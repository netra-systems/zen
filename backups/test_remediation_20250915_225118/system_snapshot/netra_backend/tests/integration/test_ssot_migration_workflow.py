"""
SSOT Integration Tests for Migration Workflow - Real Services & Authentication Required

Business Value Justification (BVJ):
- Segment: Platform/Internal
- Business Goal: Migration reliability ensures zero-downtime deployments and system stability
- Value Impact: Prevents data corruption during system updates that would destroy business value  
- Strategic Impact: Enables confident system evolution without business disruption

CRITICAL REQUIREMENTS per CLAUDE.md:
- Uses SSotBaseTestCase with real services integration
- ALL tests use authentication with real JWT tokens (E2E AUTH MANDATE)
- NO mocks in integration tests - uses real PostgreSQL/ClickHouse/Redis
- Tests complete migration workflows with real database connections
- Uses IsolatedEnvironment for all configuration access
- Validates multi-service migration coordination

SSOT Compliance:
- Inherits from SSotBaseTestCase for environment isolation
- Uses real_services_fixture for actual Docker services
- Authentication via test_framework/ssot/e2e_auth_helper.py
- Tests actual migration safety in multi-user scenarios
- Comprehensive metrics and monitoring
"""

import pytest
import asyncio
import uuid
import time
import json
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path

from test_framework.ssot.base_test_case import SSotBaseTestCase
from test_framework.ssot.e2e_auth_helper import E2EAuthHelper, create_authenticated_user
from shared.isolated_environment import get_env

from netra_backend.app.db.migration_utils import DatabaseMigrator
from netra_backend.app.db.database_manager import get_database_manager
from netra_backend.app.db.models_postgres import User, Thread, Message

pytestmark = [
    pytest.mark.integration,
    pytest.mark.migration,
    pytest.mark.real_services,
    pytest.mark.database
]


@pytest.fixture
async def real_services_fixture():
    """
    CRITICAL: Real services fixture for integration testing.
    
    This fixture ensures Docker containers are running for:
    - PostgreSQL database
    - Redis cache
    - Auth service (if needed)
    
    BUSINESS VALUE: Tests migration behavior against actual services
    rather than mocks, ensuring production reliability.
    """
    # The unified test runner automatically starts Docker services
    # when --real-services flag is used, so this fixture mainly
    # validates that services are available
    
    env = get_env()
    database_url = env.get("DATABASE_URL")
    
    if not database_url or "mock" in database_url.lower():
        pytest.skip("Real services required - DATABASE_URL not configured or contains 'mock'")
        
    # Validate database connectivity
    try:
        migrator = DatabaseMigrator(database_url)
        if not migrator.validate_url():
            pytest.skip("Database URL validation failed - real database not available")
    except Exception as e:
        pytest.skip(f"Database connectivity test failed: {e}")
    
    yield {
        "database_url": database_url,
        "redis_available": env.get("REDIS_URL") is not None,
        "auth_service_available": env.get("AUTH_SERVICE_URL") is not None
    }


@pytest.fixture
async def authenticated_user_fixture():
    """
    CRITICAL: Authentication fixture for ALL integration tests.
    
    Per CLAUDE.md E2E AUTH MANDATE: ALL integration tests MUST use
    real authentication except for tests that validate auth itself.
    """
    env = get_env()
    environment = env.get("TEST_ENV", "test")
    
    # Create authenticated user with real JWT token
    jwt_token, user_data = await create_authenticated_user(
        environment=environment,
        email=f"integration_test_{uuid.uuid4().hex[:8]}@example.com",
        permissions=["read", "write", "migration_test"]
    )
    
    # Validate authentication works
    auth_helper = E2EAuthHelper(environment=environment)
    is_valid = await auth_helper.validate_token(jwt_token)
    
    if not is_valid:
        pytest.skip("Authentication validation failed - auth service not available")
        
    return {
        "jwt_token": jwt_token,
        "user_data": user_data,
        "auth_helper": auth_helper,
        "user_id": user_data["id"],
        "email": user_data["email"]
    }


class SSOTMigrationWorkflowTests(SSotBaseTestCase):
    """
    SSOT Integration Tests for Complete Migration Workflows.
    
    This test suite validates end-to-end migration processes using:
    - Real database connections (PostgreSQL/ClickHouse)
    - Real authentication (JWT tokens)
    - Real service coordination
    - Multi-user isolation testing
    
    CRITICAL: These tests ensure migration safety for business operations.
    """
    
    def setup_method(self, method=None):
        """Enhanced setup for migration workflow testing."""
        super().setup_method(method)
        
        # Initialize test environment
        env = self.get_env()
        self.database_url = env.get("DATABASE_URL")
        self.test_prefix = f"integration_migration_{uuid.uuid4().hex[:8]}"
        
        # Record test initialization
        self.record_metric("test_prefix", self.test_prefix)
        self.record_metric("database_url_available", bool(self.database_url))
        self.record_metric("integration_test_started", datetime.now(timezone.utc).isoformat())
        
    @pytest.mark.asyncio
    async def test_migration_workflow_with_authentication(self, real_services_fixture, authenticated_user_fixture):
        """
        Test complete migration workflow with authenticated user context.
        
        BUSINESS VALUE: Validates that migrations work correctly in
        authenticated multi-user environments, ensuring system stability.
        """
        # Extract test fixtures
        services = real_services_fixture
        auth_data = authenticated_user_fixture
        
        # Initialize authenticated migration context
        migrator = DatabaseMigrator(services["database_url"])
        
        # Record authentication metrics
        self.record_metric("user_authenticated", True)
        self.record_metric("user_id", auth_data["user_id"])
        self.record_metric("jwt_token_available", bool(auth_data["jwt_token"]))
        
        try:
            # Test migration state detection with authentication
            current_revision = migrator.get_current_revision()
            head_revision = migrator.get_head_revision()
            needs_migration = migrator.needs_migration()
            
            self.increment_db_query_count(3)  # Track database operations
            
            # Record migration state
            self.record_metric("current_revision", current_revision)
            self.record_metric("head_revision", head_revision) 
            self.record_metric("needs_migration", needs_migration)
            
            # Validate migration system works with authenticated context
            assert head_revision is not None, "Migration system should detect head revision"
            
            # Test that migration system respects user context
            # In a real system, migrations should be coordinated but not user-specific
            migration_context = {
                "user_id": auth_data["user_id"],
                "authenticated": True,
                "test_prefix": self.test_prefix
            }
            
            self.record_metric("migration_context_established", True)
            self.record_metric("multi_user_safe_migration", True)
            
            # Validate migration readiness
            if needs_migration:
                self.record_metric("migration_required", True)
                # In integration tests, we don't actually run migrations
                # but we validate the system can detect the need
                assert isinstance(needs_migration, bool), "Migration need should be boolean"
            else:
                self.record_metric("database_up_to_date", True)
                
        except Exception as e:
            self.record_metric("migration_workflow_error", str(e))
            pytest.fail(f"Migration workflow failed with authenticated user: {e}")
            
    @pytest.mark.asyncio  
    async def test_concurrent_user_migration_safety(self, real_services_fixture, authenticated_user_fixture):
        """
        Test migration safety under concurrent user sessions.
        
        BUSINESS VALUE: Ensures migrations don't disrupt active user sessions
        and maintain data integrity in multi-user scenarios.
        """
        services = real_services_fixture
        auth_data = authenticated_user_fixture
        
        # Create multiple authenticated user contexts for concurrency testing
        concurrent_users = []
        for i in range(3):
            user_token, user_data = await create_authenticated_user(
                email=f"concurrent_user_{i}_{self.test_prefix}@example.com",
                permissions=["read", "write"]
            )
            concurrent_users.append({
                "token": user_token,
                "data": user_data,
                "user_id": user_data["id"]
            })
            
        self.record_metric("concurrent_users_created", len(concurrent_users))
        
        # Initialize migration system
        migrator = DatabaseMigrator(services["database_url"])
        
        try:
            # Simulate concurrent access during migration check
            migration_check_tasks = []
            
            async def check_migration_for_user(user_context):
                """Check migration status from user perspective."""
                # In real system, each user session should see consistent migration state
                current = migrator.get_current_revision()
                head = migrator.get_head_revision()
                needs = migrator.needs_migration()
                
                return {
                    "user_id": user_context["user_id"],
                    "current_revision": current,
                    "head_revision": head,
                    "needs_migration": needs,
                    "check_time": time.time()
                }
            
            # Execute concurrent migration checks
            for user in concurrent_users:
                task = check_migration_for_user(user)
                migration_check_tasks.append(task)
                
            # Wait for all checks to complete
            migration_results = await asyncio.gather(*migration_check_tasks)
            
            self.increment_db_query_count(len(migration_results) * 3)  # Track queries
            
            # Validate concurrent access safety
            revision_consistency = True
            base_current = migration_results[0]["current_revision"]
            base_head = migration_results[0]["head_revision"]
            base_needs = migration_results[0]["needs_migration"]
            
            for result in migration_results[1:]:
                if (result["current_revision"] != base_current or 
                    result["head_revision"] != base_head or
                    result["needs_migration"] != base_needs):
                    revision_consistency = False
                    break
                    
            assert revision_consistency, "Migration state should be consistent across concurrent users"
            
            self.record_metric("concurrent_access_safe", True)
            self.record_metric("revision_consistency_maintained", revision_consistency)
            self.record_metric("concurrent_checks_completed", len(migration_results))
            
        except Exception as e:
            self.record_metric("concurrent_migration_error", str(e))
            pytest.fail(f"Concurrent user migration safety test failed: {e}")
            
    @pytest.mark.asyncio
    async def test_migration_with_real_database_operations(self, real_services_fixture, authenticated_user_fixture):
        """
        Test migration behavior with active database operations.
        
        BUSINESS VALUE: Validates that migration system works correctly
        when database is actively being used by business operations.
        """
        services = real_services_fixture
        auth_data = authenticated_user_fixture
        
        # Get database manager for real operations
        db_manager = get_database_manager()
        await db_manager.initialize()
        
        migrator = DatabaseMigrator(services["database_url"])
        
        try:
            # Create some test data to simulate active business operations
            test_data_operations = []
            
            # Simulate creating user data (would happen during normal operations)
            user_creation_context = {
                "email": f"migration_test_user_{self.test_prefix}@example.com",
                "name": f"Migration Test User {self.test_prefix}",
                "created_during_migration_check": True,
                "authenticated_user": auth_data["user_id"]
            }
            
            test_data_operations.append(user_creation_context)
            
            # Check migration state while database operations are "happening"
            migration_start_time = time.time()
            
            current_revision = migrator.get_current_revision()
            head_revision = migrator.get_head_revision() 
            needs_migration = migrator.needs_migration()
            url_valid = migrator.validate_url()
            
            migration_check_time = time.time() - migration_start_time
            
            self.increment_db_query_count(3)  # Track migration queries
            
            # Record migration performance with active database
            self.record_metric("migration_check_duration", migration_check_time)
            self.record_metric("active_operations_during_check", len(test_data_operations))
            self.record_metric("database_responsive_during_migration", migration_check_time < 5.0)
            
            # Validate migration system works with active database
            assert url_valid, "Database should remain accessible during migration checks"
            assert head_revision is not None, "Should be able to read migration scripts during operations"
            
            # Test that migration detection is not affected by concurrent operations
            assert isinstance(needs_migration, bool), "Migration detection should work with active database"
            
            self.record_metric("migration_with_active_db_successful", True)
            
        except Exception as e:
            self.record_metric("migration_active_db_error", str(e))
            pytest.fail(f"Migration with active database operations failed: {e}")
            
    @pytest.mark.asyncio
    async def test_multi_service_migration_coordination(self, real_services_fixture, authenticated_user_fixture):
        """
        Test migration coordination across multiple services.
        
        BUSINESS VALUE: Ensures migrations work correctly when multiple
        services (backend, auth, etc.) need coordinated schema changes.
        """
        services = real_services_fixture
        auth_data = authenticated_user_fixture
        
        # Initialize migration contexts for different services
        backend_migrator = DatabaseMigrator(services["database_url"])
        
        # Test service coordination metrics
        service_states = {}
        
        try:
            # Check backend migration state
            backend_current = backend_migrator.get_current_revision()
            backend_head = backend_migrator.get_head_revision()
            backend_needs_migration = backend_migrator.needs_migration()
            
            service_states["backend"] = {
                "current_revision": backend_current,
                "head_revision": backend_head,
                "needs_migration": backend_needs_migration,
                "service_healthy": backend_migrator.validate_url()
            }
            
            self.increment_db_query_count(3)
            
            # If auth service is available, check its coordination
            if services.get("auth_service_available"):
                # Auth service would have its own migration state
                # but should coordinate with backend for shared resources
                service_states["auth"] = {
                    "coordination_required": True,
                    "backend_dependency": backend_needs_migration
                }
                
            # Validate service coordination
            backend_healthy = service_states["backend"]["service_healthy"]
            assert backend_healthy, "Backend database should be healthy for coordinated migrations"
            
            # Record multi-service coordination metrics
            self.record_metric("services_coordinated", len(service_states))
            self.record_metric("backend_migration_state_detected", True)
            self.record_metric("multi_service_coordination_successful", True)
            
            # Test that authentication remains valid during migration checks
            auth_helper = auth_data["auth_helper"]
            token_still_valid = await auth_helper.validate_token(auth_data["jwt_token"])
            assert token_still_valid, "Authentication should remain valid during migration operations"
            
            self.record_metric("auth_maintained_during_migration", token_still_valid)
            
        except Exception as e:
            self.record_metric("multi_service_coordination_error", str(e))
            pytest.fail(f"Multi-service migration coordination failed: {e}")
            
    @pytest.mark.asyncio
    async def test_migration_rollback_safety_integration(self, real_services_fixture, authenticated_user_fixture):
        """
        Test migration rollback safety in integrated environment.
        
        BUSINESS VALUE: Rollback safety prevents data loss and enables
        quick recovery from failed deployments.
        """
        services = real_services_fixture
        auth_data = authenticated_user_fixture
        
        migrator = DatabaseMigrator(services["database_url"])
        
        try:
            # Capture current migration state as baseline
            baseline_current = migrator.get_current_revision()
            baseline_head = migrator.get_head_revision()
            baseline_needs_migration = migrator.needs_migration()
            
            baseline_state = {
                "current_revision": baseline_current,
                "head_revision": baseline_head,
                "needs_migration": baseline_needs_migration,
                "captured_at": datetime.now(timezone.utc).isoformat(),
                "authenticated_user": auth_data["user_id"]
            }
            
            self.increment_db_query_count(3)
            
            # Test rollback readiness validation
            # In integration tests, we test the detection logic rather than actual rollback
            rollback_scenarios = [
                {
                    "scenario": "current_matches_head",
                    "safe_to_rollback": baseline_current == baseline_head,
                    "reason": "No pending migrations"
                },
                {
                    "scenario": "pending_migrations_exist", 
                    "safe_to_rollback": baseline_needs_migration,
                    "reason": "Migrations pending, rollback may be needed"
                },
                {
                    "scenario": "fresh_database",
                    "safe_to_rollback": baseline_current is None,
                    "reason": "Fresh database, stamp rather than rollback"
                }
            ]
            
            # Evaluate rollback scenarios
            applicable_scenarios = []
            for scenario in rollback_scenarios:
                if scenario["safe_to_rollback"]:
                    applicable_scenarios.append(scenario["scenario"])
                    
            self.record_metric("rollback_scenarios_evaluated", len(rollback_scenarios))
            self.record_metric("applicable_rollback_scenarios", len(applicable_scenarios))
            self.record_metric("baseline_state_captured", True)
            
            # Validate rollback safety detection
            # The system should be able to determine rollback feasibility
            rollback_feasible = (baseline_current is not None and 
                               baseline_head is not None and
                               not migrator.validate_url() == False)
                               
            self.record_metric("rollback_feasibility_determined", True)
            self.record_metric("rollback_would_be_safe", rollback_feasible)
            
            # Test that authentication context is preserved during rollback planning
            auth_token_valid = await auth_data["auth_helper"].validate_token(auth_data["jwt_token"])
            assert auth_token_valid, "Authentication should remain valid during rollback planning"
            
            self.record_metric("auth_preserved_during_rollback_planning", True)
            
        except Exception as e:
            self.record_metric("rollback_safety_integration_error", str(e))
            pytest.fail(f"Migration rollback safety integration test failed: {e}")
            
    @pytest.mark.asyncio
    async def test_migration_monitoring_and_metrics(self, real_services_fixture, authenticated_user_fixture):
        """
        Test migration monitoring and metrics collection.
        
        BUSINESS VALUE: Comprehensive monitoring prevents silent failures
        and provides visibility for business operations.
        """
        services = real_services_fixture
        auth_data = authenticated_user_fixture
        
        migrator = DatabaseMigrator(services["database_url"])
        
        # Initialize monitoring context
        monitoring_start = time.time()
        migration_metrics = {
            "test_id": self.test_prefix,
            "user_context": auth_data["user_id"],
            "monitoring_started": datetime.now(timezone.utc).isoformat()
        }
        
        try:
            # Execute monitored migration operations
            operations = [
                ("get_current_revision", lambda: migrator.get_current_revision()),
                ("get_head_revision", lambda: migrator.get_head_revision()),
                ("validate_url", lambda: migrator.validate_url()),
                ("needs_migration", lambda: migrator.needs_migration())
            ]
            
            operation_results = {}
            operation_timings = {}
            
            for op_name, op_func in operations:
                op_start = time.time()
                try:
                    result = op_func()
                    op_duration = time.time() - op_start
                    
                    operation_results[op_name] = {
                        "success": True,
                        "result": result,
                        "duration": op_duration
                    }
                    operation_timings[op_name] = op_duration
                    self.increment_db_query_count(1 if "revision" in op_name else 0)
                    
                except Exception as e:
                    op_duration = time.time() - op_start
                    operation_results[op_name] = {
                        "success": False,
                        "error": str(e),
                        "duration": op_duration
                    }
                    
            total_monitoring_time = time.time() - monitoring_start
            
            # Record comprehensive monitoring metrics
            self.record_metric("total_monitoring_duration", total_monitoring_time)
            self.record_metric("operations_monitored", len(operations))
            
            successful_operations = sum(1 for result in operation_results.values() if result["success"])
            self.record_metric("successful_operations", successful_operations)
            self.record_metric("operation_success_rate", successful_operations / len(operations))
            
            # Performance metrics
            if operation_timings:
                avg_operation_time = sum(operation_timings.values()) / len(operation_timings)
                max_operation_time = max(operation_timings.values())
                
                self.record_metric("average_operation_time", avg_operation_time)
                self.record_metric("max_operation_time", max_operation_time)
                self.record_metric("performance_acceptable", max_operation_time < 10.0)
                
            # Business continuity metrics
            critical_operations = ["get_head_revision", "needs_migration", "validate_url"]
            critical_success = all(
                operation_results.get(op, {}).get("success", False) 
                for op in critical_operations
            )
            
            self.record_metric("critical_operations_successful", critical_success)
            self.record_metric("business_continuity_maintained", critical_success)
            
            # Validate monitoring completeness
            assert successful_operations > 0, "At least some migration operations should succeed"
            assert total_monitoring_time < 30.0, f"Monitoring should complete quickly, took {total_monitoring_time:.3f}s"
            
        except Exception as e:
            monitoring_duration = time.time() - monitoring_start
            self.record_metric("monitoring_error", str(e))
            self.record_metric("monitoring_duration_at_failure", monitoring_duration)
            pytest.fail(f"Migration monitoring test failed: {e}")
            
    def teardown_method(self, method=None):
        """Enhanced teardown with comprehensive integration metrics."""
        # Record integration test completion
        self.record_metric("integration_test_completed", datetime.now(timezone.utc).isoformat())
        
        # Get all metrics for analysis
        all_metrics = self.get_all_metrics()
        
        # Log integration test summary
        if method:
            print(f"\n=== Integration Migration Test '{method.__name__}' Summary ===")
            print(f"Duration: {all_metrics.get('execution_time', 0):.3f}s")
            print(f"Database Queries: {all_metrics.get('database_queries', 0)}")
            print(f"Test Prefix: {all_metrics.get('test_prefix', 'unknown')}")
            
            # Log business-critical integration metrics
            integration_metrics = {
                k: v for k, v in all_metrics.items()
                if any(keyword in k for keyword in [
                    'authenticated', 'migration', 'concurrent', 'coordination',
                    'rollback', 'monitoring', 'business_continuity'
                ])
            }
            
            if integration_metrics:
                print(f"Integration Business Metrics: {integration_metrics}")
                
        # Call parent teardown
        super().teardown_method(method)


# SSOT Export Control
__all__ = [
    "SSOTMigrationWorkflowTests",
    "real_services_fixture",
    "authenticated_user_fixture"
]