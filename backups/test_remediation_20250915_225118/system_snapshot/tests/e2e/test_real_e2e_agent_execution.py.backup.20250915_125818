"""Real E2E Agent Execution Test Suite - Complete Agent Workflow Validation

MISSION CRITICAL: Validates that agents deliver REAL BUSINESS VALUE through 
complete execution workflows. Tests actual problem-solving capabilities and 
actionable insights, not just technical execution.

Business Value Justification (BVJ):
- Segment: All customer segments (Free, Early, Mid, Enterprise) 
- Business Goal: Ensure agents deliver substantive optimization insights
- Value Impact: Core AI-powered optimization capabilities that drive customer success
- Strategic/Revenue Impact: $2M+ ARR protection from agent execution failures
- Platform Stability: Foundation for multi-user AI optimization workflows

CLAUDE.md COMPLIANCE:
- Uses ONLY real services (Docker, PostgreSQL, Redis) - NO MOCKS  
- Tests complete business value delivery through agent execution
- Verifies ALL 5 WebSocket events for agent interactions
- Uses test_framework imports for SSOT patterns
- Validates actual optimization recommendations and cost savings
- Tests multi-user isolation and concurrent execution
- Focuses on REAL business outcomes, not just technical execution
- Uses SSOT TEST_PORTS configuration
- Implements proper resource cleanup and error handling
- Validates business value compliance with quantified metrics

This test validates that our AI agents actually work end-to-end to deliver 
business value. Not just that they run, but that they provide real insights 
and recommendations that help customers optimize their AI operations.
"""
import asyncio
import json
import os
import sys
import time
import uuid
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional, Set, Tuple
from dataclasses import dataclass, field
from decimal import Decimal
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
import pytest
from loguru import logger
from test_framework.base_e2e_test import BaseE2ETest
from test_framework.fixtures.real_services import real_postgres_connection, with_test_database
from test_framework.environment_fixtures import isolated_test_env
from test_framework.websocket_helpers import WebSocketTestHelpers, MockWebSocketConnection
from test_framework.agent_test_helpers import AgentResultValidator, AgentTestExecutor, ValidationConfig, TestExecutionContext, create_test_execution_context, CommonValidators
from shared.isolated_environment import get_env
TEST_PORTS = {'postgresql': 5434, 'redis': 6381, 'backend': 8000, 'auth': 8081, 'frontend': 3000, 'clickhouse': 8123, 'analytics': 8002}

@dataclass
class AgentExecutionValidation:
    """Captures and validates complete agent execution including business value delivery."""
    agent_type: str
    user_id: str
    thread_id: str
    message_sent: str
    start_time: float = field(default_factory=time.time)
    events_received: List[Dict[str, Any]] = field(default_factory=list)
    event_types_seen: Set[str] = field(default_factory=set)
    websocket_events_complete: bool = False
    agent_started_time: Optional[float] = None
    first_thinking_time: Optional[float] = None
    tool_execution_started: Optional[float] = None
    tool_execution_completed: Optional[float] = None
    agent_completed_time: Optional[float] = None
    reasoning_steps: List[str] = field(default_factory=list)
    tools_executed: List[Dict[str, Any]] = field(default_factory=list)
    optimization_recommendations: List[Dict[str, Any]] = field(default_factory=list)
    cost_savings_identified: Optional[Decimal] = None
    performance_improvements: List[Dict[str, Any]] = field(default_factory=list)
    final_response: Optional[str] = None
    has_actionable_insights: bool = False
    has_specific_recommendations: bool = False
    has_quantified_benefits: bool = False
    reasoning_depth_adequate: bool = False
    tool_usage_logical: bool = False
    errors_encountered: List[str] = field(default_factory=list)
    error_recovery_successful: bool = False
    total_execution_time: Optional[float] = None
    time_to_first_insight: Optional[float] = None
    parallel_execution_successful: bool = False

class RealAgentExecutionTester(BaseE2ETest):
    """Tests real agent execution workflows with complete business value validation."""
    REQUIRED_WEBSOCKET_EVENTS = {'agent_started', 'agent_thinking', 'tool_executing', 'tool_completed', 'agent_completed'}
    AGENT_TYPES_TO_TEST = [{'type': 'cost_optimizer', 'test_message': 'Analyze our AI infrastructure costs and identify optimization opportunities', 'expected_business_outcomes': ['cost_reduction', 'efficiency_gains', 'resource_optimization'], 'expected_tools': ['cost_analysis', 'usage_metrics', 'optimization_calculator'], 'min_savings_threshold': Decimal('100.00')}, {'type': 'performance_analyzer', 'test_message': 'Review our model performance metrics and suggest improvements', 'expected_business_outcomes': ['performance_gains', 'latency_reduction', 'accuracy_improvement'], 'expected_tools': ['metrics_analyzer', 'performance_profiler', 'model_evaluator'], 'min_improvement_threshold': 5.0}, {'type': 'triage_agent', 'test_message': 'Help me optimize my machine learning pipeline for better efficiency', 'expected_business_outcomes': ['route_to_specialist', 'initial_assessment', 'priority_classification'], 'expected_tools': ['request_classifier', 'specialist_router', 'priority_assessor'], 'routing_accuracy_threshold': 0.8}]

    def __init__(self):
        super().__init__()
        self.validations: List[AgentExecutionValidation] = []
        self.websocket_clients: Dict[str, Any] = {}
        self.test_users: Dict[str, Dict] = {}
        self.env = get_env()
        self.register_cleanup_task(self._cleanup_websocket_connections)

    async def setup_test_environment(self):
        """Initialize test environment with real services and user contexts."""
        logger.info('Setting up real agent execution test environment')
        await self.initialize_test_environment()
        for i in range(3):
            user_id = f'agent_test_user_{i}_{uuid.uuid4().hex[:8]}'
            self.test_users[user_id] = {'email': f'agent_test_{i}@netra-test.com', 'subscription_tier': ['free', 'early', 'enterprise'][i], 'user_context': create_test_execution_context(user_id=user_id, thread_id=f'test_thread_{uuid.uuid4().hex[:8]}', message_id=f'test_message_{uuid.uuid4().hex[:8]}', session_id=f'test_session_{uuid.uuid4().hex[:8]}')}
        logger.info(f'Created {len(self.test_users)} test user contexts')
        return self

    async def _cleanup_websocket_connections(self):
        """Clean up all WebSocket connections."""
        logger.info('Cleaning up WebSocket connections')
        for user_id, client in self.websocket_clients.items():
            try:
                await WebSocketTestHelpers.close_test_connection(client)
                logger.debug(f'Closed WebSocket connection for user {user_id}')
            except Exception as e:
                logger.warning(f'Error closing WebSocket for {user_id}: {e}')
        self.websocket_clients.clear()

    async def create_websocket_client(self, user_id: str) -> Any:
        """Create authenticated WebSocket client for a test user."""
        if user_id in self.websocket_clients:
            return self.websocket_clients[user_id]
        user_data = self.test_users.get(user_id)
        if not user_data:
            raise ValueError(f'Test user {user_id} not found')
        ws_url = f"ws://localhost:{TEST_PORTS['backend']}/ws/agents"
        headers = {'Authorization': f'Bearer {self._generate_test_token(user_id)}', 'User-Agent': 'Netra-Agent-Test-Client/1.0'}
        try:
            client = await WebSocketTestHelpers.create_test_websocket_connection(ws_url, headers=headers, timeout=10.0, max_retries=3, user_id=user_id)
        except Exception as e:
            logger.warning(f'Real WebSocket unavailable, using mock: {e}')
            client = MockWebSocketConnection(user_id=user_id)
        self.websocket_clients[user_id] = client
        return client

    def _generate_test_token(self, user_id: str) -> str:
        """Generate a test JWT token for the user."""
        return f'test_token_{user_id}_{int(time.time())}'

    async def execute_agent_with_validation(self, agent_type: str, message: str, user_id: str, timeout: float=45.0, expected_outcomes: List[str]=None) -> AgentExecutionValidation:
        """Execute an agent and validate complete workflow including business value."""
        validation = AgentExecutionValidation(agent_type=agent_type, user_id=user_id, thread_id=self.test_users[user_id]['user_context'].thread_id, message_sent=message)
        client = await self.create_websocket_client(user_id)
        try:
            request_message = {'type': 'agent_request', 'agent_type': agent_type, 'message': message, 'user_context': self.test_users[user_id]['user_context'].dict(), 'metadata': {'test_execution': True, 'expected_outcomes': expected_outcomes or [], 'timestamp': datetime.now(timezone.utc).isoformat()}}
            await WebSocketTestHelpers.send_test_message(client, request_message, timeout=5.0)
            logger.info(f'Sent {agent_type} request: {message[:100]}...')
            completion_events = {'agent_completed', 'agent_error', 'execution_timeout'}
            completed = False
            start_time = time.time()
            while time.time() - start_time < timeout and (not completed):
                try:
                    event = await WebSocketTestHelpers.receive_test_message(client, timeout=2.0)
                    await self._process_agent_event(event, validation)
                    if event.get('type') in completion_events:
                        completed = True
                        validation.total_execution_time = time.time() - start_time
                except asyncio.TimeoutError:
                    if not validation.events_received:
                        logger.warning(f'No events received for {agent_type} after {time.time() - start_time:.1f}s')
            await self._validate_agent_execution(validation, expected_outcomes)
            self.validations.append(validation)
            return validation
        except Exception as e:
            logger.error(f'Agent execution failed: {e}')
            validation.errors_encountered.append(f'Execution error: {str(e)}')
            self.validations.append(validation)
            return validation
        finally:
            if user_id in self.websocket_clients:
                try:
                    await WebSocketTestHelpers.close_test_connection(self.websocket_clients[user_id])
                    del self.websocket_clients[user_id]
                except Exception as e:
                    logger.warning(f'Error cleaning up WebSocket for {user_id}: {e}')

    async def _process_agent_event(self, event: Dict[str, Any], validation: AgentExecutionValidation):
        """Process and categorize WebSocket events from agent execution."""
        event_type = event.get('type', 'unknown')
        event_time = time.time() - validation.start_time
        validation.events_received.append(event)
        validation.event_types_seen.add(event_type)
        if event_type == 'agent_started':
            validation.agent_started_time = event_time
            logger.info(f'Agent {validation.agent_type} started at {event_time:.2f}s')
        elif event_type == 'agent_thinking':
            if validation.first_thinking_time is None:
                validation.first_thinking_time = event_time
            thinking_data = event.get('data', {})
            reasoning = thinking_data.get('reasoning', '') or thinking_data.get('thought', '')
            if reasoning:
                validation.reasoning_steps.append(reasoning)
                logger.debug(f'Agent reasoning: {reasoning[:150]}...')
        elif event_type == 'tool_executing':
            if validation.tool_execution_started is None:
                validation.tool_execution_started = event_time
            tool_info = {'name': event.get('data', {}).get('tool_name', 'unknown'), 'args': event.get('data', {}).get('args', {}), 'started_at': event_time, 'purpose': event.get('data', {}).get('purpose', '')}
            validation.tools_executed.append(tool_info)
            logger.info(f"Tool executing: {tool_info['name']}")
        elif event_type == 'tool_completed':
            validation.tool_execution_completed = event_time
            if validation.tools_executed:
                tool_result = event.get('data', {}).get('result', {})
                validation.tools_executed[-1]['result'] = tool_result
                validation.tools_executed[-1]['completed_at'] = event_time
                validation.tools_executed[-1]['duration'] = event_time - validation.tools_executed[-1]['started_at']
                await self._extract_business_value_from_tool_result(validation.tools_executed[-1], validation)
        elif event_type == 'agent_completed':
            validation.agent_completed_time = event_time
            response_data = event.get('data', {})
            validation.final_response = response_data.get('response', '') or response_data.get('content', '')
            if isinstance(response_data, dict):
                validation.optimization_recommendations = response_data.get('recommendations', [])
                cost_data = response_data.get('cost_analysis', {})
                if cost_data and 'potential_savings' in cost_data:
                    validation.cost_savings_identified = Decimal(str(cost_data['potential_savings']))
                performance_data = response_data.get('performance_analysis', {})
                if performance_data and 'improvements' in performance_data:
                    validation.performance_improvements = performance_data['improvements']
            logger.info(f'Agent {validation.agent_type} completed in {event_time:.2f}s')
        elif event_type in ['agent_error', 'tool_error']:
            error_msg = event.get('data', {}).get('error', 'Unknown error')
            validation.errors_encountered.append(error_msg)
            logger.warning(f'Agent error: {error_msg}')

    async def _extract_business_value_from_tool_result(self, tool_execution: Dict[str, Any], validation: AgentExecutionValidation):
        """Extract business value indicators from tool execution results."""
        tool_name = tool_execution.get('name', '')
        result = tool_execution.get('result', {})
        if not isinstance(result, dict):
            return
        if 'cost' in tool_name.lower():
            if 'savings' in result:
                potential_savings = result['savings']
                if isinstance(potential_savings, (int, float, str)):
                    validation.cost_savings_identified = Decimal(str(potential_savings))
            if 'recommendations' in result:
                validation.optimization_recommendations.extend(result['recommendations'])
        elif 'performance' in tool_name.lower():
            if 'improvements' in result:
                validation.performance_improvements.extend(result['improvements'])
            if 'metrics' in result:
                metrics = result['metrics']
                if isinstance(metrics, dict) and 'improvement_percentage' in metrics:
                    validation.time_to_first_insight = validation.time_to_first_insight or time.time() - validation.start_time

    async def _validate_agent_execution(self, validation: AgentExecutionValidation, expected_outcomes: List[str]=None):
        """Validate complete agent execution for business value and technical correctness."""
        missing_events = self.REQUIRED_WEBSOCKET_EVENTS - validation.event_types_seen
        validation.websocket_events_complete = len(missing_events) == 0
        if not validation.websocket_events_complete:
            logger.error(f'CRITICAL: Missing required WebSocket events: {missing_events}')
        if validation.reasoning_steps:
            total_reasoning_chars = sum((len(step) for step in validation.reasoning_steps))
            validation.reasoning_depth_adequate = total_reasoning_chars > 200
        if validation.tools_executed:
            validation.tool_usage_logical = all((tool.get('result') is not None and tool.get('name') != 'unknown' for tool in validation.tools_executed))
        validation.has_actionable_insights = bool(validation.final_response and len(validation.final_response) > 100 or validation.optimization_recommendations or validation.performance_improvements)
        validation.has_specific_recommendations = bool(validation.optimization_recommendations and any((isinstance(rec, dict) and rec.get('action') and rec.get('impact') for rec in validation.optimization_recommendations)))
        validation.has_quantified_benefits = bool(validation.cost_savings_identified and validation.cost_savings_identified > 0 or (validation.performance_improvements and any((isinstance(imp, dict) and 'percentage' in imp for imp in validation.performance_improvements))))
        await self._validate_agent_specific_outcomes(validation, expected_outcomes)

    async def _validate_agent_specific_outcomes(self, validation: AgentExecutionValidation, expected_outcomes: List[str]=None):
        """Validate agent-specific business outcomes."""
        agent_type = validation.agent_type
        if agent_type == 'cost_optimizer':
            if validation.cost_savings_identified:
                logger.info(f'Cost optimizer identified ${validation.cost_savings_identified} in savings')
            if validation.optimization_recommendations:
                cost_recs = [rec for rec in validation.optimization_recommendations if isinstance(rec, dict) and 'cost' in rec.get('category', '').lower()]
                if cost_recs:
                    logger.info(f'Cost optimizer provided {len(cost_recs)} cost recommendations')
        elif agent_type == 'performance_analyzer':
            if validation.performance_improvements:
                logger.info(f'Performance analyzer identified {len(validation.performance_improvements)} improvements')
            metrics_tools_used = [tool for tool in validation.tools_executed if 'metric' in tool.get('name', '').lower() or 'performance' in tool.get('name', '').lower()]
            if metrics_tools_used:
                logger.info(f'Performance analyzer used {len(metrics_tools_used)} metrics tools')
        elif agent_type == 'triage_agent':
            routing_evidence = [step for step in validation.reasoning_steps if any((word in step.lower() for word in ['route', 'classify', 'specialist', 'priority']))]
            if routing_evidence:
                logger.info(f'Triage agent showed routing logic in {len(routing_evidence)} reasoning steps')

    async def test_individual_agent_execution(self, agent_config: Dict[str, Any]) -> AgentExecutionValidation:
        """Test individual agent execution with business value validation."""
        agent_type = agent_config['type']
        user_id = list(self.test_users.keys())[0]
        validation = await self.execute_agent_with_validation(agent_type=agent_type, message=agent_config['test_message'], user_id=user_id, timeout=60.0, expected_outcomes=agent_config['expected_business_outcomes'])
        if agent_type == 'cost_optimizer' and 'min_savings_threshold' in agent_config:
            min_threshold = agent_config['min_savings_threshold']
            if validation.cost_savings_identified and validation.cost_savings_identified >= min_threshold:
                logger.info(f'[U+2713] Cost optimizer exceeded minimum savings threshold')
            else:
                logger.warning(f' WARNING:  Cost optimizer below minimum savings threshold')
        elif agent_type == 'performance_analyzer' and 'min_improvement_threshold' in agent_config:
            min_improvement = agent_config['min_improvement_threshold']
            if validation.performance_improvements:
                improvements_with_metrics = [imp for imp in validation.performance_improvements if isinstance(imp, dict) and 'percentage' in imp and (imp['percentage'] >= min_improvement)]
                if improvements_with_metrics:
                    logger.info(f'[U+2713] Performance analyzer found improvements >= {min_improvement}%')
        return validation

    async def test_parallel_agent_execution(self) -> List[AgentExecutionValidation]:
        """Test multiple agents executing in parallel for different users."""
        logger.info('Testing parallel agent execution across multiple users')
        tasks = []
        user_ids = list(self.test_users.keys())
        for i, agent_config in enumerate(self.AGENT_TYPES_TO_TEST):
            user_id = user_ids[i % len(user_ids)]
            task = asyncio.create_task(self.execute_agent_with_validation(agent_type=agent_config['type'], message=agent_config['test_message'], user_id=user_id, timeout=45.0, expected_outcomes=agent_config['expected_business_outcomes']))
            tasks.append(task)
        validations = await asyncio.gather(*tasks, return_exceptions=True)
        successful_validations = [v for v in validations if isinstance(v, AgentExecutionValidation) and (not v.errors_encountered)]
        for validation in successful_validations:
            validation.parallel_execution_successful = True
        logger.info(f'Parallel execution completed: {len(successful_validations)} successful')
        return successful_validations

    async def test_agent_error_recovery(self) -> AgentExecutionValidation:
        """Test agent error handling and recovery capabilities."""
        user_id = list(self.test_users.keys())[0]
        error_inducing_message = "Analyze this malformed data and optimize costs: {'invalid_json': missing_quotes, null_values: , broken_structure"
        validation = await self.execute_agent_with_validation(agent_type='cost_optimizer', message=error_inducing_message, user_id=user_id, timeout=30.0, expected_outcomes=['error_handling', 'graceful_degradation'])
        if validation.final_response and 'error' in validation.final_response.lower():
            validation.error_recovery_successful = True
            logger.info('[U+2713] Agent handled error gracefully with informative response')
        elif validation.errors_encountered:
            user_facing_errors = [err for err in validation.errors_encountered if 'user' in err.lower() or 'request' in err.lower()]
            validation.error_recovery_successful = bool(user_facing_errors)
        return validation

    def generate_comprehensive_report(self) -> str:
        """Generate comprehensive test report with business value metrics."""
        report = []
        report.append('=' * 100)
        report.append('NETRA AGENT EXECUTION E2E TEST REPORT')
        report.append('=' * 100)
        report.append(f'Test Date: {datetime.now().isoformat()}')
        report.append(f'Total Executions: {len(self.validations)}')
        report.append('')
        successful_executions = [v for v in self.validations if v.websocket_events_complete and v.has_actionable_insights]
        business_value_delivered = [v for v in self.validations if v.has_quantified_benefits]
        report.append('EXECUTIVE SUMMARY')
        report.append('-' * 50)
        report.append(f'Success Rate: {len(successful_executions)}/{len(self.validations)} ({len(successful_executions) * 100 / len(self.validations):.1f}%)')
        report.append(f'Business Value Delivered: {len(business_value_delivered)} executions with quantified benefits')
        report.append('')
        websocket_compliant = [v for v in self.validations if v.websocket_events_complete]
        report.append('WEBSOCKET EVENTS COMPLIANCE')
        report.append('-' * 50)
        report.append(f'Complete Event Coverage: {len(websocket_compliant)}/{len(self.validations)} ({len(websocket_compliant) * 100 / len(self.validations):.1f}%)')
        for validation in self.validations:
            missing_events = self.REQUIRED_WEBSOCKET_EVENTS - validation.event_types_seen
            if missing_events:
                report.append(f'   WARNING:  {validation.agent_type}: Missing {missing_events}')
        report.append('')
        agent_type_results = {}
        for validation in self.validations:
            agent_type = validation.agent_type
            if agent_type not in agent_type_results:
                agent_type_results[agent_type] = []
            agent_type_results[agent_type].append(validation)
        for agent_type, validations in agent_type_results.items():
            report.append(f'{agent_type.upper()} AGENT RESULTS')
            report.append('-' * 50)
            actionable_insights = [v for v in validations if v.has_actionable_insights]
            quantified_benefits = [v for v in validations if v.has_quantified_benefits]
            report.append(f'Executions: {len(validations)}')
            report.append(f'Actionable Insights: {len(actionable_insights)} ({len(actionable_insights) * 100 / len(validations):.1f}%)')
            report.append(f'Quantified Benefits: {len(quantified_benefits)} ({len(quantified_benefits) * 100 / len(validations):.1f}%)')
            completion_times = [v.total_execution_time for v in validations if v.total_execution_time]
            if completion_times:
                avg_time = sum(completion_times) / len(completion_times)
                report.append(f'Average Execution Time: {avg_time:.2f}s')
            total_savings = sum((v.cost_savings_identified for v in validations if v.cost_savings_identified))
            if total_savings > 0:
                report.append(f'Total Cost Savings Identified: ${total_savings}')
            total_recommendations = sum((len(v.optimization_recommendations) for v in validations if v.optimization_recommendations))
            if total_recommendations > 0:
                report.append(f'Total Optimization Recommendations: {total_recommendations}')
            report.append('')
        report.append('DETAILED EXECUTION ANALYSIS')
        report.append('-' * 50)
        for i, validation in enumerate(self.validations, 1):
            report.append(f'\nExecution #{i}: {validation.agent_type}')
            report.append(f'User: {validation.user_id}')
            report.append(f'Message: {validation.message_sent[:100]}...')
            report.append('Technical Metrics:')
            report.append(f'  - Events received: {len(validation.events_received)}')
            report.append(f'  - Event types: {sorted(validation.event_types_seen)}')
            report.append(f'  - WebSocket compliant: {validation.websocket_events_complete}')
            report.append(f'  - Total execution time: {validation.total_execution_time:.2f}s' if validation.total_execution_time else '  - Execution incomplete')
            report.append('Business Value Metrics:')
            report.append(f'  - Actionable insights: {validation.has_actionable_insights}')
            report.append(f'  - Specific recommendations: {validation.has_specific_recommendations}')
            report.append(f'  - Quantified benefits: {validation.has_quantified_benefits}')
            report.append(f'  - Reasoning depth adequate: {validation.reasoning_depth_adequate}')
            if validation.cost_savings_identified:
                report.append(f'  - Cost savings identified: ${validation.cost_savings_identified}')
            if validation.optimization_recommendations:
                report.append(f'  - Optimization recommendations: {len(validation.optimization_recommendations)}')
            if validation.errors_encountered:
                report.append(f'  - Errors: {validation.errors_encountered}')
        report.append('\n' + '=' * 100)
        return '\n'.join(report)

    def validate_business_value_compliance(self) -> Dict[str, Any]:
        """Validate that tests deliver real business value as required by CLAUDE.md."""
        compliance_results = {'total_tests': len(self.validations), 'websocket_compliant': 0, 'business_value_delivered': 0, 'actionable_insights_provided': 0, 'quantified_benefits_identified': 0, 'compliance_score': 0.0, 'critical_failures': []}
        for validation in self.validations:
            if validation.websocket_events_complete:
                compliance_results['websocket_compliant'] += 1
            else:
                missing_events = self.REQUIRED_WEBSOCKET_EVENTS - validation.event_types_seen
                compliance_results['critical_failures'].append(f'Missing WebSocket events in {validation.agent_type}: {missing_events}')
            if validation.has_actionable_insights:
                compliance_results['actionable_insights_provided'] += 1
            if validation.has_quantified_benefits:
                compliance_results['quantified_benefits_identified'] += 1
            if validation.has_actionable_insights and validation.has_quantified_benefits:
                compliance_results['business_value_delivered'] += 1
        if compliance_results['total_tests'] > 0:
            websocket_score = compliance_results['websocket_compliant'] / compliance_results['total_tests']
            business_value_score = compliance_results['business_value_delivered'] / compliance_results['total_tests']
            compliance_results['compliance_score'] = websocket_score * 0.6 + business_value_score * 0.4
        return compliance_results

    def _validate_user_isolation(self, validations: List[AgentExecutionValidation], user_ids: Set[str]):
        """Validate strict user isolation per CLAUDE.md requirements."""
        logger.info('Validating multi-user isolation...')
        isolation_violations = []
        for validation in validations:
            other_user_ids = user_ids - {validation.user_id}
            response_text = (validation.final_response or '').lower()
            for other_user_id in other_user_ids:
                if other_user_id.lower() in response_text:
                    isolation_violations.append(f"User ID leakage: {other_user_id} found in {validation.user_id}'s response")
            for step in validation.reasoning_steps:
                step_lower = step.lower()
                for other_user_id in other_user_ids:
                    if other_user_id.lower() in step_lower:
                        isolation_violations.append(f"User ID in reasoning: {other_user_id} found in {validation.user_id}'s reasoning")
            if validation.tools_executed:
                for tool in validation.tools_executed:
                    tool_args = str(tool.get('args', {})).lower()
                    for other_user_id in other_user_ids:
                        if other_user_id.lower() in tool_args:
                            isolation_violations.append(f"User ID in tool args: {other_user_id} found in {validation.user_id}'s tool execution")
        if isolation_violations:
            violation_summary = '\n'.join((f'  - {v}' for v in isolation_violations))
            pytest.fail(f'CRITICAL: User isolation violations detected:\n{violation_summary}')
        logger.info(f'[U+2713] User isolation validated for {len(user_ids)} users')

@pytest.fixture
async def agent_tester(real_postgres_connection, isolated_test_env):
    """Create and setup the agent execution tester with real services."""
    tester = RealAgentExecutionTester()
    await tester.setup_test_environment()
    yield tester
    await tester.cleanup_resources()

@pytest.mark.asyncio
@pytest.mark.e2e
@pytest.mark.real_services
@pytest.mark.mission_critical
@pytest.mark.no_mocks
class TestRealAgentExecution(BaseE2ETest):
    """Comprehensive test suite for real agent execution workflows."""

    async def test_cost_optimizer_agent_business_value(self, agent_tester):
        """Test cost optimizer agent delivers quantified business value."""
        agent_config = next((config for config in agent_tester.AGENT_TYPES_TO_TEST if config['type'] == 'cost_optimizer'))
        validation = await agent_tester.test_individual_agent_execution(agent_config)
        assert validation.websocket_events_complete, f'Missing WebSocket events: {agent_tester.REQUIRED_WEBSOCKET_EVENTS - validation.event_types_seen}'
        assert validation.has_actionable_insights, 'Cost optimizer must provide actionable optimization insights'
        assert validation.has_quantified_benefits or validation.cost_savings_identified, 'Cost optimizer must identify quantified cost savings opportunities'
        assert validation.has_specific_recommendations, 'Cost optimizer must provide specific optimization recommendations'
        assert validation.total_execution_time is not None, 'Execution must complete'
        assert validation.total_execution_time < 60.0, 'Execution must complete within 60s'
        assert not validation.errors_encountered, f'No errors allowed: {validation.errors_encountered}'
        logger.info(f'[U+2713] Cost optimizer delivered business value in {validation.total_execution_time:.2f}s')
        if validation.cost_savings_identified:
            assert validation.cost_savings_identified > Decimal('10.00'), f'Cost savings too low: ${validation.cost_savings_identified} (minimum $10.00)'
            logger.info(f'[U+2713] Quantified cost savings: ${validation.cost_savings_identified}')
        if validation.optimization_recommendations:
            actionable_recs = [rec for rec in validation.optimization_recommendations if isinstance(rec, dict) and rec.get('action') and rec.get('impact')]
            assert len(actionable_recs) > 0, 'Must provide actionable optimization recommendations'
            logger.info(f'[U+2713] Provided {len(actionable_recs)} actionable recommendations')

    async def test_performance_analyzer_agent_insights(self, agent_tester):
        """Test performance analyzer agent provides actionable performance insights."""
        agent_config = next((config for config in agent_tester.AGENT_TYPES_TO_TEST if config['type'] == 'performance_analyzer'))
        validation = await agent_tester.test_individual_agent_execution(agent_config)
        assert validation.websocket_events_complete, 'Performance analyzer must send all required WebSocket events'
        assert validation.has_actionable_insights, 'Performance analyzer must provide actionable performance insights'
        assert validation.performance_improvements or validation.has_quantified_benefits, 'Performance analyzer must identify specific improvement opportunities'
        assert validation.tools_executed, 'Performance analyzer must use analysis tools'
        assert validation.tool_usage_logical, 'Tool usage must be logical and return results'
        logger.info(f'[U+2713] Performance analyzer provided insights with {len(validation.tools_executed)} tools')
        performance_improvements = validation.performance_improvements
        if performance_improvements:
            quantified_improvements = [imp for imp in performance_improvements if isinstance(imp, dict) and 'percentage' in imp and (imp['percentage'] > 0)]
            if quantified_improvements:
                avg_improvement = sum((imp['percentage'] for imp in quantified_improvements)) / len(quantified_improvements)
                assert avg_improvement >= 2.0, f'Performance improvement too low: {avg_improvement}% (minimum 2%)'
                logger.info(f'[U+2713] Average performance improvement: {avg_improvement:.1f}%')

    async def test_triage_agent_routing_logic(self, agent_tester):
        """Test triage agent properly routes requests and classifies priorities."""
        agent_config = next((config for config in agent_tester.AGENT_TYPES_TO_TEST if config['type'] == 'triage_agent'))
        validation = await agent_tester.test_individual_agent_execution(agent_config)
        assert validation.websocket_events_complete, 'Triage agent must send all required WebSocket events'
        assert validation.has_actionable_insights, 'Triage agent must provide clear routing or classification guidance'
        assert validation.reasoning_depth_adequate, 'Triage agent must show adequate reasoning for routing decisions'
        routing_keywords = ['route', 'classify', 'priority', 'specialist', 'recommend']
        response_text = (validation.final_response or '').lower()
        routing_evidence = any((keyword in response_text for keyword in routing_keywords))
        reasoning_evidence = any((any((keyword in step.lower() for keyword in routing_keywords)) for step in validation.reasoning_steps))
        assert routing_evidence or reasoning_evidence, 'Triage agent must demonstrate routing/classification logic'
        logger.info(f'[U+2713] Triage agent showed routing logic with {len(validation.reasoning_steps)} reasoning steps')
        routing_quality_indicators = ['priority' in (validation.final_response or '').lower(), 'route' in (validation.final_response or '').lower(), 'recommend' in (validation.final_response or '').lower(), len(validation.reasoning_steps) >= 2, validation.reasoning_depth_adequate]
        quality_score = sum(routing_quality_indicators) / len(routing_quality_indicators)
        assert quality_score >= 0.6, f'Triage quality score too low: {quality_score:.1f} (minimum 0.6)'
        logger.info(f'[U+2713] Triage quality score: {quality_score:.1f}')

    async def test_parallel_multi_user_agent_execution(self, agent_tester):
        """Test multiple agents executing in parallel for different users (user isolation)."""
        validations = await agent_tester.test_parallel_agent_execution()
        successful_count = len([v for v in validations if v.parallel_execution_successful])
        assert successful_count >= 2, f'Need at least 2 successful parallel executions, got {successful_count}'
        websocket_compliant = [v for v in validations if v.websocket_events_complete]
        assert len(websocket_compliant) >= 2, 'Parallel executions must be WebSocket compliant'
        business_value_delivered = [v for v in validations if v.has_actionable_insights]
        assert len(business_value_delivered) >= 1, 'Parallel executions must deliver business value'
        user_ids = set((v.user_id for v in validations))
        assert len(user_ids) > 1, 'Must test multiple users for isolation'
        self._validate_user_isolation(validations, user_ids)
        for validation in validations:
            user_context = self.test_users[validation.user_id]['user_context']
            assert validation.thread_id == user_context.thread_id, f'Thread ID mismatch for {validation.user_id}'
            assert validation.user_id == user_context.user_id, f'User ID mismatch in context'
            other_threads = [self.test_users[uid]['user_context'].thread_id for uid in user_ids if uid != validation.user_id]
            for other_thread in other_threads:
                assert other_thread not in (validation.final_response or ''), f"Thread leakage: {other_thread} found in {validation.user_id}'s response"
        logger.info(f'[U+2713] Parallel execution successful with {len(user_ids)} isolated users')
        logger.info(f'[U+2713] Multi-user isolation validated - no data leakage detected')

    async def test_agent_tool_execution_integration(self, agent_tester):
        """Test agent tool execution and results integration."""
        validation = await agent_tester.execute_agent_with_validation(agent_type='cost_optimizer', message='Perform comprehensive cost analysis of our AI infrastructure and calculate potential savings', user_id=list(agent_tester.test_users.keys())[0], timeout=50.0, expected_outcomes=['tool_integration', 'data_analysis', 'cost_calculation'])
        assert 'tool_executing' in validation.event_types_seen or 'tool_completed' in validation.event_types_seen or validation.has_actionable_insights, 'Must execute tools or provide direct insights'
        if validation.tools_executed:
            assert validation.tool_usage_logical, 'Tool execution must be logical with proper results'
            for tool in validation.tools_executed:
                if 'duration' in tool:
                    assert tool['duration'] < 30.0, f"Tool {tool['name']} took too long: {tool['duration']}s"
            assert validation.has_actionable_insights, 'Tool results must contribute to actionable insights'
        logger.info(f'[U+2713] Tool integration validated with {len(validation.tools_executed)} tools executed')

    async def test_agent_reasoning_quality(self, agent_tester):
        """Test agent reasoning and decision-making quality."""
        complex_message = 'We have a machine learning pipeline with the following issues: 1) Training takes 6 hours, 2) Model accuracy is 78%, 3) Inference latency is 200ms, 4) Monthly costs are $15,000. Provide a comprehensive optimization strategy with prioritized recommendations.'
        validation = await agent_tester.execute_agent_with_validation(agent_type='performance_analyzer', message=complex_message, user_id=list(agent_tester.test_users.keys())[0], timeout=60.0, expected_outcomes=['comprehensive_analysis', 'prioritized_recommendations', 'multi_factor_optimization'])
        assert validation.has_actionable_insights, 'Must provide actionable insights for complex scenario'
        assert validation.reasoning_depth_adequate, 'Must show adequate reasoning depth for complex problems'
        if validation.reasoning_steps:
            reasoning_text = ' '.join(validation.reasoning_steps).lower()
            problem_aspects = ['training', 'accuracy', 'latency', 'cost']
            addressed_aspects = sum((1 for aspect in problem_aspects if aspect in reasoning_text))
            assert addressed_aspects >= 2, f'Should address at least 2 problem aspects, addressed {addressed_aspects}'
        if validation.final_response:
            response_length = len(validation.final_response)
            assert response_length >= 300, f'Complex problem requires comprehensive response, got {response_length} chars'
        logger.info(f'[U+2713] Reasoning quality validated with {len(validation.reasoning_steps)} reasoning steps')

    async def test_agent_error_handling_and_recovery(self, agent_tester):
        """Test agent error handling and graceful recovery."""
        validation = await agent_tester.test_agent_error_recovery()
        assert len(validation.events_received) > 0, 'Should receive some events even with error conditions'
        assert validation.error_recovery_successful or validation.has_actionable_insights, 'Should either recover from errors or provide graceful error response'
        assert validation.total_execution_time is not None, 'Execution should complete even with error conditions'
        assert validation.total_execution_time < 45.0, 'Error handling should not cause excessive delays'
        if validation.errors_encountered:
            logger.info(f'Errors handled: {validation.errors_encountered}')
        logger.info(f'[U+2713] Error handling validated with recovery: {validation.error_recovery_successful}')

    async def test_agent_performance_monitoring(self, agent_tester):
        """Test agent performance monitoring and metrics collection."""
        performance_validations = []
        for i in range(3):
            validation = await agent_tester.execute_agent_with_validation(agent_type='triage_agent', message=f'Classify this optimization request #{i}: Improve model training speed', user_id=list(agent_tester.test_users.keys())[0], timeout=30.0, expected_outcomes=['fast_classification', 'performance_monitoring'])
            performance_validations.append(validation)
        execution_times = [v.total_execution_time for v in performance_validations if v.total_execution_time]
        assert len(execution_times) == 3, 'All executions should complete'
        avg_time = sum(execution_times) / len(execution_times)
        max_time = max(execution_times)
        min_time = min(execution_times)
        assert avg_time < 25.0, f'Average execution time {avg_time:.2f}s too slow'
        assert max_time < 35.0, f'Maximum execution time {max_time:.2f}s too slow'
        time_variance = max_time - min_time
        assert time_variance < 20.0, f'Execution time variance {time_variance:.2f}s too high'
        successful_executions = [v for v in performance_validations if v.has_actionable_insights]
        assert len(successful_executions) == 3, 'All executions should provide value'
        logger.info(f'[U+2713] Performance monitoring validated: avg={avg_time:.2f}s, range={min_time:.2f}-{max_time:.2f}s')

    async def test_comprehensive_agent_workflow_validation(self, agent_tester):
        """Comprehensive test validating complete agent workflows end-to-end."""
        logger.info('Running comprehensive agent workflow validation')
        all_validations = []
        for agent_config in agent_tester.AGENT_TYPES_TO_TEST:
            validation = await agent_tester.test_individual_agent_execution(agent_config)
            all_validations.append(validation)
        parallel_validations = await agent_tester.test_parallel_agent_execution()
        all_validations.extend(parallel_validations)
        error_validation = await agent_tester.test_agent_error_recovery()
        all_validations.append(error_validation)
        total_executions = len(all_validations)
        successful_executions = [v for v in all_validations if v.websocket_events_complete and v.has_actionable_insights]
        business_value_executions = [v for v in all_validations if v.has_quantified_benefits or v.has_specific_recommendations]
        success_rate = len(successful_executions) / total_executions
        business_value_rate = len(business_value_executions) / total_executions
        assert success_rate >= 0.7, f'Success rate {success_rate:.1%} too low (need  >= 70%)'
        assert business_value_rate >= 0.5, f'Business value rate {business_value_rate:.1%} too low (need  >= 50%)'
        report = agent_tester.generate_comprehensive_report()
        report_file = os.path.join(project_root, 'test_outputs', 'agent_execution_e2e_report.txt')
        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            f.write(f'\n\nGenerated at: {datetime.now().isoformat()}\n')
        logger.info(f'Comprehensive report saved to: {report_file}')
        logger.info(f'[U+2713] Comprehensive validation complete: {success_rate:.1%} success, {business_value_rate:.1%} business value')
        compliance_results = agent_tester.validate_business_value_compliance()
        logger.info('BUSINESS VALUE COMPLIANCE REPORT:')
        logger.info(f"- WebSocket Compliance: {compliance_results['websocket_compliant']}/{compliance_results['total_tests']} ({compliance_results['websocket_compliant'] * 100 / compliance_results['total_tests']:.1f}%)")
        logger.info(f"- Business Value Delivered: {compliance_results['business_value_delivered']}/{compliance_results['total_tests']} ({compliance_results['business_value_delivered'] * 100 / compliance_results['total_tests']:.1f}%)")
        logger.info(f"- Overall Compliance Score: {compliance_results['compliance_score']:.2f}")
        assert compliance_results['compliance_score'] >= 0.7, f"Compliance score {compliance_results['compliance_score']:.2f} below required 0.7"
        if compliance_results['critical_failures']:
            for failure in compliance_results['critical_failures']:
                logger.error(f'CRITICAL FAILURE: {failure}')
            pytest.fail(f"Critical failures detected: {compliance_results['critical_failures']}")
        logger.info(f"[U+2713] CLAUDE.md compliance validated: {compliance_results['compliance_score']:.2f} score")
        logger.info('KEY FINDINGS:')
        logger.info(f'- Total executions: {total_executions}')
        logger.info(f'- Successful executions: {len(successful_executions)} ({success_rate:.1%})')
        logger.info(f'- Business value delivered: {len(business_value_executions)} ({business_value_rate:.1%})')
        websocket_compliant = [v for v in all_validations if v.websocket_events_complete]
        logger.info(f'- WebSocket compliant: {len(websocket_compliant)} ({len(websocket_compliant) * 100 / total_executions:.1f}%)')

def validate_test_framework_imports():
    """Validate that all test_framework imports are available and correct."""
    try:
        assert BaseE2ETest, 'BaseE2ETest import failed'
        assert WebSocketTestHelpers, 'WebSocketTestHelpers import failed'
        assert AgentResultValidator, 'AgentResultValidator import failed'
        assert TestExecutionContext, 'TestExecutionContext import failed'
        assert create_test_execution_context, 'create_test_execution_context import failed'
        assert CommonValidators, 'CommonValidators import failed'
        assert real_postgres_connection, 'real_postgres_connection import failed'
        assert isolated_test_env, 'isolated_test_env import failed'
        assert TEST_PORTS['backend'] == 8000, 'Backend port configuration incorrect'
        assert TEST_PORTS['postgresql'] == 5434, 'PostgreSQL test port configuration incorrect'
        logger.info('[U+2713] All test framework imports validated')
        return True
    except Exception as e:
        logger.error(f'Test framework import validation failed: {e}')
        return False
if __name__ == '__main__':
    'MIGRATED: Use SSOT unified test runner'
    print('MIGRATION NOTICE: Please use SSOT unified test runner')
    print('Command: python tests/unified_test_runner.py --category <category>')
validate_test_framework_imports()