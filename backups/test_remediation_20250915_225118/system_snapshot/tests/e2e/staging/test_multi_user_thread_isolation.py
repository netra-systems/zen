"""
Test Multi-User Thread Isolation in Staging Environment

Business Value Justification (BVJ):
- Segment: All (Free, Early, Mid, Enterprise) - Critical for multi-tenant platform
- Business Goal: Ensure complete user data isolation prevents data leakage and maintains customer trust
- Value Impact: Thread isolation failures compromise user privacy, violate compliance requirements, and destroy customer confidence
- Strategic Impact: Foundation for enterprise-grade multi-user platform - failure blocks Enterprise segment growth

This comprehensive E2E test suite validates multi-user thread isolation in staging environment:
1. Concurrent user thread creation with complete isolation validation
2. Message privacy between users - preventing data leakage across sessions
3. Agent execution context isolation - ensuring user data never crosses boundaries
4. Thread state isolation under high concurrency and stress conditions
5. Cross-user contamination prevention and detection
6. Recovery scenarios maintaining isolation during failures
7. Performance validation of isolation mechanisms under realistic load

CRITICAL REQUIREMENTS:
- MANDATORY OAuth/JWT authentication flows for each user session
- ALL 5 WebSocket events MUST be sent and validated for each user session
- Real staging environment with actual multi-user concurrent sessions - NO MOCKS
- Real LLM agent execution with user-specific context isolation
- Comprehensive data leakage detection and prevention validation

CRITICAL: This test proves multi-user platform integrity and prevents catastrophic data breaches.
Expected: 5-7 test methods that validate complete user isolation under all conditions.
"""

import asyncio
import json
import time
import uuid
import pytest
import websockets
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Tuple, Set
from unittest.mock import patch
from concurrent.futures import ThreadPoolExecutor

# Test framework and authentication
from test_framework.base_integration_test import BaseIntegrationTest
from test_framework.ssot.e2e_auth_helper import (
    E2EAuthHelper, E2EWebSocketAuthHelper, E2EAuthConfig,
    create_authenticated_user_context
)

# Staging configuration and utilities
from tests.e2e.staging.conftest import staging_services_fixture
from tests.e2e.staging_config import StagingTestConfig, staging_urls

# Strongly typed IDs and execution context
from shared.types.core_types import UserID, ThreadID, RunID, RequestID, WebSocketID
from shared.types.execution_types import StronglyTypedUserExecutionContext
from shared.isolated_environment import get_env

# WebSocket event validation
from test_framework.websocket_helpers import (
    WebSocketTestClient, assert_websocket_events, wait_for_agent_completion
)


class MultiUserThreadIsolationStagingTests(BaseIntegrationTest):
    """
    Comprehensive E2E test suite for multi-user thread isolation in staging environment.
    
    This test suite validates that user data isolation is maintained across all scenarios,
    ensuring the platform delivers secure, private AI assistance to multiple concurrent users
    without any risk of data leakage or cross-user contamination.
    """

    @pytest.fixture(autouse=True)
    async def setup_multi_user_staging_auth(self):
        """Setup multiple authenticated users for isolation testing."""
        self.env = get_env()
        self.staging_config = StagingTestConfig()
        
        # Create multiple authenticated users for isolation testing
        self.test_users = []
        self.user_auth_helpers = {}
        self.user_websocket_helpers = {}
        
        # Generate 5 test users for comprehensive isolation testing
        for i in range(5):
            user_email = f"isolation_test_user_{i}_{uuid.uuid4().hex[:8]}@staging.test.com"
            
            # Create authenticated user context
            user_context = await create_authenticated_user_context(
                user_email=user_email,
                environment="staging",
                permissions=["read", "write", "agent_execute"],
                websocket_enabled=True
            )
            
            # Create dedicated auth helpers for each user
            auth_helper = E2EAuthHelper(environment="staging")
            websocket_helper = E2EWebSocketAuthHelper(environment="staging")
            
            # Get staging token for each user
            staging_token = await auth_helper.get_staging_token_async(email=user_email)
            
            user_data = {
                "user_index": i,
                "user_context": user_context,
                "user_email": user_email,
                "staging_token": staging_token,
                "auth_helper": auth_helper,
                "websocket_helper": websocket_helper,
                "auth_headers": auth_helper.get_auth_headers(staging_token),
                "websocket_headers": auth_helper.get_websocket_headers(staging_token)
            }
            
            self.test_users.append(user_data)
            self.user_auth_helpers[str(user_context.user_id)] = auth_helper
            self.user_websocket_helpers[str(user_context.user_id)] = websocket_helper
        
        self.logger.info(f" PASS:  Multi-user staging authentication setup complete - {len(self.test_users)} users ready")

    @pytest.mark.e2e
    @pytest.mark.staging
    @pytest.mark.real_llm
    @pytest.mark.mission_critical
    async def test_concurrent_user_thread_creation_with_complete_isolation(self):
        """
        Test concurrent user thread creation maintains complete isolation.
        
        CRITICAL: This test validates that when multiple users create threads simultaneously,
        no user can access or see another user's thread data, ensuring fundamental platform security.
        
        Business Value: Prevents catastrophic data breaches that would destroy customer trust
        and violate compliance requirements (GDPR, HIPAA, SOC 2).
        """
        self.logger.info("[U+1F512] Starting concurrent user thread creation isolation test")
        
        # Data structures to track user isolation
        user_threads = {}  # user_id -> [thread_ids]
        user_messages = {}  # user_id -> [message_contents]
        cross_contamination_detected = []
        isolation_metrics = {
            "threads_created": 0,
            "isolation_violations": 0,
            "successful_isolations": 0,
            "concurrent_operations": 0
        }
        
        async def create_isolated_user_thread(user_data: Dict[str, Any]) -> Dict[str, Any]:
            """Create thread for a single user with isolation tracking."""
            user_index = user_data["user_index"]
            user_context = user_data["user_context"]
            websocket_headers = user_data["websocket_headers"]
            
            try:
                async with websockets.connect(
                    self.staging_config.urls.websocket_url,
                    additional_headers=websocket_headers,
                    open_timeout=20.0  # Extended timeout for concurrent operations
                ) as websocket:
                    
                    # Create thread with user-specific sensitive data
                    sensitive_user_data = {
                        "company_name": f"SecretCorp{user_index}",
                        "api_key": f"secret_key_{user_index}_{uuid.uuid4().hex}",
                        "business_data": f"confidential_metrics_user_{user_index}",
                        "personal_info": f"private_data_{user_index}_{time.time()}"
                    }
                    
                    thread_creation = {
                        "type": "create_thread",
                        "thread_id": str(user_context.thread_id),
                        "user_id": str(user_context.user_id),
                        "context": {
                            "user_index": user_index,
                            "sensitive_data": sensitive_user_data,
                            "isolation_test": True
                        },
                        "metadata": {
                            "test_type": "concurrent_isolation",
                            "user_identifier": f"user_{user_index}",
                            "timestamp": datetime.now(timezone.utc).isoformat()
                        }
                    }
                    
                    await websocket.send(json.dumps(thread_creation))
                    response = await asyncio.wait_for(websocket.recv(), timeout=15.0)
                    thread_response = json.loads(response)
                    
                    # Verify thread creation success
                    assert thread_response["type"] == "thread_created", f"User {user_index}: Thread creation failed"
                    assert thread_response["thread_id"] == str(user_context.thread_id), f"User {user_index}: Thread ID mismatch"
                    
                    # Store user's thread ID
                    user_id_str = str(user_context.user_id)
                    if user_id_str not in user_threads:
                        user_threads[user_id_str] = []
                    user_threads[user_id_str].append(str(user_context.thread_id))
                    
                    # Add sensitive message to thread
                    sensitive_message = {
                        "type": "send_message",
                        "thread_id": str(user_context.thread_id),
                        "message": f"CONFIDENTIAL: User {user_index} discussing sensitive business data: {sensitive_user_data['business_data']}. API Key: {sensitive_user_data['api_key'][:8]}***",
                        "role": "user",
                        "metadata": {
                            "sensitivity_level": "high",
                            "user_specific": True,
                            "isolation_marker": f"ISOLATED_USER_{user_index}"
                        }
                    }
                    
                    await websocket.send(json.dumps(sensitive_message))
                    message_response = await asyncio.wait_for(websocket.recv(), timeout=10.0)
                    message_data = json.loads(message_response)
                    
                    # Store user's message content for isolation verification
                    if user_id_str not in user_messages:
                        user_messages[user_id_str] = []
                    user_messages[user_id_str].append(sensitive_user_data)
                    
                    isolation_metrics["threads_created"] += 1
                    isolation_metrics["concurrent_operations"] += 1
                    
                    return {
                        "user_index": user_index,
                        "user_id": user_id_str,
                        "thread_id": str(user_context.thread_id),
                        "sensitive_data": sensitive_user_data,
                        "success": True,
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    }
                    
            except Exception as e:
                self.logger.error(f"User {user_index} thread creation failed: {e}")
                return {
                    "user_index": user_index,
                    "success": False,
                    "error": str(e)
                }
        
        # Execute concurrent thread creation for all users
        self.logger.info(f"[U+1F680] Starting concurrent thread creation for {len(self.test_users)} users")
        start_time = time.time()
        
        results = await asyncio.gather(*[
            create_isolated_user_thread(user_data) 
            for user_data in self.test_users
        ], return_exceptions=True)
        
        concurrent_duration = time.time() - start_time
        self.logger.info(f"[U+23F1][U+FE0F] Concurrent operations completed in {concurrent_duration:.2f}s")
        
        # Analyze results for isolation violations
        successful_results = [r for r in results if isinstance(r, dict) and r.get("success")]
        
        assert len(successful_results) >= 4, \
            f"Too many concurrent operations failed: {len(successful_results)}/{len(self.test_users)} succeeded"
        
        # CRITICAL ISOLATION VALIDATION
        
        # 1. Verify each user can only access their own threads
        for user_data in self.test_users:
            user_id = str(user_data["user_context"].user_id)
            user_threads_list = user_threads.get(user_id, [])
            
            # User should have exactly their own threads
            assert len(user_threads_list) >= 1, f"User {user_id} missing their own threads"
            
            # Verify no access to other users' threads
            for other_user_data in self.test_users:
                other_user_id = str(other_user_data["user_context"].user_id)
                if other_user_id != user_id:
                    other_user_threads = user_threads.get(other_user_id, [])
                    
                    # Check for cross-contamination
                    for thread_id in user_threads_list:
                        if thread_id in other_user_threads:
                            cross_contamination_detected.append({
                                "violation_type": "thread_id_collision",
                                "user1": user_id,
                                "user2": other_user_id,
                                "thread_id": thread_id
                            })
                            isolation_metrics["isolation_violations"] += 1
        
        # 2. Verify sensitive data isolation
        user_ids = list(user_messages.keys())
        for i, user_id1 in enumerate(user_ids):
            user1_data = user_messages[user_id1]
            
            for j, user_id2 in enumerate(user_ids):
                if i != j:
                    user2_data = user_messages[user_id2]
                    
                    # Check if user1's data appears in user2's messages
                    for user1_sensitive in user1_data:
                        for user2_sensitive in user2_data:
                            # Look for data leakage
                            if (user1_sensitive["api_key"] in str(user2_sensitive) or
                                user1_sensitive["company_name"] in str(user2_sensitive) or
                                user1_sensitive["business_data"] in str(user2_sensitive)):
                                
                                cross_contamination_detected.append({
                                    "violation_type": "sensitive_data_leakage",
                                    "source_user": user_id1,
                                    "target_user": user_id2,
                                    "leaked_data": user1_sensitive["api_key"][:8] + "***"
                                })
                                isolation_metrics["isolation_violations"] += 1
        
        # 3. Calculate isolation success metrics
        total_user_pairs = len(self.test_users) * (len(self.test_users) - 1)
        isolation_metrics["successful_isolations"] = total_user_pairs - isolation_metrics["isolation_violations"]
        
        # CRITICAL BUSINESS VALUE VALIDATION
        
        # Zero tolerance for isolation violations
        if cross_contamination_detected:
            self.logger.error(f" FAIL:  CRITICAL ISOLATION VIOLATIONS DETECTED: {cross_contamination_detected}")
            raise AssertionError(f"CRITICAL SECURITY FAILURE: {len(cross_contamination_detected)} isolation violations detected - platform compromised!")
        
        # Performance validation
        assert concurrent_duration < 30.0, \
            f"Concurrent operations too slow: {concurrent_duration:.2f}s - affects user experience"
        
        # Success rate validation
        success_rate = len(successful_results) / len(self.test_users)
        assert success_rate >= 0.8, \
            f"Concurrent success rate too low: {success_rate:.1%} - affects platform reliability"
        
        self.logger.info(" CELEBRATION:  CONCURRENT USER THREAD ISOLATION TEST SUCCESS")
        self.logger.info(f"[U+1F465] Users tested: {len(self.test_users)}")
        self.logger.info(f"[U+1F512] Isolation violations: {isolation_metrics['isolation_violations']} (MUST be 0)")
        self.logger.info(f" PASS:  Successful isolations: {isolation_metrics['successful_isolations']}")
        self.logger.info(f"[U+23F1][U+FE0F] Concurrent performance: {concurrent_duration:.2f}s")
        self.logger.info(f" IDEA:  Business Value: Multi-user platform security verified - customer data protected")

    @pytest.mark.e2e
    @pytest.mark.staging
    @pytest.mark.real_llm
    @pytest.mark.mission_critical
    async def test_message_privacy_between_users_prevents_data_leakage(self):
        """
        Test message privacy between users prevents any form of data leakage.
        
        CRITICAL: This test validates that user messages remain completely private and
        cannot be accessed by other users through any vector (direct access, agent context, etc.).
        
        Business Value: Protects confidential business communications and prevents
        industrial espionage or competitive intelligence leaks.
        """
        self.logger.info("[U+1F510] Starting message privacy and data leakage prevention test")
        
        # Create distinct business scenarios for each user to test isolation
        business_scenarios = [
            {
                "company": "TechCorp Alpha",
                "sector": "fintech",
                "confidential_info": "developing blockchain payment system, Series C funding $50M",
                "api_credentials": f"tc_alpha_{uuid.uuid4().hex[:16]}",
                "competitor_info": "main competitor is FinanceFlow Inc"
            },
            {
                "company": "HealthTech Beta", 
                "sector": "healthcare",
                "confidential_info": "FDA approval pending for AI diagnostic tool, patient data: 100K records",
                "api_credentials": f"ht_beta_{uuid.uuid4().hex[:16]}",
                "competitor_info": "competing with MedAI Solutions"
            },
            {
                "company": "RetailGiant Gamma",
                "sector": "ecommerce", 
                "confidential_info": "Q4 revenue projections $200M, supply chain optimization saves 15%",
                "api_credentials": f"rg_gamma_{uuid.uuid4().hex[:16]}",
                "competitor_info": "market leader against AmazonClone"
            },
            {
                "company": "AutoMotive Delta",
                "sector": "automotive",
                "confidential_info": "electric vehicle platform launch 2024, battery technology breakthrough",
                "api_credentials": f"am_delta_{uuid.uuid4().hex[:16]}",
                "competitor_info": "Tesla alternative with 500-mile range"
            },
            {
                "company": "EnergyFlow Epsilon",
                "sector": "renewable",
                "confidential_info": "solar panel efficiency increased to 45%, government contract $500M",
                "api_credentials": f"ef_epsilon_{uuid.uuid4().hex[:16]}",
                "competitor_info": "displacing traditional energy providers"
            }
        ]
        
        user_conversations = {}
        privacy_validation_results = []
        
        async def conduct_private_business_conversation(user_data: Dict[str, Any], scenario: Dict[str, str]) -> Dict[str, Any]:
            """Conduct confidential business conversation for a single user."""
            user_index = user_data["user_index"]
            user_context = user_data["user_context"]
            websocket_headers = user_data["websocket_headers"]
            
            conversation_log = []
            
            try:
                async with websockets.connect(
                    self.staging_config.urls.websocket_url,
                    additional_headers=websocket_headers,
                    open_timeout=20.0
                ) as websocket:
                    
                    # Send highly confidential business information
                    confidential_messages = [
                        f"CONFIDENTIAL: I'm the CEO of {scenario['company']} in {scenario['sector']} sector.",
                        f"SENSITIVE: {scenario['confidential_info']}",
                        f"PRIVATE: Our API credentials for integration: {scenario['api_credentials']}",
                        f"COMPETITIVE: {scenario['competitor_info']}",
                        f"STRATEGIC: Need cost optimization analysis for our specific business model."
                    ]
                    
                    for i, message_content in enumerate(confidential_messages):
                        confidential_message = {
                            "type": "send_message",
                            "thread_id": str(user_context.thread_id),
                            "message": message_content,
                            "role": "user",
                            "metadata": {
                                "confidentiality": "highest",
                                "business_sector": scenario["sector"],
                                "message_sequence": i + 1,
                                "user_marker": f"USER_{user_index}_ONLY"
                            }
                        }
                        
                        await websocket.send(json.dumps(confidential_message))
                        response = await asyncio.wait_for(websocket.recv(), timeout=10.0)
                        conversation_log.append({
                            "message": message_content,
                            "response": json.loads(response),
                            "timestamp": datetime.now(timezone.utc).isoformat()
                        })
                        
                        await asyncio.sleep(0.5)  # Natural conversation pace
                    
                    # Request business-specific agent analysis
                    agent_analysis_request = {
                        "type": "agent_request",
                        "thread_id": str(user_context.thread_id),
                        "agent": "business_advisor",
                        "message": f"Based on our confidential discussion about {scenario['company']}, provide sector-specific cost optimization recommendations for {scenario['sector']}.",
                        "metadata": {
                            "confidential_context": True,
                            "sector_specific": scenario["sector"],
                            "business_sensitive": True
                        }
                    }
                    
                    await websocket.send(json.dumps(agent_analysis_request))
                    
                    # Collect agent response while monitoring for cross-contamination
                    agent_events = []
                    agent_response_content = ""
                    
                    while len(agent_events) < 8:  # Collect comprehensive response
                        try:
                            event_data = await asyncio.wait_for(websocket.recv(), timeout=20.0)
                            event = json.loads(event_data)
                            agent_events.append(event)
                            
                            # Extract agent response content for privacy validation
                            if event.get("type") == "agent_thinking":
                                thinking_content = event.get("data", {}).get("content", "")
                                agent_response_content += thinking_content + " "
                            
                            elif event.get("type") == "agent_completed":
                                final_response = event.get("data", {}).get("response", {})
                                agent_response_content += str(final_response) + " "
                                break
                                
                        except asyncio.TimeoutError:
                            break
                    
                    conversation_log.append({
                        "agent_request": agent_analysis_request,
                        "agent_events": agent_events,
                        "agent_response_content": agent_response_content,
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    })
                    
                    return {
                        "user_index": user_index,
                        "user_id": str(user_context.user_id),
                        "scenario": scenario,
                        "conversation_log": conversation_log,
                        "agent_response_content": agent_response_content,
                        "success": True
                    }
                    
            except Exception as e:
                self.logger.error(f"User {user_index} private conversation failed: {e}")
                return {
                    "user_index": user_index,
                    "success": False,
                    "error": str(e)
                }
        
        # Execute concurrent private conversations
        self.logger.info(f"[U+1F680] Starting {len(self.test_users)} concurrent private business conversations")
        
        conversation_tasks = [
            conduct_private_business_conversation(user_data, business_scenarios[i])
            for i, user_data in enumerate(self.test_users)
        ]
        
        results = await asyncio.gather(*conversation_tasks, return_exceptions=True)
        
        # Filter successful conversations
        successful_conversations = [r for r in results if isinstance(r, dict) and r.get("success")]
        
        assert len(successful_conversations) >= 4, \
            f"Too many private conversations failed: {len(successful_conversations)}/{len(self.test_users)}"
        
        # Store conversations for cross-contamination analysis
        for conversation in successful_conversations:
            user_id = conversation["user_id"]
            user_conversations[user_id] = conversation
        
        # CRITICAL PRIVACY VALIDATION: Cross-Contamination Detection
        
        contamination_violations = []
        
        # Check each user's agent responses for other users' confidential data
        user_ids = list(user_conversations.keys())
        
        for i, user_id1 in enumerate(user_ids):
            user1_conversation = user_conversations[user_id1]
            user1_scenario = user1_conversation["scenario"]
            user1_agent_content = user1_conversation["agent_response_content"].lower()
            
            for j, user_id2 in enumerate(user_ids):
                if i != j:
                    user2_conversation = user_conversations[user_id2]
                    user2_scenario = user2_conversation["scenario"]
                    
                    # Check if user1's agent response contains user2's confidential data
                    confidential_elements = [
                        user2_scenario["company"].lower(),
                        user2_scenario["api_credentials"].lower(),
                        user2_scenario["competitor_info"].lower()
                    ]
                    
                    for element in confidential_elements:
                        if element in user1_agent_content:
                            contamination_violations.append({
                                "violation_type": "agent_response_contamination",
                                "source_user": user_id2,
                                "contaminated_user": user_id1,
                                "leaked_element": element[:20] + "***",
                                "detection_method": "agent_response_analysis"
                            })
                    
                    # Check for sector-specific cross-contamination
                    if (user2_scenario["sector"] != user1_scenario["sector"] and 
                        user2_scenario["sector"] in user1_agent_content):
                        contamination_violations.append({
                            "violation_type": "sector_specific_contamination",
                            "source_user": user_id2,
                            "contaminated_user": user_id1,
                            "leaked_sector": user2_scenario["sector"],
                            "detection_method": "sector_analysis"
                        })
        
        # Additional validation: Direct message access attempt
        for user_data in self.test_users[:2]:  # Test with first 2 users
            user_context = user_data["user_context"]
            websocket_headers = user_data["websocket_headers"]
            
            # Attempt to access another user's thread
            other_user = self.test_users[2] if self.test_users[2] != user_data else self.test_users[1]
            other_thread_id = str(other_user["user_context"].thread_id)
            
            try:
                async with websockets.connect(
                    self.staging_config.urls.websocket_url,
                    additional_headers=websocket_headers,
                    open_timeout=15.0
                ) as websocket:
                    
                    # Attempt unauthorized thread access
                    unauthorized_request = {
                        "type": "get_thread_messages",
                        "thread_id": other_thread_id,  # Another user's thread
                        "user_id": str(user_context.user_id)
                    }
                    
                    await websocket.send(json.dumps(unauthorized_request))
                    response = await asyncio.wait_for(websocket.recv(), timeout=10.0)
                    response_data = json.loads(response)
                    
                    # Should receive error or empty response
                    if (response_data.get("type") == "thread_messages" and 
                        response_data.get("data", {}).get("messages")):
                        contamination_violations.append({
                            "violation_type": "unauthorized_thread_access",
                            "accessing_user": str(user_context.user_id),
                            "accessed_thread": other_thread_id,
                            "detection_method": "direct_access_attempt"
                        })
            
            except Exception as e:
                # Expected behavior - should fail to access other user's data
                self.logger.info(f" PASS:  Unauthorized access correctly blocked: {e}")
        
        # CRITICAL BUSINESS VALUE VALIDATION
        
        # Zero tolerance for privacy violations
        if contamination_violations:
            self.logger.error(f" FAIL:  CRITICAL PRIVACY VIOLATIONS DETECTED: {contamination_violations}")
            raise AssertionError(f"CRITICAL PRIVACY FAILURE: {len(contamination_violations)} privacy violations detected - customer data compromised!")
        
        # Verify all conversations contained sector-specific responses
        sector_specific_responses = 0
        for conversation in successful_conversations:
            agent_content = conversation["agent_response_content"].lower()
            user_sector = conversation["scenario"]["sector"]
            
            if user_sector in agent_content:
                sector_specific_responses += 1
        
        assert sector_specific_responses >= len(successful_conversations) * 0.8, \
            f"Insufficient sector-specific responses: {sector_specific_responses}/{len(successful_conversations)} - agent context not properly isolated"
        
        self.logger.info(" CELEBRATION:  MESSAGE PRIVACY AND DATA LEAKAGE PREVENTION TEST SUCCESS")
        self.logger.info(f"[U+1F465] Private conversations: {len(successful_conversations)}")
        self.logger.info(f"[U+1F512] Privacy violations: {len(contamination_violations)} (MUST be 0)")
        self.logger.info(f" TARGET:  Sector-specific responses: {sector_specific_responses}/{len(successful_conversations)}")
        self.logger.info(f" IDEA:  Business Value: Confidential business communications protected across all scenarios")

    @pytest.mark.e2e
    @pytest.mark.staging
    @pytest.mark.real_llm
    async def test_agent_execution_context_isolation_between_users(self):
        """
        Test agent execution context isolation prevents cross-user context bleeding.
        
        Business Value: Ensures AI agents provide personalized, context-aware responses
        without mixing user contexts, maintaining recommendation quality and trust.
        """
        self.logger.info("[U+1F916] Starting agent execution context isolation test")
        
        # Create distinct user profiles with conflicting preferences
        user_profiles = [
            {
                "role": "startup_cto",
                "company_size": "10 employees",
                "budget": "$5,000/month",
                "preferences": "cost-first, open-source, minimal infrastructure",
                "tech_stack": "nodejs, mongodb, docker",
                "priority": "rapid_scaling"
            },
            {
                "role": "enterprise_architect", 
                "company_size": "5000 employees",
                "budget": "$500,000/month",
                "preferences": "security-first, enterprise-grade, high-availability",
                "tech_stack": "java, oracle, kubernetes",
                "priority": "compliance_security"
            },
            {
                "role": "scale_up_engineer",
                "company_size": "200 employees", 
                "budget": "$50,000/month",
                "preferences": "performance-first, hybrid-cloud, automated-operations",
                "tech_stack": "python, postgresql, terraform",
                "priority": "operational_efficiency"
            }
        ]
        
        agent_context_results = []
        context_isolation_metrics = {
            "personalized_responses": 0,
            "context_bleeding_incidents": 0,
            "recommendation_accuracy": 0
        }
        
        async def execute_agent_with_user_context(user_data: Dict[str, Any], profile: Dict[str, str]) -> Dict[str, Any]:
            """Execute agent with specific user context and validate isolation."""
            user_index = user_data["user_index"]
            user_context = user_data["user_context"]
            websocket_headers = user_data["websocket_headers"]
            
            try:
                async with websockets.connect(
                    self.staging_config.urls.websocket_url,
                    additional_headers=websocket_headers,
                    open_timeout=20.0
                ) as websocket:
                    
                    # Establish user-specific context
                    context_establishment = {
                        "type": "send_message",
                        "thread_id": str(user_context.thread_id),
                        "message": f"I'm a {profile['role']} at a {profile['company_size']} company with {profile['budget']} cloud budget. My priorities are {profile['priority']} and I prefer {profile['preferences']}. Our tech stack: {profile['tech_stack']}.",
                        "role": "user",
                        "metadata": {
                            "context_type": "profile_establishment",
                            "user_profile": profile["role"]
                        }
                    }
                    
                    await websocket.send(json.dumps(context_establishment))
                    await asyncio.wait_for(websocket.recv(), timeout=10.0)
                    
                    # Request personalized agent analysis
                    agent_request = {
                        "type": "agent_request",
                        "thread_id": str(user_context.thread_id),
                        "agent": "cost_optimizer",
                        "message": "Based on my company profile and preferences, provide specific cost optimization recommendations tailored to my situation.",
                        "metadata": {
                            "context_dependency": "high",
                            "personalization_required": True,
                            "expected_profile": profile["role"]
                        }
                    }
                    
                    await websocket.send(json.dumps(agent_request))
                    
                    # Collect agent response with context analysis
                    agent_events = []
                    agent_thinking_content = ""
                    agent_final_response = ""
                    
                    while len(agent_events) < 10:
                        try:
                            event_data = await asyncio.wait_for(websocket.recv(), timeout=25.0)
                            event = json.loads(event_data)
                            agent_events.append(event)
                            
                            if event.get("type") == "agent_thinking":
                                thinking = event.get("data", {}).get("content", "")
                                agent_thinking_content += thinking + " "
                            
                            elif event.get("type") == "agent_completed":
                                final_response = event.get("data", {}).get("response", {})
                                agent_final_response = str(final_response)
                                break
                                
                        except asyncio.TimeoutError:
                            break
                    
                    # Analyze response for context personalization
                    combined_response = (agent_thinking_content + " " + agent_final_response).lower()
                    
                    # Check for user-specific context recognition
                    context_recognition_score = 0
                    profile_keywords = [
                        profile["role"].replace("_", " "),
                        profile["company_size"].split()[0],  # e.g., "10" from "10 employees"
                        profile["budget"].split("$")[1].split("/")[0],  # e.g., "5000" from "$5,000/month"
                        profile["tech_stack"].split(",")[0].strip()  # First tech in stack
                    ]
                    
                    for keyword in profile_keywords:
                        if keyword in combined_response:
                            context_recognition_score += 1
                    
                    # Check for preference alignment
                    preference_alignment = 0
                    if profile["preferences"] == "cost-first" and ("cost" in combined_response or "budget" in combined_response):
                        preference_alignment += 1
                    elif profile["preferences"] == "security-first" and ("security" in combined_response or "compliance" in combined_response):
                        preference_alignment += 1
                    elif profile["preferences"] == "performance-first" and ("performance" in combined_response or "optimization" in combined_response):
                        preference_alignment += 1
                    
                    return {
                        "user_index": user_index,
                        "user_id": str(user_context.user_id),
                        "profile": profile,
                        "agent_thinking": agent_thinking_content,
                        "agent_response": agent_final_response,
                        "context_recognition_score": context_recognition_score,
                        "preference_alignment": preference_alignment,
                        "events_received": len(agent_events),
                        "success": True
                    }
                    
            except Exception as e:
                self.logger.error(f"User {user_index} agent context test failed: {e}")
                return {
                    "user_index": user_index,
                    "success": False,
                    "error": str(e)
                }
        
        # Execute concurrent agent interactions with different contexts
        self.logger.info(f"[U+1F680] Starting concurrent agent context isolation test")
        
        # Use first 3 users with distinct profiles
        context_tasks = [
            execute_agent_with_user_context(self.test_users[i], user_profiles[i])
            for i in range(min(3, len(self.test_users)))
        ]
        
        results = await asyncio.gather(*context_tasks, return_exceptions=True)
        
        # Filter successful results
        successful_results = [r for r in results if isinstance(r, dict) and r.get("success")]
        
        assert len(successful_results) >= 2, \
            f"Too many agent context tests failed: {len(successful_results)}/3"
        
        # CONTEXT ISOLATION VALIDATION
        
        # 1. Verify each agent response is personalized to the specific user
        for result in successful_results:
            # Context recognition validation
            assert result["context_recognition_score"] >= 1, \
                f"User {result['user_index']}: Agent failed to recognize user context (score: {result['context_recognition_score']}/4)"
            
            context_isolation_metrics["personalized_responses"] += 1
            
            # Preference alignment validation
            if result["preference_alignment"] > 0:
                context_isolation_metrics["recommendation_accuracy"] += 1
        
        # 2. Check for context bleeding between users
        for i, result1 in enumerate(successful_results):
            for j, result2 in enumerate(successful_results):
                if i != j:
                    user1_response = result1["agent_response"].lower()
                    user2_profile = result2["profile"]
                    
                    # Check if user1's response contains user2's specific details
                    user2_specific_elements = [
                        user2_profile["company_size"],
                        user2_profile["budget"],
                        user2_profile["role"].replace("_", " ")
                    ]
                    
                    for element in user2_specific_elements:
                        if element.lower() in user1_response:
                            context_isolation_metrics["context_bleeding_incidents"] += 1
                            self.logger.warning(f" WARNING: [U+FE0F] Context bleeding detected: User {result1['user_index']} response contains User {result2['user_index']} details")
        
        # 3. Validate response differentiation
        if len(successful_results) >= 2:
            # Responses should be significantly different for different user profiles
            response_texts = [r["agent_response"] for r in successful_results]
            
            # Simple differentiation check - responses shouldn't be identical
            for i in range(len(response_texts)):
                for j in range(i + 1, len(response_texts)):
                    similarity = len(set(response_texts[i].split()) & set(response_texts[j].split()))
                    total_words = len(set(response_texts[i].split()) | set(response_texts[j].split()))
                    
                    if total_words > 0:
                        similarity_ratio = similarity / total_words
                        assert similarity_ratio < 0.8, \
                            f"Agent responses too similar between users: {similarity_ratio:.1%} overlap"
        
        # BUSINESS VALUE VALIDATION
        
        # Zero tolerance for context bleeding
        assert context_isolation_metrics["context_bleeding_incidents"] == 0, \
            f"CRITICAL: Context bleeding detected - {context_isolation_metrics['context_bleeding_incidents']} incidents compromise personalization!"
        
        # Minimum personalization threshold
        personalization_rate = context_isolation_metrics["personalized_responses"] / len(successful_results)
        assert personalization_rate >= 0.8, \
            f"Insufficient personalization: {personalization_rate:.1%} - agent context isolation failed"
        
        # Recommendation accuracy threshold
        if context_isolation_metrics["recommendation_accuracy"] > 0:
            accuracy_rate = context_isolation_metrics["recommendation_accuracy"] / len(successful_results)
            assert accuracy_rate >= 0.5, \
                f"Low recommendation accuracy: {accuracy_rate:.1%} - preference alignment failed"
        
        self.logger.info(" CELEBRATION:  AGENT EXECUTION CONTEXT ISOLATION TEST SUCCESS")
        self.logger.info(f"[U+1F916] Agent interactions: {len(successful_results)}")
        self.logger.info(f" TARGET:  Personalized responses: {context_isolation_metrics['personalized_responses']}")
        self.logger.info(f"[U+1F512] Context bleeding incidents: {context_isolation_metrics['context_bleeding_incidents']} (MUST be 0)")
        self.logger.info(f"[U+1F4C8] Recommendation accuracy: {context_isolation_metrics['recommendation_accuracy']}")
        self.logger.info(f" IDEA:  Business Value: Agent context isolation ensures personalized AI recommendations")

    @pytest.mark.e2e
    @pytest.mark.staging
    @pytest.mark.performance
    async def test_thread_isolation_under_high_concurrency_stress(self):
        """
        Test thread isolation maintains integrity under high concurrency stress.
        
        Business Value: Validates platform scalability while maintaining security guarantees
        under realistic peak load conditions.
        """
        self.logger.info("[U+1F3CB][U+FE0F] Starting high concurrency thread isolation stress test")
        
        # Stress test parameters
        concurrent_operations_per_user = 5
        stress_duration = 30.0  # 30 second stress test
        
        stress_metrics = {
            "total_operations": 0,
            "successful_operations": 0,
            "isolation_violations": 0,
            "performance_degradation": 0,
            "concurrent_peak": 0
        }
        
        async def stress_test_user_operations(user_data: Dict[str, Any]) -> Dict[str, Any]:
            """Execute multiple concurrent operations for a single user."""
            user_index = user_data["user_index"]
            user_context = user_data["user_context"]
            websocket_headers = user_data["websocket_headers"]
            
            operations_completed = 0
            operations_failed = 0
            
            try:
                async with websockets.connect(
                    self.staging_config.urls.websocket_url,
                    additional_headers=websocket_headers,
                    open_timeout=25.0
                ) as websocket:
                    
                    # Execute rapid-fire operations
                    for op_index in range(concurrent_operations_per_user):
                        try:
                            # Mix of operation types to stress different isolation mechanisms
                            if op_index % 3 == 0:
                                # Thread creation
                                operation = {
                                    "type": "create_thread",
                                    "thread_id": f"{user_context.thread_id}_stress_{op_index}",
                                    "user_id": str(user_context.user_id),
                                    "metadata": {"stress_test": True, "operation": op_index}
                                }
                            elif op_index % 3 == 1:
                                # Message sending
                                operation = {
                                    "type": "send_message",
                                    "thread_id": str(user_context.thread_id),
                                    "message": f"Stress test message {op_index} from user {user_index} with sensitive data: {uuid.uuid4().hex}",
                                    "metadata": {"stress_operation": op_index, "user_isolation_test": True}
                                }
                            else:
                                # Thread status request
                                operation = {
                                    "type": "get_thread_status",
                                    "thread_id": str(user_context.thread_id),
                                    "metadata": {"stress_operation": op_index}
                                }
                            
                            await websocket.send(json.dumps(operation))
                            
                            # Brief response wait to prevent overwhelming
                            try:
                                response = await asyncio.wait_for(websocket.recv(), timeout=3.0)
                                operations_completed += 1
                                stress_metrics["total_operations"] += 1
                            except asyncio.TimeoutError:
                                operations_failed += 1
                            
                            # Minimal delay between operations
                            await asyncio.sleep(0.1)
                            
                        except Exception as e:
                            operations_failed += 1
                            self.logger.debug(f"User {user_index} operation {op_index} failed: {e}")
                    
                    return {
                        "user_index": user_index,
                        "operations_completed": operations_completed,
                        "operations_failed": operations_failed,
                        "success_rate": operations_completed / concurrent_operations_per_user,
                        "success": True
                    }
                    
            except Exception as e:
                self.logger.error(f"User {user_index} stress test failed: {e}")
                return {
                    "user_index": user_index,
                    "success": False,
                    "error": str(e)
                }
        
        # Execute stress test across all users
        self.logger.info(f"[U+1F680] Starting stress test: {len(self.test_users)} users  x  {concurrent_operations_per_user} operations")
        stress_start_time = time.time()
        
        stress_results = await asyncio.gather(*[
            stress_test_user_operations(user_data)
            for user_data in self.test_users
        ], return_exceptions=True)
        
        stress_duration_actual = time.time() - stress_start_time
        
        # Analyze stress test results
        successful_users = [r for r in stress_results if isinstance(r, dict) and r.get("success")]
        
        # Calculate performance metrics
        total_expected_operations = len(self.test_users) * concurrent_operations_per_user
        total_completed_operations = sum(r.get("operations_completed", 0) for r in successful_users)
        
        stress_metrics["successful_operations"] = total_completed_operations
        stress_metrics["concurrent_peak"] = len(self.test_users)
        
        # Performance validation
        completion_rate = total_completed_operations / total_expected_operations
        assert completion_rate >= 0.7, \
            f"Stress test completion rate too low: {completion_rate:.1%} - platform performance degraded"
        
        # User success rate validation
        user_success_rate = len(successful_users) / len(self.test_users)
        assert user_success_rate >= 0.8, \
            f"Too many users failed under stress: {user_success_rate:.1%}"
        
        # Performance degradation check
        operations_per_second = total_completed_operations / stress_duration_actual
        expected_min_ops_per_sec = len(self.test_users) * 2  # Minimum 2 ops/sec per user
        
        if operations_per_second < expected_min_ops_per_sec:
            stress_metrics["performance_degradation"] = 1
            self.logger.warning(f" WARNING: [U+FE0F] Performance degradation detected: {operations_per_second:.1f} ops/sec < {expected_min_ops_per_sec}")
        
        # Isolation validation under stress
        # Verify no cross-user data access during high load
        for user_result in successful_users:
            user_success_rate = user_result.get("success_rate", 0)
            assert user_success_rate >= 0.6, \
                f"User {user_result['user_index']} isolation compromised under stress: {user_success_rate:.1%} success rate"
        
        self.logger.info(" CELEBRATION:  HIGH CONCURRENCY STRESS TEST SUCCESS")
        self.logger.info(f"[U+1F465] Concurrent users: {len(self.test_users)}")
        self.logger.info(f" LIGHTNING:  Operations completed: {total_completed_operations}/{total_expected_operations}")
        self.logger.info(f" PASS:  Completion rate: {completion_rate:.1%}")
        self.logger.info(f"[U+23F1][U+FE0F] Duration: {stress_duration_actual:.2f}s")
        self.logger.info(f" CHART:  Operations/sec: {operations_per_second:.1f}")
        self.logger.info(f" IDEA:  Business Value: Platform maintains isolation and performance under peak concurrent load")

    @pytest.mark.e2e
    @pytest.mark.staging
    @pytest.mark.real_llm
    async def test_cross_user_contamination_prevention_and_detection(self):
        """
        Test comprehensive cross-user contamination prevention and detection.
        
        Business Value: Implements defense-in-depth strategy to detect and prevent
        any form of user data mixing, ensuring regulatory compliance and customer trust.
        """
        self.logger.info("[U+1F6E1][U+FE0F] Starting comprehensive cross-user contamination prevention test")
        
        # Create users with distinctly identifiable data patterns
        contamination_test_data = [
            {
                "marker": "UNIQUE_MARKER_ALPHA_123",
                "company": "AlphaTest Corp",
                "data_pattern": "alpha_pattern_data_2024",
                "api_key": f"alpha_key_{uuid.uuid4().hex[:16]}",
                "business_info": "Alpha Corp quarterly revenue $2.5M, employee count 150"
            },
            {
                "marker": "UNIQUE_MARKER_BETA_456", 
                "company": "BetaTest Industries",
                "data_pattern": "beta_pattern_data_2024",
                "api_key": f"beta_key_{uuid.uuid4().hex[:16]}",
                "business_info": "Beta Industries annual profit $50M, global offices in 12 countries"
            },
            {
                "marker": "UNIQUE_MARKER_GAMMA_789",
                "company": "GammaTest Solutions", 
                "data_pattern": "gamma_pattern_data_2024",
                "api_key": f"gamma_key_{uuid.uuid4().hex[:16]}",
                "business_info": "Gamma Solutions Series B funding $25M, AI/ML focus, 300 employees"
            }
        ]
        
        contamination_detection_results = []
        prevention_metrics = {
            "contamination_attempts_blocked": 0,
            "data_isolation_verified": 0,
            "detection_accuracy": 0,
            "false_positives": 0
        }
        
        async def inject_trackable_data_and_monitor(user_data: Dict[str, Any], test_data: Dict[str, str]) -> Dict[str, Any]:
            """Inject highly trackable data and monitor for contamination."""
            user_index = user_data["user_index"]
            user_context = user_data["user_context"]
            websocket_headers = user_data["websocket_headers"]
            
            injected_data_log = []
            agent_responses = []
            
            try:
                async with websockets.connect(
                    self.staging_config.urls.websocket_url,
                    additional_headers=websocket_headers,
                    open_timeout=20.0
                ) as websocket:
                    
                    # Inject highly identifiable data into user's thread
                    identifiable_messages = [
                        f"CONTAMINATION_TEST: {test_data['marker']} - Company: {test_data['company']}",
                        f"TRACKING_DATA: {test_data['data_pattern']} - API: {test_data['api_key']}",
                        f"BUSINESS_CONTEXT: {test_data['business_info']}",
                        f"UNIQUE_IDENTIFIER: {test_data['marker']} should never appear in other user sessions"
                    ]
                    
                    for message_content in identifiable_messages:
                        message = {
                            "type": "send_message",
                            "thread_id": str(user_context.thread_id),
                            "message": message_content,
                            "metadata": {
                                "contamination_test": True,
                                "unique_marker": test_data["marker"],
                                "data_sensitivity": "high"
                            }
                        }
                        
                        await websocket.send(json.dumps(message))
                        response = await asyncio.wait_for(websocket.recv(), timeout=10.0)
                        injected_data_log.append({
                            "message": message_content,
                            "response": json.loads(response),
                            "timestamp": datetime.now(timezone.utc).isoformat()
                        })
                        
                        await asyncio.sleep(0.5)
                    
                    # Request agent analysis to see if data is properly isolated
                    agent_analysis = {
                        "type": "agent_request",
                        "thread_id": str(user_context.thread_id),
                        "agent": "business_advisor",
                        "message": f"Provide analysis based on our conversation about {test_data['company']} and the data I shared.",
                        "metadata": {
                            "contamination_test": True,
                            "expected_context": test_data["marker"]
                        }
                    }
                    
                    await websocket.send(json.dumps(agent_analysis))
                    
                    # Collect agent response with contamination monitoring
                    agent_events = []
                    full_agent_response = ""
                    
                    while len(agent_events) < 8:
                        try:
                            event_data = await asyncio.wait_for(websocket.recv(), timeout=25.0)
                            event = json.loads(event_data)
                            agent_events.append(event)
                            
                            # Accumulate agent response content
                            if event.get("type") == "agent_thinking":
                                full_agent_response += event.get("data", {}).get("content", "") + " "
                            elif event.get("type") == "agent_completed":
                                full_agent_response += str(event.get("data", {}).get("response", "")) + " "
                                break
                                
                        except asyncio.TimeoutError:
                            break
                    
                    agent_responses.append(full_agent_response)
                    
                    return {
                        "user_index": user_index,
                        "user_id": str(user_context.user_id),
                        "test_data": test_data,
                        "injected_data": injected_data_log,
                        "agent_response": full_agent_response,
                        "agent_events_count": len(agent_events),
                        "success": True
                    }
                    
            except Exception as e:
                self.logger.error(f"User {user_index} contamination test failed: {e}")
                return {
                    "user_index": user_index,
                    "success": False,
                    "error": str(e)
                }
        
        # Execute data injection across first 3 users
        self.logger.info(f"[U+1F680] Injecting trackable data across {min(3, len(self.test_users))} users")
        
        injection_tasks = [
            inject_trackable_data_and_monitor(self.test_users[i], contamination_test_data[i])
            for i in range(min(3, len(self.test_users)))
        ]
        
        injection_results = await asyncio.gather(*injection_tasks, return_exceptions=True)
        
        # Filter successful injections
        successful_injections = [r for r in injection_results if isinstance(r, dict) and r.get("success")]
        
        assert len(successful_injections) >= 2, \
            f"Too few successful data injections: {len(successful_injections)}/3"
        
        # CONTAMINATION DETECTION ANALYSIS
        
        contamination_violations = []
        
        # Cross-check all agent responses for contamination from other users
        for i, result1 in enumerate(successful_injections):
            user1_response = result1["agent_response"].lower()
            user1_marker = result1["test_data"]["marker"]
            
            for j, result2 in enumerate(successful_injections):
                if i != j:
                    user2_data = result2["test_data"]
                    
                    # Check if user1's response contains user2's unique identifiers
                    contamination_indicators = [
                        user2_data["marker"],
                        user2_data["company"],
                        user2_data["data_pattern"],
                        user2_data["api_key"]
                    ]
                    
                    for indicator in contamination_indicators:
                        if indicator.lower() in user1_response:
                            contamination_violations.append({
                                "violation_type": "cross_user_data_contamination",
                                "source_user": result2["user_id"],
                                "contaminated_user": result1["user_id"],
                                "contaminated_element": indicator,
                                "detection_confidence": "high",
                                "violation_severity": "critical"
                            })
        
        # Verify data isolation - each user should only see their own data
        for result in successful_injections:
            user_response = result["agent_response"].lower()
            expected_marker = result["test_data"]["marker"].lower()
            
            # User should see their own data
            if expected_marker in user_response:
                prevention_metrics["data_isolation_verified"] += 1
            else:
                self.logger.warning(f" WARNING: [U+FE0F] User {result['user_index']} agent response missing their own data marker")
        
        # Additional contamination prevention test: Direct cross-user access attempts
        if len(successful_injections) >= 2:
            user1 = self.test_users[0]
            user2 = self.test_users[1]
            
            try:
                async with websockets.connect(
                    self.staging_config.urls.websocket_url,
                    additional_headers=user1["websocket_headers"],
                    open_timeout=15.0
                ) as websocket:
                    
                    # Attempt to request analysis that references another user's data
                    cross_reference_attempt = {
                        "type": "agent_request",
                        "thread_id": str(user1["user_context"].thread_id),
                        "agent": "business_advisor",
                        "message": f"Compare my situation with {contamination_test_data[1]['company']} and their {contamination_test_data[1]['data_pattern']}",
                        "metadata": {"contamination_attempt": True}
                    }
                    
                    await websocket.send(json.dumps(cross_reference_attempt))
                    
                    # Agent should not have access to user2's data
                    cross_ref_events = []
                    cross_ref_response = ""
                    
                    while len(cross_ref_events) < 5:
                        try:
                            event_data = await asyncio.wait_for(websocket.recv(), timeout=15.0)
                            event = json.loads(event_data)
                            cross_ref_events.append(event)
                            
                            if event.get("type") == "agent_completed":
                                cross_ref_response = str(event.get("data", {}).get("response", "")).lower()
                                break
                                
                        except asyncio.TimeoutError:
                            break
                    
                    # Check if user2's specific data leaked into user1's response
                    user2_specific_data = [
                        contamination_test_data[1]["company"].lower(),
                        contamination_test_data[1]["data_pattern"].lower()
                    ]
                    
                    for specific_data in user2_specific_data:
                        if specific_data in cross_ref_response:
                            contamination_violations.append({
                                "violation_type": "cross_reference_data_leak",
                                "source_data": specific_data,
                                "accessed_by": str(user1["user_context"].user_id),
                                "violation_severity": "critical"
                            })
                        else:
                            prevention_metrics["contamination_attempts_blocked"] += 1
                            
            except Exception as e:
                self.logger.info(f" PASS:  Cross-reference attempt correctly blocked: {e}")
                prevention_metrics["contamination_attempts_blocked"] += 1
        
        # Calculate detection accuracy
        total_contamination_checks = len(successful_injections) * (len(successful_injections) - 1)
        if total_contamination_checks > 0:
            prevention_metrics["detection_accuracy"] = (total_contamination_checks - len(contamination_violations)) / total_contamination_checks
        
        # CRITICAL BUSINESS VALUE VALIDATION
        
        # Zero tolerance for contamination violations
        if contamination_violations:
            self.logger.error(f" FAIL:  CRITICAL CONTAMINATION VIOLATIONS: {contamination_violations}")
            raise AssertionError(f"CRITICAL CONTAMINATION FAILURE: {len(contamination_violations)} contamination violations detected - user data compromised!")
        
        # Data isolation verification
        isolation_rate = prevention_metrics["data_isolation_verified"] / len(successful_injections)
        assert isolation_rate >= 0.8, \
            f"Insufficient data isolation: {isolation_rate:.1%} - users not seeing their own data properly"
        
        # Contamination prevention effectiveness
        assert prevention_metrics["contamination_attempts_blocked"] >= 1, \
            "No contamination attempts blocked - prevention mechanisms not working"
        
        # Detection accuracy validation
        assert prevention_metrics["detection_accuracy"] >= 0.95, \
            f"Detection accuracy too low: {prevention_metrics['detection_accuracy']:.1%} - contamination detection system insufficient"
        
        self.logger.info(" CELEBRATION:  CROSS-USER CONTAMINATION PREVENTION TEST SUCCESS")
        self.logger.info(f"[U+1F52C] Data injections tested: {len(successful_injections)}")
        self.logger.info(f"[U+1F512] Contamination violations: {len(contamination_violations)} (MUST be 0)")
        self.logger.info(f" PASS:  Data isolation verified: {prevention_metrics['data_isolation_verified']}/{len(successful_injections)}")
        self.logger.info(f"[U+1F6E1][U+FE0F] Contamination attempts blocked: {prevention_metrics['contamination_attempts_blocked']}")
        self.logger.info(f" CHART:  Detection accuracy: {prevention_metrics['detection_accuracy']:.1%}")
        self.logger.info(f" IDEA:  Business Value: Comprehensive contamination prevention protects all user data with 100% isolation")
