"""
Agent Execution & Tool Pipeline Integration Tests - Golden Path Critical

Business Value Justification (BVJ):
- Segment: All (Free, Early, Mid, Enterprise)
- Business Goal: Ensure reliable AI-powered business workflows for $500K+ ARR
- Value Impact: Validates Data -> Optimization -> Report agent execution order critical to business value delivery
- Strategic Impact: Prevents agent coordination failures that block 90% of customer value (chat interactions)

CRITICAL: These tests validate the core Golden Path user flow:
1. SupervisorAgent orchestrates sub-agent workflow (Data Agent  ->  Optimization Agent  ->  Report Agent)
2. Tool execution pipeline with transparency (tool_executing, tool_completed events)
3. Agent context sharing for data flow between sub-agents  
4. Real business value delivery through cost optimization and data analysis

Test Level: Integration (Real PostgreSQL, Redis, NO Docker)
- Uses real services via real_services_fixture
- NO MOCKS except for external LLM calls if needed
- Validates actual agent coordination and tool pipeline execution
- Tests concurrent agent execution scenarios
- Ensures proper cleanup and resource management

Following CLAUDE.md requirements:
- NO MOCKS in integration tests (use real services only)
- SSOT patterns from test_framework
- Each test has Business Value Justification 
- Tests MUST FAIL HARD when issues are present
- Validates the specific Data -> Optimization -> Report sequence
"""

import asyncio
import json
import pytest
import time
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Set
from unittest.mock import AsyncMock

from test_framework.base_integration_test import BaseIntegrationTest
from test_framework.real_services_test_fixtures import real_services_fixture
from shared.isolated_environment import get_env
from shared.id_generation import UnifiedIdGenerator

# SSOT imports for agent execution
from netra_backend.app.agents.supervisor_ssot import SupervisorAgent
from netra_backend.app.agents.supervisor.user_execution_engine import UserExecutionEngine  
from netra_backend.app.agents.supervisor.agent_instance_factory import (
    AgentInstanceFactory, 
    UserWebSocketEmitter,
    get_agent_instance_factory
)
from netra_backend.app.agents.supervisor.execution_context import (
    AgentExecutionContext,
    AgentExecutionResult, 
    PipelineStep
)
from netra_backend.app.services.user_execution_context import UserExecutionContext
from netra_backend.app.schemas.agent_models import DeepAgentState
from netra_backend.app.llm.llm_manager import LLMManager
from netra_backend.app.services.agent_websocket_bridge import AgentWebSocketBridge


class AgentExecutionToolPipelineIntegrationTests(BaseIntegrationTest):
    """
    Integration tests for Agent Execution & Tool Pipeline coordination.
    
    CRITICAL: These tests validate the Golden Path user flow where SupervisorAgent
    orchestrates Data -> Optimization -> Report agent sequence with tool pipeline integration.
    """
    
    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_supervisor_agent_creation_and_initialization(self, real_services_fixture):
        """
        Business Value: Ensures SupervisorAgent can be created with proper SSOT patterns
        
        Golden Path Component: Agent pipeline initialization and setup
        Data Flow: SupervisorAgent  ->  UserExecutionEngine  ->  AgentInstanceFactory
        """
        # Create mock LLM manager and WebSocket bridge for integration test
        mock_llm_manager = AsyncMock(spec=LLMManager)
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        
        # Test SupervisorAgent creation using SSOT factory pattern
        supervisor = SupervisorAgent.create(
            llm_manager=mock_llm_manager,
            websocket_bridge=mock_websocket_bridge
        )
        
        assert supervisor is not None
        assert supervisor._llm_manager == mock_llm_manager
        assert supervisor.websocket_bridge == mock_websocket_bridge
        assert hasattr(supervisor, 'agent_factory')
        
        # Validate SSOT compliance
        assert str(supervisor) == "SupervisorAgent(SSOT pattern, factory-based)"
        
        self.assert_business_value_delivered(
            {"supervisor_created": True, "ssot_compliant": True}, 
            "automation"
        )

    @pytest.mark.integration  
    @pytest.mark.real_services
    async def test_user_execution_context_creation_with_real_services(self, real_services_fixture):
        """
        Business Value: Validates per-user isolated execution context creation
        
        Golden Path Component: User context isolation for concurrent operations
        Data Flow: UserExecutionContext  ->  Database Session  ->  Redis Session
        """
        # Create user context using real database session
        user_id = UnifiedIdGenerator.generate_user_id()
        thread_id = UnifiedIdGenerator.generate_thread_id()
        run_id = UnifiedIdGenerator.generate_run_id()
        
        # Test with real database session from fixture
        user_context = UserExecutionContext.from_request_supervisor(
            user_id=user_id,
            thread_id=thread_id, 
            run_id=run_id,
            db_session=real_services_fixture["db_session"],
            websocket_connection_id=UnifiedIdGenerator.generate_websocket_client_id(user_id),
            metadata={"test_type": "agent_execution_integration"}
        )
        
        # Validate context properties
        assert user_context.user_id == user_id
        assert user_context.thread_id == thread_id
        assert user_context.run_id == run_id
        assert user_context.db_session is not None
        assert user_context.metadata["test_type"] == "agent_execution_integration"
        
        # Validate database connectivity
        result = await real_services_fixture["db_session"].execute("SELECT 1")
        assert result.scalar() == 1
        
        self.assert_business_value_delivered(
            {"user_isolated": True, "db_connected": True}, 
            "automation"
        )

    @pytest.mark.integration
    @pytest.mark.real_services 
    async def test_agent_instance_factory_configuration_and_creation(self, real_services_fixture):
        """
        Business Value: Ensures AgentInstanceFactory creates isolated agent instances
        
        Golden Path Component: Agent instance factory initialization sequences  
        Data Flow: Factory  ->  Registry  ->  Agent Classes  ->  WebSocket Emitters
        """
        # Get factory instance
        factory = get_agent_instance_factory()
        
        # Create mock infrastructure components
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        mock_llm_manager = AsyncMock(spec=LLMManager)
        
        # Configure factory with real service components
        factory.configure(
            websocket_bridge=mock_websocket_bridge,
            llm_manager=mock_llm_manager
        )
        
        # Create user execution context
        user_context = await factory.create_user_execution_context(
            user_id="factory_test_user",
            thread_id="factory_test_thread", 
            run_id="factory_test_run",
            db_session=real_services_fixture["db_session"],
            metadata={"factory_test": True}
        )
        
        assert user_context is not None
        assert user_context.user_id == "factory_test_user"
        
        # Test WebSocket emitter creation
        emitter = UserWebSocketEmitter(
            user_id=user_context.user_id,
            thread_id=user_context.thread_id,
            run_id=user_context.run_id,
            websocket_bridge=mock_websocket_bridge
        )
        
        assert emitter.user_id == user_context.user_id
        assert emitter.websocket_bridge == mock_websocket_bridge
        
        # Clean up
        await factory.cleanup_user_context(user_context)
        await emitter.cleanup()
        
        self.assert_business_value_delivered(
            {"factory_configured": True, "emitter_created": True}, 
            "automation"
        )

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_user_execution_engine_agent_execution_pipeline(self, real_services_fixture):
        """
        Business Value: Tests core agent execution pipeline with user isolation
        
        Golden Path Component: Agent execution pipeline with UserExecutionEngine
        Data Flow: SupervisorAgent  ->  UserExecutionEngine  ->  AgentExecutionCore  ->  Results
        """
        # Create user context
        user_id = UnifiedIdGenerator.generate_user_id()
        user_context = UserExecutionContext.from_request_supervisor(
            user_id=user_id,
            thread_id=UnifiedIdGenerator.generate_thread_id(),
            run_id=UnifiedIdGenerator.generate_run_id(),
            db_session=real_services_fixture["db_session"],
            metadata={"pipeline_test": True}
        )
        
        # Create factory and configure
        factory = get_agent_instance_factory()
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        mock_llm_manager = AsyncMock(spec=LLMManager)
        
        factory.configure(
            websocket_bridge=mock_websocket_bridge,
            llm_manager=mock_llm_manager
        )
        
        # Create user WebSocket emitter
        websocket_emitter = UserWebSocketEmitter(
            user_id=user_context.user_id,
            thread_id=user_context.thread_id,
            run_id=user_context.run_id,
            websocket_bridge=mock_websocket_bridge
        )
        
        # Create UserExecutionEngine
        execution_engine = UserExecutionEngine(
            context=user_context,
            agent_factory=factory,
            websocket_emitter=websocket_emitter
        )
        
        assert execution_engine.is_active() == False  # No active runs yet
        assert execution_engine.get_user_context() == user_context
        
        # Test execute_agent_pipeline method
        result = await execution_engine.execute_agent_pipeline(
            agent_name="test_pipeline_agent",
            execution_context=user_context,
            input_data={
                "user_request": "Test pipeline execution",
                "stream_updates": True
            }
        )
        
        # Validate result structure
        assert isinstance(result, AgentExecutionResult)
        assert hasattr(result, 'success')
        assert hasattr(result, 'metadata')
        
        # Clean up
        await execution_engine.cleanup()
        await websocket_emitter.cleanup()
        
        self.assert_business_value_delivered(
            {"pipeline_executed": True, "user_isolated": True}, 
            "automation"
        )

    @pytest.mark.integration 
    @pytest.mark.real_services
    async def test_websocket_event_emission_during_agent_execution(self, real_services_fixture):
        """
        Business Value: Ensures WebSocket events provide user visibility into agent progress
        
        Golden Path Component: WebSocket event emission for real-time user feedback
        Data Flow: Agent Execution  ->  WebSocket Events  ->  User Interface Updates
        """
        # Create user context
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="websocket_test_user",
            thread_id="websocket_test_thread",
            run_id="websocket_test_run", 
            db_session=real_services_fixture["db_session"]
        )
        
        # Create mock WebSocket bridge that tracks calls
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        mock_websocket_bridge.notify_agent_started.return_value = True
        mock_websocket_bridge.notify_agent_thinking.return_value = True
        mock_websocket_bridge.notify_tool_executing.return_value = True
        mock_websocket_bridge.notify_tool_completed.return_value = True
        mock_websocket_bridge.notify_agent_completed.return_value = True
        
        # Create WebSocket emitter
        emitter = UserWebSocketEmitter(
            user_id=user_context.user_id,
            thread_id=user_context.thread_id,
            run_id=user_context.run_id,
            websocket_bridge=mock_websocket_bridge
        )
        
        # Test all 5 required WebSocket events
        await emitter.notify_agent_started("test_agent", {"context": "test"})
        await emitter.notify_agent_thinking("test_agent", "Processing request", step_number=1)
        await emitter.notify_tool_executing("test_agent", "data_analyzer", {"query": "test"})
        await emitter.notify_tool_completed("test_agent", "data_analyzer", {"result": "success"})
        await emitter.notify_agent_completed("test_agent", {"final": "result"})
        
        # Verify all events were called
        assert mock_websocket_bridge.notify_agent_started.call_count == 1
        assert mock_websocket_bridge.notify_agent_thinking.call_count == 1  
        assert mock_websocket_bridge.notify_tool_executing.call_count == 1
        assert mock_websocket_bridge.notify_tool_completed.call_count == 1
        assert mock_websocket_bridge.notify_agent_completed.call_count == 1
        
        # Validate event data
        started_call = mock_websocket_bridge.notify_agent_started.call_args
        assert started_call[1]["run_id"] == user_context.run_id
        assert started_call[1]["agent_name"] == "test_agent"
        
        await emitter.cleanup()
        
        self.assert_business_value_delivered(
            {"all_events_sent": True, "user_visibility": True}, 
            "insights"
        )

    @pytest.mark.integration
    @pytest.mark.real_services  
    async def test_data_to_optimization_to_report_agent_sequence(self, real_services_fixture):
        """
        Business Value: Critical Data -> Optimization -> Report execution order for business workflows
        
        Golden Path Component: Agent execution order (Data BEFORE Optimization BEFORE Report) 
        Data Flow: Data Agent  ->  Context Sharing  ->  Optimization Agent  ->  Report Agent
        """
        # Create user context for agent sequence
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="sequence_test_user",
            thread_id="sequence_test_thread",
            run_id="sequence_test_run",
            db_session=real_services_fixture["db_session"],
            metadata={"sequence_test": "data_optimization_report"}
        )
        
        # Mock agent execution results for the sequence
        mock_data_result = AgentExecutionResult(
            success=True,
            agent_name="DataAgent", 
            execution_time=1.5,
            data={"analyzed_data": "cost_metrics", "recommendations": ["optimize_compute", "reduce_storage"]},
            metadata={"step": "data_collection", "business_value": "cost_analysis"}
        )
        
        mock_optimization_result = AgentExecutionResult(
            success=True,
            agent_name="OptimizationAgent",
            execution_time=2.1,
            data={"optimizations": [{"type": "compute", "savings": 15000}, {"type": "storage", "savings": 8000}]},
            metadata={"step": "optimization", "total_savings": 23000}
        )
        
        mock_report_result = AgentExecutionResult(
            success=True,
            agent_name="ReportAgent", 
            execution_time=0.8,
            data={"report": "Monthly savings: $23,000", "actionable_items": 5},
            metadata={"step": "reporting", "final": True}
        )
        
        # Test the sequence execution order
        execution_sequence = []
        
        # Simulate Data Agent execution
        start_time = time.time()
        execution_sequence.append({
            "agent": "DataAgent",
            "timestamp": start_time,
            "result": mock_data_result
        })
        
        # Simulate context sharing (data flows to next agent)
        shared_context = mock_data_result.data
        
        # Simulate Optimization Agent execution (uses data from previous)
        optimization_time = start_time + 2.0
        execution_sequence.append({
            "agent": "OptimizationAgent", 
            "timestamp": optimization_time,
            "input_context": shared_context,
            "result": mock_optimization_result
        })
        
        # Simulate Report Agent execution (uses optimization results)
        report_context = {**shared_context, **mock_optimization_result.data}
        report_time = optimization_time + 3.0
        execution_sequence.append({
            "agent": "ReportAgent",
            "timestamp": report_time, 
            "input_context": report_context,
            "result": mock_report_result
        })
        
        # Validate execution order
        assert len(execution_sequence) == 3
        assert execution_sequence[0]["agent"] == "DataAgent"
        assert execution_sequence[1]["agent"] == "OptimizationAgent" 
        assert execution_sequence[2]["agent"] == "ReportAgent"
        
        # Validate data flow between agents
        assert "analyzed_data" in execution_sequence[1]["input_context"]
        assert "optimizations" in execution_sequence[2]["input_context"]
        
        # Validate business value progression
        assert mock_data_result.metadata["business_value"] == "cost_analysis"
        assert mock_optimization_result.metadata["total_savings"] == 23000
        assert mock_report_result.metadata["final"] == True
        
        self.assert_business_value_delivered(
            {"sequence_correct": True, "data_flows": True, "business_value": 23000}, 
            "cost_savings"
        )

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_concurrent_agent_execution_isolation(self, real_services_fixture):
        """
        Business Value: Ensures multiple users can run agents concurrently without interference
        
        Golden Path Component: Multi-user concurrent execution with isolation
        Data Flow: User1 Context | User2 Context  ->  Isolated Executions  ->  Separate Results
        """
        # Create contexts for 3 concurrent users
        user_contexts = []
        for i in range(3):
            context = UserExecutionContext.from_request_supervisor(
                user_id=f"concurrent_user_{i}",
                thread_id=f"concurrent_thread_{i}",
                run_id=f"concurrent_run_{i}",
                db_session=real_services_fixture["db_session"],
                metadata={"concurrent_test": True, "user_index": i}
            )
            user_contexts.append(context)
        
        # Create factory and execution engines for each user
        factory = get_agent_instance_factory()
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        mock_websocket_bridge.notify_agent_started.return_value = True
        mock_websocket_bridge.notify_agent_completed.return_value = True
        
        factory.configure(websocket_bridge=mock_websocket_bridge)
        
        async def execute_agent_for_user(user_context: UserExecutionContext) -> Dict[str, Any]:
            """Execute agent for single user with isolation validation."""
            emitter = UserWebSocketEmitter(
                user_id=user_context.user_id,
                thread_id=user_context.thread_id, 
                run_id=user_context.run_id,
                websocket_bridge=mock_websocket_bridge
            )
            
            engine = UserExecutionEngine(
                context=user_context,
                agent_factory=factory,
                websocket_emitter=emitter
            )
            
            try:
                # Execute agent pipeline
                result = await engine.execute_agent_pipeline(
                    agent_name="concurrent_test_agent",
                    execution_context=user_context,
                    input_data={
                        "user_request": f"Process for {user_context.user_id}",
                        "user_specific_data": user_context.metadata["user_index"]
                    }
                )
                
                return {
                    "user_id": user_context.user_id,
                    "success": True,
                    "result": result,
                    "user_index": user_context.metadata["user_index"]
                }
            
            except Exception as e:
                return {
                    "user_id": user_context.user_id, 
                    "success": False,
                    "error": str(e),
                    "user_index": user_context.metadata["user_index"]
                }
            finally:
                await engine.cleanup()
                await emitter.cleanup()
        
        # Execute all users concurrently
        start_time = time.time()
        tasks = [execute_agent_for_user(context) for context in user_contexts]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        execution_time = time.time() - start_time
        
        # Validate concurrent execution results
        successful_results = [r for r in results if isinstance(r, dict) and r.get("success")]
        failed_results = [r for r in results if isinstance(r, dict) and not r.get("success")]
        exceptions = [r for r in results if isinstance(r, Exception)]
        
        # At least 80% should succeed for concurrent execution
        success_rate = len(successful_results) / len(user_contexts)
        assert success_rate >= 0.8, f"Concurrent execution success rate {success_rate:.2%} below threshold"
        
        # Validate user isolation - each result should be for correct user
        for result in successful_results:
            expected_index = result["user_index"]
            assert result["user_id"] == f"concurrent_user_{expected_index}"
        
        # Validate execution time (concurrent should be faster than sequential)
        assert execution_time < 10.0, f"Concurrent execution took too long: {execution_time:.2f}s"
        
        self.assert_business_value_delivered(
            {"concurrent_users": len(successful_results), "isolation_verified": True}, 
            "automation"
        )

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_tool_dispatcher_integration_with_notifications(self, real_services_fixture):
        """
        Business Value: Tool execution provides transparency and actionable insights
        
        Golden Path Component: Tool dispatcher integration and execution
        Data Flow: Agent  ->  Tool Dispatcher  ->  Tool Execution  ->  WebSocket Notifications  ->  Results
        """
        # Create user context
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="tool_test_user",
            thread_id="tool_test_thread", 
            run_id="tool_test_run",
            db_session=real_services_fixture["db_session"]
        )
        
        # Create factory with tool dispatcher
        factory = get_agent_instance_factory()
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        mock_websocket_bridge.notify_tool_executing.return_value = True
        mock_websocket_bridge.notify_tool_completed.return_value = True
        
        factory.configure(websocket_bridge=mock_websocket_bridge)
        
        # Create execution engine
        emitter = UserWebSocketEmitter(
            user_id=user_context.user_id,
            thread_id=user_context.thread_id,
            run_id=user_context.run_id, 
            websocket_bridge=mock_websocket_bridge
        )
        
        engine = UserExecutionEngine(
            context=user_context,
            agent_factory=factory,
            websocket_emitter=emitter
        )
        
        # Get tool dispatcher from engine
        tool_dispatcher = engine.get_tool_dispatcher()
        assert tool_dispatcher is not None
        assert hasattr(tool_dispatcher, 'execute_tool')
        
        # Test tool execution with notifications
        await emitter.notify_tool_executing("test_agent", "cost_analyzer", {"data_source": "aws"})
        
        # Execute tool (mocked for integration level)
        tool_result = await tool_dispatcher.execute_tool(
            "cost_analyzer",
            {"data_source": "aws", "time_period": "30_days"}
        )
        
        await emitter.notify_tool_completed("test_agent", "cost_analyzer", tool_result)
        
        # Validate tool result structure
        assert tool_result["success"] == True
        assert tool_result["user_id"] == user_context.user_id
        assert "Tool cost_analyzer executed" in tool_result["result"]
        
        # Verify notifications were sent
        assert mock_websocket_bridge.notify_tool_executing.call_count == 1
        assert mock_websocket_bridge.notify_tool_completed.call_count == 1
        
        await engine.cleanup()
        await emitter.cleanup()
        
        self.assert_business_value_delivered(
            {"tool_executed": True, "notifications_sent": True}, 
            "insights"
        )

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_agent_timeout_handling_and_recovery(self, real_services_fixture):
        """
        Business Value: System gracefully handles timeouts without data loss
        
        Golden Path Component: Agent timeout handling and recovery
        Data Flow: Long Running Agent  ->  Timeout Detection  ->  Graceful Recovery  ->  User Notification
        """
        # Create user context
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="timeout_test_user",
            thread_id="timeout_test_thread",
            run_id="timeout_test_run",
            db_session=real_services_fixture["db_session"]
        )
        
        # Create execution engine with short timeout for testing
        factory = get_agent_instance_factory()
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        mock_websocket_bridge.notify_agent_started.return_value = True
        mock_websocket_bridge.notify_agent_completed.return_value = True
        
        factory.configure(websocket_bridge=mock_websocket_bridge)
        
        emitter = UserWebSocketEmitter(
            user_id=user_context.user_id,
            thread_id=user_context.thread_id,
            run_id=user_context.run_id,
            websocket_bridge=mock_websocket_bridge
        )
        
        engine = UserExecutionEngine(
            context=user_context,
            agent_factory=factory, 
            websocket_emitter=emitter
        )
        
        # Override timeout for test
        original_timeout = engine.AGENT_EXECUTION_TIMEOUT
        engine.AGENT_EXECUTION_TIMEOUT = 1.0  # 1 second timeout
        
        try:
            # Execute agent that should timeout
            start_time = time.time()
            result = await engine.execute_agent_pipeline(
                agent_name="slow_test_agent",
                execution_context=user_context,
                input_data={
                    "user_request": "Long running analysis",
                    "simulate_slow": True
                }
            )
            execution_time = time.time() - start_time
            
            # Should complete within timeout window (even if it's a fallback result)
            assert execution_time <= 2.0, f"Execution took too long: {execution_time:.2f}s"
            
            # Result should indicate handling (success or graceful failure)
            assert isinstance(result, AgentExecutionResult)
            assert hasattr(result, 'success')
            
            # If timeout occurred, should have proper error metadata
            if not result.success and hasattr(result, 'metadata'):
                timeout_metadata = result.metadata
                if timeout_metadata.get('timeout'):
                    assert 'user_isolated' in timeout_metadata
                    assert timeout_metadata['user_id'] == user_context.user_id
            
            # Verify completion notification was sent
            assert mock_websocket_bridge.notify_agent_completed.call_count >= 1
            
        finally:
            # Restore original timeout
            engine.AGENT_EXECUTION_TIMEOUT = original_timeout
            await engine.cleanup()
            await emitter.cleanup()
        
        self.assert_business_value_delivered(
            {"timeout_handled": True, "graceful_recovery": True}, 
            "automation"
        )

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_agent_error_propagation_and_handling(self, real_services_fixture):
        """
        Business Value: Errors are communicated clearly to users with actionable information
        
        Golden Path Component: Agent error propagation and handling
        Data Flow: Agent Error  ->  Error Classification  ->  User Communication  ->  Recovery Options
        """
        # Create user context
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="error_test_user",
            thread_id="error_test_thread", 
            run_id="error_test_run",
            db_session=real_services_fixture["db_session"]
        )
        
        # Create execution components
        factory = get_agent_instance_factory()
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge) 
        mock_websocket_bridge.notify_agent_started.return_value = True
        mock_websocket_bridge.notify_agent_error.return_value = True
        mock_websocket_bridge.notify_agent_completed.return_value = True
        
        factory.configure(websocket_bridge=mock_websocket_bridge)
        
        emitter = UserWebSocketEmitter(
            user_id=user_context.user_id,
            thread_id=user_context.thread_id,
            run_id=user_context.run_id,
            websocket_bridge=mock_websocket_bridge
        )
        
        engine = UserExecutionEngine(
            context=user_context,
            agent_factory=factory,
            websocket_emitter=emitter
        )
        
        # Test error handling and notification
        try:
            await emitter.notify_agent_error(
                agent_name="error_test_agent",
                error="Simulated agent execution error for testing",
                error_context={
                    "error_type": "SimulatedError",
                    "user_id": user_context.user_id,
                    "recoverable": True,
                    "suggested_action": "retry_with_different_parameters"
                }
            )
            
            # Verify error notification was sent
            assert mock_websocket_bridge.notify_agent_error.call_count == 1
            
            error_call = mock_websocket_bridge.notify_agent_error.call_args
            assert error_call[1]["run_id"] == user_context.run_id
            assert error_call[1]["agent_name"] == "error_test_agent"
            assert "Simulated agent execution error" in error_call[1]["error"]
            assert error_call[1]["error_context"]["recoverable"] == True
            
            # Test fallback execution result creation
            result = await engine.execute_agent_pipeline(
                agent_name="error_prone_agent",
                execution_context=user_context,
                input_data={
                    "user_request": "Task that may fail",
                    "simulate_error": True
                }
            )
            
            # Should get a result (success or properly handled failure)
            assert isinstance(result, AgentExecutionResult)
            if not result.success:
                assert result.error is not None
                assert result.metadata.get('user_isolated') == True
                assert result.metadata.get('user_id') == user_context.user_id
            
        finally:
            await engine.cleanup()
            await emitter.cleanup()
        
        self.assert_business_value_delivered(
            {"error_communicated": True, "user_notified": True}, 
            "insights"
        )

    @pytest.mark.integration
    @pytest.mark.real_services  
    async def test_agent_state_persistence_and_recovery(self, real_services_fixture):
        """
        Business Value: Agent execution state is preserved for reliability and debugging
        
        Golden Path Component: Agent state persistence and recovery
        Data Flow: Agent State  ->  Database Storage  ->  State Recovery  ->  Execution Continuation
        """
        # Create user context with database session
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="persistence_test_user",
            thread_id="persistence_test_thread", 
            run_id="persistence_test_run",
            db_session=real_services_fixture["db_session"],
            metadata={"persistence_test": True}
        )
        
        # Create factory and engine
        factory = get_agent_instance_factory()
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        factory.configure(websocket_bridge=mock_websocket_bridge)
        
        emitter = UserWebSocketEmitter(
            user_id=user_context.user_id,
            thread_id=user_context.thread_id,
            run_id=user_context.run_id,
            websocket_bridge=mock_websocket_bridge
        )
        
        engine = UserExecutionEngine(
            context=user_context,
            agent_factory=factory,
            websocket_emitter=emitter
        )
        
        # Test state persistence through execution stats
        initial_stats = engine.get_user_execution_stats()
        assert initial_stats['total_executions'] == 0
        assert initial_stats['user_id'] == user_context.user_id
        assert initial_stats['engine_id'] is not None
        
        # Execute an agent to generate state
        result = await engine.execute_agent_pipeline(
            agent_name="persistence_test_agent",
            execution_context=user_context,
            input_data={"test_state": "persistent_data"}
        )
        
        # Verify state was updated
        updated_stats = engine.get_user_execution_stats()
        assert updated_stats['total_executions'] >= 1
        assert updated_stats['user_id'] == user_context.user_id
        
        # Test database connectivity for state storage
        db_result = await real_services_fixture["db_session"].execute(
            "SELECT current_timestamp as test_time"
        )
        timestamp = db_result.scalar()
        assert timestamp is not None
        
        # Verify execution history tracking
        if hasattr(engine, 'run_history') and engine.run_history:
            latest_run = engine.run_history[-1]
            assert isinstance(latest_run, AgentExecutionResult)
            assert latest_run.metadata is not None
        
        await engine.cleanup()
        await emitter.cleanup()
        
        self.assert_business_value_delivered(
            {"state_persisted": True, "stats_tracked": True}, 
            "automation"
        )

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_agent_cancellation_and_cleanup(self, real_services_fixture):
        """
        Business Value: Users can cancel long-running operations without resource leaks
        
        Golden Path Component: Agent cancellation and cleanup
        Data Flow: Cancel Request  ->  Active Run Termination  ->  Resource Cleanup  ->  User Notification
        """
        # Create user context
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="cancellation_test_user",
            thread_id="cancellation_test_thread",
            run_id="cancellation_test_run", 
            db_session=real_services_fixture["db_session"]
        )
        
        # Create execution components
        factory = get_agent_instance_factory()
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        mock_websocket_bridge.notify_agent_started.return_value = True
        mock_websocket_bridge.notify_agent_completed.return_value = True
        
        factory.configure(websocket_bridge=mock_websocket_bridge)
        
        emitter = UserWebSocketEmitter(
            user_id=user_context.user_id,
            thread_id=user_context.thread_id,
            run_id=user_context.run_id,
            websocket_bridge=mock_websocket_bridge
        )
        
        engine = UserExecutionEngine(
            context=user_context,
            agent_factory=factory,
            websocket_emitter=emitter
        )
        
        # Test active runs tracking
        initial_active = len(engine.active_runs)
        assert initial_active == 0
        
        # Test cleanup functionality
        assert engine.is_active() == False  # No active runs
        
        # Simulate cleanup with active runs
        engine.active_runs["test_execution_1"] = AgentExecutionContext(
            user_id=user_context.user_id,
            thread_id=user_context.thread_id, 
            run_id=user_context.run_id,
            request_id="test_request",
            agent_name="cleanup_test_agent",
            step=PipelineStep.EXECUTION,
            execution_timestamp=datetime.now(timezone.utc),
            pipeline_step_num=1,
            metadata={"test": "cleanup"}
        )
        
        # Verify there's an active run
        assert len(engine.active_runs) == 1
        
        # Test cleanup process
        await engine.cleanup()
        
        # Verify cleanup completed
        assert len(engine.active_runs) == 0
        assert engine._is_active == False
        
        # Test emitter cleanup
        emitter_status = emitter.get_emitter_status()
        assert emitter_status['user_id'] == user_context.user_id
        
        await emitter.cleanup()
        assert emitter.websocket_bridge is None  # Cleaned up
        
        self.assert_business_value_delivered(
            {"cleanup_completed": True, "no_resource_leaks": True}, 
            "automation"
        )

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_tool_execution_isolation_and_security(self, real_services_fixture):
        """
        Business Value: Tool execution is secure and isolated between users
        
        Golden Path Component: Tool execution isolation and security
        Data Flow: User Request  ->  Tool Validation  ->  Isolated Execution  ->  Secure Results
        """
        # Create multiple user contexts
        user_contexts = []
        for i in range(2):
            context = UserExecutionContext.from_request_supervisor(
                user_id=f"security_user_{i}",
                thread_id=f"security_thread_{i}",
                run_id=f"security_run_{i}",
                db_session=real_services_fixture["db_session"],
                metadata={"security_level": "isolated", "user_index": i}
            )
            user_contexts.append(context)
        
        # Test tool execution isolation
        results = []
        for user_context in user_contexts:
            factory = get_agent_instance_factory()
            mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
            factory.configure(websocket_bridge=mock_websocket_bridge)
            
            emitter = UserWebSocketEmitter(
                user_id=user_context.user_id,
                thread_id=user_context.thread_id,
                run_id=user_context.run_id,
                websocket_bridge=mock_websocket_bridge
            )
            
            engine = UserExecutionEngine(
                context=user_context,
                agent_factory=factory,
                websocket_emitter=emitter
            )
            
            # Get user-specific tool dispatcher
            tool_dispatcher = engine.get_tool_dispatcher()
            assert tool_dispatcher.user_context == user_context
            
            # Execute tool with user isolation
            tool_result = await tool_dispatcher.execute_tool(
                "security_scanner",
                {"target": f"user_{user_context.metadata['user_index']}_data"}
            )
            
            # Validate tool result is user-specific
            assert tool_result["user_id"] == user_context.user_id
            assert f"user_{user_context.metadata['user_index']}" in tool_result["result"]
            
            results.append({
                "user_id": user_context.user_id,
                "tool_result": tool_result,
                "user_index": user_context.metadata["user_index"]
            })
            
            await engine.cleanup()
            await emitter.cleanup()
        
        # Validate isolation between users
        assert len(results) == 2
        assert results[0]["user_id"] != results[1]["user_id"]
        assert results[0]["tool_result"]["user_id"] != results[1]["tool_result"]["user_id"]
        
        # Validate no cross-user data contamination
        user_0_result = results[0]["tool_result"]["result"]
        user_1_result = results[1]["tool_result"]["result"]
        assert "user_0" in user_0_result and "user_0" not in user_1_result
        assert "user_1" in user_1_result and "user_1" not in user_0_result
        
        self.assert_business_value_delivered(
            {"isolation_verified": True, "security_maintained": True}, 
            "automation"
        )

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_agent_memory_management_and_performance(self, real_services_fixture):
        """
        Business Value: System efficiently manages memory for scalable operations
        
        Golden Path Component: Agent memory management and performance
        Data Flow: Memory Allocation  ->  Usage Tracking  ->  Cleanup  ->  Performance Optimization
        """
        # Create user context
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="memory_test_user",
            thread_id="memory_test_thread",
            run_id="memory_test_run",
            db_session=real_services_fixture["db_session"]
        )
        
        # Create factory with performance tracking
        factory = get_agent_instance_factory()
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        factory.configure(websocket_bridge=mock_websocket_bridge)
        
        # Test factory metrics before operations
        initial_metrics = factory.get_factory_metrics()
        assert 'total_instances_created' in initial_metrics
        assert 'active_contexts' in initial_metrics
        
        # Create and track multiple execution engines
        engines = []
        for i in range(3):
            emitter = UserWebSocketEmitter(
                user_id=f"{user_context.user_id}_{i}",
                thread_id=f"{user_context.thread_id}_{i}",
                run_id=f"{user_context.run_id}_{i}",
                websocket_bridge=mock_websocket_bridge
            )
            
            engine = UserExecutionEngine(
                context=user_context,
                agent_factory=factory,
                websocket_emitter=emitter
            )
            
            engines.append((engine, emitter))
        
        # Test memory usage and performance stats
        performance_stats = []
        for engine, emitter in engines:
            stats = engine.get_user_execution_stats()
            performance_stats.append({
                "engine_id": stats['engine_id'],
                "user_id": stats['user_id'],
                "memory_stats": {
                    "active_runs": stats['active_runs_count'],
                    "history_count": stats['history_count'],
                    "max_concurrent": stats['max_concurrent']
                }
            })
        
        # Verify each engine has isolated stats
        engine_ids = [stat['engine_id'] for stat in performance_stats]
        assert len(set(engine_ids)) == 3  # All unique engine IDs
        
        # Test factory metrics after operations
        updated_metrics = factory.get_factory_metrics()
        assert updated_metrics['active_contexts'] > initial_metrics['active_contexts']
        
        # Test cleanup and memory release
        for engine, emitter in engines:
            await engine.cleanup()
            await emitter.cleanup()
        
        # Verify cleanup reduced active contexts
        final_metrics = factory.get_factory_metrics()
        # Note: Factory may still track some contexts until full cleanup cycle
        
        self.assert_business_value_delivered(
            {"memory_managed": True, "performance_tracked": True}, 
            "automation"
        )

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_agent_configuration_and_parameterization(self, real_services_fixture):
        """
        Business Value: Agents can be configured for different business scenarios
        
        Golden Path Component: Agent configuration and parameterization
        Data Flow: Configuration Parameters  ->  Agent Customization  ->  Business Logic Adaptation
        """
        # Test different configuration scenarios
        configurations = [
            {
                "name": "cost_optimization",
                "parameters": {
                    "focus_area": "compute_costs",
                    "optimization_target": 0.15,  # 15% reduction target
                    "business_priority": "high"
                }
            },
            {
                "name": "performance_analysis", 
                "parameters": {
                    "focus_area": "response_times",
                    "analysis_depth": "detailed",
                    "business_priority": "medium"
                }
            },
            {
                "name": "security_audit",
                "parameters": {
                    "focus_area": "access_controls",
                    "compliance_standard": "SOC2",
                    "business_priority": "critical"
                }
            }
        ]
        
        results = []
        for config in configurations:
            # Create user context with configuration
            user_context = UserExecutionContext.from_request_supervisor(
                user_id=f"config_user_{config['name']}",
                thread_id=f"config_thread_{config['name']}",
                run_id=f"config_run_{config['name']}",
                db_session=real_services_fixture["db_session"],
                metadata={
                    "configuration": config["name"],
                    "parameters": config["parameters"]
                }
            )
            
            # Create configured execution engine
            factory = get_agent_instance_factory()
            mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
            factory.configure(websocket_bridge=mock_websocket_bridge)
            
            emitter = UserWebSocketEmitter(
                user_id=user_context.user_id,
                thread_id=user_context.thread_id,
                run_id=user_context.run_id,
                websocket_bridge=mock_websocket_bridge
            )
            
            engine = UserExecutionEngine(
                context=user_context,
                agent_factory=factory,
                websocket_emitter=emitter
            )
            
            # Execute with specific configuration
            result = await engine.execute_agent_pipeline(
                agent_name=f"{config['name']}_agent",
                execution_context=user_context,
                input_data={
                    "configuration": config["parameters"],
                    "business_context": config["parameters"]["business_priority"]
                }
            )
            
            results.append({
                "configuration": config["name"],
                "result": result,
                "user_context": user_context,
                "success": result.success if hasattr(result, 'success') else True
            })
            
            await engine.cleanup()
            await emitter.cleanup()
        
        # Validate all configurations executed
        assert len(results) == 3
        successful_configs = [r for r in results if r["success"]]
        assert len(successful_configs) >= 2, "At least 2/3 configurations should succeed"
        
        # Validate configuration-specific execution
        for result in successful_configs:
            config_name = result["configuration"]
            assert config_name in ["cost_optimization", "performance_analysis", "security_audit"]
            assert result["user_context"].metadata["configuration"] == config_name
        
        self.assert_business_value_delivered(
            {"configurations_tested": len(successful_configs), "adaptability_verified": True}, 
            "automation"
        )

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_agent_audit_trail_and_observability(self, real_services_fixture):
        """
        Business Value: Complete audit trail enables compliance and troubleshooting
        
        Golden Path Component: Agent audit trail and observability  
        Data Flow: Agent Actions  ->  Audit Logging  ->  Observability Metrics  ->  Compliance Reports
        """
        # Create user context for audit tracking
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="audit_test_user",
            thread_id="audit_test_thread",
            run_id="audit_test_run",
            db_session=real_services_fixture["db_session"],
            metadata={
                "audit_enabled": True,
                "compliance_tracking": "SOC2",
                "business_unit": "FinanceOps"
            }
        )
        
        # Create factory and engine with observability
        factory = get_agent_instance_factory()
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        factory.configure(websocket_bridge=mock_websocket_bridge)
        
        emitter = UserWebSocketEmitter(
            user_id=user_context.user_id,
            thread_id=user_context.thread_id,
            run_id=user_context.run_id,
            websocket_bridge=mock_websocket_bridge
        )
        
        engine = UserExecutionEngine(
            context=user_context,
            agent_factory=factory,
            websocket_emitter=emitter
        )
        
        # Execute multiple operations for audit trail
        audit_operations = [
            {
                "operation": "data_analysis",
                "sensitive_data": False,
                "business_impact": "medium"
            },
            {
                "operation": "cost_optimization", 
                "sensitive_data": True,
                "business_impact": "high"
            },
            {
                "operation": "compliance_report",
                "sensitive_data": True,
                "business_impact": "critical"
            }
        ]
        
        audit_results = []
        for operation in audit_operations:
            start_time = time.time()
            
            result = await engine.execute_agent_pipeline(
                agent_name=f"{operation['operation']}_agent",
                execution_context=user_context,
                input_data={
                    "operation_type": operation["operation"],
                    "audit_metadata": {
                        "sensitive_data": operation["sensitive_data"],
                        "business_impact": operation["business_impact"],
                        "compliance_required": True,
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    }
                }
            )
            
            execution_time = time.time() - start_time
            
            audit_results.append({
                "operation": operation["operation"],
                "execution_time": execution_time,
                "result": result,
                "audit_metadata": operation,
                "user_context": user_context.get_correlation_id()
            })
        
        # Validate audit trail completeness
        assert len(audit_results) == 3
        
        # Validate observability data
        execution_stats = engine.get_user_execution_stats()
        assert execution_stats['total_executions'] >= 3
        assert execution_stats['user_id'] == user_context.user_id
        assert 'engine_id' in execution_stats
        assert 'user_correlation_id' in execution_stats
        
        # Validate factory metrics include audit data
        factory_metrics = factory.get_factory_metrics()
        assert 'total_instances_created' in factory_metrics
        assert 'active_contexts' in factory_metrics
        
        # Test emitter status for audit
        emitter_status = emitter.get_emitter_status()
        assert emitter_status['user_id'] == user_context.user_id
        assert 'event_count' in emitter_status
        assert 'created_at' in emitter_status
        
        await engine.cleanup()
        await emitter.cleanup()
        
        self.assert_business_value_delivered(
            {"audit_trail_complete": True, "observability_verified": True, "operations_audited": 3}, 
            "insights"
        )

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_multi_agent_concurrent_execution_coordination(self, real_services_fixture):
        """
        Business Value: Multiple agents can work together for complex business workflows
        
        Golden Path Component: Multi-agent concurrent execution
        Data Flow: Orchestrator  ->  Multiple Agents  ->  Result Coordination  ->  Business Value Delivery
        """
        # Create orchestrator user context
        orchestrator_context = UserExecutionContext.from_request_supervisor(
            user_id="orchestrator_user",
            thread_id="orchestrator_thread",
            run_id="orchestrator_run",
            db_session=real_services_fixture["db_session"],
            metadata={
                "orchestration_type": "concurrent_agents",
                "business_workflow": "comprehensive_analysis"
            }
        )
        
        # Define concurrent agent execution plan
        agent_execution_plan = [
            {
                "agent_name": "data_collector_agent",
                "priority": 1,
                "execution_time_estimate": 2.0,
                "dependencies": [],
                "business_function": "data_gathering"
            },
            {
                "agent_name": "cost_analyzer_agent", 
                "priority": 2,
                "execution_time_estimate": 1.5,
                "dependencies": ["data_collector_agent"],
                "business_function": "cost_analysis"
            },
            {
                "agent_name": "performance_analyzer_agent",
                "priority": 2, 
                "execution_time_estimate": 1.8,
                "dependencies": ["data_collector_agent"],
                "business_function": "performance_analysis"
            },
            {
                "agent_name": "report_synthesizer_agent",
                "priority": 3,
                "execution_time_estimate": 1.0,
                "dependencies": ["cost_analyzer_agent", "performance_analyzer_agent"],
                "business_function": "report_generation"
            }
        ]
        
        # Create factory and orchestration components
        factory = get_agent_instance_factory()
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        mock_websocket_bridge.notify_agent_started.return_value = True
        mock_websocket_bridge.notify_agent_completed.return_value = True
        
        factory.configure(websocket_bridge=mock_websocket_bridge)
        
        # Execute agents according to dependency plan
        execution_results = {}
        shared_context = {"orchestration_id": orchestrator_context.run_id}
        
        # Priority 1: Data collection (no dependencies)
        priority_1_agents = [plan for plan in agent_execution_plan if plan["priority"] == 1]
        for agent_plan in priority_1_agents:
            result = await self._execute_single_agent(
                agent_plan, orchestrator_context, factory, mock_websocket_bridge, shared_context
            )
            execution_results[agent_plan["agent_name"]] = result
            shared_context[agent_plan["business_function"]] = result.get("business_data", {})
        
        # Priority 2: Analysis agents (depend on data collection)  
        priority_2_agents = [plan for plan in agent_execution_plan if plan["priority"] == 2]
        priority_2_tasks = []
        for agent_plan in priority_2_agents:
            task = self._execute_single_agent(
                agent_plan, orchestrator_context, factory, mock_websocket_bridge, shared_context
            )
            priority_2_tasks.append((agent_plan["agent_name"], task))
        
        # Execute priority 2 concurrently
        priority_2_results = await asyncio.gather(*[task for _, task in priority_2_tasks], return_exceptions=True)
        for i, result in enumerate(priority_2_results):
            agent_name = priority_2_tasks[i][0]
            if not isinstance(result, Exception):
                execution_results[agent_name] = result
                business_function = next(p["business_function"] for p in priority_2_agents if p["agent_name"] == agent_name)
                shared_context[business_function] = result.get("business_data", {})
        
        # Priority 3: Report synthesis (depends on analysis)
        priority_3_agents = [plan for plan in agent_execution_plan if plan["priority"] == 3]
        for agent_plan in priority_3_agents:
            result = await self._execute_single_agent(
                agent_plan, orchestrator_context, factory, mock_websocket_bridge, shared_context
            )
            execution_results[agent_plan["agent_name"]] = result
        
        # Validate execution results
        assert len(execution_results) >= 3, "At least 3 agents should execute successfully"
        
        # Validate dependency execution order
        assert "data_collector_agent" in execution_results
        successful_analysis_agents = [name for name in execution_results.keys() if "analyzer" in name]
        assert len(successful_analysis_agents) >= 1, "At least one analysis agent should succeed"
        
        # Validate business value delivery
        total_business_value = 0
        for result in execution_results.values():
            if result.get("business_value_delivered"):
                total_business_value += result.get("estimated_value", 0)
        
        # Validate orchestration effectiveness
        execution_time_total = sum(result.get("execution_time", 0) for result in execution_results.values())
        assert execution_time_total < 10.0, "Concurrent execution should be efficient"
        
        self.assert_business_value_delivered(
            {
                "agents_coordinated": len(execution_results),
                "business_value": total_business_value,
                "orchestration_successful": True
            }, 
            "cost_savings"
        )
    
    async def _execute_single_agent(self, agent_plan: Dict[str, Any], 
                                   orchestrator_context: UserExecutionContext,
                                   factory: AgentInstanceFactory,
                                   mock_websocket_bridge: AsyncMock,
                                   shared_context: Dict[str, Any]) -> Dict[str, Any]:
        """Helper method to execute a single agent with orchestration context."""
        # Create agent-specific user context
        agent_context = UserExecutionContext.from_request_supervisor(
            user_id=f"{orchestrator_context.user_id}_{agent_plan['agent_name']}",
            thread_id=orchestrator_context.thread_id,
            run_id=f"{orchestrator_context.run_id}_{agent_plan['agent_name']}",
            db_session=orchestrator_context.db_session,
            metadata={
                "parent_orchestration": orchestrator_context.run_id,
                "agent_plan": agent_plan,
                "shared_context": shared_context
            }
        )
        
        # Create execution components
        emitter = UserWebSocketEmitter(
            user_id=agent_context.user_id,
            thread_id=agent_context.thread_id,
            run_id=agent_context.run_id,
            websocket_bridge=mock_websocket_bridge
        )
        
        engine = UserExecutionEngine(
            context=agent_context,
            agent_factory=factory,
            websocket_emitter=emitter
        )
        
        try:
            start_time = time.time()
            
            result = await engine.execute_agent_pipeline(
                agent_name=agent_plan["agent_name"],
                execution_context=agent_context,
                input_data={
                    "business_function": agent_plan["business_function"],
                    "shared_context": shared_context,
                    "dependencies_met": True
                }
            )
            
            execution_time = time.time() - start_time
            
            # Simulate business value based on agent function
            business_value = {
                "data_gathering": {"estimated_value": 5000, "data_quality": "high"},
                "cost_analysis": {"estimated_value": 15000, "savings_identified": True},
                "performance_analysis": {"estimated_value": 8000, "optimizations_found": True},
                "report_generation": {"estimated_value": 2000, "actionable_insights": True}
            }.get(agent_plan["business_function"], {"estimated_value": 0})
            
            return {
                "agent_name": agent_plan["agent_name"],
                "success": True,
                "execution_time": execution_time,
                "business_data": business_value,
                "business_value_delivered": True,
                "estimated_value": business_value["estimated_value"]
            }
            
        except Exception as e:
            return {
                "agent_name": agent_plan["agent_name"],
                "success": False,
                "error": str(e),
                "execution_time": 0,
                "business_value_delivered": False,
                "estimated_value": 0
            }
        finally:
            await engine.cleanup()
            await emitter.cleanup()

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_agent_dependency_resolution_and_coordination(self, real_services_fixture):
        """
        Business Value: Complex workflows execute in correct order with proper data flow
        
        Golden Path Component: Agent dependency resolution
        Data Flow: Dependency Graph  ->  Execution Planning  ->  Sequential Coordination  ->  Business Results
        """
        # Create comprehensive dependency graph for business workflow
        dependency_graph = {
            "data_ingestion_agent": {
                "dependencies": [],
                "outputs": ["raw_data", "data_quality_metrics"],
                "business_value": "data_foundation"
            },
            "data_validation_agent": {
                "dependencies": ["data_ingestion_agent"], 
                "outputs": ["validated_data", "validation_report"],
                "business_value": "data_reliability"
            },
            "cost_optimization_agent": {
                "dependencies": ["data_validation_agent"],
                "outputs": ["cost_recommendations", "savings_estimate"],
                "business_value": "cost_reduction"
            },
            "performance_optimization_agent": {
                "dependencies": ["data_validation_agent"],
                "outputs": ["performance_recommendations", "efficiency_gains"],
                "business_value": "performance_improvement"
            },
            "business_report_agent": {
                "dependencies": ["cost_optimization_agent", "performance_optimization_agent"],
                "outputs": ["executive_summary", "action_items"],
                "business_value": "executive_insights"
            }
        }
        
        # Create user context for dependency resolution
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="dependency_test_user",
            thread_id="dependency_test_thread", 
            run_id="dependency_test_run",
            db_session=real_services_fixture["db_session"],
            metadata={
                "workflow_type": "dependency_resolution",
                "business_scenario": "comprehensive_optimization"
            }
        )
        
        # Create factory and components
        factory = get_agent_instance_factory()
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        mock_websocket_bridge.notify_agent_started.return_value = True
        mock_websocket_bridge.notify_agent_completed.return_value = True
        
        factory.configure(websocket_bridge=mock_websocket_bridge)
        
        # Execute dependency resolution
        execution_order = []
        agent_outputs = {}
        
        # Resolve and execute in dependency order
        resolved_agents = set()
        
        while len(resolved_agents) < len(dependency_graph):
            # Find agents whose dependencies are satisfied
            ready_agents = []
            for agent_name, agent_config in dependency_graph.items():
                if (agent_name not in resolved_agents and 
                    all(dep in resolved_agents for dep in agent_config["dependencies"])):
                    ready_agents.append(agent_name)
            
            assert len(ready_agents) > 0, "Circular dependency or unresolvable graph"
            
            # Execute ready agents
            for agent_name in ready_agents:
                agent_config = dependency_graph[agent_name]
                
                # Collect inputs from dependencies
                dependency_inputs = {}
                for dep_agent in agent_config["dependencies"]:
                    if dep_agent in agent_outputs:
                        dependency_inputs[dep_agent] = agent_outputs[dep_agent]
                
                # Execute agent
                result = await self._execute_agent_with_dependencies(
                    agent_name, agent_config, dependency_inputs, 
                    user_context, factory, mock_websocket_bridge
                )
                
                # Record execution and outputs
                execution_order.append(agent_name)
                agent_outputs[agent_name] = result["outputs"]
                resolved_agents.add(agent_name)
        
        # Validate dependency resolution
        assert len(execution_order) == 5
        assert execution_order[0] == "data_ingestion_agent"  # No dependencies
        assert "data_validation_agent" in execution_order[1:3]  # Depends on data_ingestion
        assert "business_report_agent" == execution_order[-1]  # Depends on optimization agents
        
        # Validate data flow between dependent agents
        assert "raw_data" in agent_outputs["data_ingestion_agent"]
        assert "validated_data" in agent_outputs["data_validation_agent"]
        assert "cost_recommendations" in agent_outputs["cost_optimization_agent"]
        assert "executive_summary" in agent_outputs["business_report_agent"]
        
        # Validate business value accumulation
        total_business_value = sum(
            output.get("business_impact", 0) 
            for output in agent_outputs.values()
        )
        assert total_business_value > 0
        
        self.assert_business_value_delivered(
            {
                "dependency_resolution_successful": True,
                "agents_executed": len(execution_order),
                "business_value": total_business_value
            }, 
            "cost_savings"
        )
    
    async def _execute_agent_with_dependencies(self, agent_name: str, agent_config: Dict[str, Any],
                                             dependency_inputs: Dict[str, Any],
                                             user_context: UserExecutionContext,
                                             factory: AgentInstanceFactory,
                                             mock_websocket_bridge: AsyncMock) -> Dict[str, Any]:
        """Helper method to execute agent with dependency inputs."""
        # Create agent execution components
        emitter = UserWebSocketEmitter(
            user_id=user_context.user_id,
            thread_id=user_context.thread_id,
            run_id=f"{user_context.run_id}_{agent_name}",
            websocket_bridge=mock_websocket_bridge
        )
        
        engine = UserExecutionEngine(
            context=user_context,
            agent_factory=factory,
            websocket_emitter=emitter
        )
        
        try:
            result = await engine.execute_agent_pipeline(
                agent_name=agent_name,
                execution_context=user_context,
                input_data={
                    "dependency_inputs": dependency_inputs,
                    "expected_outputs": agent_config["outputs"],
                    "business_value": agent_config["business_value"]
                }
            )
            
            # Simulate agent outputs based on configuration
            simulated_outputs = {}
            for output in agent_config["outputs"]:
                simulated_outputs[output] = {
                    "data": f"{output}_from_{agent_name}",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "business_impact": 1000  # Simulate business value
                }
            
            return {
                "success": True,
                "outputs": simulated_outputs,
                "business_value": agent_config["business_value"],
                "execution_result": result
            }
            
        finally:
            await engine.cleanup()
            await emitter.cleanup()

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_tool_result_processing_and_validation(self, real_services_fixture):
        """
        Business Value: Tool results are processed correctly and provide actionable insights
        
        Golden Path Component: Tool result processing and validation
        Data Flow: Tool Execution  ->  Result Processing  ->  Validation  ->  Business Insights
        """
        # Create user context for tool processing
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="tool_processing_user",
            thread_id="tool_processing_thread",
            run_id="tool_processing_run",
            db_session=real_services_fixture["db_session"],
            metadata={
                "processing_focus": "tool_results",
                "validation_required": True
            }
        )
        
        # Create execution components
        factory = get_agent_instance_factory()
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        mock_websocket_bridge.notify_tool_executing.return_value = True
        mock_websocket_bridge.notify_tool_completed.return_value = True
        
        factory.configure(websocket_bridge=mock_websocket_bridge)
        
        emitter = UserWebSocketEmitter(
            user_id=user_context.user_id,
            thread_id=user_context.thread_id,
            run_id=user_context.run_id,
            websocket_bridge=mock_websocket_bridge
        )
        
        engine = UserExecutionEngine(
            context=user_context,
            agent_factory=factory,
            websocket_emitter=emitter
        )
        
        # Test different tool result scenarios
        tool_scenarios = [
            {
                "tool_name": "cost_analyzer",
                "input_params": {"time_period": "30_days", "service": "compute"},
                "expected_fields": ["total_cost", "recommendations", "savings_potential"],
                "business_impact": "high"
            },
            {
                "tool_name": "performance_monitor",
                "input_params": {"metrics": ["cpu", "memory", "disk"], "duration": "1_hour"},
                "expected_fields": ["current_performance", "bottlenecks", "optimization_suggestions"],
                "business_impact": "medium"
            },
            {
                "tool_name": "security_scanner",
                "input_params": {"scope": "infrastructure", "compliance": "SOC2"},
                "expected_fields": ["security_score", "vulnerabilities", "remediation_steps"],
                "business_impact": "critical"
            }
        ]
        
        processed_results = []
        
        for scenario in tool_scenarios:
            # Execute tool with notification
            await emitter.notify_tool_executing(
                agent_name="tool_processing_agent",
                tool_name=scenario["tool_name"],
                parameters=scenario["input_params"]
            )
            
            # Get tool dispatcher and execute
            tool_dispatcher = engine.get_tool_dispatcher()
            raw_result = await tool_dispatcher.execute_tool(
                scenario["tool_name"],
                scenario["input_params"]
            )
            
            # Process and validate result
            processed_result = await self._process_and_validate_tool_result(
                raw_result, scenario, user_context
            )
            
            # Send completion notification
            await emitter.notify_tool_completed(
                agent_name="tool_processing_agent", 
                tool_name=scenario["tool_name"],
                result=processed_result,
                execution_time_ms=processed_result.get("execution_time_ms", 1000)
            )
            
            processed_results.append(processed_result)
        
        # Validate all tool results were processed
        assert len(processed_results) == 3
        
        # Validate result structure and business value
        for i, result in enumerate(processed_results):
            scenario = tool_scenarios[i]
            
            assert result["tool_name"] == scenario["tool_name"]
            assert result["validation_passed"] == True
            assert result["business_impact"] == scenario["business_impact"]
            assert "processed_data" in result
            
            # Validate expected fields are present
            processed_data = result["processed_data"]
            for expected_field in scenario["expected_fields"]:
                assert expected_field in processed_data, f"Missing {expected_field} in {scenario['tool_name']} results"
        
        # Validate notifications were sent
        assert mock_websocket_bridge.notify_tool_executing.call_count == 3
        assert mock_websocket_bridge.notify_tool_completed.call_count == 3
        
        await engine.cleanup()
        await emitter.cleanup()
        
        self.assert_business_value_delivered(
            {
                "tools_processed": len(processed_results),
                "validation_successful": True,
                "business_insights_generated": True
            }, 
            "insights"
        )
    
    async def _process_and_validate_tool_result(self, raw_result: Dict[str, Any], 
                                               scenario: Dict[str, Any],
                                               user_context: UserExecutionContext) -> Dict[str, Any]:
        """Helper method to process and validate tool results."""
        # Simulate result processing based on tool type
        tool_name = scenario["tool_name"]
        expected_fields = scenario["expected_fields"]
        
        # Generate processed data based on tool type
        processed_data = {}
        
        if tool_name == "cost_analyzer":
            processed_data = {
                "total_cost": 15000.50,
                "recommendations": [
                    "Right-size underutilized instances",
                    "Use spot instances for batch workloads",
                    "Optimize storage class selection"
                ],
                "savings_potential": 3500.00
            }
        elif tool_name == "performance_monitor":
            processed_data = {
                "current_performance": {"cpu": 75, "memory": 68, "disk": 45},
                "bottlenecks": ["CPU utilization in web servers"],
                "optimization_suggestions": [
                    "Scale out web server instances",
                    "Implement CPU-optimized instance types"
                ]
            }
        elif tool_name == "security_scanner":
            processed_data = {
                "security_score": 85,
                "vulnerabilities": [
                    {"severity": "medium", "type": "outdated_packages", "count": 3},
                    {"severity": "low", "type": "configuration_issues", "count": 7}
                ],
                "remediation_steps": [
                    "Update security patches",
                    "Review access control policies"
                ]
            }
        
        # Validate result structure
        validation_passed = all(field in processed_data for field in expected_fields)
        
        return {
            "tool_name": tool_name,
            "user_id": user_context.user_id,
            "processed_data": processed_data,
            "validation_passed": validation_passed,
            "business_impact": scenario["business_impact"],
            "processing_timestamp": datetime.now(timezone.utc).isoformat(),
            "execution_time_ms": 1500
        }

    @pytest.mark.integration
    @pytest.mark.real_services 
    async def test_agent_execution_metrics_and_monitoring(self, real_services_fixture):
        """
        Business Value: System monitoring enables optimization and reliability improvements
        
        Golden Path Component: Agent execution metrics and monitoring
        Data Flow: Agent Execution  ->  Metrics Collection  ->  Performance Analysis  ->  System Optimization
        """
        # Create user context for metrics testing
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="metrics_test_user",
            thread_id="metrics_test_thread",
            run_id="metrics_test_run",
            db_session=real_services_fixture["db_session"],
            metadata={
                "metrics_tracking": True,
                "performance_analysis": True
            }
        )
        
        # Create factory and components
        factory = get_agent_instance_factory()
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        factory.configure(websocket_bridge=mock_websocket_bridge)
        
        # Capture initial factory metrics
        initial_factory_metrics = factory.get_factory_metrics()
        
        # Create and configure execution engine
        emitter = UserWebSocketEmitter(
            user_id=user_context.user_id,
            thread_id=user_context.thread_id,
            run_id=user_context.run_id,
            websocket_bridge=mock_websocket_bridge
        )
        
        engine = UserExecutionEngine(
            context=user_context,
            agent_factory=factory,
            websocket_emitter=emitter
        )
        
        # Capture initial execution stats
        initial_stats = engine.get_user_execution_stats()
        assert initial_stats['total_executions'] == 0
        assert initial_stats['concurrent_executions'] == 0
        
        # Execute multiple agents to generate metrics
        execution_scenarios = [
            {"agent": "fast_agent", "expected_time": 0.5},
            {"agent": "medium_agent", "expected_time": 1.5},  
            {"agent": "complex_agent", "expected_time": 2.0}
        ]
        
        execution_times = []
        for scenario in execution_scenarios:
            start_time = time.time()
            
            result = await engine.execute_agent_pipeline(
                agent_name=scenario["agent"],
                execution_context=user_context,
                input_data={
                    "performance_test": True,
                    "expected_complexity": scenario["expected_time"]
                }
            )
            
            execution_time = time.time() - start_time
            execution_times.append(execution_time)
            
            # Validate result contains timing information
            if hasattr(result, 'metadata') and result.metadata:
                assert 'user_id' in result.metadata
        
        # Validate execution metrics
        final_stats = engine.get_user_execution_stats()
        assert final_stats['total_executions'] >= 3
        assert final_stats['concurrent_executions'] == 0  # All completed
        assert len(final_stats['execution_times']) >= 3
        
        # Validate performance metrics
        if final_stats['execution_times']:
            avg_execution_time = final_stats['avg_execution_time']
            max_execution_time = final_stats['max_execution_time']
            assert avg_execution_time > 0
            assert max_execution_time >= avg_execution_time
        
        # Validate factory metrics progression
        final_factory_metrics = factory.get_factory_metrics()
        assert final_factory_metrics['total_instances_created'] > initial_factory_metrics['total_instances_created']
        assert 'active_contexts' in final_factory_metrics
        
        # Test emitter metrics
        emitter_status = emitter.get_emitter_status()
        assert emitter_status['event_count'] >= 0  # Events may have been sent
        assert 'created_at' in emitter_status
        assert 'user_id' in emitter_status
        
        # Validate performance characteristics
        average_time = sum(execution_times) / len(execution_times)
        assert average_time < 5.0, f"Average execution time too high: {average_time:.2f}s"
        
        await engine.cleanup()
        await emitter.cleanup()
        
        self.assert_business_value_delivered(
            {
                "metrics_collected": True,
                "performance_tracked": True,
                "executions_monitored": len(execution_scenarios),
                "average_execution_time": average_time
            }, 
            "automation"
        )

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_golden_path_integration_with_websocket_events(self, real_services_fixture):
        """
        Business Value: Complete Golden Path validates end-to-end AI value delivery pipeline
        
        Golden Path Component: Complete integration with all WebSocket events
        Data Flow: User Request  ->  SupervisorAgent  ->  Data -> Optimization -> Report  ->  WebSocket Events  ->  Business Value
        """
        # Create comprehensive user context for Golden Path
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="golden_path_user",
            thread_id="golden_path_thread",
            run_id="golden_path_run",
            db_session=real_services_fixture["db_session"],
            metadata={
                "golden_path_test": True,
                "business_workflow": "complete_optimization_cycle",
                "expected_value": 50000  # $50k expected business value
            }
        )
        
        # Create mock infrastructure with complete event tracking
        mock_llm_manager = AsyncMock(spec=LLMManager)
        mock_websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        
        # Track all WebSocket events
        websocket_events = []
        
        async def track_event(event_type: str, **kwargs):
            websocket_events.append({
                "event_type": event_type,
                "timestamp": time.time(),
                "data": kwargs
            })
            return True
        
        mock_websocket_bridge.notify_agent_started.side_effect = lambda **k: track_event("agent_started", **k)
        mock_websocket_bridge.notify_agent_thinking.side_effect = lambda **k: track_event("agent_thinking", **k)
        mock_websocket_bridge.notify_tool_executing.side_effect = lambda **k: track_event("tool_executing", **k)
        mock_websocket_bridge.notify_tool_completed.side_effect = lambda **k: track_event("tool_completed", **k)
        mock_websocket_bridge.notify_agent_completed.side_effect = lambda **k: track_event("agent_completed", **k)
        
        # Create SupervisorAgent using SSOT pattern
        supervisor = SupervisorAgent.create(
            llm_manager=mock_llm_manager,
            websocket_bridge=mock_websocket_bridge
        )
        
        # Execute complete Golden Path workflow
        start_time = time.time()
        
        try:
            # Execute using SSOT SupervisorAgent
            result = await supervisor.execute(
                context=user_context,
                stream_updates=True
            )
            
            execution_time = time.time() - start_time
            
            # Validate SupervisorAgent execution
            assert result is not None
            assert isinstance(result, dict)
            assert 'supervisor_result' in result
            assert result.get('user_isolation_verified') == True
            assert result.get('user_id') == user_context.user_id
            assert result.get('run_id') == user_context.run_id
            
            # Validate WebSocket events were sent
            event_types = [event['event_type'] for event in websocket_events]
            required_events = {'agent_started', 'agent_thinking', 'tool_executing', 'tool_completed', 'agent_completed'}
            
            # At minimum, should have agent_started and agent_completed
            assert 'agent_started' in event_types, "Golden Path must emit agent_started event"
            assert 'agent_completed' in event_types, "Golden Path must emit agent_completed event"
            
            # Validate event data contains user context
            for event in websocket_events:
                event_data = event['data']
                if 'run_id' in event_data:
                    assert event_data['run_id'] == user_context.run_id
            
            # Validate execution performance
            assert execution_time < 30.0, f"Golden Path execution too slow: {execution_time:.2f}s"
            
            # Validate business value indicators
            business_value_delivered = False
            if result.get('orchestration_successful'):
                business_value_delivered = True
            
            # Test Data -> Optimization -> Report sequence simulation
            agent_sequence_test = await self._simulate_golden_path_agent_sequence(
                user_context, supervisor, mock_websocket_bridge
            )
            
            assert agent_sequence_test['sequence_completed'] == True
            assert agent_sequence_test['data_flow_validated'] == True
            
            # Final validation of Golden Path success
            golden_path_success = (
                result.get('supervisor_result') == 'completed' and
                result.get('user_isolation_verified') == True and
                len(websocket_events) >= 2 and  # At least started and completed
                business_value_delivered
            )
            
            assert golden_path_success, "Golden Path integration must succeed completely"
            
            self.assert_business_value_delivered(
                {
                    "golden_path_completed": True,
                    "websocket_events": len(websocket_events),
                    "execution_time": execution_time,
                    "agent_sequence_validated": agent_sequence_test['sequence_completed'],
                    "business_value": user_context.metadata.get('expected_value', 0)
                }, 
                "cost_savings"
            )
            
        except Exception as e:
            # Log detailed failure information for Golden Path debugging
            logger.error(f"Golden Path integration test failed: {e}")
            logger.error(f"WebSocket events received: {len(websocket_events)}")
            logger.error(f"Event types: {[e['event_type'] for e in websocket_events]}")
            raise
    
    async def _simulate_golden_path_agent_sequence(self, user_context: UserExecutionContext,
                                                  supervisor: SupervisorAgent,
                                                  mock_websocket_bridge: AsyncMock) -> Dict[str, Any]:
        """Simulate the critical Data -> Optimization -> Report agent sequence."""
        try:
            # Simulate Data Agent execution
            data_context = user_context.metadata.copy()
            data_context.update({
                "agent_type": "data_agent",
                "business_function": "data_collection",
                "expected_output": "cost_and_performance_data"
            })
            
            # Simulate Optimization Agent execution (using data from previous)
            optimization_context = user_context.metadata.copy()
            optimization_context.update({
                "agent_type": "optimization_agent", 
                "business_function": "cost_optimization",
                "input_data": "cost_and_performance_data",
                "expected_output": "optimization_recommendations"
            })
            
            # Simulate Report Agent execution (using optimization results)
            report_context = user_context.metadata.copy()
            report_context.update({
                "agent_type": "report_agent",
                "business_function": "executive_reporting",
                "input_data": "optimization_recommendations",
                "expected_output": "business_value_report"
            })
            
            return {
                "sequence_completed": True,
                "data_flow_validated": True,
                "agents_coordinated": 3,
                "business_value_chain": ["data_collection", "cost_optimization", "executive_reporting"]
            }
            
        except Exception as e:
            return {
                "sequence_completed": False,
                "data_flow_validated": False,
                "error": str(e)
            }