# Golden Path Integration Test Fix Plan - Issue #267

**Business Justification:** Golden Path integration tests protect $500K+ ARR by validating the core user flow (login â†’ AI responses). Currently 10/19 tests failing (89% failure rate) due to agent orchestration initialization errors.

**Target:** Unit, Integration (non-Docker), and E2E staging tests - NO Docker dependencies for reliable CI/CD execution.

## Executive Summary

The Golden Path integration tests in `tests/integration/golden_path/test_agent_orchestration_execution_comprehensive.py` are failing due to interface misalignments between test expectations and SSOT implementations. Primary error: **"No agent registry configured - cannot create agent 'supervisor_orchestration'"**.

### Root Cause Analysis

1. **Agent Registry Configuration Failure** (Primary Issue)
   - `AgentInstanceFactory` not properly configured with agent registry
   - Missing call to `configure_agent_instance_factory()` during test setup
   - Empty agent class registry causing agent creation failures

2. **Interface Misalignment Issues** (Secondary)
   - WebSocket bridge mock expecting `send_event()` but implementation uses different methods
   - SupervisorAgent constructor parameter changes
   - ExecutionEngineFactory missing `create_for_user()` method
   - Missing execution engine methods

3. **Test Infrastructure Dependencies** (Tertiary)
   - Tests expecting Docker services but need to run without Docker
   - Missing compatibility layers for test isolation

## Test Plan Structure

### Phase 1: Unit Tests - Reproduce Core Issues (HIGH PRIORITY)

**Target:** Create failing unit tests that reproduce each specific issue with minimal dependencies

#### 1.1 Agent Registry Configuration Tests
**File:** `tests/unit/test_golden_path_agent_registry_issues.py`

```python
class TestAgentRegistryConfiguration(SSotAsyncTestCase):
    """Unit tests reproducing agent registry configuration failures."""
    
    async def test_agent_instance_factory_missing_registry_error(self):
        """FAILING TEST: Reproduce 'No agent registry configured' error."""
        # Should FAIL initially - this reproduces the exact error from Golden Path tests
        factory = AgentInstanceFactory()
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="test_user", thread_id="test_thread", run_id="test_run"
        )
        
        # This should fail with "No agent registry configured"
        with self.assertRaises(ValueError) as cm:
            await factory.create_agent_instance("supervisor_orchestration", user_context)
        
        self.assertIn("No agent registry configured", str(cm.exception))
    
    async def test_configure_agent_instance_factory_fixes_registry(self):
        """PASSING TEST: Verify factory configuration resolves registry issues."""
        factory = AgentInstanceFactory()
        
        # Create test agent class registry
        from netra_backend.app.agents.supervisor.agent_class_registry import AgentClassRegistry
        registry = AgentClassRegistry()
        
        # Register a test agent class
        from netra_backend.app.agents.base_agent import BaseAgent
        class TestSupervisorAgent(BaseAgent):
            async def execute(self, *args, **kwargs):
                return {"status": "success", "agent": "test_supervisor"}
        
        registry.register("supervisor_orchestration", TestSupervisorAgent, "Test supervisor")
        registry.freeze()
        
        # Configure factory with registry
        factory.configure(agent_class_registry=registry)
        
        # Now agent creation should work
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="test_user", thread_id="test_thread", run_id="test_run"
        )
        
        agent = await factory.create_agent_instance("supervisor_orchestration", user_context)
        self.assertIsNotNone(agent)
        self.assertIsInstance(agent, TestSupervisorAgent)
```

#### 1.2 WebSocket Bridge Interface Tests
**File:** `tests/unit/test_golden_path_websocket_interfaces.py`

```python
class TestWebSocketBridgeInterfaces(SSotAsyncTestCase):
    """Unit tests reproducing WebSocket bridge interface mismatches."""
    
    async def test_websocket_bridge_mock_interface_mismatch(self):
        """FAILING TEST: Reproduce WebSocket bridge mock attribute errors."""
        # This reproduces the "Mock object has no attribute 'send_event'" error
        from unittest.mock import AsyncMock, MagicMock
        from netra_backend.app.services.agent_websocket_bridge import AgentWebSocketBridge
        
        # Test expectations vs actual interface
        websocket_bridge = AsyncMock(spec=AgentWebSocketBridge)
        
        # Current test expectation (FAILS)
        with self.assertRaises(AttributeError):
            websocket_bridge.send_event.assert_called()
        
        # Need to verify actual AgentWebSocketBridge interface
        real_bridge = AgentWebSocketBridge()
        actual_methods = [method for method in dir(real_bridge) if not method.startswith('_')]
        
        # Log actual interface for test correction
        logger.info(f"AgentWebSocketBridge methods: {sorted(actual_methods)}")
    
    async def test_websocket_bridge_correct_interface(self):
        """PASSING TEST: Verify correct WebSocket bridge interface."""
        from netra_backend.app.services.agent_websocket_bridge import AgentWebSocketBridge
        
        bridge = AgentWebSocketBridge()
        
        # Verify actual methods exist (these should pass)
        self.assertTrue(hasattr(bridge, 'notify_agent_started'))
        self.assertTrue(hasattr(bridge, 'notify_agent_completed'))
        self.assertTrue(hasattr(bridge, 'notify_agent_thinking'))
        self.assertTrue(hasattr(bridge, 'notify_tool_executing'))
        self.assertTrue(hasattr(bridge, 'notify_tool_completed'))
```

#### 1.3 SupervisorAgent Constructor Tests
**File:** `tests/unit/test_golden_path_supervisor_constructor.py`

```python
class TestSupervisorAgentConstructor(SSotAsyncTestCase):
    """Unit tests reproducing SupervisorAgent constructor parameter issues."""
    
    async def test_supervisor_agent_constructor_parameter_mismatch(self):
        """FAILING TEST: Reproduce SupervisorAgent constructor TypeError."""
        from netra_backend.app.agents.supervisor_ssot import SupervisorAgent
        from unittest.mock import MagicMock
        
        mock_llm_manager = MagicMock()
        
        # Test current constructor signature
        try:
            supervisor = SupervisorAgent(llm_manager=mock_llm_manager)
            self.assertIsNotNone(supervisor)
        except TypeError as e:
            # Log the actual error for analysis
            logger.error(f"SupervisorAgent constructor TypeError: {e}")
            # This should help identify correct constructor parameters
            
    async def test_supervisor_agent_fallback_constructor(self):
        """PASSING TEST: Test fallback constructor approach."""
        from netra_backend.app.agents.supervisor_ssot import SupervisorAgent
        from unittest.mock import MagicMock
        
        mock_llm_manager = MagicMock()
        
        # Test no-parameter constructor (fallback)
        try:
            supervisor = SupervisorAgent()
            supervisor.llm_manager = mock_llm_manager  # Inject after creation
            self.assertIsNotNone(supervisor)
            self.assertEqual(supervisor.llm_manager, mock_llm_manager)
        except Exception as e:
            self.fail(f"Fallback constructor should work: {e}")
```

### Phase 2: Integration Tests - Validate Factory Patterns (MEDIUM PRIORITY)

**Target:** Test factory method integration and execution engine functionality

#### 2.1 ExecutionEngineFactory Tests
**File:** `tests/integration/test_golden_path_execution_engine_factory.py`

```python
class TestExecutionEngineFactoryIntegration(SSotAsyncTestCase):
    """Integration tests for ExecutionEngineFactory factory methods."""
    
    async def test_execution_engine_factory_create_for_user_method(self):
        """FAILING/PASSING TEST: Verify ExecutionEngineFactory.create_for_user() exists."""
        from netra_backend.app.agents.supervisor.execution_engine_factory import ExecutionEngineFactory
        from netra_backend.app.services.agent_websocket_bridge import AgentWebSocketBridge
        
        websocket_bridge = AgentWebSocketBridge()
        factory = ExecutionEngineFactory(websocket_bridge=websocket_bridge)
        
        # Verify method exists
        self.assertTrue(hasattr(factory, 'create_for_user'))
        
        # Test method call
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="test_user", thread_id="test_thread", run_id="test_run"
        )
        
        engine = await factory.create_for_user(user_context)
        self.assertIsNotNone(engine)
    
    async def test_user_execution_engine_required_methods(self):
        """FAILING TEST: Verify UserExecutionEngine has all required methods."""
        from netra_backend.app.agents.supervisor.execution_engine_factory import ExecutionEngineFactory
        from netra_backend.app.services.agent_websocket_bridge import AgentWebSocketBridge
        
        websocket_bridge = AgentWebSocketBridge()
        factory = ExecutionEngineFactory(websocket_bridge=websocket_bridge)
        
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="test_user", thread_id="test_thread", run_id="test_run"
        )
        
        engine = await factory.create_for_user(user_context)
        
        # Verify required methods exist
        required_methods = [
            'execute_agent_pipeline',
            'set_execution_state',
            'get_execution_state',
            'set_agent_state',
            'get_agent_state',
            'get_user_context',
            'is_active',
            'cleanup'
        ]
        
        for method_name in required_methods:
            self.assertTrue(hasattr(engine, method_name), 
                          f"UserExecutionEngine missing method: {method_name}")
```

#### 2.2 Agent Workflow Integration Tests
**File:** `tests/integration/test_golden_path_agent_workflow.py`

```python
class TestGoldenPathAgentWorkflow(SSotAsyncTestCase):
    """Integration tests for complete agent workflow without Docker."""
    
    async def async_setup_method(self, method):
        """Setup agent infrastructure for workflow tests."""
        await super().async_setup_method(method)
        
        # Configure agent factory for tests
        await self._configure_agent_factory_for_integration_tests()
    
    async def _configure_agent_factory_for_integration_tests(self):
        """Configure agent factory with minimal test dependencies."""
        from netra_backend.app.agents.supervisor.agent_instance_factory import configure_agent_instance_factory
        from netra_backend.app.agents.supervisor.agent_class_registry import get_agent_class_registry
        from netra_backend.app.services.agent_websocket_bridge import AgentWebSocketBridge
        
        # Create test agent class registry
        registry = get_agent_class_registry()
        if len(registry) == 0:
            await self._populate_minimal_test_agent_registry(registry)
            registry.freeze()
        
        # Configure factory
        websocket_bridge = AgentWebSocketBridge()
        await configure_agent_instance_factory(
            agent_class_registry=registry,
            websocket_bridge=websocket_bridge,
            llm_manager=self.mock_llm_manager
        )
    
    async def test_basic_agent_creation_workflow(self):
        """PASSING TEST: Verify basic agent creation works after proper configuration."""
        from netra_backend.app.agents.supervisor.agent_instance_factory import get_agent_instance_factory
        
        factory = get_agent_instance_factory()
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="test_user", thread_id="test_thread", run_id="test_run"
        )
        
        # Should work after configuration
        agent = await factory.create_agent_instance("triage", user_context)
        self.assertIsNotNone(agent)
    
    async def test_supervisor_agent_execution_integration(self):
        """INTEGRATION TEST: Test supervisor agent execution with mocked dependencies."""
        from netra_backend.app.agents.supervisor_ssot import SupervisorAgent
        from unittest.mock import AsyncMock, MagicMock
        
        # Create supervisor with fallback constructor
        supervisor = SupervisorAgent()
        supervisor.llm_manager = self.mock_llm_manager
        
        # Mock WebSocket bridge with correct interface
        websocket_bridge = MagicMock()
        websocket_bridge.notify_agent_started = AsyncMock(return_value=True)
        websocket_bridge.notify_agent_completed = AsyncMock(return_value=True)
        supervisor.websocket_bridge = websocket_bridge
        
        # Execute workflow
        user_context = UserExecutionContext.from_request_supervisor(
            user_id="test_user", thread_id="test_thread", run_id="test_run"
        )
        
        result = await supervisor.execute(context=user_context, stream_updates=True)
        
        # Verify basic execution
        self.assertIsNotNone(result)
        
        # Verify WebSocket events were called
        websocket_bridge.notify_agent_started.assert_called()
        websocket_bridge.notify_agent_completed.assert_called()
```

### Phase 3: E2E Staging Tests - Validate Complete Flow (LOW PRIORITY)

**Target:** End-to-end tests that validate complete Golden Path without Docker dependencies

#### 3.1 Golden Path E2E Tests
**File:** `tests/e2e/test_golden_path_staging_validation.py`

```python
class TestGoldenPathStagingValidation(SSotAsyncTestCase):
    """E2E tests for Golden Path validation in staging environment."""
    
    @pytest.mark.e2e
    @pytest.mark.staging
    async def test_complete_golden_path_user_flow(self):
        """E2E TEST: Validate complete user flow from login to AI response."""
        # This test validates the complete business flow
        # Implementation depends on staging environment setup
        pass
    
    @pytest.mark.e2e  
    @pytest.mark.staging
    async def test_websocket_events_in_staging(self):
        """E2E TEST: Validate all 5 critical WebSocket events in staging."""
        # This test validates WebSocket event delivery
        # Implementation depends on staging WebSocket setup
        pass
```

## Incremental Testing Approach

### Stage 1: Reproduce Issues (Week 1)
1. **Create failing unit tests** for each identified issue
2. **Validate test failures** reproduce exact Golden Path errors
3. **Document interface mismatches** and constructor issues

### Stage 2: Fix Infrastructure (Week 1-2)
1. **Fix agent registry configuration** - Configure AgentInstanceFactory properly
2. **Fix WebSocket bridge mocks** - Use correct method names in tests
3. **Fix constructor parameters** - Handle SupervisorAgent constructor changes

### Stage 3: Validate Integration (Week 2)
1. **Run integration tests** with fixed infrastructure
2. **Validate factory patterns** work correctly
3. **Test agent workflow execution** end-to-end

### Stage 4: E2E Validation (Week 2-3)
1. **Create staging E2E tests** for complete validation
2. **Validate WebSocket events** in real environment
3. **Performance and load testing** for business continuity

## Test Execution Commands

### Run Unit Tests (No Dependencies)
```bash
# All unit tests for Golden Path issues
python -m pytest tests/unit/test_golden_path_* -v

# Specific issue reproduction
python -m pytest tests/unit/test_golden_path_agent_registry_issues.py::TestAgentRegistryConfiguration::test_agent_instance_factory_missing_registry_error -v
```

### Run Integration Tests (No Docker)
```bash
# All integration tests for Golden Path
python -m pytest tests/integration/test_golden_path_* -v -m "not docker"

# Factory pattern validation
python -m pytest tests/integration/test_golden_path_execution_engine_factory.py -v
```

### Run E2E Tests (Staging)
```bash
# Staging environment tests
python -m pytest tests/e2e/test_golden_path_staging_validation.py -v -m "staging"
```

## Success Criteria

### Unit Test Success (Phase 1)
- [ ] All agent registry configuration errors reproduced and documented
- [ ] WebSocket bridge interface mismatches identified and corrected
- [ ] SupervisorAgent constructor issues resolved with fallback patterns
- [ ] 100% of identified issues have corresponding failing unit tests

### Integration Test Success (Phase 2)
- [ ] ExecutionEngineFactory.create_for_user() method works correctly
- [ ] UserExecutionEngine has all required methods
- [ ] Agent workflow integration tests pass with mocked dependencies
- [ ] Factory configuration resolves agent creation issues

### E2E Test Success (Phase 3)
- [ ] Complete Golden Path user flow works in staging
- [ ] All 5 critical WebSocket events delivered in staging
- [ ] Performance metrics meet business requirements
- [ ] Business value validation passes for $500K+ ARR protection

## Business Impact Protection

### Revenue Protection
- **$500K+ ARR protected** by ensuring core agent orchestration works
- **Chat functionality restored** - 90% of platform value delivery
- **User experience improved** - Real-time agent progress visibility

### Platform Stability
- **CI/CD reliability improved** - Tests run without Docker dependencies
- **Development velocity increased** - Faster feedback on agent changes
- **Production deployment confidence** - Comprehensive test coverage

### Technical Debt Reduction
- **Interface standardization** - Consistent WebSocket bridge usage
- **Factory pattern validation** - Proper dependency injection patterns
- **Test infrastructure improvement** - Reliable non-Docker testing

## Risk Mitigation

### High Risk Issues
1. **Agent Registry Configuration** - Primary blocker for all agent creation
2. **WebSocket Event Delivery** - Critical for user experience
3. **Constructor Parameter Changes** - Breaks existing agent initialization

### Medium Risk Issues
1. **Factory Method Availability** - Affects user isolation patterns
2. **Engine Method Completeness** - May impact advanced workflows
3. **Mock Interface Alignment** - Test reliability and maintenance

### Low Risk Issues
1. **E2E Test Dependencies** - Can be developed in parallel
2. **Performance Optimization** - Secondary to functionality
3. **Staging Environment Setup** - Infrastructure concern

## Implementation Timeline

### Week 1: Foundation (High Priority)
- **Day 1-2:** Create failing unit tests for core issues
- **Day 3-4:** Fix agent registry configuration and WebSocket mocks
- **Day 5:** Validate unit test fixes and integration setup

### Week 2: Integration (Medium Priority)  
- **Day 1-2:** Implement factory pattern integration tests
- **Day 3-4:** Fix remaining constructor and method issues
- **Day 5:** Validate complete integration test suite

### Week 3: Validation (Low Priority)
- **Day 1-2:** Create E2E staging tests
- **Day 3-4:** Performance and load testing
- **Day 5:** Final validation and documentation

This comprehensive test plan ensures systematic resolution of Golden Path integration test failures while protecting the $500K+ ARR dependent on reliable agent orchestration functionality.