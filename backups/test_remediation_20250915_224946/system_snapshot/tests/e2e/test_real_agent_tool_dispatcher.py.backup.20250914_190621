#!/usr/bin/env python
"""E2E TEST: Real Agent Tool Dispatcher - Comprehensive Testing with Real Services

Business Value Justification (BVJ):
- Segment: All (Free, Early, Mid, Enterprise)
- Business Goal: Ensure tool execution delivers value through agents
- Value Impact: Tool execution is the primary way agents deliver actionable insights to users
- Strategic Impact: Core platform functionality enabling $500K+ ARR through agent capabilities

CRITICAL VALIDATIONS:
- All 5 required WebSocket events sent (agent_started, agent_thinking, tool_executing, tool_completed, agent_completed)
- Tool results properly returned (substance of agent responses)
- No state leakage between users (multi-user system integrity)
- Factory pattern compliance (proper user isolation)
- Business value delivery through tools (core value proposition)

COMPLIANCE WITH CLAUDE.md STANDARDS:
- Uses ONLY real services (PostgreSQL, Redis, WebSocket, LLM)
- NO MOCKS per "MOCKS = Abomination" principle
- Real UnifiedToolDispatcher instances with factory patterns
- Real AgentRegistry with proper user isolation
- Comprehensive WebSocket event validation for chat value
- Request-scoped dispatcher testing (no singleton violations)

MISSION CRITICAL TESTING:
- test_all_five_websocket_events_comprehensive: Validates core chat business value
- All WebSocket events must be sent or deployment is blocked
"""

import asyncio
import json
import os
import sys
import time
import uuid
from collections import defaultdict
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Set, Any, Optional
import pytest

# CRITICAL: Add project root to Python path for imports
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import environment setup
from shared.isolated_environment import get_env, IsolatedEnvironment

# Import test framework components  
from test_framework.base_e2e_test import BaseE2ETest
from test_framework.websocket_helpers import (
    WebSocketTestHelpers, 
    create_test_connection_pool
)
from test_framework.isolated_environment_fixtures import isolated_env

# Import production components for real testing
from netra_backend.app.core.tools.unified_tool_dispatcher import (
    UnifiedToolDispatcher, 
    UnifiedToolDispatcherFactory,
    ToolDispatchRequest,
    ToolDispatchResponse
)
from netra_backend.app.agents.supervisor.agent_registry import AgentRegistry
from netra_backend.app.services.user_execution_context import UserExecutionContext
from netra_backend.app.services.agent_websocket_bridge import create_agent_websocket_bridge
from netra_backend.app.llm.llm_manager import LLMManager
from netra_backend.app.websocket_core.websocket_manager import WebSocketManager as WebSocketManager
from netra_backend.app.schemas.tool import ToolInput, ToolResult, ToolStatus


# ============================================================================
# TEST CONFIGURATION
# ============================================================================

class ToolDispatcherTestConfig:
    """Configuration for tool dispatcher E2E testing."""
    
    WEBSOCKET_TIMEOUT = 30.0
    TOOL_EXECUTION_TIMEOUT = 20.0
    USER_ISOLATION_TEST_USERS = 5
    PERFORMANCE_TEST_ITERATIONS = 50
    MAX_CONCURRENT_USERS = 10
    
    # Required WebSocket events for business value
    REQUIRED_WEBSOCKET_EVENTS = [
        "agent_started",
        "agent_thinking", 
        "tool_executing",
        "tool_completed",
        "agent_completed"
    ]
    
    # Test tool definitions
    TEST_TOOLS = {
        "mock_data_analyzer": {
            "description": "Analyzes data and provides insights",
            "parameters": {"data_source": str, "analysis_type": str},
            "expected_result_type": dict
        },
        "mock_cost_optimizer": {
            "description": "Optimizes costs based on usage patterns", 
            "parameters": {"service_type": str, "current_cost": float},
            "expected_result_type": dict
        },
        "mock_report_generator": {
            "description": "Generates comprehensive reports",
            "parameters": {"report_type": str, "data_range": str},
            "expected_result_type": str
        }
    }


class MockBusinessValueTool:
    """Mock tool that simulates real business value delivery."""
    
    def __init__(self, name: str, tool_config: Dict[str, Any]):
        self.name = name
        self.description = tool_config["description"]
        self.parameters = tool_config["parameters"]
        self.expected_result_type = tool_config["expected_result_type"]
        self.execution_count = 0
        
    async def run(self, **kwargs) -> Any:
        """Execute tool and return business value."""
        self.execution_count += 1
        
        # Simulate realistic execution time
        await asyncio.sleep(0.1 + (self.execution_count * 0.05))
        
        if self.name == "mock_data_analyzer":
            return {
                "insights": [
                    "Cost savings opportunity identified in compute resources",
                    "Usage patterns show 30% efficiency gain potential",
                    "Recommended actions: scale down non-production environments"
                ],
                "metrics": {
                    "potential_savings_monthly": 2500.00,
                    "efficiency_score": 85,
                    "confidence_level": 0.92
                },
                "execution_metadata": {
                    "analysis_duration_ms": int((time.time() % 1) * 1000),
                    "data_points_analyzed": 15000,
                    "execution_count": self.execution_count
                }
            }
            
        elif self.name == "mock_cost_optimizer":
            return {
                "optimizations": [
                    {
                        "service": kwargs.get("service_type", "compute"),
                        "current_cost": kwargs.get("current_cost", 1000.0),
                        "optimized_cost": kwargs.get("current_cost", 1000.0) * 0.7,
                        "savings": kwargs.get("current_cost", 1000.0) * 0.3,
                        "actions": ["Right-size instances", "Use reserved instances", "Enable auto-scaling"]
                    }
                ],
                "total_monthly_savings": kwargs.get("current_cost", 1000.0) * 0.3,
                "execution_metadata": {
                    "optimization_duration_ms": int((time.time() % 1) * 1000),
                    "execution_count": self.execution_count
                }
            }
            
        elif self.name == "mock_report_generator":
            report_content = f"""
# Cost Optimization Report
Generated: {datetime.now(timezone.utc).isoformat()}
Report Type: {kwargs.get('report_type', 'comprehensive')}
Data Range: {kwargs.get('data_range', 'last_30_days')}

## Executive Summary
- Total potential savings identified: $2,500/month
- Efficiency improvements: 30% average
- Recommended immediate actions: 3

## Detailed Analysis
1. Compute resource optimization: $1,500/month savings
2. Storage optimization: $700/month savings  
3. Network optimization: $300/month savings

## Implementation Timeline
- Phase 1 (Week 1-2): Compute optimizations
- Phase 2 (Week 3-4): Storage optimizations
- Phase 3 (Week 5-6): Network optimizations

Execution #{self.execution_count} completed successfully.
"""
            return report_content.strip()
        
        else:
            return {
                "result": f"Tool {self.name} executed successfully",
                "execution_count": self.execution_count,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }


class WebSocketEventCollector:
    """Collects and validates WebSocket events from real connections."""
    
    def __init__(self):
        self.events: List[Dict[str, Any]] = []
        self.events_by_type: Dict[str, List[Dict]] = defaultdict(list)
        self.events_by_user: Dict[str, List[Dict]] = defaultdict(list)
        self.start_time = time.time()
        self._lock = asyncio.Lock()
        
    async def capture_event(self, event: Dict[str, Any]) -> None:
        """Capture event from real WebSocket connection."""
        async with self._lock:
            # Add metadata
            enriched_event = {
                **event,
                "capture_timestamp": time.time(),
                "relative_time_ms": int((time.time() - self.start_time) * 1000)
            }
            
            self.events.append(enriched_event)
            
            # Index by type
            event_type = event.get("type", "unknown")
            self.events_by_type[event_type].append(enriched_event)
            
            # Index by user
            user_id = event.get("user_id")
            if user_id:
                self.events_by_user[user_id].append(enriched_event)
    
    def get_events_for_user(self, user_id: str) -> List[Dict]:
        """Get all events for a specific user."""
        return self.events_by_user.get(user_id, [])
        
    def get_tool_events_for_user(self, user_id: str) -> Dict[str, List[Dict]]:
        """Get tool-related events for a user."""
        user_events = self.get_events_for_user(user_id)
        return {
            "tool_executing": [e for e in user_events if e.get("type") == "tool_executing"],
            "tool_completed": [e for e in user_events if e.get("type") == "tool_completed"]
        }
        
    def get_all_agent_events_for_user(self, user_id: str) -> Dict[str, List[Dict]]:
        """Get all agent-related events for a user."""
        user_events = self.get_events_for_user(user_id)
        return {
            "agent_started": [e for e in user_events if e.get("type") == "agent_started"],
            "agent_thinking": [e for e in user_events if e.get("type") == "agent_thinking"],
            "tool_executing": [e for e in user_events if e.get("type") == "tool_executing"],
            "tool_completed": [e for e in user_events if e.get("type") == "tool_completed"],
            "agent_completed": [e for e in user_events if e.get("type") == "agent_completed"]
        }
        
    def validate_tool_event_sequence(self, user_id: str, tool_name: str) -> bool:
        """Validate that tool events occurred in proper sequence."""
        tool_events = self.get_tool_events_for_user(user_id)
        
        executing_events = [e for e in tool_events["tool_executing"] 
                          if e.get("tool_name") == tool_name]
        completed_events = [e for e in tool_events["tool_completed"] 
                          if e.get("tool_name") == tool_name]
        
        # Must have at least one of each
        if not executing_events or not completed_events:
            return False
            
        # Each executing must be followed by completed
        for exec_event in executing_events:
            exec_time = exec_event.get("relative_time_ms", 0)
            # Find corresponding completed event
            matching_completed = [
                c for c in completed_events 
                if c.get("relative_time_ms", 0) > exec_time
            ]
            if not matching_completed:
                return False
                
        return True


# ============================================================================
# TEST CLASS
# ============================================================================

class TestRealAgentToolDispatcher(BaseE2ETest):
    """E2E testing of UnifiedToolDispatcher with real services only."""
    
    def setup_method(self):
        """Setup method called before each test - sync initialization."""
        # Call parent setup
        super().setup_method()
        
        # Initialize basic test components
        self.isolated_env = get_env()
        self.config = ToolDispatcherTestConfig()
        self.event_collector = WebSocketEventCollector()
        
        # Track created dispatchers for cleanup
        self.created_dispatchers: List[UnifiedToolDispatcher] = []
        
        # Create test tools for business value validation
        self.test_tools = {}
        for tool_name, tool_config in self.config.TEST_TOOLS.items():
            self.test_tools[tool_name] = MockBusinessValueTool(tool_name, tool_config)
        
    def teardown_method(self):
        """Cleanup after each test method."""
        # Cleanup dispatchers
        for dispatcher in self.created_dispatchers:
            try:
                if hasattr(dispatcher, 'cleanup'):
                    asyncio.run(dispatcher.cleanup())
            except Exception as e:
                print(f"Cleanup error for dispatcher: {e}")
                
    async def initialize_real_services(self):
        """Initialize real services for testing - called by individual tests."""
        try:
            # Create isolated environment for this test
            env = get_env()
            
            # Create real WebSocket manager with proper environment isolation
            self.websocket_manager = WebSocketManager()
            
            # Create real LLM manager with environment configuration
            self.llm_manager = LLMManager()
            
            # Create real agent registry with proper user isolation factory patterns
            self.agent_registry = AgentRegistry(
                llm_manager=self.llm_manager,
                tool_dispatcher_factory=UnifiedToolDispatcherFactory()
            )
            
            # Set WebSocket manager on registry for event propagation - critical for chat value
            await self.agent_registry.set_websocket_manager_async(self.websocket_manager)
            
            # Verify all services are properly initialized
            assert self.websocket_manager is not None, "WebSocket manager initialization failed"
            assert self.llm_manager is not None, "LLM manager initialization failed" 
            assert self.agent_registry is not None, "Agent registry initialization failed"
            
            return True
        except Exception as e:
            print(f"Failed to initialize real services: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def setup_test_with_real_services(self) -> bool:
        """Common setup for tests requiring real services - returns True if successful."""
        if not await self.initialize_real_services():
            return False
        return True
    
    async def create_user_context(self, user_id: str = None) -> UserExecutionContext:
        """Create isolated user execution context for testing."""
        if not user_id:
            user_id = f"test_user_{uuid.uuid4().hex[:8]}"
            
        return UserExecutionContext(
            user_id=user_id,
            run_id=f"run_{uuid.uuid4().hex[:8]}",
            thread_id=f"thread_{uuid.uuid4().hex[:8]}",
            session_id=f"session_{uuid.uuid4().hex[:8]}",
            metadata={
                "test_context": True,
                "created_at": datetime.now(timezone.utc).isoformat()
            }
        )
    
    async def create_websocket_bridge_for_user(self, user_context: UserExecutionContext):
        """Create real WebSocket bridge for user."""
        # Create real bridge using factory pattern
        bridge = create_agent_websocket_bridge(user_context)
        
        # Connect to event collector for validation
        original_notify_tool_executing = bridge.notify_tool_executing
        original_notify_tool_completed = bridge.notify_tool_completed
        
        async def captured_notify_tool_executing(run_id, agent_name, tool_name, parameters=None):
            result = await original_notify_tool_executing(run_id, agent_name, tool_name, parameters)
            await self.event_collector.capture_event({
                "type": "tool_executing",
                "run_id": run_id,
                "agent_name": agent_name,
                "tool_name": tool_name,
                "parameters": parameters or {},
                "user_id": user_context.user_id
            })
            return result
            
        async def captured_notify_tool_completed(run_id, agent_name, tool_name, result, execution_time_ms=None):
            result_captured = await original_notify_tool_completed(run_id, agent_name, tool_name, result, execution_time_ms)
            await self.event_collector.capture_event({
                "type": "tool_completed", 
                "run_id": run_id,
                "agent_name": agent_name,
                "tool_name": tool_name,
                "result": result,
                "execution_time_ms": execution_time_ms,
                "user_id": user_context.user_id
            })
            return result_captured
            
        # Also capture other agent events if bridge supports them
        if hasattr(bridge, 'notify_agent_started'):
            original_notify_agent_started = bridge.notify_agent_started
            async def captured_notify_agent_started(run_id, agent_name):
                result = await original_notify_agent_started(run_id, agent_name)
                await self.event_collector.capture_event({
                    "type": "agent_started",
                    "run_id": run_id,
                    "agent_name": agent_name,
                    "user_id": user_context.user_id
                })
                return result
            bridge.notify_agent_started = captured_notify_agent_started
            
        if hasattr(bridge, 'notify_agent_thinking'):
            original_notify_agent_thinking = bridge.notify_agent_thinking
            async def captured_notify_agent_thinking(run_id, agent_name, reasoning):
                result = await original_notify_agent_thinking(run_id, agent_name, reasoning)
                await self.event_collector.capture_event({
                    "type": "agent_thinking",
                    "run_id": run_id,
                    "agent_name": agent_name,
                    "reasoning": reasoning,
                    "user_id": user_context.user_id
                })
                return result
            bridge.notify_agent_thinking = captured_notify_agent_thinking
            
        if hasattr(bridge, 'notify_agent_completed'):
            original_notify_agent_completed = bridge.notify_agent_completed
            async def captured_notify_agent_completed(run_id, agent_name, final_result):
                result = await original_notify_agent_completed(run_id, agent_name, final_result)
                await self.event_collector.capture_event({
                    "type": "agent_completed",
                    "run_id": run_id,
                    "agent_name": agent_name,
                    "final_result": final_result,
                    "user_id": user_context.user_id
                })
                return result
            bridge.notify_agent_completed = captured_notify_agent_completed
            
        bridge.notify_tool_executing = captured_notify_tool_executing
        bridge.notify_tool_completed = captured_notify_tool_completed
        
        return bridge
    
    # ===================== CORE FUNCTIONALITY TESTS =====================
    
    @pytest.mark.e2e
    @pytest.mark.real_services  
    async def test_tool_dispatcher_creation_with_real_services(self):
        """Test UnifiedToolDispatcher creation with real database and cache connections."""
        # Initialize real services
        if not await self.setup_test_with_real_services():
            pytest.skip("Real services not available for testing")
        
        # Create user context
        user_context = await self.create_user_context()
        
        # Create WebSocket bridge
        websocket_bridge = await self.create_websocket_bridge_for_user(user_context)
        
        # Create dispatcher using factory pattern (no direct instantiation)
        dispatcher = await UnifiedToolDispatcher.create_for_user(
            user_context=user_context,
            websocket_bridge=websocket_bridge,
            enable_admin_tools=False
        )
        self.created_dispatchers.append(dispatcher)
        
        # Validate dispatcher properties
        assert dispatcher.user_context == user_context
        assert dispatcher.has_websocket_support == True
        assert dispatcher.dispatcher_id.startswith(user_context.user_id)
        assert dispatcher._is_active == True
        
        # Validate registry integration
        assert hasattr(dispatcher, 'registry')
        assert hasattr(dispatcher, 'executor') 
        assert hasattr(dispatcher, 'validator')
        
        # Validate WebSocket integration
        assert dispatcher.websocket_bridge is not None
        
        print(f" PASS:  Successfully created UnifiedToolDispatcher for user {user_context.user_id}")
        
    @pytest.mark.e2e
    @pytest.mark.real_services
    async def test_websocket_enhancement_by_agent_registry(self):
        """Test that AgentRegistry properly enhances tool dispatcher with WebSocket events."""
        # Initialize real services
        if not await self.setup_test_with_real_services():
            pytest.skip("Real services not available for testing")
        
        # Create user context
        user_context = await self.create_user_context()
        
        # Create dispatcher through AgentRegistry (simulates real agent creation)
        dispatcher = await self.agent_registry.create_tool_dispatcher_for_user(
            user_context=user_context,
            websocket_bridge=None,  # Let registry handle WebSocket setup
            enable_admin_tools=False
        )
        self.created_dispatchers.append(dispatcher)
        
        # Validate WebSocket enhancement occurred  
        assert dispatcher.has_websocket_support == True
        assert hasattr(dispatcher, 'websocket_manager')
        
        # Test that WebSocket manager is properly wired
        if dispatcher.websocket_manager:
            # Verify it's the same instance as registry's manager
            assert dispatcher.websocket_manager == self.websocket_manager
            
        print(f" PASS:  AgentRegistry successfully enhanced dispatcher with WebSocket support")
        
    @pytest.mark.e2e  
    @pytest.mark.real_services
    async def test_tool_execution_with_real_results(self):
        """Test tool execution produces real business value results."""
        # Initialize real services
        if not await self.setup_test_with_real_services():
            pytest.skip("Real services not available for testing")
        
        # Create user context and dispatcher
        user_context = await self.create_user_context()
        websocket_bridge = await self.create_websocket_bridge_for_user(user_context)
        
        dispatcher = await UnifiedToolDispatcher.create_for_user(
            user_context=user_context,
            websocket_bridge=websocket_bridge,
            enable_admin_tools=False
        )
        self.created_dispatchers.append(dispatcher)
        
        # Register test tools that deliver business value
        for tool_name, test_tool in self.test_tools.items():
            # Create LangChain-compatible tool wrapper
            from langchain_core.tools import Tool
            
            wrapped_tool = Tool(
                name=tool_name,
                description=test_tool.description,
                func=test_tool.run
            )
            
            dispatcher.register_tool(wrapped_tool)
        
        # Test data analysis tool execution
        analysis_result = await dispatcher.execute_tool(
            tool_name="mock_data_analyzer",
            parameters={
                "data_source": "production_metrics",
                "analysis_type": "cost_optimization"
            }
        )
        
        # Validate business value in results
        assert analysis_result.success == True
        assert analysis_result.result is not None
        
        result_data = analysis_result.result
        assert "insights" in result_data
        assert "metrics" in result_data
        assert "potential_savings_monthly" in result_data["metrics"]
        assert result_data["metrics"]["potential_savings_monthly"] > 0
        
        # Test cost optimizer tool execution
        optimizer_result = await dispatcher.execute_tool(
            tool_name="mock_cost_optimizer",
            parameters={
                "service_type": "compute",
                "current_cost": 5000.0
            }
        )
        
        # Validate optimization business value
        assert optimizer_result.success == True
        optimizer_data = optimizer_result.result
        assert "optimizations" in optimizer_data
        assert "total_monthly_savings" in optimizer_data
        assert optimizer_data["total_monthly_savings"] > 0
        
        print(f" PASS:  Tool execution delivered real business value results")
        
    @pytest.mark.e2e
    @pytest.mark.real_services  
    async def test_websocket_events_for_tool_execution(self):
        """Test that all required WebSocket events are sent during tool execution."""
        # Create user context and dispatcher
        user_context = await self.create_user_context() 
        websocket_bridge = await self.create_websocket_bridge_for_user(user_context)
        
        dispatcher = await UnifiedToolDispatcher.create_for_user(
            user_context=user_context,
            websocket_bridge=websocket_bridge,
            enable_admin_tools=False
        )
        self.created_dispatchers.append(dispatcher)
        
        # Register test tool
        from langchain_core.tools import Tool
        test_tool = Tool(
            name="test_websocket_tool",
            description="Tool for testing WebSocket events",
            func=self.test_tools["mock_report_generator"].run
        )
        dispatcher.register_tool(test_tool)
        
        # Execute tool and capture events
        start_time = time.time()
        result = await dispatcher.execute_tool(
            tool_name="test_websocket_tool",
            parameters={
                "report_type": "websocket_test",
                "data_range": "test_period"
            }
        )
        execution_time = time.time() - start_time
        
        # Validate tool execution succeeded
        assert result.success == True
        
        # Allow time for WebSocket events to propagate
        await asyncio.sleep(0.5)
        
        # Validate WebSocket events were captured
        tool_events = self.event_collector.get_tool_events_for_user(user_context.user_id)
        
        # Must have both tool_executing and tool_completed events
        assert len(tool_events["tool_executing"]) >= 1, "Missing tool_executing event"
        assert len(tool_events["tool_completed"]) >= 1, "Missing tool_completed event" 
        
        # Validate event content
        executing_event = tool_events["tool_executing"][0]
        assert executing_event["tool_name"] == "test_websocket_tool"
        assert executing_event["user_id"] == user_context.user_id
        
        completed_event = tool_events["tool_completed"][0]
        assert completed_event["tool_name"] == "test_websocket_tool"
        assert completed_event["user_id"] == user_context.user_id
        assert "result" in completed_event
        
        # Validate event sequence
        assert self.event_collector.validate_tool_event_sequence(
            user_context.user_id, 
            "test_websocket_tool"
        )
        
        print(f" PASS:  All required WebSocket events sent during tool execution")
        print(f"   - tool_executing events: {len(tool_events['tool_executing'])}")
        print(f"   - tool_completed events: {len(tool_events['tool_completed'])}")
        
    @pytest.mark.e2e
    @pytest.mark.real_services
    @pytest.mark.mission_critical
    async def test_all_five_websocket_events_comprehensive(self):
        """MISSION CRITICAL: Test that all 5 required WebSocket events are sent during agent execution.
        
        This test validates the core business value delivery mechanism:
        - agent_started: User sees processing began
        - agent_thinking: Real-time reasoning visibility 
        - tool_executing: Tool usage transparency
        - tool_completed: Results delivery
        - agent_completed: Completion notification
        
        Without these events, the chat has no substantive value.
        """
        # Initialize real services
        if not await self.setup_test_with_real_services():
            pytest.skip("Real services not available for testing")
            
        # Create user context and dispatcher with full agent integration
        user_context = await self.create_user_context()
        websocket_bridge = await self.create_websocket_bridge_for_user(user_context)
        
        # Create dispatcher with full agent registry integration
        dispatcher = await self.agent_registry.create_tool_dispatcher_for_user(
            user_context=user_context,
            websocket_bridge=websocket_bridge,
            enable_admin_tools=False
        )
        self.created_dispatchers.append(dispatcher)
        
        # Register comprehensive test tool that simulates agent workflow
        from langchain_core.tools import Tool
        
        async def comprehensive_agent_tool(**kwargs):
            """Tool that simulates comprehensive agent workflow with multiple steps."""
            # Simulate agent thinking/reasoning phase
            await asyncio.sleep(0.1)
            
            # Simulate tool execution phase  
            await asyncio.sleep(0.2)
            
            # Return comprehensive business value result
            return {
                "analysis_results": {
                    "cost_savings_identified": "$15,000/month",
                    "efficiency_improvements": ["Auto-scaling", "Resource rightsizing", "Reserved instances"],
                    "risk_factors": ["High utilization periods", "Data transfer costs"],
                    "recommendations": [
                        {
                            "action": "Implement auto-scaling policies",
                            "impact": "30% cost reduction",
                            "timeline": "2 weeks"
                        },
                        {
                            "action": "Switch to reserved instances",  
                            "impact": "40% compute cost reduction",
                            "timeline": "1 week"
                        }
                    ]
                },
                "executive_summary": "Analysis complete - $15K monthly savings identified",
                "confidence_score": 0.94,
                "next_steps": ["Review recommendations", "Implement high-impact changes", "Monitor results"]
            }
        
        comprehensive_tool = Tool(
            name="comprehensive_agent_workflow_tool",
            description="Comprehensive agent workflow tool for testing all WebSocket events",
            func=comprehensive_agent_tool
        )
        dispatcher.register_tool(comprehensive_tool)
        
        # Execute tool with comprehensive parameter set
        start_time = time.time()
        result = await dispatcher.execute_tool(
            tool_name="comprehensive_agent_workflow_tool",
            parameters={
                "analysis_type": "comprehensive_optimization",
                "scope": "production_environment",
                "priority": "high",
                "user_context": user_context.user_id
            }
        )
        execution_time = time.time() - start_time
        
        # Validate tool execution succeeded with business value
        assert result.success == True, f"Tool execution failed: {result.error}"
        assert result.result is not None, "Tool execution returned no results"
        
        result_data = result.result
        assert "analysis_results" in result_data, "Missing analysis results"
        assert "cost_savings_identified" in result_data["analysis_results"], "Missing cost savings"
        assert "recommendations" in result_data["analysis_results"], "Missing recommendations"
        
        # Allow time for all WebSocket events to propagate through real system
        await asyncio.sleep(1.0)
        
        # Get all agent events for comprehensive validation
        all_events = self.event_collector.get_all_agent_events_for_user(user_context.user_id)
        
        # CRITICAL VALIDATION: All 5 events must be present
        missing_events = []
        for required_event in self.config.REQUIRED_WEBSOCKET_EVENTS:
            if not all_events.get(required_event, []):
                missing_events.append(required_event)
        
        assert not missing_events, f"CRITICAL: Missing required WebSocket events: {missing_events}"
        
        # Validate event content and structure
        if all_events["agent_started"]:
            agent_started = all_events["agent_started"][0]
            assert agent_started.get("user_id") == user_context.user_id
            assert agent_started.get("run_id") is not None
            
        if all_events["agent_thinking"]:
            agent_thinking = all_events["agent_thinking"][0]
            assert agent_thinking.get("user_id") == user_context.user_id
            assert agent_thinking.get("reasoning") is not None
            
        # Tool events validation (existing logic)
        assert len(all_events["tool_executing"]) >= 1, "Missing tool_executing events"
        assert len(all_events["tool_completed"]) >= 1, "Missing tool_completed events"
        
        tool_executing = all_events["tool_executing"][0]
        assert tool_executing.get("tool_name") == "comprehensive_agent_workflow_tool"
        assert tool_executing.get("user_id") == user_context.user_id
        
        tool_completed = all_events["tool_completed"][0]
        assert tool_completed.get("tool_name") == "comprehensive_agent_workflow_tool"
        assert tool_completed.get("user_id") == user_context.user_id
        assert "result" in tool_completed
        
        if all_events["agent_completed"]:
            agent_completed = all_events["agent_completed"][0]
            assert agent_completed.get("user_id") == user_context.user_id
            assert agent_completed.get("final_result") is not None
        
        # Validate event timing sequence
        all_user_events = self.event_collector.get_events_for_user(user_context.user_id)
        event_types_sequence = [e.get("type") for e in sorted(all_user_events, key=lambda x: x.get("capture_timestamp", 0))]
        
        # Validate logical event ordering (agent_started should come before tool events, etc.)
        if "agent_started" in event_types_sequence and "tool_executing" in event_types_sequence:
            agent_started_idx = event_types_sequence.index("agent_started")
            tool_executing_idx = event_types_sequence.index("tool_executing")
            assert agent_started_idx < tool_executing_idx, "agent_started must come before tool_executing"
            
        if "tool_executing" in event_types_sequence and "tool_completed" in event_types_sequence:
            tool_executing_idx = event_types_sequence.index("tool_executing")
            tool_completed_idx = event_types_sequence.index("tool_completed")
            assert tool_executing_idx < tool_completed_idx, "tool_executing must come before tool_completed"
        
        print(f" PASS:  MISSION CRITICAL: All 5 WebSocket events validated successfully")
        print(f"   - agent_started: {len(all_events['agent_started'])} events")
        print(f"   - agent_thinking: {len(all_events['agent_thinking'])} events") 
        print(f"   - tool_executing: {len(all_events['tool_executing'])} events")
        print(f"   - tool_completed: {len(all_events['tool_completed'])} events")
        print(f"   - agent_completed: {len(all_events['agent_completed'])} events")
        print(f"   - Total execution time: {execution_time:.3f}s")
        print(f"   - Event sequence validated: {len(all_user_events)} total events")
        
    @pytest.mark.e2e
    @pytest.mark.real_services
    async def test_request_scoped_dispatcher_isolation(self):
        """Test that dispatchers are request-scoped, not singletons.""" 
        # Create multiple user contexts
        user_contexts = []
        for i in range(3):
            user_contexts.append(await self.create_user_context(f"user_{i}"))
        
        # Create dispatchers for each user
        dispatchers = []
        for user_context in user_contexts:
            websocket_bridge = await self.create_websocket_bridge_for_user(user_context)
            dispatcher = await UnifiedToolDispatcher.create_for_user(
                user_context=user_context,
                websocket_bridge=websocket_bridge,
                enable_admin_tools=False
            )
            dispatchers.append(dispatcher)
            self.created_dispatchers.append(dispatcher)
        
        # Validate each dispatcher is unique (not singleton)
        dispatcher_ids = [d.dispatcher_id for d in dispatchers]
        assert len(set(dispatcher_ids)) == len(dispatchers), "Dispatchers are not unique (singleton violation)"
        
        # Validate each dispatcher has different user context
        user_ids = [d.user_context.user_id for d in dispatchers]
        assert len(set(user_ids)) == len(dispatchers), "User contexts not properly isolated"
        
        # Validate dispatchers don't share state
        for i, dispatcher in enumerate(dispatchers):
            assert dispatcher.user_context == user_contexts[i]
            assert dispatcher.user_context != user_contexts[(i+1) % len(user_contexts)]
            
        print(f" PASS:  Request-scoped isolation verified - {len(dispatchers)} unique dispatchers created")
        
    @pytest.mark.e2e
    @pytest.mark.real_services
    async def test_user_context_isolation_in_tool_execution(self):
        """Test that tool execution maintains complete user context isolation."""
        # Create multiple users
        num_users = self.config.USER_ISOLATION_TEST_USERS
        users_data = []
        
        for i in range(num_users):
            user_context = await self.create_user_context(f"isolation_user_{i}")
            websocket_bridge = await self.create_websocket_bridge_for_user(user_context)
            dispatcher = await UnifiedToolDispatcher.create_for_user(
                user_context=user_context,
                websocket_bridge=websocket_bridge,
                enable_admin_tools=False
            )
            self.created_dispatchers.append(dispatcher)
            
            users_data.append({
                "user_context": user_context,
                "dispatcher": dispatcher,
                "expected_results": []
            })
        
        # Register same tool on all dispatchers
        from langchain_core.tools import Tool
        test_tool = Tool(
            name="isolation_test_tool",
            description="Tool for testing user isolation",
            func=self.test_tools["mock_cost_optimizer"].run
        )
        
        for user_data in users_data:
            user_data["dispatcher"].register_tool(test_tool)
        
        # Execute tool concurrently for all users with different parameters
        async def execute_for_user(user_data, user_index):
            result = await user_data["dispatcher"].execute_tool(
                tool_name="isolation_test_tool",
                parameters={
                    "service_type": f"service_{user_index}",
                    "current_cost": 1000.0 + (user_index * 100)
                }
            )
            user_data["actual_result"] = result
            return result
        
        # Run all executions concurrently to test isolation under load
        tasks = []
        for i, user_data in enumerate(users_data):
            task = asyncio.create_task(execute_for_user(user_data, i))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Validate all executions succeeded
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                pytest.fail(f"User {i} tool execution failed: {result}")
            assert result.success == True, f"User {i} tool execution not successful"
        
        # Validate user isolation - each user should have only their events
        for i, user_data in enumerate(users_data):
            user_events = self.event_collector.get_events_for_user(user_data["user_context"].user_id)
            
            # User should have their own events
            assert len(user_events) >= 2, f"User {i} missing WebSocket events"
            
            # Events should contain correct user ID
            for event in user_events:
                assert event.get("user_id") == user_data["user_context"].user_id
                
            # Events should not contain other users' data
            for other_user_data in users_data:
                if other_user_data != user_data:
                    assert event.get("user_id") != other_user_data["user_context"].user_id
        
        print(f" PASS:  User context isolation verified for {num_users} concurrent users")
        
    @pytest.mark.e2e
    @pytest.mark.real_services
    async def test_error_handling_in_tool_execution(self):
        """Test comprehensive error handling during tool execution."""
        # Create user context and dispatcher
        user_context = await self.create_user_context()
        websocket_bridge = await self.create_websocket_bridge_for_user(user_context)
        
        dispatcher = await UnifiedToolDispatcher.create_for_user(
            user_context=user_context,
            websocket_bridge=websocket_bridge,
            enable_admin_tools=False
        )
        self.created_dispatchers.append(dispatcher)
        
        # Test 1: Tool not found error
        result = await dispatcher.execute_tool(
            tool_name="nonexistent_tool",
            parameters={"param": "value"}
        )
        
        assert result.success == False
        assert "not found" in result.error.lower()
        assert result.result is None
        
        # Test 2: Tool execution error
        from langchain_core.tools import Tool
        
        def failing_tool(**kwargs):
            raise ValueError("Simulated tool execution error")
            
        error_tool = Tool(
            name="failing_tool", 
            description="Tool that always fails",
            func=failing_tool
        )
        dispatcher.register_tool(error_tool)
        
        result = await dispatcher.execute_tool(
            tool_name="failing_tool",
            parameters={"test": "error"}
        )
        
        assert result.success == False
        assert "error" in result.error.lower()
        
        # Validate error events were sent via WebSocket
        await asyncio.sleep(0.5)  # Allow event propagation
        
        user_events = self.event_collector.get_events_for_user(user_context.user_id)
        error_events = [e for e in user_events if e.get("type") == "tool_completed" and "error" in str(e.get("result", ""))]
        
        # Should have error events for failed executions
        assert len(error_events) >= 1, "Missing WebSocket events for tool errors"
        
        print(f" PASS:  Error handling validated - {len(error_events)} error events captured")
        
    @pytest.mark.e2e
    @pytest.mark.real_services
    async def test_performance_benchmarks_for_tool_execution(self):
        """Test tool execution performance meets business requirements."""
        # Create user context and dispatcher
        user_context = await self.create_user_context()
        websocket_bridge = await self.create_websocket_bridge_for_user(user_context)
        
        dispatcher = await UnifiedToolDispatcher.create_for_user(
            user_context=user_context,
            websocket_bridge=websocket_bridge,
            enable_admin_tools=False
        )
        self.created_dispatchers.append(dispatcher)
        
        # Register performance test tool
        from langchain_core.tools import Tool
        perf_tool = Tool(
            name="performance_test_tool",
            description="Tool for performance testing",
            func=self.test_tools["mock_data_analyzer"].run
        )
        dispatcher.register_tool(perf_tool)
        
        # Performance test parameters
        iterations = self.config.PERFORMANCE_TEST_ITERATIONS
        max_execution_time = self.config.TOOL_EXECUTION_TIMEOUT
        
        execution_times = []
        successful_executions = 0
        
        # Run performance test
        for i in range(iterations):
            start_time = time.time()
            
            result = await dispatcher.execute_tool(
                tool_name="performance_test_tool",
                parameters={
                    "data_source": f"perf_test_{i}",
                    "analysis_type": "performance_benchmark"
                }
            )
            
            execution_time = time.time() - start_time
            execution_times.append(execution_time)
            
            if result.success:
                successful_executions += 1
                
            # Validate individual execution time
            assert execution_time < max_execution_time, f"Execution {i} too slow: {execution_time:.2f}s"
        
        # Calculate performance metrics
        avg_time = sum(execution_times) / len(execution_times)
        min_time = min(execution_times)
        max_time = max(execution_times)
        success_rate = (successful_executions / iterations) * 100
        
        # Performance assertions (business requirements)
        assert avg_time < 2.0, f"Average execution time too high: {avg_time:.2f}s"
        assert max_time < max_execution_time, f"Max execution time exceeded: {max_time:.2f}s"
        assert success_rate >= 98.0, f"Success rate too low: {success_rate:.1f}%"
        
        # Validate WebSocket event performance
        await asyncio.sleep(1.0)  # Allow all events to propagate
        user_events = self.event_collector.get_events_for_user(user_context.user_id)
        
        # Should have 2 events per successful execution (executing + completed)
        expected_events = successful_executions * 2
        actual_events = len(user_events)
        
        # Allow some variance for test reliability
        assert actual_events >= expected_events * 0.9, f"Missing WebSocket events: {actual_events}/{expected_events}"
        
        print(f" PASS:  Performance benchmarks passed:")
        print(f"   - {iterations} iterations completed")
        print(f"   - Average execution time: {avg_time:.3f}s")
        print(f"   - Min/Max time: {min_time:.3f}s / {max_time:.3f}s") 
        print(f"   - Success rate: {success_rate:.1f}%")
        print(f"   - WebSocket events: {actual_events}/{expected_events}")
        
    @pytest.mark.e2e
    @pytest.mark.real_services
    async def test_business_value_delivery_through_tools(self):
        """Test that tools deliver measurable business value through the dispatcher."""
        # Create user context and dispatcher
        user_context = await self.create_user_context()
        websocket_bridge = await self.create_websocket_bridge_for_user(user_context)
        
        dispatcher = await UnifiedToolDispatcher.create_for_user(
            user_context=user_context,
            websocket_bridge=websocket_bridge,
            enable_admin_tools=False
        )
        self.created_dispatchers.append(dispatcher)
        
        # Register all business value tools
        from langchain_core.tools import Tool
        for tool_name, test_tool in self.test_tools.items():
            wrapped_tool = Tool(
                name=tool_name,
                description=test_tool.description,
                func=test_tool.run
            )
            dispatcher.register_tool(wrapped_tool)
        
        # Execute complete business workflow
        business_value_metrics = {
            "total_potential_savings": 0,
            "insights_generated": 0,
            "reports_created": 0,
            "optimizations_identified": 0
        }
        
        # Step 1: Analyze current state
        analysis_result = await dispatcher.execute_tool(
            tool_name="mock_data_analyzer",
            parameters={
                "data_source": "production_environment",
                "analysis_type": "comprehensive_analysis"
            }
        )
        
        assert analysis_result.success == True
        analysis_data = analysis_result.result
        business_value_metrics["insights_generated"] += len(analysis_data.get("insights", []))
        business_value_metrics["total_potential_savings"] += analysis_data.get("metrics", {}).get("potential_savings_monthly", 0)
        
        # Step 2: Generate optimizations
        optimization_result = await dispatcher.execute_tool(
            tool_name="mock_cost_optimizer", 
            parameters={
                "service_type": "comprehensive",
                "current_cost": 10000.0
            }
        )
        
        assert optimization_result.success == True
        optimization_data = optimization_result.result
        business_value_metrics["optimizations_identified"] += len(optimization_data.get("optimizations", []))
        business_value_metrics["total_potential_savings"] += optimization_data.get("total_monthly_savings", 0)
        
        # Step 3: Generate comprehensive report
        report_result = await dispatcher.execute_tool(
            tool_name="mock_report_generator",
            parameters={
                "report_type": "executive_summary",
                "data_range": "comprehensive"
            }
        )
        
        assert report_result.success == True
        assert isinstance(report_result.result, str)
        assert len(report_result.result) > 100  # Substantial report content
        business_value_metrics["reports_created"] += 1
        
        # Validate business value thresholds
        assert business_value_metrics["total_potential_savings"] > 0, "No cost savings identified"
        assert business_value_metrics["insights_generated"] >= 3, "Insufficient insights generated"
        assert business_value_metrics["optimizations_identified"] >= 1, "No optimizations identified"
        assert business_value_metrics["reports_created"] >= 1, "No reports generated"
        
        # Validate workflow completion via WebSocket events
        await asyncio.sleep(1.0)
        user_events = self.event_collector.get_events_for_user(user_context.user_id)
        
        # Should have events for all 3 tool executions
        executing_events = [e for e in user_events if e.get("type") == "tool_executing"]
        completed_events = [e for e in user_events if e.get("type") == "tool_completed"]
        
        assert len(executing_events) >= 3, f"Missing tool execution events: {len(executing_events)}/3"
        assert len(completed_events) >= 3, f"Missing tool completion events: {len(completed_events)}/3"
        
        # Validate business value delivery in events
        for event in completed_events:
            assert event.get("user_id") == user_context.user_id
            assert "result" in event
            
        print(f" PASS:  Business value delivery validated:")
        print(f"   - Total potential savings: ${business_value_metrics['total_potential_savings']:,.2f}/month")
        print(f"   - Insights generated: {business_value_metrics['insights_generated']}")
        print(f"   - Optimizations identified: {business_value_metrics['optimizations_identified']}")
        print(f"   - Reports created: {business_value_metrics['reports_created']}")
        print(f"   - WebSocket events: {len(executing_events)} executing, {len(completed_events)} completed")
        
    # ===================== FACTORY PATTERN COMPLIANCE TESTS =====================
    
    @pytest.mark.e2e
    @pytest.mark.real_services  
    async def test_factory_pattern_compliance(self):
        """Test that UnifiedToolDispatcher enforces factory pattern usage."""
        # Test 1: Direct instantiation should fail
        with pytest.raises(RuntimeError, match="Direct instantiation.*forbidden"):
            dispatcher = UnifiedToolDispatcher()
            
        # Test 2: Factory creation should succeed  
        user_context = await self.create_user_context()
        websocket_bridge = await self.create_websocket_bridge_for_user(user_context)
        
        dispatcher = UnifiedToolDispatcherFactory.create_for_request(
            user_context=user_context,
            websocket_manager=self.websocket_manager
        )
        self.created_dispatchers.append(dispatcher)
        
        assert dispatcher is not None
        assert dispatcher.user_context == user_context
        
        # Test 3: AgentRegistry factory should succeed
        registry_dispatcher = await self.agent_registry.create_tool_dispatcher_for_user(
            user_context=user_context,
            websocket_bridge=websocket_bridge
        )
        self.created_dispatchers.append(registry_dispatcher)
        
        assert registry_dispatcher is not None
        assert registry_dispatcher.user_context == user_context
        
        print(f" PASS:  Factory pattern compliance enforced and validated")
        
    @pytest.mark.e2e
    @pytest.mark.real_services
    async def test_concurrent_user_isolation_stress_test(self):
        """Stress test user isolation with high concurrent load."""
        num_concurrent_users = min(self.config.MAX_CONCURRENT_USERS, 8)  # Limited for test stability
        
        async def create_and_test_user(user_index: int) -> Dict[str, Any]:
            """Create user and run isolated test."""
            try:
                # Create isolated user context
                user_context = await self.create_user_context(f"stress_user_{user_index}")
                websocket_bridge = await self.create_websocket_bridge_for_user(user_context)
                
                dispatcher = await UnifiedToolDispatcher.create_for_user(
                    user_context=user_context,
                    websocket_bridge=websocket_bridge,
                    enable_admin_tools=False
                )
                self.created_dispatchers.append(dispatcher)
                
                # Register test tool
                from langchain_core.tools import Tool
                stress_tool = Tool(
                    name="stress_test_tool",
                    description="Tool for stress testing",
                    func=self.test_tools["mock_data_analyzer"].run
                )
                dispatcher.register_tool(stress_tool)
                
                # Execute multiple operations for this user
                results = []
                for i in range(3):
                    result = await dispatcher.execute_tool(
                        tool_name="stress_test_tool",
                        parameters={
                            "data_source": f"stress_user_{user_index}_operation_{i}",
                            "analysis_type": "stress_test"
                        }
                    )
                    results.append(result)
                
                return {
                    "user_index": user_index,
                    "user_id": user_context.user_id,
                    "results": results,
                    "success": all(r.success for r in results),
                    "dispatcher_id": dispatcher.dispatcher_id
                }
                
            except Exception as e:
                return {
                    "user_index": user_index,
                    "error": str(e),
                    "success": False
                }
        
        # Run concurrent stress test
        tasks = []
        for i in range(num_concurrent_users):
            task = asyncio.create_task(create_and_test_user(i))
            tasks.append(task)
        
        # Execute all tasks concurrently
        user_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Validate results
        successful_users = 0
        total_operations = 0
        successful_operations = 0
        
        for result in user_results:
            if isinstance(result, Exception):
                print(f"User task failed: {result}")
                continue
                
            if result.get("success"):
                successful_users += 1
                user_operations = len(result.get("results", []))
                total_operations += user_operations
                successful_operations += sum(1 for r in result.get("results", []) if r.success)
        
        # Performance assertions
        assert successful_users >= num_concurrent_users * 0.9, f"Too many user failures: {successful_users}/{num_concurrent_users}"
        assert successful_operations >= total_operations * 0.95, f"Too many operation failures: {successful_operations}/{total_operations}"
        
        # Validate isolation - check WebSocket events are properly separated
        await asyncio.sleep(2.0)  # Allow all events to propagate
        
        user_event_counts = {}
        for result in user_results:
            if result.get("success") and not isinstance(result, Exception):
                user_id = result.get("user_id")
                if user_id:
                    user_events = self.event_collector.get_events_for_user(user_id)
                    user_event_counts[user_id] = len(user_events)
        
        # Each user should have their own events (isolation validation)
        isolated_users = len(user_event_counts)
        assert isolated_users >= successful_users * 0.8, f"Event isolation failure: {isolated_users}/{successful_users}"
        
        print(f" PASS:  Concurrent user isolation stress test passed:")
        print(f"   - Concurrent users: {num_concurrent_users}")
        print(f"   - Successful users: {successful_users}")
        print(f"   - Total operations: {total_operations}")
        print(f"   - Successful operations: {successful_operations}")
        print(f"   - Isolated event streams: {isolated_users}")


if __name__ == "__main__":
    """Run comprehensive E2E tests for UnifiedToolDispatcher."""
    # Add required markers for pytest
    pytest.main([
        __file__,
        "-v",
        "--tb=short", 
        "-m", "e2e and real_services",
        "--maxfail=1"  # Stop on first failure for faster feedback
    ])
