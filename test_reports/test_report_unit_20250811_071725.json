{
  "timestamp": "2025-08-11T07:17:25.213893",
  "test_level": "unit",
  "configuration": {
    "description": "Unit tests for isolated components (1-2 minutes)",
    "purpose": "Development validation, component testing",
    "backend_args": [
      "--category",
      "unit",
      "-v"
    ],
    "frontend_args": [
      "--category",
      "unit"
    ],
    "timeout": 120,
    "run_coverage": false,
    "run_both": true
  },
  "results": {
    "backend": {
      "status": "failed",
      "duration": 24.06718897819519,
      "exit_code": 1,
      "output": "================================================================================\nNETRA AI PLATFORM - BACKEND TEST RUNNER\n================================================================================\nTest Configuration:\n  Category: unit\n  Parallel: disabled\n  Coverage: disabled\n  Fail Fast: disabled\n  Environment: testing\n\nRunning command:\n  pytest app/tests/services app/tests/core -vv --tb=short --asyncio-mode=auto --color=yes --strict-markers --disable-warnings -p no:warnings\n================================================================================\n\u001b[1m============================= test session starts =============================\u001b[0m\nplatform win32 -- Python 3.12.4, pytest-8.4.1, pluggy-1.6.0 -- C:\\Users\\antho\\miniconda3\\python.exe\ncachedir: .pytest_cache\nmetadata: {'Python': '3.12.4', 'Platform': 'Windows-11-10.0.26100-SP0', 'Packages': {'pytest': '8.4.1', 'pluggy': '1.6.0'}, 'Plugins': {'anyio': '4.9.0', 'Faker': '37.4.2', 'langsmith': '0.4.10', 'asyncio': '0.21.1', 'cov': '6.2.1', 'html': '4.1.1', 'json-report': '1.5.0', 'metadata': '3.1.1', 'mock': '3.14.1', 'timeout': '2.4.0', 'xdist': '3.8.0', 'typeguard': '4.4.4'}}\nrootdir: C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, Faker-37.4.2, langsmith-0.4.10, asyncio-0.21.1, cov-6.2.1, html-4.1.1, json-report-1.5.0, metadata-3.1.1, mock-3.14.1, timeout-2.4.0, xdist-3.8.0, typeguard-4.4.4\nasyncio: mode=Mode.AUTO\n\u001b[1mcollecting ... \u001b[0mcollected 533 items\n\napp\\tests\\services\\agents\\test_sub_agent.py::test_agent_node_is_coroutine \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\napp\\tests\\services\\agents\\test_supervisor_service.py::test_supervisor_end_to_end \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\napp\\tests\\services\\agents\\test_tools.py::test_tool_dispatcher \u001b[32mPASSED\u001b[0m\u001b[32m     [  0%]\u001b[0m\napp\\tests\\services\\agents\\test_tools.py::test_tool_dispatcher_tool_not_found \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\napp\\tests\\services\\agents\\test_tools.py::test_tool_dispatcher_tool_error \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\test_tool_builder.py::test_tool_builder_and_dispatcher \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_advanced_optimization_for_core_function.py::test_advanced_optimization_for_core_function_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_advanced_optimization_for_core_function.py \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestToolMetadata::test_tool_metadata_creation_basic \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestToolMetadata::test_tool_metadata_creation_full \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestToolMetadata::test_tool_metadata_validation_missing_required \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestToolMetadata::test_tool_metadata_dict_conversion \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestToolMetadata::test_tool_metadata_json_serialization \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestToolMetadata::test_tool_metadata_edge_cases \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_instantiation \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_get_metadata \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_execute_wrapper \u001b[31mERROR\u001b[0m\u001b[31m [  3%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_execute_failure \u001b[32mPASSED\u001b[0m\u001b[31m [  3%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_concrete_tool_run_method \u001b[32mPASSED\u001b[0m\u001b[31m [  3%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_with_llm_name \u001b[32mPASSED\u001b[0m\u001b[31m [  3%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_multiple_executions \u001b[32mPASSED\u001b[0m\u001b[31m [  3%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_concurrent_execution \u001b[32mPASSED\u001b[0m\u001b[31m [  3%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_execute_without_metadata_attribute \u001b[31mFAILED\u001b[0m\u001b[31m [  4%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_inheritance_chain \u001b[32mPASSED\u001b[0m\u001b[31m [  4%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_with_complex_kwargs \u001b[32mPASSED\u001b[0m\u001b[31m [  4%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_exception_types \u001b[31mFAILED\u001b[0m\u001b[31m [  4%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_async_delay \u001b[32mPASSED\u001b[0m\u001b[31m [  4%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_basic_functionality \u001b[32mPASSED\u001b[0m\u001b[31m [  5%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_empty_logs \u001b[32mPASSED\u001b[0m\u001b[31m [  5%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_single_log \u001b[32mPASSED\u001b[0m\u001b[31m [  5%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_large_costs \u001b[32mPASSED\u001b[0m\u001b[31m [  5%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_zero_costs \u001b[32mPASSED\u001b[0m\u001b[31m [  5%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_fractional_cents \u001b[32mPASSED\u001b[0m\u001b[31m [  6%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_exception_handling \u001b[32mPASSED\u001b[0m\u001b[31m [  6%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_async_execution \u001b[31mFAILED\u001b[0m\u001b[31m [  6%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_concurrent_logs \u001b[32mPASSED\u001b[0m\u001b[31m [  6%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_negative_costs \u001b[32mPASSED\u001b[0m\u001b[31m [  6%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_mixed_model_types \u001b[32mPASSED\u001b[0m\u001b[31m [  6%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_partial_failure \u001b[32mPASSED\u001b[0m\u001b[31m [  7%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_rounding_edge_cases \u001b[31mFAILED\u001b[0m\u001b[31m [  7%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_reduction_quality_preservation.py::test_cost_reduction_quality_preservation_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_reduction_quality_preservation.py \u001b[32mPASSED\u001b[0m\u001b[31m [  7%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_simulation_for_increased_usage.py::test_cost_simulation_for_increased_usage_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_simulation_for_increased_usage.py \u001b[32mPASSED\u001b[0m\u001b[31m [  7%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_kv_cache_optimization_audit.py::test_kv_cache_optimization_audit_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_kv_cache_optimization_audit.py \u001b[32mPASSED\u001b[0m\u001b[31m [  7%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_basic_functionality \u001b[31mERROR\u001b[0m\u001b[31m [  8%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_empty_logs \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_single_log \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_high_latency_values \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_zero_latency \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_sub_millisecond_latency \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_exception_handling \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_async_execution \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_large_dataset \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_varied_latencies \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_partial_failure \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_edge_case_rounding \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_negative_latencies \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_mixed_response_formats \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_extreme_values \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_concurrent_execution_timing \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_multi_objective_optimization.py::test_multi_objective_optimization_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_multi_objective_optimization.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 11%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_new_model_effectiveness_analysis.py::test_new_model_effectiveness_analysis_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_new_model_effectiveness_analysis.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 11%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_tool_latency_optimization.py::test_tool_latency_optimization_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_tool_latency_optimization.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 11%]\u001b[0m\napp\\tests\\services\\test_agent_message_processing.py::TestAgentMessageProcessing::test_process_user_message \u001b[31mERROR\u001b[0m\u001b[31m [ 11%]\u001b[0m\napp\\tests\\services\\test_agent_message_processing.py::TestAgentMessageProcessing::test_handle_tool_execution \u001b[31mFAILED\u001b[0m\u001b[31m [ 11%]\u001b[0m\napp\\tests\\services\\test_agent_message_processing.py::TestAgentMessageProcessing::test_message_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 12%]\u001b[0m\napp\\tests\\services\\test_agent_service.py::test_run_agent \u001b[32mPASSED\u001b[0m\u001b[31m          [ 12%]\u001b[0m\napp\\tests\\services\\test_agent_service_advanced.py::test_agent_service_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 12%]\u001b[0m\napp\\tests\\services\\test_agent_service_advanced.py::test_agent_service_process_request \u001b[32mPASSED\u001b[0m\u001b[31m [ 12%]\u001b[0m\napp\\tests\\services\\test_agent_service_advanced.py::test_agent_service_websocket_message_handling \u001b[32mPASSED\u001b[0m\u001b[31m [ 12%]\u001b[0m\napp\\tests\\services\\test_clickhouse_service.py::TestClickHouseConnection::test_client_initialization \u001b[31mERROR\u001b[0m\u001b[31m [ 12%]\u001b[0m\napp\\tests\\services\\test_clickhouse_service.py::TestClickHouseConnection::test_list_corpus_tables \u001b[32mPASSED\u001b[0m\u001b[31m [ 13%]\u001b[0m\napp\\tests\\services\\test_clickhouse_service.py::TestClickHouseConnection::test_basic_query_execution \u001b[31mERROR\u001b[0m\u001b[31m [ 13%]\u001b[0m\napp\\tests\\services\\test_clickhouse_service.py::TestClickHouseConnection::test_query_with_parameters \u001b[31mERROR\u001b[0m\u001b[31m [ 13%]\u001b[0m\napp\\tests\\services\\test_clickhouse_service.py::TestBasicOperations::test_show_tables \u001b[31mERROR\u001b[0m\u001b[31m [ 13%]\u001b[0m\napp\\tests\\services\\test_corpus_service.py::TestCorpusService::test_corpus_status_enum \u001b[32mPASSED\u001b[0m\u001b[31m [ 13%]\u001b[0m\napp\\tests\\services\\test_corpus_service.py::TestCorpusService::test_corpus_schema \u001b[32mPASSED\u001b[0m\u001b[31m [ 14%]\u001b[0m\napp\\tests\\services\\test_corpus_service.py::TestCorpusService::test_corpus_create_schema \u001b[32mPASSED\u001b[0m\u001b[31m [ 14%]\u001b[0m\napp\\tests\\services\\test_corpus_service.py::TestCorpusService::test_corpus_service_import \u001b[32mPASSED\u001b[0m\u001b[31m [ 14%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestUnitOfWork::test_uow_initialization \u001b[31mERROR\u001b[0m\u001b[31m [ 14%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestUnitOfWork::test_uow_transaction_commit \u001b[31mERROR\u001b[0m\u001b[31m [ 14%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestUnitOfWork::test_uow_transaction_rollback \u001b[31mERROR\u001b[0m\u001b[31m [ 15%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestUnitOfWork::test_uow_nested_transactions \u001b[31mERROR\u001b[0m\u001b[31m [ 15%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestUnitOfWork::test_uow_concurrent_access \u001b[31mERROR\u001b[0m\u001b[31m [ 15%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_create \u001b[31mERROR\u001b[0m\u001b[31m [ 15%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_bulk_create \u001b[31mERROR\u001b[0m\u001b[31m [ 15%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_get_by_id \u001b[31mERROR\u001b[0m\u001b[31m [ 15%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_get_many \u001b[31mERROR\u001b[0m\u001b[31m [ 16%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_update \u001b[31mERROR\u001b[0m\u001b[31m [ 16%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_delete \u001b[31mERROR\u001b[0m\u001b[31m [ 16%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_soft_delete \u001b[31mERROR\u001b[0m\u001b[31m [ 16%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_pagination \u001b[31mERROR\u001b[0m\u001b[31m [ 16%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestMessageRepository::test_get_messages_by_thread \u001b[31mERROR\u001b[0m\u001b[31m [ 17%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestMessageRepository::test_get_messages_with_pagination \u001b[31mERROR\u001b[0m\u001b[31m [ 17%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestMessageRepository::test_get_latest_messages \u001b[31mERROR\u001b[0m\u001b[31m [ 17%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestThreadRepository::test_get_threads_by_user \u001b[31mERROR\u001b[0m\u001b[31m [ 17%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestThreadRepository::test_get_active_threads \u001b[31mERROR\u001b[0m\u001b[31m [ 17%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestThreadRepository::test_archive_thread \u001b[31mERROR\u001b[0m\u001b[31m [ 18%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestRunRepository::test_create_run_with_tools \u001b[31mERROR\u001b[0m\u001b[31m [ 18%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestRunRepository::test_update_run_status \u001b[31mERROR\u001b[0m\u001b[31m [ 18%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestRunRepository::test_get_active_runs \u001b[31mERROR\u001b[0m\u001b[31m [ 18%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestReferenceRepository::test_create_reference_with_metadata \u001b[31mERROR\u001b[0m\u001b[31m [ 18%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestReferenceRepository::test_get_references_by_message \u001b[31mERROR\u001b[0m\u001b[31m [ 18%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestReferenceRepository::test_search_references \u001b[31mERROR\u001b[0m\u001b[31m [ 19%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_process_demo_chat_new_session \u001b[32mPASSED\u001b[0m\u001b[31m [ 19%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_process_demo_chat_existing_session \u001b[32mPASSED\u001b[0m\u001b[31m [ 19%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_get_industry_templates_valid \u001b[32mPASSED\u001b[0m\u001b[31m [ 19%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_get_industry_templates_invalid \u001b[32mPASSED\u001b[0m\u001b[31m [ 19%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_calculate_roi_financial \u001b[32mPASSED\u001b[0m\u001b[31m [ 20%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_calculate_roi_different_industries \u001b[32mPASSED\u001b[0m\u001b[31m [ 20%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_generate_synthetic_metrics \u001b[32mPASSED\u001b[0m\u001b[31m [ 20%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_generate_report \u001b[32mPASSED\u001b[0m\u001b[31m [ 20%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_generate_report_session_not_found \u001b[32mPASSED\u001b[0m\u001b[31m [ 20%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_get_session_status \u001b[32mPASSED\u001b[0m\u001b[31m [ 21%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_get_session_status_not_found \u001b[32mPASSED\u001b[0m\u001b[31m [ 21%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_submit_feedback \u001b[32mPASSED\u001b[0m\u001b[31m [ 21%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_track_demo_interaction \u001b[32mPASSED\u001b[0m\u001b[31m [ 21%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_get_analytics_summary \u001b[32mPASSED\u001b[0m\u001b[31m [ 21%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_generate_demo_response \u001b[32mPASSED\u001b[0m\u001b[31m [ 21%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_error_handling_redis_failure \u001b[32mPASSED\u001b[0m\u001b[31m [ 22%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_optimization_fallback_low_quality \u001b[31mFAILED\u001b[0m\u001b[31m [ 22%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_data_analysis_fallback_parsing_error \u001b[31mFAILED\u001b[0m\u001b[31m [ 22%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_action_plan_fallback_context_missing \u001b[31mFAILED\u001b[0m\u001b[31m [ 22%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_report_fallback_validation_failed \u001b[31mFAILED\u001b[0m\u001b[31m [ 22%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_triage_fallback_timeout \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_error_message_fallback_llm_error \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_fallback_with_circular_reasoning \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_fallback_with_hallucination_risk \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_fallback_with_rate_limit \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_fallback_considers_retry_count \u001b[31mFAILED\u001b[0m\u001b[31m [ 24%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_fallback_includes_diagnostic_tips \u001b[31mFAILED\u001b[0m\u001b[31m [ 24%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_fallback_with_previous_responses \u001b[31mFAILED\u001b[0m\u001b[31m [ 24%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_format_response_with_placeholders \u001b[31mFAILED\u001b[0m\u001b[31m [ 24%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_get_diagnostic_tips_for_failure_reason \u001b[32mPASSED\u001b[0m\u001b[31m [ 24%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_get_recovery_suggestions \u001b[31mFAILED\u001b[0m\u001b[31m [ 24%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_fallback_response_quality \u001b[31mFAILED\u001b[0m\u001b[31m [ 25%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_fallback_service_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 25%]\u001b[0m\napp\\tests\\services\\test_generation_service.py::test_update_job_status <- ..\\v2\\app\\tests\\services\\test_generation_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 25%]\u001b[0m\napp\\tests\\services\\test_generation_service.py::test_get_corpus_from_clickhouse <- ..\\v2\\app\\tests\\services\\test_generation_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 25%]\u001b[0m\napp\\tests\\services\\test_generation_service.py::test_save_corpus_to_clickhouse <- ..\\v2\\app\\tests\\services\\test_generation_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 25%]\u001b[0m\napp\\tests\\services\\test_job_store_service.py::TestJobStore::test_job_store_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 26%]\u001b[0m\napp\\tests\\services\\test_job_store_service.py::TestJobStore::test_set_and_get_job \u001b[32mPASSED\u001b[0m\u001b[31m [ 26%]\u001b[0m\napp\\tests\\services\\test_job_store_service.py::TestJobStore::test_update_job_status \u001b[32mPASSED\u001b[0m\u001b[31m [ 26%]\u001b[0m\napp\\tests\\services\\test_job_store_service.py::TestJobStore::test_nonexistent_job \u001b[32mPASSED\u001b[0m\u001b[31m [ 26%]\u001b[0m\napp\\tests\\services\\test_job_store_service.py::TestJobStore::test_global_job_store \u001b[32mPASSED\u001b[0m\u001b[31m [ 26%]\u001b[0m\napp\\tests\\services\\test_key_manager.py::test_load_from_settings_success <- ..\\v2\\app\\tests\\services\\test_key_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 27%]\u001b[0m\napp\\tests\\services\\test_key_manager.py::test_load_from_settings_jwt_key_too_short <- ..\\v2\\app\\tests\\services\\test_key_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 27%]\u001b[0m\napp\\tests\\services\\test_llm_cache_service.py::test_llm_cache_service_initialization \u001b[31mFAILED\u001b[0m\u001b[31m [ 27%]\u001b[0m\napp\\tests\\services\\test_llm_cache_service.py::test_cache_set_and_get \u001b[31mFAILED\u001b[0m\u001b[31m [ 27%]\u001b[0m\napp\\tests\\services\\test_llm_cache_service.py::test_cache_expiration \u001b[31mFAILED\u001b[0m\u001b[31m [ 27%]\u001b[0m\napp\\tests\\services\\test_llm_cache_service.py::test_cache_size_limit \u001b[31mFAILED\u001b[0m\u001b[31m [ 27%]\u001b[0m\napp\\tests\\services\\test_llm_cache_service.py::test_cache_stats \u001b[31mFAILED\u001b[0m\u001b[31m    [ 28%]\u001b[0m\napp\\tests\\services\\test_message_handlers.py::test_message_handler_service_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 28%]\u001b[0m\napp\\tests\\services\\test_message_handlers.py::test_handle_start_agent \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\napp\\tests\\services\\test_message_handlers.py::test_websocket_schema_imports \u001b[32mPASSED\u001b[0m\u001b[31m [ 28%]\u001b[0m\napp\\tests\\services\\test_overall_supervisor.py::test_overall_supervisor_workflow \u001b[32mPASSED\u001b[0m\u001b[31m [ 28%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestPermissionServiceConstants::test_role_hierarchy_ordering \u001b[32mPASSED\u001b[0m\u001b[31m [ 29%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestPermissionServiceConstants::test_role_permissions_inheritance \u001b[32mPASSED\u001b[0m\u001b[31m [ 29%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestPermissionServiceConstants::test_critical_permissions_restricted \u001b[32mPASSED\u001b[0m\u001b[31m [ 29%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestDetectDeveloperStatus::test_detect_developer_with_dev_mode_env \u001b[32mPASSED\u001b[0m\u001b[31m [ 29%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestDetectDeveloperStatus::test_detect_developer_with_netra_email \u001b[32mPASSED\u001b[0m\u001b[31m [ 29%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestDetectDeveloperStatus::test_detect_developer_with_dev_environment \u001b[32mPASSED\u001b[0m\u001b[31m [ 30%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestDetectDeveloperStatus::test_detect_developer_priority_order \u001b[32mPASSED\u001b[0m\u001b[31m [ 30%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestDetectDeveloperStatus::test_detect_developer_with_none_email \u001b[32mPASSED\u001b[0m\u001b[31m [ 30%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestUpdateUserRole::test_update_user_role_auto_elevate_to_developer \u001b[32mPASSED\u001b[0m\u001b[31m [ 30%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestUpdateUserRole::test_update_user_role_no_elevation_for_admin \u001b[32mPASSED\u001b[0m\u001b[31m [ 30%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestUpdateUserRole::test_update_user_role_no_elevation_for_super_admin \u001b[32mPASSED\u001b[0m\u001b[31m [ 30%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestUpdateUserRole::test_update_user_role_skip_developer_check \u001b[32mPASSED\u001b[0m\u001b[31m [ 31%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestUpdateUserRole::test_update_user_role_already_developer \u001b[32mPASSED\u001b[0m\u001b[31m [ 31%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestUpdateUserRole::test_update_user_role_power_user_elevation \u001b[32mPASSED\u001b[0m\u001b[31m [ 31%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestCheckPermission::test_check_permission_standard_user \u001b[32mPASSED\u001b[0m\u001b[31m [ 31%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestCheckPermission::test_check_permission_super_admin \u001b[32mPASSED\u001b[0m\u001b[31m [ 31%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestCheckPermission::test_check_permission_developer \u001b[32mPASSED\u001b[0m\u001b[31m [ 32%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestCheckPermission::test_check_permission_invalid_role \u001b[32mPASSED\u001b[0m\u001b[31m [ 32%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestCheckPermission::test_check_permission_none_role \u001b[32mPASSED\u001b[0m\u001b[31m [ 32%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestGetUserPermissions::test_get_user_permissions_standard_user \u001b[32mPASSED\u001b[0m\u001b[31m [ 32%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestGetUserPermissions::test_get_user_permissions_super_admin \u001b[32mPASSED\u001b[0m\u001b[31m [ 32%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestGetUserPermissions::test_get_user_permissions_invalid_role \u001b[32mPASSED\u001b[0m\u001b[31m [ 33%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestAdminChecks::test_is_admin_or_higher \u001b[32mPASSED\u001b[0m\u001b[31m [ 33%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestAdminChecks::test_is_developer_or_higher \u001b[32mPASSED\u001b[0m\u001b[31m [ 33%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestPermissionGroups::test_has_any_permission \u001b[32mPASSED\u001b[0m\u001b[31m [ 33%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestPermissionGroups::test_has_all_permissions \u001b[32mPASSED\u001b[0m\u001b[31m [ 33%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestSecurityEdgeCases::test_sql_injection_in_permission_check \u001b[32mPASSED\u001b[0m\u001b[31m [ 33%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestSecurityEdgeCases::test_case_sensitivity_in_roles \u001b[32mPASSED\u001b[0m\u001b[31m [ 34%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestSecurityEdgeCases::test_permission_escalation_attempt \u001b[32mPASSED\u001b[0m\u001b[31m [ 34%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestIntegrationScenarios::test_new_user_onboarding_flow \u001b[32mPASSED\u001b[0m\u001b[31m [ 34%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestIntegrationScenarios::test_production_environment_security \u001b[32mPASSED\u001b[0m\u001b[31m [ 34%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_high_quality_optimization_content \u001b[31mFAILED\u001b[0m\u001b[31m [ 34%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_low_quality_generic_content \u001b[32mPASSED\u001b[0m\u001b[31m [ 35%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_detect_circular_reasoning \u001b[32mPASSED\u001b[0m\u001b[31m [ 35%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_data_analysis_with_metrics \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_action_plan_completeness \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_with_strict_mode \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_error_message_clarity \u001b[31mFAILED\u001b[0m\u001b[31m [ 36%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_report_redundancy \u001b[31mFAILED\u001b[0m\u001b[31m [ 36%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_domain_specific_term_recognition \u001b[31mFAILED\u001b[0m\u001b[31m [ 36%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_caching_mechanism \u001b[31mFAILED\u001b[0m\u001b[31m [ 36%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_retry_suggestions_for_failed_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 36%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_hallucination_risk_detection \u001b[31mFAILED\u001b[0m\u001b[31m [ 36%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_triage_content_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_context_aware_validation \u001b[32mPASSED\u001b[0m\u001b[31m [ 37%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_quality_level_classification \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityMonitoringServiceInitialization::test_initialization_with_defaults \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityMonitoringServiceInitialization::test_initialization_with_custom_config \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityMonitoringServiceInitialization::test_initialization_with_metrics_collector \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsCollection::test_collect_response_quality_metrics \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsCollection::test_collect_system_metrics \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsCollection::test_collect_error_metrics \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsCollection::test_batch_metrics_collection \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityThresholds::test_set_quality_thresholds \u001b[31mFAILED\u001b[0m\u001b[31m [ 39%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityThresholds::test_validate_against_thresholds \u001b[31mFAILED\u001b[0m\u001b[31m [ 39%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityThresholds::test_dynamic_threshold_adjustment \u001b[31mFAILED\u001b[0m\u001b[31m [ 39%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestAlerting::test_trigger_alert_on_threshold_breach \u001b[31mFAILED\u001b[0m\u001b[31m [ 39%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestAlerting::test_alert_rate_limiting \u001b[31mFAILED\u001b[0m\u001b[31m [ 39%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestAlerting::test_alert_escalation \u001b[31mFAILED\u001b[0m\u001b[31m [ 39%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsAggregation::test_aggregate_metrics_by_time_window \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsAggregation::test_calculate_statistics \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsAggregation::test_trend_analysis \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityReporting::test_generate_quality_report \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityReporting::test_export_metrics_to_json \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityReporting::test_generate_sla_compliance_report \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestAnomalyDetection::test_detect_anomalies_zscore \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestAnomalyDetection::test_detect_anomalies_iqr \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestAnomalyDetection::test_real_time_anomaly_detection \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestPerformanceMonitoring::test_monitor_response_times \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestPerformanceMonitoring::test_monitor_throughput \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestIntegration::test_integration_with_agent_service \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestIntegration::test_integration_with_database \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\napp\\tests\\services\\test_schema_validation_service.py::TestSchemaValidationService::test_validate_schema \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\napp\\tests\\services\\test_schema_validation_service.py::TestSchemaValidationService::test_schema_service_import \u001b[32mPASSED\u001b[0m\u001b[31m [ 42%]\u001b[0m\napp\\tests\\services\\test_schema_validation_service.py::TestSchemaValidationService::test_user_data_schema \u001b[32mPASSED\u001b[0m\u001b[31m [ 42%]\u001b[0m\napp\\tests\\services\\test_schema_validation_service.py::TestSchemaValidationService::test_invalid_user_data \u001b[32mPASSED\u001b[0m\u001b[31m [ 43%]\u001b[0m\napp\\tests\\services\\test_schema_validation_service.py::TestSchemaValidationService::test_websocket_message_schema \u001b[32mPASSED\u001b[0m\u001b[31m [ 43%]\u001b[0m\napp\\tests\\services\\test_schema_validation_service.py::TestSchemaValidationService::test_agent_message_schema \u001b[32mPASSED\u001b[0m\u001b[31m [ 43%]\u001b[0m\napp\\tests\\services\\test_security_service.py::test_encrypt_and_decrypt <- ..\\v2\\app\\tests\\services\\test_security_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 43%]\u001b[0m\napp\\tests\\services\\test_security_service.py::test_create_and_validate_access_token <- ..\\v2\\app\\tests\\services\\test_security_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 43%]\u001b[0m\napp\\tests\\services\\test_security_service.py::test_invalid_token <- ..\\v2\\app\\tests\\services\\test_security_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 44%]\u001b[0m\napp\\tests\\services\\test_state_persistence.py::TestStatePersistence::test_save_agent_state \u001b[31mFAILED\u001b[0m\u001b[31m [ 44%]\u001b[0m\napp\\tests\\services\\test_state_persistence.py::TestStatePersistence::test_restore_agent_state \u001b[31mFAILED\u001b[0m\u001b[31m [ 44%]\u001b[0m\napp\\tests\\services\\test_state_persistence.py::TestStatePersistence::test_cleanup_old_states \u001b[31mFAILED\u001b[0m\u001b[31m [ 44%]\u001b[0m\napp\\tests\\services\\test_state_persistence.py::TestStatePersistence::test_state_versioning \u001b[31mFAILED\u001b[0m\u001b[31m [ 44%]\u001b[0m\napp\\tests\\services\\test_state_persistence.py::TestStatePersistence::test_concurrent_state_updates \u001b[31mFAILED\u001b[0m\u001b[31m [ 45%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_get_all_options \u001b[31mFAILED\u001b[0m\u001b[31m [ 45%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_get_option_by_id \u001b[31mFAILED\u001b[0m\u001b[31m [ 45%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_get_option_by_name \u001b[31mFAILED\u001b[0m\u001b[31m [ 45%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_create_option \u001b[32mPASSED\u001b[0m\u001b[31m [ 45%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_update_option \u001b[32mPASSED\u001b[0m\u001b[31m [ 45%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_delete_option \u001b[32mPASSED\u001b[0m\u001b[31m [ 46%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_delete_option_not_found \u001b[32mPASSED\u001b[0m\u001b[31m [ 46%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_autofill_catalog_empty \u001b[32mPASSED\u001b[0m\u001b[31m [ 46%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_autofill_catalog_existing_data \u001b[32mPASSED\u001b[0m\u001b[31m [ 46%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service.py::TestSyntheticDataService::test_service_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 46%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service.py::TestSyntheticDataService::test_workload_categories_enum \u001b[32mPASSED\u001b[0m\u001b[31m [ 47%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service.py::TestSyntheticDataService::test_generation_status_enum \u001b[32mPASSED\u001b[0m\u001b[31m [ 47%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_creation_with_clickhouse_table \u001b[31mFAILED\u001b[0m\u001b[31m [ 47%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_status_transitions \u001b[31mFAILED\u001b[0m\u001b[31m [ 47%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_content_upload_batch \u001b[31mFAILED\u001b[0m\u001b[31m [ 47%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 48%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_availability_check \u001b[31mFAILED\u001b[0m\u001b[31m [ 48%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_fallback_to_default \u001b[31mFAILED\u001b[0m\u001b[31m [ 48%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_caching_mechanism \u001b[31mFAILED\u001b[0m\u001b[31m [ 48%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_deletion_cascade \u001b[31mFAILED\u001b[0m\u001b[31m [ 48%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_metadata_tracking \u001b[31mFAILED\u001b[0m\u001b[31m [ 48%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_concurrent_access \u001b[31mFAILED\u001b[0m\u001b[31m [ 49%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_workload_distribution_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 49%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_temporal_pattern_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 49%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_tool_invocation_patterns \u001b[31mFAILED\u001b[0m\u001b[31m [ 49%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_error_scenario_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 49%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_trace_hierarchy_generation \u001b[32mPASSED\u001b[0m\u001b[31m [ 50%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_domain_specific_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_statistical_distribution_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_custom_tool_catalog_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_incremental_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_generation_with_corpus_sampling \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_batch_ingestion_to_clickhouse \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_streaming_ingestion_with_backpressure \u001b[32mPASSED\u001b[0m\u001b[31m [ 51%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_ingestion_error_recovery \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_ingestion_deduplication \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_table_creation_on_demand \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_ingestion_metrics_tracking \u001b[32mPASSED\u001b[0m\u001b[31m [ 52%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_parallel_batch_ingestion \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_ingestion_with_transformation \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_ingestion_circuit_breaker \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_ingestion_progress_tracking \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_websocket_connection_management \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-221' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 53%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_generation_progress_broadcast \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-223' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 53%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_batch_completion_notifications \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-225' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 53%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_error_notification_handling \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-227' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 53%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_websocket_reconnection_handling \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-229' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 53%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_multiple_client_subscriptions \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-231' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 54%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_websocket_message_queuing \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-237' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 54%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_websocket_heartbeat \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-239' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 54%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_generation_completion_notification \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-241' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 54%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_websocket_rate_limiting \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-243' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 54%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_schema_validation \u001b[32mPASSED\u001b[0m\u001b[31m [ 54%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_statistical_distribution_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_referential_integrity_validation \u001b[32mPASSED\u001b[0m\u001b[31m [ 55%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_temporal_consistency_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_data_completeness_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_anomaly_detection_in_generated_data \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_correlation_preservation \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_quality_metrics_calculation \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_data_diversity_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_validation_report_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_high_throughput_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_memory_efficient_streaming \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_horizontal_scaling \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask was destroyed but it is pending!\ntask: <Task pending name='Task-266' coro=<TestPerformanceScalability.test_memory_efficient_streaming.<locals>.monitor_memory() running at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\tests\\services\\test_synthetic_data_service_v3.py:1137> wait_for=<Future pending cb=[Task.task_wakeup()]>>\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 57%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_batch_size_optimization \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_connection_pooling_efficiency \u001b[32mPASSED\u001b[0m\u001b[31m [ 57%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_cache_effectiveness \u001b[32mPASSED\u001b[0m\u001b[31m [ 57%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_auto_scaling_behavior \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_resource_limit_handling \u001b[32mPASSED\u001b[0m\u001b[31m [ 58%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_query_optimization \u001b[32mPASSED\u001b[0m\u001b[31m [ 58%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_burst_load_handling \u001b[32mPASSED\u001b[0m\u001b[31m [ 58%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_corpus_unavailable_fallback \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_clickhouse_connection_recovery \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_generation_checkpoint_recovery \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_websocket_disconnect_recovery \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_memory_overflow_handling \u001b[32mPASSED\u001b[0m\u001b[31m [ 59%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_circuit_breaker_operation \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_dead_letter_queue_processing \u001b[32mPASSED\u001b[0m\u001b[31m [ 59%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_transaction_rollback \u001b[32mPASSED\u001b[0m\u001b[31m [ 60%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_idempotent_generation \u001b[32mPASSED\u001b[0m\u001b[31m [ 60%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_graceful_degradation \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_generation_job_monitoring \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_detailed_metrics_dashboard \u001b[32mPASSED\u001b[0m\u001b[31m [ 60%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_corpus_usage_analytics \u001b[32mPASSED\u001b[0m\u001b[31m [ 60%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_audit_log_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_performance_profiling \u001b[32mPASSED\u001b[0m\u001b[31m [ 61%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_alert_configuration \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_job_cancellation_by_admin \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_resource_usage_tracking \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_admin_diagnostic_tools \u001b[32mPASSED\u001b[0m\u001b[31m [ 62%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_batch_job_management \u001b[32mPASSED\u001b[0m\u001b[31m [ 62%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_complete_generation_workflow \u001b[31mERROR\u001b[0m\u001b[31m [ 62%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_multi_tenant_generation \u001b[31mERROR\u001b[0m\u001b[31m [ 62%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_real_time_streaming_pipeline \n\u001b[1m------------------------------- live log setup --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-287' coro=<SyntheticDataService.generate_monitored() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\services\\synthetic_data_service.py:1733> exception=AttributeError(\"'GenerationConfig' object has no attribute 'num_logs'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\services\\synthetic_data_service.py\", line 1742, in generate_monitored\n    record = await self._generate_single_record(config, None, i)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\services\\synthetic_data_service.py\", line 307, in _generate_single_record\n    timestamp = self._generate_timestamp(config, index)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\services\\synthetic_data_service.py\", line 352, in _generate_timestamp\n    hour_offset = (index / config.num_logs) * 24\n                           ^^^^^^^^^^^^^^^\nAttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mERROR\u001b[0m\u001b[31m                                                                    [ 62%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_failure_recovery_integration \u001b[31mERROR\u001b[0m\u001b[31m [ 63%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_cross_component_validation \u001b[31mERROR\u001b[0m\u001b[31m [ 63%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_performance_under_load \u001b[31mERROR\u001b[0m\u001b[31m [ 63%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_data_consistency_verification \u001b[31mERROR\u001b[0m\u001b[31m [ 63%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_monitoring_integration \u001b[31mERROR\u001b[0m\u001b[31m [ 63%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_security_and_access_control \u001b[31mERROR\u001b[0m\u001b[31m [ 63%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_cleanup_and_retention \u001b[31mERROR\u001b[0m\u001b[31m [ 64%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_ml_driven_pattern_generation \u001b[32mPASSED\u001b[0m\u001b[31m [ 64%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_anomaly_injection_strategies \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_cross_correlation_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_temporal_event_sequences \u001b[32mPASSED\u001b[0m\u001b[31m [ 64%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_geo_distributed_simulation \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_adaptive_generation_feedback \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_multi_model_workload_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_compliance_aware_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_cost_optimized_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_versioned_corpus_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 66%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_spec_version_is_3 \u001b[32mPASSED\u001b[0m\u001b[31m [ 66%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_enhanced_admin_visibility_section_exists \u001b[32mPASSED\u001b[0m\u001b[31m [ 66%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_data_agent_integration_section_exists \u001b[32mPASSED\u001b[0m\u001b[31m [ 66%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_log_structure_coherence_section_exists \u001b[32mPASSED\u001b[0m\u001b[31m [ 66%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_comprehensive_testing_framework_defined \u001b[32mPASSED\u001b[0m\u001b[31m [ 66%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_version_3_improvements_documented \u001b[32mPASSED\u001b[0m\u001b[31m [ 67%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_admin_prompts_categories_defined \u001b[32mPASSED\u001b[0m\u001b[31m [ 67%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_clustering_algorithms_defined \u001b[32mPASSED\u001b[0m\u001b[31m [ 67%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_alerting_rules_defined \u001b[32mPASSED\u001b[0m\u001b[31m [ 67%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_migration_guide_present \u001b[32mPASSED\u001b[0m\u001b[31m [ 67%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_workload_categories_match_spec \u001b[32mPASSED\u001b[0m\u001b[31m [ 68%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_generation_status_enum_matches_spec \u001b[32mPASSED\u001b[0m\u001b[31m [ 68%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_corpus_service_exists \u001b[32mPASSED\u001b[0m\u001b[31m [ 68%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_websocket_manager_exists \u001b[32mPASSED\u001b[0m\u001b[31m [ 68%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_synthetic_data_service_has_required_methods \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_clickhouse_integration_configured \u001b[32mPASSED\u001b[0m\u001b[31m [ 69%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_agent_tools_available \u001b[32mPASSED\u001b[0m\u001b[31m [ 69%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_data_clustering_tool_exists \u001b[32mPASSED\u001b[0m\u001b[31m [ 69%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_test_file_created \u001b[32mPASSED\u001b[0m\u001b[31m [ 69%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_spec_xml_is_valid \u001b[32mPASSED\u001b[0m\u001b[31m [ 69%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_corpus_lifecycle_states \u001b[32mPASSED\u001b[0m\u001b[31m [ 69%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_workload_distribution_calculation \u001b[32mPASSED\u001b[0m\u001b[31m [ 70%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_batch_size_optimization_range \u001b[32mPASSED\u001b[0m\u001b[31m [ 70%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_websocket_message_types \u001b[32mPASSED\u001b[0m\u001b[31m [ 70%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_quality_metrics_calculation \u001b[32mPASSED\u001b[0m\u001b[31m [ 70%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_circuit_breaker_configuration \u001b[32mPASSED\u001b[0m\u001b[31m [ 70%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_ingestion_rate_targets \u001b[32mPASSED\u001b[0m\u001b[31m [ 71%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_temporal_pattern_types \u001b[32mPASSED\u001b[0m\u001b[31m [ 71%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_tool_catalog_structure \u001b[32mPASSED\u001b[0m\u001b[31m [ 71%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_admin_dashboard_update_frequency \u001b[32mPASSED\u001b[0m\u001b[31m [ 71%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestGetOrCreateThread::test_get_existing_thread_success \u001b[32mPASSED\u001b[0m\u001b[31m [ 71%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestGetOrCreateThread::test_create_new_thread_success \u001b[32mPASSED\u001b[0m\u001b[31m [ 72%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestGetOrCreateThread::test_handle_integrity_error_race_condition \u001b[32mPASSED\u001b[0m\u001b[31m [ 72%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestGetOrCreateThread::test_handle_sqlalchemy_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 72%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestGetOrCreateThread::test_handle_unexpected_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 72%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestCreateMessage::test_create_message_success_minimal \u001b[32mPASSED\u001b[0m\u001b[31m [ 72%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestCreateMessage::test_create_message_success_full_params \u001b[32mPASSED\u001b[0m\u001b[31m [ 72%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestCreateMessage::test_create_message_sqlalchemy_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 73%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestCreateMessage::test_create_message_unexpected_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 73%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestGetThreadMessages::test_get_thread_messages_success \u001b[32mPASSED\u001b[0m\u001b[31m [ 73%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestGetThreadMessages::test_get_thread_messages_with_limit \u001b[32mPASSED\u001b[0m\u001b[31m [ 73%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestGetThreadMessages::test_get_thread_messages_empty_thread \u001b[32mPASSED\u001b[0m\u001b[31m [ 73%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestCreateRun::test_create_run_success_minimal \u001b[32mPASSED\u001b[0m\u001b[31m [ 74%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestCreateRun::test_create_run_success_full_params \u001b[32mPASSED\u001b[0m\u001b[31m [ 74%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestUpdateRunStatus::test_update_run_status_to_completed \u001b[32mPASSED\u001b[0m\u001b[31m [ 74%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestUpdateRunStatus::test_update_run_status_to_failed_with_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 74%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestUpdateRunStatus::test_update_run_status_to_in_progress \u001b[32mPASSED\u001b[0m\u001b[31m [ 74%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestUpdateRunStatus::test_update_run_status_run_not_found \u001b[32mPASSED\u001b[0m\u001b[31m [ 75%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestThreadServiceIntegration::test_complete_conversation_flow \u001b[32mPASSED\u001b[0m\u001b[31m [ 75%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::TestThreadServiceIntegration::test_error_handling_workflow \u001b[32mPASSED\u001b[0m\u001b[31m [ 75%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_tool_registration \u001b[32mPASSED\u001b[0m\u001b[31m [ 75%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_tool_execution_with_validation \u001b[32mPASSED\u001b[0m\u001b[31m [ 75%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_tool_chain_execution \u001b[32mPASSED\u001b[0m\u001b[31m [ 75%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_tool_error_handling \u001b[32mPASSED\u001b[0m\u001b[31m [ 76%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_unknown_tool_handling \u001b[32mPASSED\u001b[0m\u001b[31m [ 76%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_concurrent_tool_execution \u001b[32mPASSED\u001b[0m\u001b[31m [ 76%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_tool_metadata_tracking \u001b[32mPASSED\u001b[0m\u001b[31m [ 76%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_create_user \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\n(trapped) error reading bcrypt version\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\passlib\\handlers\\bcrypt.py\", line 620, in _load_backend_mixin\n    version = _bcrypt.__about__.__version__\n              ^^^^^^^^^^^^^^^^^\nAttributeError: module 'bcrypt' has no attribute '__about__'\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 76%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_get_user_by_email \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_get_user_by_email_not_found \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_update_user \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_delete_user \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_get_user_by_id \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_get_multiple_users \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_password_hashing_security \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_create_user_with_duplicate_email \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_user_service_singleton \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestSecretManager::test_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestSecretManager::test_load_from_environment \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestSecretManager::test_secret_manager_client_creation_success \u001b[32mPASSED\u001b[0m\u001b[31m [ 79%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestSecretManager::test_secret_manager_client_creation_failure \u001b[32mPASSED\u001b[0m\u001b[31m [ 79%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestSecretManager::test_load_from_secret_manager_success \u001b[32mPASSED\u001b[0m\u001b[31m [ 79%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestSecretManager::test_load_secrets_fallback_to_env \u001b[32mPASSED\u001b[0m\u001b[31m [ 79%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 79%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_validate_valid_config \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nLLM configuration warnings: LLM 'default' is missing API key, LLM 'analysis' is missing API key, LLM 'triage' is missing API key, LLM 'data' is missing API key, LLM 'optimizations_core' is missing API key, LLM 'actions_to_meet_goals' is missing API key, LLM 'reporting' is missing API key\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 80%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_validate_database_config_missing_url \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration validation failed: Database configuration errors: Database URL is not configured\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 80%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_validate_database_config_invalid_url \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration validation failed: Database configuration errors: Database URL must be a PostgreSQL connection string\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 80%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_validate_auth_config_missing_jwt_secret \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration validation failed: Authentication configuration errors: JWT secret key is not configured\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 80%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_validate_auth_config_production_dev_secret \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration validation failed: Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 80%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_get_validation_report_success \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nLLM configuration warnings: LLM 'default' is missing API key, LLM 'analysis' is missing API key, LLM 'triage' is missing API key, LLM 'data' is missing API key, LLM 'optimizations_core' is missing API key, LLM 'actions_to_meet_goals' is missing API key, LLM 'reporting' is missing API key\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 81%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_get_validation_report_failure \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration validation failed: Database configuration errors: Database URL is not configured\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 81%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 81%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_get_environment_development \u001b[32mPASSED\u001b[0m\u001b[31m [ 81%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_get_environment_testing \u001b[32mPASSED\u001b[0m\u001b[31m [ 81%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_get_environment_production \u001b[32mPASSED\u001b[0m\u001b[31m [ 81%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_create_base_config_development \u001b[32mPASSED\u001b[0m\u001b[31m [ 82%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_create_base_config_unknown_defaults_to_dev \u001b[32mPASSED\u001b[0m\u001b[31m [ 82%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_load_configuration_success \u001b[32mPASSED\u001b[0m\u001b[31m [ 82%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_load_configuration_validation_failure \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration loading failed: Validation failed\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 82%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_get_config_caching \u001b[32mPASSED\u001b[0m\u001b[31m [ 82%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_reload_config \u001b[32mPASSED\u001b[0m\u001b[31m [ 83%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigurationFunctions::test_get_config \u001b[32mPASSED\u001b[0m\u001b[31m [ 83%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigurationFunctions::test_reload_config \u001b[32mPASSED\u001b[0m\u001b[31m [ 83%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigurationIntegration::test_full_configuration_flow \u001b[32mPASSED\u001b[0m\u001b[31m [ 83%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigurationIntegration::test_testing_configuration \u001b[32mPASSED\u001b[0m\u001b[31m [ 83%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigurationIntegration::test_configuration_error_handling \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration loading failed: Test error\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 84%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorCodes::test_error_code_values \u001b[32mPASSED\u001b[0m\u001b[31m [ 84%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorCodes::test_error_severity_values \u001b[32mPASSED\u001b[0m\u001b[31m [ 84%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorDetails::test_error_details_creation \u001b[32mPASSED\u001b[0m\u001b[31m [ 84%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorDetails::test_error_details_with_context \u001b[32mPASSED\u001b[0m\u001b[31m [ 84%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorDetails::test_error_details_serialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 84%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_netra_exception_basic \u001b[32mPASSED\u001b[0m\u001b[31m [ 85%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_netra_exception_with_code \u001b[32mPASSED\u001b[0m\u001b[31m [ 85%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_netra_exception_to_dict \u001b[32mPASSED\u001b[0m\u001b[31m [ 85%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_configuration_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 85%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_validation_error_with_errors \u001b[32mPASSED\u001b[0m\u001b[31m [ 85%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_authentication_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 86%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_authorization_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 86%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_token_expired_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 86%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_database_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 86%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_record_not_found_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 86%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_service_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 87%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_llm_request_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 87%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_websocket_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 87%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandler::test_handle_netra_exception \u001b[32mPASSED\u001b[0m\u001b[31m [ 87%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandler::test_handle_pydantic_validation_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 87%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandler::test_handle_sqlalchemy_integrity_error \u001b[32mPASSED\u001b[0m\u001b[31m [ 87%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandler::test_handle_http_exception \u001b[32mPASSED\u001b[0m\u001b[31m [ 88%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandler::test_handle_unknown_exception \u001b[32mPASSED\u001b[0m\u001b[31m [ 88%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandler::test_get_http_status_code_mapping \u001b[32mPASSED\u001b[0m\u001b[31m [ 88%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_trace_id_context \u001b[32mPASSED\u001b[0m\u001b[31m [ 88%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_request_id_context \u001b[32mPASSED\u001b[0m\u001b[31m [ 88%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_user_id_context \u001b[32mPASSED\u001b[0m\u001b[31m [ 89%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_custom_context \u001b[32mPASSED\u001b[0m\u001b[31m [ 89%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_get_all_context \u001b[32mPASSED\u001b[0m\u001b[31m [ 89%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_clear_context \u001b[32mPASSED\u001b[0m\u001b[31m [ 89%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_error_context_manager \u001b[32mPASSED\u001b[0m\u001b[31m [ 89%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_get_enriched_error_context \u001b[32mPASSED\u001b[0m\u001b[31m [ 90%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandlerFunctions::test_handle_exception_function \u001b[32mPASSED\u001b[0m\u001b[31m [ 90%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandlerFunctions::test_get_http_status_code_function \u001b[32mPASSED\u001b[0m\u001b[31m [ 90%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandlerFunctions::test_netra_exception_handler \u001b[32mPASSED\u001b[0m\u001b[31m [ 90%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandlerFunctions::test_validation_exception_handler \u001b[32mPASSED\u001b[0m\u001b[31m [ 90%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandlerFunctions::test_http_exception_handler \u001b[32mPASSED\u001b[0m\u001b[31m [ 90%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandlerFunctions::test_general_exception_handler \u001b[32mPASSED\u001b[0m\u001b[31m [ 91%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorResponseModel::test_error_response_creation \u001b[32mPASSED\u001b[0m\u001b[31m [ 91%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorResponseModel::test_error_response_with_details \u001b[32mPASSED\u001b[0m\u001b[31m [ 91%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorResponseModel::test_error_response_serialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 91%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseServiceMixin::test_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 91%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseServiceMixin::test_update_metrics_success \u001b[32mPASSED\u001b[0m\u001b[31m [ 92%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseServiceMixin::test_update_metrics_failure \u001b[32mPASSED\u001b[0m\u001b[31m [ 92%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseServiceMixin::test_update_metrics_average_calculation \u001b[32mPASSED\u001b[0m\u001b[31m [ 92%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseServiceMixin::test_create_background_task \u001b[32mPASSED\u001b[0m\u001b[31m [ 92%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseServiceMixin::test_cancel_background_tasks \u001b[32mPASSED\u001b[0m\u001b[31m [ 92%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 93%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_initialize_success \u001b[32mPASSED\u001b[0m\u001b[31m [ 93%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_initialize_failure \u001b[32mPASSED\u001b[0m\u001b[31m [ 93%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_initialize_idempotent \u001b[32mPASSED\u001b[0m\u001b[31m [ 93%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_shutdown \u001b[32mPASSED\u001b[0m\u001b[31m [ 93%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_health_check_healthy \u001b[32mPASSED\u001b[0m\u001b[31m [ 93%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_health_check_with_dependencies \u001b[32mPASSED\u001b[0m\u001b[31m [ 94%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_health_check_exception \u001b[32mPASSED\u001b[0m\u001b[31m [ 94%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestDatabaseService::test_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 94%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestDatabaseService::test_set_session_factory \u001b[32mPASSED\u001b[0m\u001b[31m [ 94%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestDatabaseService::test_get_db_session_no_factory \u001b[32mPASSED\u001b[0m\u001b[31m [ 94%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestDatabaseService::test_get_db_session_success \u001b[32mPASSED\u001b[0m\u001b[31m [ 95%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestDatabaseService::test_get_db_session_exception \u001b[32mPASSED\u001b[0m\u001b[31m [ 95%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_create_entity \u001b[32mPASSED\u001b[0m\u001b[31m [ 95%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_get_by_id_found \u001b[32mPASSED\u001b[0m\u001b[31m [ 95%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_get_by_id_not_found \u001b[32mPASSED\u001b[0m\u001b[31m [ 95%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_update_entity_success \u001b[32mPASSED\u001b[0m\u001b[31m [ 96%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_update_entity_not_found \u001b[32mPASSED\u001b[0m\u001b[31m [ 96%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_delete_entity_success \u001b[32mPASSED\u001b[0m\u001b[31m [ 96%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_delete_entity_not_found \u001b[32mPASSED\u001b[0m\u001b[31m [ 96%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_exists_true \u001b[32mPASSED\u001b[0m\u001b[31m [ 96%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_exists_false \u001b[32mPASSED\u001b[0m\u001b[31m [ 96%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestAsyncTaskService::test_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestAsyncTaskService::test_start_background_tasks \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestAsyncTaskService::test_start_background_tasks_idempotent \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestAsyncTaskService::test_stop_background_tasks \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_register_service \u001b[32mPASSED\u001b[0m\u001b[31m [ 98%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_get_service_not_found \u001b[32mPASSED\u001b[0m\u001b[31m [ 98%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_get_all_services \u001b[32mPASSED\u001b[0m\u001b[31m [ 98%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_initialize_all \u001b[32mPASSED\u001b[0m\u001b[31m [ 98%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_shutdown_all \u001b[32mPASSED\u001b[0m\u001b[31m [ 98%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_health_check_all \u001b[32mPASSED\u001b[0m\u001b[31m [ 99%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_health_check_all_with_exception \u001b[32mPASSED\u001b[0m\u001b[31m [ 99%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceModels::test_service_health_creation \u001b[32mPASSED\u001b[0m\u001b[31m [ 99%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceModels::test_service_metrics_creation \u001b[32mPASSED\u001b[0m\u001b[31m [ 99%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestGlobalServiceRegistry::test_global_registry_exists \u001b[32mPASSED\u001b[0m\u001b[31m [ 99%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestGlobalServiceRegistry::test_global_registry_operations \u001b[32mPASSED\u001b[0m\u001b[31m [100%]\u001b[0m\n\n=================================== ERRORS ====================================\n\u001b[31m\u001b[1m________ ERROR at setup of TestBaseTool.test_base_tool_execute_wrapper ________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:397: in pytest_fixture_setup\n    \u001b[0mloop = outcome.get_result()\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\tests\\conftest.py\u001b[0m:45: in event_loop\n    \u001b[0mloop = asyncio.get_event_loop()\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\asyncio\\events.py\u001b[0m:702: in get_event_loop\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mRuntimeError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mThere is no current event loop in thread \u001b[39;49;00m\u001b[33m%r\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   RuntimeError: There is no current event loop in thread 'MainThread'.\u001b[0m\n---------------------------- Captured stderr setup ----------------------------\nC:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\fixtures.py:1127: PluggyTeardownRaisedWarning: A plugin raised an exception during an old-style hookwrapper teardown.\nPlugin: asyncio, Hook: pytest_fixture_setup\nRuntimeError: There is no current event loop in thread 'MainThread'.\nFor more information see https://pluggy.readthedocs.io/en/stable/api_reference.html#pluggy.PluggyTeardownRaisedWarning\n  result = ihook.pytest_fixture_setup(fixturedef=self, request=request)\n\u001b[31m\u001b[1m_ ERROR at setup of TestLatencyAnalyzer.test_latency_analyzer_basic_functionality _\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:397: in pytest_fixture_setup\n    \u001b[0mloop = outcome.get_result()\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\tests\\conftest.py\u001b[0m:45: in event_loop\n    \u001b[0mloop = asyncio.get_event_loop()\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\asyncio\\events.py\u001b[0m:702: in get_event_loop\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mRuntimeError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mThere is no current event loop in thread \u001b[39;49;00m\u001b[33m%r\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   RuntimeError: There is no current event loop in thread 'MainThread'.\u001b[0m\n---------------------------- Captured stderr setup ----------------------------\nC:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\fixtures.py:1127: PluggyTeardownRaisedWarning: A plugin raised an exception during an old-style hookwrapper teardown.\nPlugin: asyncio, Hook: pytest_fixture_setup\nRuntimeError: There is no current event loop in thread 'MainThread'.\nFor more information see https://pluggy.readthedocs.io/en/stable/api_reference.html#pluggy.PluggyTeardownRaisedWarning\n  result = ihook.pytest_fixture_setup(fixturedef=self, request=request)\n\u001b[31m\u001b[1m___ ERROR at setup of TestAgentMessageProcessing.test_process_user_message ____\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:397: in pytest_fixture_setup\n    \u001b[0mloop = outcome.get_result()\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\tests\\conftest.py\u001b[0m:45: in event_loop\n    \u001b[0mloop = asyncio.get_event_loop()\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\asyncio\\events.py\u001b[0m:702: in get_event_loop\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mRuntimeError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mThere is no current event loop in thread \u001b[39;49;00m\u001b[33m%r\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   RuntimeError: There is no current event loop in thread 'MainThread'.\u001b[0m\n---------------------------- Captured stderr setup ----------------------------\nC:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\fixtures.py:1127: PluggyTeardownRaisedWarning: A plugin raised an exception during an old-style hookwrapper teardown.\nPlugin: asyncio, Hook: pytest_fixture_setup\nRuntimeError: There is no current event loop in thread 'MainThread'.\nFor more information see https://pluggy.readthedocs.io/en/stable/api_reference.html#pluggy.PluggyTeardownRaisedWarning\n  result = ihook.pytest_fixture_setup(fixturedef=self, request=request)\n\u001b[31m\u001b[1m____ ERROR at setup of TestClickHouseConnection.test_client_initialization ____\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:281: in _asyncgen_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m____ ERROR at setup of TestClickHouseConnection.test_basic_query_execution ____\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:281: in _asyncgen_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m____ ERROR at setup of TestClickHouseConnection.test_query_with_parameters ____\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:281: in _asyncgen_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m___________ ERROR at setup of TestBasicOperations.test_show_tables ____________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:281: in _asyncgen_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m__________ ERROR at setup of TestUnitOfWork.test_uow_initialization ___________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m________ ERROR at setup of TestUnitOfWork.test_uow_transaction_commit _________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_______ ERROR at setup of TestUnitOfWork.test_uow_transaction_rollback ________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m________ ERROR at setup of TestUnitOfWork.test_uow_nested_transactions ________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_________ ERROR at setup of TestUnitOfWork.test_uow_concurrent_access _________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_________ ERROR at setup of TestBaseRepository.test_repository_create _________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m______ ERROR at setup of TestBaseRepository.test_repository_bulk_create _______\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_______ ERROR at setup of TestBaseRepository.test_repository_get_by_id ________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m________ ERROR at setup of TestBaseRepository.test_repository_get_many ________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_________ ERROR at setup of TestBaseRepository.test_repository_update _________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_________ ERROR at setup of TestBaseRepository.test_repository_delete _________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m______ ERROR at setup of TestBaseRepository.test_repository_soft_delete _______\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_______ ERROR at setup of TestBaseRepository.test_repository_pagination _______\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_____ ERROR at setup of TestMessageRepository.test_get_messages_by_thread _____\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m__ ERROR at setup of TestMessageRepository.test_get_messages_with_pagination __\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m______ ERROR at setup of TestMessageRepository.test_get_latest_messages _______\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_______ ERROR at setup of TestThreadRepository.test_get_threads_by_user _______\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_______ ERROR at setup of TestThreadRepository.test_get_active_threads ________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_________ ERROR at setup of TestThreadRepository.test_archive_thread __________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_______ ERROR at setup of TestRunRepository.test_create_run_with_tools ________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_________ ERROR at setup of TestRunRepository.test_update_run_status __________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m__________ ERROR at setup of TestRunRepository.test_get_active_runs ___________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_ ERROR at setup of TestReferenceRepository.test_create_reference_with_metadata _\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m__ ERROR at setup of TestReferenceRepository.test_get_references_by_message ___\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m______ ERROR at setup of TestReferenceRepository.test_search_references _______\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_____ ERROR at setup of TestIntegration.test_complete_generation_workflow _____\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_______ ERROR at setup of TestIntegration.test_multi_tenant_generation ________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_____ ERROR at setup of TestIntegration.test_real_time_streaming_pipeline _____\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_____ ERROR at setup of TestIntegration.test_failure_recovery_integration _____\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m______ ERROR at setup of TestIntegration.test_cross_component_validation ______\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m________ ERROR at setup of TestIntegration.test_performance_under_load ________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m____ ERROR at setup of TestIntegration.test_data_consistency_verification _____\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m________ ERROR at setup of TestIntegration.test_monitoring_integration ________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m_____ ERROR at setup of TestIntegration.test_security_and_access_control ______\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n\u001b[31m\u001b[1m________ ERROR at setup of TestIntegration.test_cleanup_and_retention _________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:319: in _async_fixture_wrapper\n    \u001b[0mfixture, request.instance, fixturedef.unittest\u001b[90m\u001b[39;49;00m\n                               ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FixtureDef' object has no attribute 'unittest'\u001b[0m\n================================== FAILURES ===================================\n\u001b[31m\u001b[1m_______ TestBaseTool.test_base_tool_execute_without_metadata_attribute ________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py\u001b[0m:282: in test_base_tool_execute_without_metadata_attribute\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m pytest.raises(\u001b[96mAttributeError\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   Failed: DID NOT RAISE <class 'AttributeError'>\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestBaseTool.test_base_tool_exception_types _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py\u001b[0m:336: in test_base_tool_exception_types\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[96mstr\u001b[39;49;00m(exc_info.value) == \u001b[33m\"\u001b[39;49;00m\u001b[33mCustom error\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert \"'Custom error'\" == 'Custom error'\u001b[0m\n\u001b[1m\u001b[31mE     \u001b[0m\n\u001b[1m\u001b[31mE     \u001b[0m\u001b[91m- Custom error\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n\u001b[1m\u001b[31mE     \u001b[92m+ 'Custom error'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n\u001b[1m\u001b[31mE     ? +            +\u001b[90m\u001b[39;49;00m\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestCostAnalyzer.test_cost_analyzer_async_execution _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py\u001b[0m:170: in test_cost_analyzer_async_execution\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mTest prompt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m result\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'Test prompt' in 'Analyzed current costs. Total estimated cost: $0.03'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestCostAnalyzer.test_cost_analyzer_rounding_edge_cases ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py\u001b[0m:268: in test_cost_analyzer_rounding_edge_cases\n    \u001b[0m\u001b[94massert\u001b[39;49;00m expected \u001b[95min\u001b[39;49;00m result, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFailed for costs \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcosts\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: Failed for costs [0.005, 0.005, 0.005]\u001b[0m\n\u001b[1m\u001b[31mE   assert '$0.02' in 'Analyzed current costs. Total estimated cost: $0.01'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestLatencyAnalyzer.test_latency_analyzer_empty_logs _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:96: in test_latency_analyzer_empty_logs\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m latency_analyzer(mock_context)\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916314624'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[0m\n---------------------------- Captured stderr call -----------------------------\nC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py:96: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  result = await latency_analyzer(mock_context)\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestLatencyAnalyzer.test_latency_analyzer_single_log _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:107: in test_latency_analyzer_single_log\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m latency_analyzer(mock_context)\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916699568'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestLatencyAnalyzer.test_latency_analyzer_high_latency_values ________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:121: in test_latency_analyzer_high_latency_values\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m latency_analyzer(mock_context)\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916462656'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestLatencyAnalyzer.test_latency_analyzer_zero_latency ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:130: in test_latency_analyzer_zero_latency\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m latency_analyzer(mock_context)\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916267584'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______ TestLatencyAnalyzer.test_latency_analyzer_sub_millisecond_latency ______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:143: in test_latency_analyzer_sub_millisecond_latency\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m latency_analyzer(mock_context)\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699910739680'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestLatencyAnalyzer.test_latency_analyzer_exception_handling _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:155: in test_latency_analyzer_exception_handling\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[96mstr\u001b[39;49;00m(exc_info.value) == \u001b[33m\"\u001b[39;49;00m\u001b[33mAPI Error\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert \"1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916522368'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\" == 'API Error'\u001b[0m\n\u001b[1m\u001b[31mE     \u001b[0m\n\u001b[1m\u001b[31mE     \u001b[0m\u001b[91m- API Error\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n\u001b[1m\u001b[31mE     \u001b[92m+ 1 validation error for latency_analyzer\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n\u001b[1m\u001b[31mE     \u001b[92m+   Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916522368'>, input_type=Mock]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n\u001b[1m\u001b[31mE     \u001b[92m+     For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestLatencyAnalyzer.test_latency_analyzer_async_execution __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:169: in test_latency_analyzer_async_execution\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m latency_analyzer(mock_context)\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699912347968'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestLatencyAnalyzer.test_latency_analyzer_large_dataset ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:188: in test_latency_analyzer_large_dataset\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m latency_analyzer(mock_context)\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916520160'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestLatencyAnalyzer.test_latency_analyzer_varied_latencies __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:211: in test_latency_analyzer_varied_latencies\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m latency_analyzer(mock_context)\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940102464'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestLatencyAnalyzer.test_latency_analyzer_partial_failure __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:229: in test_latency_analyzer_partial_failure\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mPrediction service unavailable\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m \u001b[96mstr\u001b[39;49;00m(exc_info.value)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert 'Prediction service unavailable' in \"1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940115232'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\"\u001b[0m\n\u001b[1m\u001b[31mE    +  where \"1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940115232'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\" = str(1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940115232'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type)\u001b[0m\n\u001b[1m\u001b[31mE    +    where 1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940115232'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type = <ExceptionInfo 1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=mod...' id='2699940115232'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type tblen=8>.value\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestLatencyAnalyzer.test_latency_analyzer_edge_case_rounding _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:247: in test_latency_analyzer_edge_case_rounding\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m latency_analyzer(mock_context)\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940205904'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestLatencyAnalyzer.test_latency_analyzer_negative_latencies _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:260: in test_latency_analyzer_negative_latencies\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m latency_analyzer(mock_context)\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940215936'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______ TestLatencyAnalyzer.test_latency_analyzer_mixed_response_formats _______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:277: in test_latency_analyzer_mixed_response_formats\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m latency_analyzer(mock_context)\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940784432'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestLatencyAnalyzer.test_latency_analyzer_extreme_values ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:288: in test_latency_analyzer_extreme_values\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m latency_analyzer(mock_context)\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699906843616'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____ TestLatencyAnalyzer.test_latency_analyzer_concurrent_execution_timing ____\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:306: in test_latency_analyzer_concurrent_execution_timing\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m latency_analyzer(mock_context)\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940777904'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestAgentMessageProcessing.test_handle_tool_execution ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_agent_message_processing.py\u001b[0m:43: in test_handle_tool_execution\n    \u001b[0magent_service = AgentService(mock_db, mock_llm)\u001b[90m\u001b[39;49;00m\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: AgentService.__init__() takes 2 positional arguments but 3 were given\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestAgentMessageProcessing.test_message_validation ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_agent_message_processing.py\u001b[0m:69: in test_message_validation\n    \u001b[0magent_service = AgentService(mock_db, mock_llm)\u001b[90m\u001b[39;49;00m\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: AgentService.__init__() takes 2 positional arguments but 3 were given\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_optimization_fallback_low_quality _\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:60: in test_generate_optimization_fallback_low_quality\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(response) > \u001b[94m100\u001b[39;49;00m  \u001b[90m# Should be substantial\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert 4 > 100\u001b[0m\n\u001b[1m\u001b[31mE    +  where 4 = len({'response': \"I need more specific information about your Optimize my GPU workload to provide actionable optimization recommendations. Could you provide:\\n\\u2022 Current performance metrics (latency, throughput)\\n\\u2022 Resource constraints (memory, compute)\\n\\u2022 Target improvements (e.g., 20% latency reduction)\\nThis will help me generate specific, measurable optimization strategies.\\n\\n**Quality Issues Detected:**\\n\\U0001f4ca **Specificity Issue**: The response lacked specific details and metrics.\\n\\U0001f3af **Actionability Issue**: The response didn't provide clear action steps.\\n\\U0001f4c8 **Quantification Issue**: Missing numerical values and measurements.\\n\\U0001f504 **Logic Issue**: Circular reasoning detected in the response.\\n\\U0001f4dd **Generic Content**: Found 5 generic phrases.\", 'metadata': {'is_fallback': True, 'failure_reason': 'low_quality', 'content_type': 'optimization', 'agent': 'optimization_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Provide specific metrics and constraints for better recommendations', 'Tip: Include current performance baselines for targeted optimization', \"Tip: Specify measurable goals (e.g., '20% latency reduction')\"], 'recovery_options': [{'action': 'Use Optimization Template', 'description': 'Start with our proven optimization patterns', 'link': '/templates/optimization'}, {'action': 'Run Diagnostics', 'description': 'Analyze your system to identify bottlenecks', 'link': '/tools/diagnostics'}, {'action': 'Schedule Consultation', 'description': 'Get expert help for complex optimizations', 'link': '/support/consultation'}]})\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_data_analysis_fallback_parsing_error _\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:82: in test_generate_data_analysis_fallback_parsing_error\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33msystem logs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m response\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'system logs' in {'response': 'I encountered an issue processing the data for Analyze system logs. This typically occurs with:\\n\\u2022 Inconsistent data formats\\n\\u2022 Missing required fields\\n\\u2022 Encoding issues\\nCould you verify the data format and provide a sample?', 'metadata': {'is_fallback': True, 'failure_reason': 'parsing_error', 'content_type': 'data_analysis', 'agent': 'data_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Validate JSON structure before submission', 'Tip: Check for special characters in input data', 'Tip: Ensure consistent data types across fields'], 'recovery_options': [{'action': 'Data Validation Tool', 'description': 'Check and fix data format issues', 'link': '/tools/data-validator'}, {'action': 'Sample Analysis', 'description': 'Try with a smaller data sample first', 'link': '/tools/sample-analysis'}, {'action': 'Analysis Templates', 'description': 'Use pre-built analysis patterns', 'link': '/templates/analysis'}]}\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_action_plan_fallback_context_missing _\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:100: in test_generate_action_plan_fallback_context_missing\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdeployment plan\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m response\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'deployment plan' in {'response': 'I need more information to provide a valuable response for Create deployment plan. Please provide:\\n\\u2022 Specific details about your use case\\n\\u2022 Current metrics or configuration\\n\\u2022 Desired outcomes or improvements\\n\\u2022 Any constraints or requirements\\nThis will help me generate actionable recommendations.', 'metadata': {'is_fallback': True, 'failure_reason': 'context_missing', 'content_type': 'action_plan', 'agent': 'action_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Include system specifications in your request', 'Tip: Provide current configuration details', 'Tip: Share relevant performance metrics'], 'recovery_options': [{'action': 'Planning Wizard', 'description': 'Guided process for action plan creation', 'link': '/tools/planning-wizard'}, {'action': 'Best Practices', 'description': 'Review proven implementation patterns', 'link': '/docs/best-practices'}, {'action': 'Example Plans', 'description': 'See successful optimization plans', 'link': '/examples/action-plans'}]}\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_report_fallback_validation_failed _\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:118: in test_generate_report_fallback_validation_failed\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mperformance report\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m response\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'performance report' in {'response': 'I need more information to provide a valuable response for Generate performance report. Please provide:\\n\\u2022 Specific details about your use case\\n\\u2022 Current metrics or configuration\\n\\u2022 Desired outcomes or improvements\\n\\u2022 Any constraints or requirements\\nThis will help me generate actionable recommendations.', 'metadata': {'is_fallback': True, 'failure_reason': 'validation_failed', 'content_type': 'report', 'agent': 'reporting_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Provide more specific details about your request'], 'recovery_options': [{'action': 'Report Builder', 'description': 'Interactive report generation tool', 'link': '/tools/report-builder'}, {'action': 'Metrics Dashboard', 'description': 'View real-time metrics and trends', 'link': '/dashboard/metrics'}, {'action': 'Export Templates', 'description': 'Download report templates', 'link': '/templates/reports'}]}\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______ TestFallbackResponseService.test_generate_triage_fallback_timeout ______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:136: in test_generate_triage_fallback_timeout\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33msystem issue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m response\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'system issue' in {'response': 'The analysis for Diagnose system issue is taking longer than expected. You can:\\n\\u2022 Reduce the scope of analysis\\n\\u2022 Process in smaller batches\\n\\u2022 Use our quick optimization templates\\n\\u2022 Schedule for batch processing', 'metadata': {'is_fallback': True, 'failure_reason': 'timeout', 'content_type': 'triage', 'agent': 'triage_agent', 'retry_count': 2, 'can_retry': True}, 'diagnostics': ['Tip: Provide more specific details about your request'], 'recovery_options': [{'action': 'Refine Request', 'description': 'Add more specific details and constraints', 'link': '/help/requests'}, {'action': 'View Examples', 'description': 'See successful request examples', 'link': '/examples'}, {'action': 'Get Help', 'description': 'Contact support for assistance', 'link': '/support'}], 'retry_info': {'attempts': 2, 'max_attempts': 3, 'next_action': 'Consider alternative approach'}}\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_error_message_fallback_llm_error __\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:155: in test_generate_error_message_fallback_llm_error\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33merror\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m response.lower()\u001b[90m\u001b[39;49;00m\n                      ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'lower'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_fallback_with_circular_reasoning __\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:173: in test_generate_fallback_with_circular_reasoning\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel performance\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m response\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'model performance' in {'response': 'Let me provide a more concrete optimization approach for Improve model performance:\\n1. **Measure**: First, profile your current system using tools like [specific profiler]\\n2. **Identify**: Look for bottlenecks in [specific areas]\\n3. **Apply**: Implement specific techniques like [concrete optimization]\\n4. **Verify**: Measure improvements against baseline\\nWould you like me to elaborate on any of these steps?', 'metadata': {'is_fallback': True, 'failure_reason': 'circular_reasoning', 'content_type': 'optimization', 'agent': 'optimization_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Ask for specific optimization techniques', 'Tip: Request quantified recommendations', 'Tip: Focus on measurable improvements'], 'recovery_options': [{'action': 'Use Optimization Template', 'description': 'Start with our proven optimization patterns', 'link': '/templates/optimization'}, {'action': 'Run Diagnostics', 'description': 'Analyze your system to identify bottlenecks', 'link': '/tools/diagnostics'}, {'action': 'Schedule Consultation', 'description': 'Get expert help for complex optimizations', 'link': '/support/consultation'}]}\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_fallback_with_hallucination_risk __\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:202: in test_generate_fallback_with_hallucination_risk\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mfuture trends\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m response\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert 'future trends' in {'response': \"I need more information to provide a valuable response for Analyze future trends. Please provide:\\n\\u2022 Specific details about your use case\\n\\u2022 Current metrics or configuration\\n\\u2022 Desired outcomes or improvements\\n\\u2022 Any constraints or requirements\\nThis will help me generate actionable recommendations.\\n\\n**Quality Issues Detected:**\\n\\U0001f4ca **Specificity Issue**: The response lacked specific details and metrics.\\n\\U0001f3af **Actionability Issue**: The response didn't provide clear action steps.\\n\\U0001f4c8 **Quantification Issue**: Missing numerical values and measurements.\", 'metadata': {'is_fallback': True, 'failure_reason': 'hallucination_risk', 'content_type': 'data_analysis', 'agent': 'data_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Verify all metrics against your actual data', 'Tip: Cross-reference recommendations with documentation', 'Tip: Start with small-scale testing'], 'recovery_options': [{'action': 'Data Validation Tool', 'description': 'Check and fix data format issues', 'link': '/tools/data-validator'}, {'action': 'Sample Analysis', 'description': 'Try with a smaller data sample first', 'link': '/tools/sample-analysis'}, {'action': 'Analysis Templates', 'description': 'Use pre-built analysis patterns', 'link': '/templates/analysis'}]}\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____ TestFallbackResponseService.test_generate_fallback_with_rate_limit ______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:220: in test_generate_fallback_with_rate_limit\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[96many\u001b[39;49;00m(term \u001b[95min\u001b[39;49;00m response.lower() \u001b[94mfor\u001b[39;49;00m term \u001b[95min\u001b[39;49;00m [\u001b[33m\"\u001b[39;49;00m\u001b[33mmoment\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mshortly\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mwait\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtry\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:220: in <genexpr>\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[96many\u001b[39;49;00m(term \u001b[95min\u001b[39;49;00m response.lower() \u001b[94mfor\u001b[39;49;00m term \u001b[95min\u001b[39;49;00m [\u001b[33m\"\u001b[39;49;00m\u001b[33mmoment\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mshortly\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mwait\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtry\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n                       ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'lower'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__ TestFallbackResponseService.test_generate_fallback_considers_retry_count ___\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:243: in test_generate_fallback_considers_retry_count\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(response2) > \u001b[94m0\u001b[39;49;00m  \u001b[90m# Should still provide helpful response\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: object of type 'coroutine' has no len()\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_fallback_includes_diagnostic_tips _\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:264: in test_generate_fallback_includes_diagnostic_tips\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mCSV data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m response\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'CSV data' in {'response': 'I encountered an issue processing the data for Process CSV data. This typically occurs with:\\n\\u2022 Inconsistent data formats\\n\\u2022 Missing required fields\\n\\u2022 Encoding issues\\nCould you verify the data format and provide a sample?', 'metadata': {'is_fallback': True, 'failure_reason': 'parsing_error', 'content_type': 'data_analysis', 'agent': 'data_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Validate JSON structure before submission', 'Tip: Check for special characters in input data', 'Tip: Ensure consistent data types across fields'], 'recovery_options': [{'action': 'Data Validation Tool', 'description': 'Check and fix data format issues', 'link': '/tools/data-validator'}, {'action': 'Sample Analysis', 'description': 'Try with a smaller data sample first', 'link': '/tools/sample-analysis'}, {'action': 'Analysis Templates', 'description': 'Use pre-built analysis patterns', 'link': '/templates/analysis'}]}\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_fallback_with_previous_responses __\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:284: in test_generate_fallback_with_previous_responses\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdatabase queries\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m response\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert 'database queries' in {'response': \"The optimization analysis for Optimize database queries requires additional context. To provide value-driven recommendations, I need:\\n\\u2022 Baseline performance data\\n\\u2022 System architecture details\\n\\u2022 Specific bottlenecks you're experiencing\\nWith this information, I can suggest targeted optimizations with expected improvements.\", 'metadata': {'is_fallback': True, 'failure_reason': 'low_quality', 'content_type': 'optimization', 'agent': 'optimization_agent', 'retry_count': 1, 'can_retry': True}, 'diagnostics': ['Tip: Provide specific metrics and constraints for better recommendations', 'Tip: Include current performance baselines for targeted optimization', \"Tip: Specify measurable goals (e.g., '20% latency reduction')\"], 'recovery_options': [{'action': 'Use Optimization Template', 'description': 'Start with our proven optimization patterns', 'link': '/templates/optimization'}, {'action': 'Run Diagnostics', 'description': 'Analyze your system to identify bottlenecks', 'link': '/tools/diagnostics'}, {'action': 'Schedule Consultation', 'description': 'Get expert help for complex optimizations', 'link': '/support/consultation'}], 'retry_info': {'attempts': 1, 'max_attempts': 3, 'next_action': 'Retry with more context'}}\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____ TestFallbackResponseService.test_format_response_with_placeholders ______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:292: in test_format_response_with_placeholders\n    \u001b[0mformatted = fallback_service._format_response(\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FallbackResponseService' object has no attribute '_format_response'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestFallbackResponseService.test_get_recovery_suggestions __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:313: in test_get_recovery_suggestions\n    \u001b[0msuggestions = fallback_service._get_recovery_suggestions(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: FallbackResponseService._get_recovery_suggestions() takes 2 positional arguments but 3 were given\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestFallbackResponseService.test_fallback_response_quality __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_fallback_response_service.py\u001b[0m:347: in test_fallback_response_quality\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(response) >= \u001b[94m50\u001b[39;49;00m  \u001b[90m# Minimum length\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert 4 >= 50\u001b[0m\n\u001b[1m\u001b[31mE    +  where 4 = len({'response': 'I need more specific information about your Test request to provide actionable optimization recommendations. Could you provide:\\n\\u2022 Current performance metrics (latency, throughput)\\n\\u2022 Resource constraints (memory, compute)\\n\\u2022 Target improvements (e.g., 20% latency reduction)\\nThis will help me generate specific, measurable optimization strategies.', 'metadata': {'is_fallback': True, 'failure_reason': 'low_quality', 'content_type': 'optimization', 'agent': 'test_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Provide specific metrics and constraints for better recommendations', 'Tip: Include current performance baselines for targeted optimization', \"Tip: Specify measurable goals (e.g., '20% latency reduction')\"], 'recovery_options': [{'action': 'Use Optimization Template', 'description': 'Start with our proven optimization patterns', 'link': '/templates/optimization'}, {'action': 'Run Diagnostics', 'description': 'Analyze your system to identify bottlenecks', 'link': '/tools/diagnostics'}, {'action': 'Schedule Consultation', 'description': 'Get expert help for complex optimizations', 'link': '/support/consultation'}]})\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________________ test_llm_cache_service_initialization ____________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_llm_cache_service.py\u001b[0m:8: in test_llm_cache_service_initialization\n    \u001b[0mcache_service = LLMCacheService(ttl=\u001b[94m3600\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________________________ test_cache_set_and_get ____________________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_llm_cache_service.py\u001b[0m:15: in test_cache_set_and_get\n    \u001b[0mcache_service = LLMCacheService(ttl=\u001b[94m3600\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________________________ test_cache_expiration ____________________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_llm_cache_service.py\u001b[0m:29: in test_cache_expiration\n    \u001b[0mcache_service = LLMCacheService(ttl=\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n                    ^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________________________ test_cache_size_limit ____________________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_llm_cache_service.py\u001b[0m:43: in test_cache_size_limit\n    \u001b[0mcache_service = LLMCacheService(ttl=\u001b[94m3600\u001b[39;49;00m, max_size=\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________________________ test_cache_stats _______________________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_llm_cache_service.py\u001b[0m:54: in test_cache_stats\n    \u001b[0mcache_service = LLMCacheService(ttl=\u001b[94m3600\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________________________ test_handle_start_agent ___________________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_message_handlers.py\u001b[0m:38: in test_handle_start_agent\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m handler_service.handle_start_agent(\u001b[33m\"\u001b[39;49;00m\u001b[33muser_123\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, payload, mock_session)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\message_handlers.py\u001b[0m:43: in handle_start_agent\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.thread_service.create_message(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: object Mock can't be used in 'await' expression\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_message_handlers.py\u001b[0m:43: in test_handle_start_agent\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33msupervisor\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m \u001b[96mstr\u001b[39;49;00m(e).lower() \u001b[95mor\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33magent\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m \u001b[96mstr\u001b[39;49;00m(e).lower()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert ('supervisor' in \"object mock can't be used in 'await' expression\" or 'agent' in \"object mock can't be used in 'await' expression\")\u001b[0m\n\u001b[1m\u001b[31mE    +  where \"object mock can't be used in 'await' expression\" = <built-in method lower of str object at 0x00000274A10A4C30>()\u001b[0m\n\u001b[1m\u001b[31mE    +    where <built-in method lower of str object at 0x00000274A10A4C30> = \"object Mock can't be used in 'await' expression\".lower\u001b[0m\n\u001b[1m\u001b[31mE    +      where \"object Mock can't be used in 'await' expression\" = str(TypeError(\"object Mock can't be used in 'await' expression\"))\u001b[0m\n\u001b[1m\u001b[31mE    +  and   \"object mock can't be used in 'await' expression\" = <built-in method lower of str object at 0x00000274A10A4C30>()\u001b[0m\n\u001b[1m\u001b[31mE    +    where <built-in method lower of str object at 0x00000274A10A4C30> = \"object Mock can't be used in 'await' expression\".lower\u001b[0m\n\u001b[1m\u001b[31mE    +      where \"object Mock can't be used in 'await' expression\" = str(TypeError(\"object Mock can't be used in 'await' expression\"))\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___ TestQualityGateService.test_validate_high_quality_optimization_content ____\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_gate_service.py\u001b[0m:58: in test_validate_high_quality_optimization_content\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result.passed \u001b[95mis\u001b[39;49;00m \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.39999999999999997, actionability_score=0.5, quantification_score=1.0, relevance_score=0.5, completeness_score=0.2, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=77, sentence_count=12, numeric_values_count=0, specific_terms_count=0, overall_score=0.5700000000000001, quality_level=<QualityLevel.ACCEPTABLE: 'acceptable'>, issues=['Contains 3 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.']}, fallback_response=None).passed\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______ TestQualityGateService.test_validate_data_analysis_with_metrics _______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_gate_service.py\u001b[0m:129: in test_validate_data_analysis_with_metrics\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result.passed \u001b[95mis\u001b[39;49;00m \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.6, actionability_score=0.3, quantification_score=0.9999999999999999, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=59, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.6449999999999999, quality_level=<QualityLevel.ACCEPTABLE: 'acceptable'>, issues=[], suggestions=['Include clear action steps or commands']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Provide step-by-step actionable instructions with specific commands or code.']}, fallback_response=None).passed\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestQualityGateService.test_validate_action_plan_completeness ________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_gate_service.py\u001b[0m:161: in test_validate_action_plan_completeness\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result.passed \u001b[95mis\u001b[39;49;00m \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.4, actionability_score=0.45, quantification_score=0.5499999999999999, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=89, sentence_count=3, numeric_values_count=0, specific_terms_count=0, overall_score=0.41250000000000003, quality_level=<QualityLevel.POOR: 'poor'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add verification steps and success criteria']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.']}, fallback_response=None).passed\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestQualityGateService.test_validate_with_strict_mode ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_gate_service.py\u001b[0m:190: in test_validate_with_strict_mode\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result_normal.passed \u001b[95mis\u001b[39;49;00m \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.0, completeness_score=0.0, novelty_score=0.0, clarity_score=0.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=0, sentence_count=0, numeric_values_count=0, specific_terms_count=0, overall_score=0.0, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[\"Validation error: name 'metrics' is not defined\"], suggestions=[]), retry_suggested=False, retry_prompt_adjustments=None, fallback_response=None).passed\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:08.925 | ERROR    | app.services.quality_gate_service:validate_content:273 | Error validating content: name 'metrics' is not defined\n2025-08-11 07:17:08.925 | ERROR    | app.services.quality_gate_service:validate_content:273 | Error validating content: name 'metrics' is not defined\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestQualityGateService.test_validate_error_message_clarity __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_gate_service.py\u001b[0m:216: in test_validate_error_message_clarity\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result.passed \u001b[95mis\u001b[39;49;00m \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.0, completeness_score=0.0, novelty_score=0.0, clarity_score=0.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=0, sentence_count=0, numeric_values_count=0, specific_terms_count=0, overall_score=0.0, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[\"Validation error: name 'metrics' is not defined\"], suggestions=[]), retry_suggested=False, retry_prompt_adjustments=None, fallback_response=None).passed\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:08.932 | ERROR    | app.services.quality_gate_service:validate_content:273 | Error validating content: name 'metrics' is not defined\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestQualityGateService.test_validate_report_redundancy ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_gate_service.py\u001b[0m:247: in test_validate_report_redundancy\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result.metrics.redundancy_ratio > \u001b[94m0.5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 0.4 > 0.5\u001b[0m\n\u001b[1m\u001b[31mE    +  where 0.4 = QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.6, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=1, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.4, word_count=53, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.35, quality_level=<QualityLevel.POOR: 'poor'>, issues=['Contains 1 generic phrases', 'Contains 5 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Reduce redundant information']).redundancy_ratio\u001b[0m\n\u001b[1m\u001b[31mE    +    where QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.6, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=1, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.4, word_count=53, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.35, quality_level=<QualityLevel.POOR: 'poor'>, issues=['Contains 1 generic phrases', 'Contains 5 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Reduce redundant information']) = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.6, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=1, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.4, word_count=53, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.35, quality_level=<QualityLevel.POOR: 'poor'>, issues=['Contains 1 generic phrases', 'Contains 5 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Reduce redundant information']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.', 'Include numerical values for all claims. Show before/after metrics with percentages.']}, fallback_response=None).metrics\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestQualityGateService.test_domain_specific_term_recognition _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_gate_service.py\u001b[0m:265: in test_domain_specific_term_recognition\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result.passed \u001b[95mis\u001b[39;49;00m \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.6, actionability_score=0.55, quantification_score=0.9, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=44, sentence_count=7, numeric_values_count=0, specific_terms_count=0, overall_score=0.5925, quality_level=<QualityLevel.ACCEPTABLE: 'acceptable'>, issues=[], suggestions=[]), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': []}, fallback_response=None).passed\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestQualityGateService.test_caching_mechanism ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_gate_service.py\u001b[0m:288: in test_caching_mechanism\n    \u001b[0m\u001b[94massert\u001b[39;49;00m cache_key \u001b[95min\u001b[39;49;00m quality_service.validation_cache\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'quality:general:c1402699b07df03b830d11ee43df782f' in {}\u001b[0m\n\u001b[1m\u001b[31mE    +  where {} = <app.services.quality_gate_service.QualityGateService object at 0x00000274A100ED20>.validation_cache\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:08.954 | ERROR    | app.services.quality_gate_service:validate_content:273 | Error validating content: name 'metrics' is not defined\n2025-08-11 07:17:08.954 | ERROR    | app.services.quality_gate_service:validate_content:273 | Error validating content: name 'metrics' is not defined\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____ TestQualityGateService.test_retry_suggestions_for_failed_validation _____\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_gate_service.py\u001b[0m:301: in test_retry_suggestions_for_failed_validation\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result.retry_suggested \u001b[95mis\u001b[39;49;00m \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=9, sentence_count=2, numeric_values_count=0, specific_terms_count=0, overall_score=0.125, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=['Contains 2 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Include before/after performance metrics']), retry_suggested=False, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.', 'Include numerical values for all claims. Show before/after metrics with percentages.']}, fallback_response=None).retry_suggested\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestQualityGateService.test_hallucination_risk_detection ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_gate_service.py\u001b[0m:323: in test_hallucination_risk_detection\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result.metrics.hallucination_risk > \u001b[94m0.5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 0.2 > 0.5\u001b[0m\n\u001b[1m\u001b[31mE    +  where 0.2 = QualityMetrics(specificity_score=0.05, actionability_score=0.0, quantification_score=0.15, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=41, sentence_count=4, numeric_values_count=0, specific_terms_count=0, overall_score=0.28, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes']).hallucination_risk\u001b[0m\n\u001b[1m\u001b[31mE    +    where QualityMetrics(specificity_score=0.05, actionability_score=0.0, quantification_score=0.15, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=41, sentence_count=4, numeric_values_count=0, specific_terms_count=0, overall_score=0.28, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes']) = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.05, actionability_score=0.0, quantification_score=0.15, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=41, sentence_count=4, numeric_values_count=0, specific_terms_count=0, overall_score=0.28, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes']), retry_suggested=False, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.', 'Include numerical values for all claims. Show before/after metrics with percentages.']}, fallback_response=None).metrics\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestQualityGateService.test_triage_content_validation ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_gate_service.py\u001b[0m:348: in test_triage_content_validation\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result.passed \u001b[95mis\u001b[39;49;00m \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.0, completeness_score=0.0, novelty_score=0.0, clarity_score=0.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=0, sentence_count=0, numeric_values_count=0, specific_terms_count=0, overall_score=0.0, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[\"Validation error: name 'metrics' is not defined\"], suggestions=[]), retry_suggested=False, retry_prompt_adjustments=None, fallback_response=None).passed\u001b[0m\n-------------------------- Captured stderr teardown ---------------------------\n2025-08-11 07:17:08.973 | ERROR    | app.services.quality_gate_service:validate_content:273 | Error validating content: name 'metrics' is not defined\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestQualityGateService.test_quality_level_classification ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_gate_service.py\u001b[0m:393: in test_quality_level_classification\n    \u001b[0mmetrics.quality_level = quality_service._classify_quality_level(score)\u001b[90m\u001b[39;49;00m\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityGateService' object has no attribute '_classify_quality_level'. Did you mean: '_determine_quality_level'?\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestQualityMonitoringServiceInitialization.test_initialization_with_defaults _\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:44: in test_initialization_with_defaults\n    \u001b[0m\u001b[94massert\u001b[39;49;00m service.metrics_store \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'metrics_store'. Did you mean: 'metrics_buffer'?\u001b[0m\n\u001b[31m\u001b[1m_ TestQualityMonitoringServiceInitialization.test_initialization_with_custom_config _\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:55: in test_initialization_with_custom_config\n    \u001b[0mservice = QualityMonitoringService(config=config)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: QualityMonitoringService.__init__() got an unexpected keyword argument 'config'\u001b[0m\n\u001b[31m\u001b[1m_ TestQualityMonitoringServiceInitialization.test_initialization_with_metrics_collector _\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1387: in patched\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.decoration_helper(patched,\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\contextlib.py\u001b[0m:137: in __enter__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mnext\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.gen)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1369: in decoration_helper\n    \u001b[0marg = exit_stack.enter_context(patching)\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\contextlib.py\u001b[0m:526: in enter_context\n    \u001b[0mresult = _enter(cm)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n                      ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <module 'app.services.quality_monitoring_service' from 'C:\\\\Users\\\\antho\\\\OneDrive\\\\Desktop\\\\Netra\\\\netra-core-generation-1\\\\app\\\\services\\\\quality_monitoring_service.py'> does not have the attribute 'MetricsCollector'\u001b[0m\n\u001b[31m\u001b[1m_________ TestMetricsCollection.test_collect_response_quality_metrics _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:86: in test_collect_response_quality_metrics\n    \u001b[0mmetrics = \u001b[94mawait\u001b[39;49;00m service.collect_response_metrics(response)\u001b[90m\u001b[39;49;00m\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'collect_response_metrics'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestMetricsCollection.test_collect_system_metrics ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:103: in test_collect_system_metrics\n    \u001b[0mmetrics = \u001b[94mawait\u001b[39;49;00m service.collect_system_metrics()\u001b[90m\u001b[39;49;00m\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'collect_system_metrics'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestMetricsCollection.test_collect_error_metrics _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:121: in test_collect_error_metrics\n    \u001b[0mmetrics = \u001b[94mawait\u001b[39;49;00m service.collect_error_metrics(error)\u001b[90m\u001b[39;49;00m\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'collect_error_metrics'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestMetricsCollection.test_batch_metrics_collection _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:138: in test_batch_metrics_collection\n    \u001b[0mbatch_metrics = \u001b[94mawait\u001b[39;49;00m service.collect_batch_metrics(responses)\u001b[90m\u001b[39;49;00m\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'collect_batch_metrics'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestQualityThresholds.test_set_quality_thresholds ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:158: in test_set_quality_thresholds\n    \u001b[0mservice.set_thresholds(thresholds)\u001b[90m\u001b[39;49;00m\n    ^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'set_thresholds'. Did you mean: '_check_thresholds'?\u001b[0m\n\u001b[31m\u001b[1m___________ TestQualityThresholds.test_validate_against_thresholds ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:167: in test_validate_against_thresholds\n    \u001b[0mservice.set_thresholds({\u001b[33m\"\u001b[39;49;00m\u001b[33mmin_confidence\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m0.8\u001b[39;49;00m})\u001b[90m\u001b[39;49;00m\n    ^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'set_thresholds'. Did you mean: '_check_thresholds'?\u001b[0m\n\u001b[31m\u001b[1m___________ TestQualityThresholds.test_dynamic_threshold_adjustment ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:185: in test_dynamic_threshold_adjustment\n    \u001b[0mnew_threshold = service.calculate_dynamic_threshold(\u001b[90m\u001b[39;49;00m\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'calculate_dynamic_threshold'\u001b[0m\n\u001b[31m\u001b[1m_____________ TestAlerting.test_trigger_alert_on_threshold_breach _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:201: in test_trigger_alert_on_threshold_breach\n    \u001b[0mservice.set_thresholds({\u001b[33m\"\u001b[39;49;00m\u001b[33mmin_confidence\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m0.8\u001b[39;49;00m})\u001b[90m\u001b[39;49;00m\n    ^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'set_thresholds'. Did you mean: '_check_thresholds'?\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________________ TestAlerting.test_alert_rate_limiting ____________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:218: in test_alert_rate_limiting\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch.object(service, \u001b[33m'\u001b[39;49;00m\u001b[33msend_alert\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, new_callable=AsyncMock) \u001b[94mas\u001b[39;49;00m mock_alert:\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n                      ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <app.services.quality_monitoring_service.QualityMonitoringService object at 0x00000274A1172630> does not have the attribute 'send_alert'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________________ TestAlerting.test_alert_escalation ______________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:238: in test_alert_escalation\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch.object(service, \u001b[33m'\u001b[39;49;00m\u001b[33mescalate_alert\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, new_callable=AsyncMock) \u001b[94mas\u001b[39;49;00m mock_escalate:\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n                      ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <app.services.quality_monitoring_service.QualityMonitoringService object at 0x00000274A10C5040> does not have the attribute 'escalate_alert'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestMetricsAggregation.test_aggregate_metrics_by_time_window _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:260: in test_aggregate_metrics_by_time_window\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m service.store_metric(metric)\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'store_metric'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestMetricsAggregation.test_calculate_statistics _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:279: in test_calculate_statistics\n    \u001b[0mstats = \u001b[94mawait\u001b[39;49;00m service.calculate_statistics(values)\u001b[90m\u001b[39;49;00m\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'calculate_statistics'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestMetricsAggregation.test_trend_analysis __________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:301: in test_trend_analysis\n    \u001b[0mtrend = \u001b[94mawait\u001b[39;49;00m service.analyze_trend(timestamps, values)\u001b[90m\u001b[39;49;00m\n                  ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'analyze_trend'. Did you mean: '_analyze_trends'?\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestQualityReporting.test_generate_quality_report ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:317: in test_generate_quality_report\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m service.store_metric({\u001b[33m\"\u001b[39;49;00m\u001b[33mconfidence\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m0.9\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mlatency\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m1.2\u001b[39;49;00m})\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'store_metric'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestQualityReporting.test_export_metrics_to_json _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:340: in test_export_metrics_to_json\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m service.store_metric(metric)\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'store_metric'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestQualityReporting.test_generate_sla_compliance_report ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:365: in test_generate_sla_compliance_report\n    \u001b[0mreport = \u001b[94mawait\u001b[39;49;00m service.check_sla_compliance(sla_targets, actual_metrics)\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'check_sla_compliance'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestAnomalyDetection.test_detect_anomalies_zscore ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:384: in test_detect_anomalies_zscore\n    \u001b[0manomalies = \u001b[94mawait\u001b[39;49;00m service.detect_anomalies(values, method=\u001b[33m\"\u001b[39;49;00m\u001b[33mzscore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, threshold=\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n                      ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'detect_anomalies'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________ TestAnomalyDetection.test_detect_anomalies_iqr ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:397: in test_detect_anomalies_iqr\n    \u001b[0manomalies = \u001b[94mawait\u001b[39;49;00m service.detect_anomalies(values, method=\u001b[33m\"\u001b[39;49;00m\u001b[33miqr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n                      ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'detect_anomalies'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestAnomalyDetection.test_real_time_anomaly_detection ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:408: in test_real_time_anomaly_detection\n    \u001b[0mservice.configure_anomaly_detector(\u001b[90m\u001b[39;49;00m\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'configure_anomaly_detector'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestPerformanceMonitoring.test_monitor_response_times ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:441: in test_monitor_response_times\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m service.record_response_time(rt)\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'record_response_time'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestPerformanceMonitoring.test_monitor_throughput ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:457: in test_monitor_throughput\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m service.record_request()\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'record_request'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestIntegration.test_integration_with_agent_service _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:482: in test_integration_with_agent_service\n    \u001b[0mmetrics = \u001b[94mawait\u001b[39;49;00m service.collect_agent_metrics(mock_agent_instance)\u001b[90m\u001b[39;49;00m\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'collect_agent_metrics'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________ TestIntegration.test_integration_with_database ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:493: in test_integration_with_database\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch(\u001b[33m'\u001b[39;49;00m\u001b[33mapp.services.database.metrics_repository\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m mock_repo:\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n                      ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <module 'app.services.database' (namespace) from ['C:\\\\Users\\\\antho\\\\OneDrive\\\\Desktop\\\\Netra\\\\netra-core-generation-1\\\\app\\\\services\\\\database']> does not have the attribute 'metrics_repository'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestSchemaValidationService.test_validate_schema _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_schema_validation_service.py\u001b[0m:24: in test_validate_schema\n    \u001b[0mmock_engine.connect.return_value.\u001b[92m__aenter__\u001b[39;49;00m.return_value = mock_conn\u001b[90m\u001b[39;49;00m\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:660: in __getattr__\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(name)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: __aenter__\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________________________ test_encrypt_and_decrypt ___________________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_security_service.py\u001b[0m:15: in test_encrypt_and_decrypt\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SecurityService' object has no attribute 'encrypt'\u001b[0m\n\u001b[31m\u001b[1m_________________ TestStatePersistence.test_save_agent_state __________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_state_persistence.py\u001b[0m:16: in test_save_agent_state\n    \u001b[0mpersistence_service = StatePersistenceService(mock_db)\u001b[90m\u001b[39;49;00m\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestStatePersistence.test_restore_agent_state ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_state_persistence.py\u001b[0m:53: in test_restore_agent_state\n    \u001b[0mpersistence_service = StatePersistenceService(mock_db)\u001b[90m\u001b[39;49;00m\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestStatePersistence.test_cleanup_old_states _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_state_persistence.py\u001b[0m:67: in test_cleanup_old_states\n    \u001b[0mpersistence_service = StatePersistenceService(mock_db)\u001b[90m\u001b[39;49;00m\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestStatePersistence.test_state_versioning __________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_state_persistence.py\u001b[0m:89: in test_state_versioning\n    \u001b[0mpersistence_service = StatePersistenceService(mock_db)\u001b[90m\u001b[39;49;00m\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestStatePersistence.test_concurrent_state_updates ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_state_persistence.py\u001b[0m:102: in test_concurrent_state_updates\n    \u001b[0mpersistence_service = StatePersistenceService(mock_db)\u001b[90m\u001b[39;49;00m\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestSupplyCatalogService.test_get_all_options ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_supply_catalog_service.py\u001b[0m:37: in test_get_all_options\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result[\u001b[94m0\u001b[39;49;00m].name == \u001b[33m\"\u001b[39;49;00m\u001b[33mgpt-4\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert <MagicMock name='gpt-4.name' id='2699940775456'> == 'gpt-4'\u001b[0m\n\u001b[1m\u001b[31mE    +  where <MagicMock name='gpt-4.name' id='2699940775456'> = <MagicMock name='gpt-4' id='2699940152336'>.name\u001b[0m\n\u001b[31m\u001b[1m_______________ TestSupplyCatalogService.test_get_option_by_id ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_supply_catalog_service.py\u001b[0m:49: in test_get_option_by_id\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result.name == \u001b[33m\"\u001b[39;49;00m\u001b[33mgpt-4\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert <MagicMock name='gpt-4.name' id='2699940829600'> == 'gpt-4'\u001b[0m\n\u001b[1m\u001b[31mE    +  where <MagicMock name='gpt-4.name' id='2699940829600'> = <MagicMock name='gpt-4' id='2699942156320'>.name\u001b[0m\n\u001b[31m\u001b[1m______________ TestSupplyCatalogService.test_get_option_by_name _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_supply_catalog_service.py\u001b[0m:60: in test_get_option_by_name\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result.name == \u001b[33m\"\u001b[39;49;00m\u001b[33mgpt-4\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert <MagicMock name='gpt-4.name' id='2699940163616'> == 'gpt-4'\u001b[0m\n\u001b[1m\u001b[31mE    +  where <MagicMock name='gpt-4.name' id='2699940163616'> = <MagicMock name='gpt-4' id='2699942152144'>.name\u001b[0m\n\u001b[31m\u001b[1m_______ TestCorpusManagement.test_corpus_creation_with_clickhouse_table _______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:165: in test_corpus_creation_with_clickhouse_table\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m corpus_service.create_corpus(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\corpus_service.py\u001b[0m:69: in create_corpus\n    \u001b[0mdb_corpus = models.Corpus(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m<string>\u001b[0m:4: in __init__\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\sqlalchemy\\orm\\state.py\u001b[0m:571: in _initialize_instance\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m util.safe_reraise():\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\u001b[0m:224: in __exit__\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m exc_value.with_traceback(exc_tb)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\sqlalchemy\\orm\\state.py\u001b[0m:569: in _initialize_instance\n    \u001b[0mmanager.original_init(*mixed[\u001b[94m1\u001b[39;49;00m:], **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\sqlalchemy\\orm\\decl_base.py\u001b[0m:2179: in _declarative_constructor\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mTypeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: 'domain' is an invalid keyword argument for Corpus\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestCorpusManagement.test_corpus_status_transitions _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:188: in test_corpus_status_transitions\n    \u001b[0m\u001b[94massert\u001b[39;49;00m corpus_service.is_valid_transition(from_status, to_status)\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'CorpusService' object has no attribute 'is_valid_transition'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestCorpusManagement.test_corpus_content_upload_batch ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:205: in test_corpus_content_upload_batch\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m corpus_service.upload_corpus_content(\u001b[90m\u001b[39;49;00m\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'CorpusService' object has no attribute 'upload_corpus_content'. Did you mean: '_copy_corpus_content'?\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestCorpusManagement.test_corpus_validation _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:229: in test_corpus_validation\n    \u001b[0m\u001b[94massert\u001b[39;49;00m corpus_service.validate_corpus_record(valid_record) == \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'CorpusService' object has no attribute 'validate_corpus_record'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestCorpusManagement.test_corpus_availability_check _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:241: in test_corpus_availability_check\n    \u001b[0mis_available, record_count = \u001b[94mawait\u001b[39;49;00m corpus_service.check_corpus_availability(\u001b[90m\u001b[39;49;00m\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'CorpusService' object has no attribute 'check_corpus_availability'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestCorpusManagement.test_corpus_fallback_to_default _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:251: in test_corpus_fallback_to_default\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch(\u001b[33m'\u001b[39;49;00m\u001b[33mapp.services.corpus_service.get_default_corpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m mock_default:\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n                      ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <module 'app.services.corpus_service' from 'C:\\\\Users\\\\antho\\\\OneDrive\\\\Desktop\\\\Netra\\\\netra-core-generation-1\\\\app\\\\services\\\\corpus_service.py'> does not have the attribute 'get_default_corpus'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestCorpusManagement.test_corpus_caching_mechanism ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:268: in test_corpus_caching_mechanism\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch.object(corpus_service, \u001b[33m'\u001b[39;49;00m\u001b[33m_fetch_corpus_content\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m mock_fetch:\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n                      ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <app.services.corpus_service.CorpusService object at 0x00000274A100A990> does not have the attribute '_fetch_corpus_content'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestCorpusManagement.test_corpus_deletion_cascade ______________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:913: in assert_called\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAssertionError\u001b[39;49;00m(msg)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: Expected 'execute' to have been called.\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:287: in test_corpus_deletion_cascade\n    \u001b[0mmock_clickhouse_client.execute.assert_called()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: Expected 'execute' to have been called.\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:09.937 | ERROR    | app.core.unified_logging:_log:313 | Failed to delete corpus 02e36fb4-229b-4109-8ed6-eb718c00283b: 'str' object does not support item assignment\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestCorpusManagement.test_corpus_metadata_tracking ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:294: in test_corpus_metadata_tracking\n    \u001b[0mmetadata = corpus_service.create_corpus_metadata(\u001b[90m\u001b[39;49;00m\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'CorpusService' object has no attribute 'create_corpus_metadata'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestCorpusManagement.test_corpus_concurrent_access ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:320: in test_corpus_concurrent_access\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[95mnot\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(r, \u001b[96mException\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m r \u001b[95min\u001b[39;49;00m results)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert False\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = all(<generator object TestCorpusManagement.test_corpus_concurrent_access.<locals>.<genexpr> at 0x00000274A11B12A0>)\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______ TestDataGenerationEngine.test_workload_distribution_generation ________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:331: in test_workload_distribution_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestDataGenerationEngine.test_temporal_pattern_generation __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:355: in test_temporal_pattern_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_with_temporal_patterns(config)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:679: in generate_with_temporal_patterns\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestDataGenerationEngine.test_tool_invocation_patterns ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:379: in test_tool_invocation_patterns\n    \u001b[0m\u001b[94massert\u001b[39;49;00m current[\u001b[33m\"\u001b[39;49;00m\u001b[33mend_time\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] <= next_inv[\u001b[33m\"\u001b[39;49;00m\u001b[33mstart_time\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert datetime.datetime(2025, 8, 11, 14, 17, 10, 487585) <= datetime.datetime(2025, 8, 11, 14, 17, 10, 213188)\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestDataGenerationEngine.test_error_scenario_generation ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:390: in test_error_scenario_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_with_errors(config)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:733: in generate_with_errors\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestDataGenerationEngine.test_domain_specific_generation ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:434: in test_domain_specific_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_domain_specific(config)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:812: in generate_domain_specific\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______ TestDataGenerationEngine.test_statistical_distribution_generation ______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:455: in test_statistical_distribution_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_with_distribution(config)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:845: in generate_with_distribution\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestDataGenerationEngine.test_custom_tool_catalog_generation _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:479: in test_custom_tool_catalog_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_with_custom_tools(config)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:874: in generate_with_custom_tools\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestDataGenerationEngine.test_incremental_generation _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:502: in test_incremental_generation\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m generation_service.generate_incremental(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:895: in generate_incremental\n    \u001b[0mbatch = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.generate_batch(config, batch_size)\u001b[90m\u001b[39;49;00m\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:668: in generate_batch\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestDataGenerationEngine.test_generation_with_corpus_sampling ________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:523: in test_generation_with_corpus_sampling\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_from_corpus(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:914: in generate_from_corpus\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, corpus_content, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestRealTimeIngestion.test_batch_ingestion_to_clickhouse ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:544: in test_batch_ingestion_to_clickhouse\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m ingestion_service.ingest_batch(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1055: in ingest_batch\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._ingest_batch(table_name, records)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:597: in _ingest_batch\n    \u001b[0mrecord[\u001b[33m\"\u001b[39;49;00m\u001b[33mrequest_payload\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = json.dumps(record[\u001b[33m\"\u001b[39;49;00m\u001b[33mrequest_payload\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   KeyError: 'request_payload'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestRealTimeIngestion.test_ingestion_error_recovery _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:596: in test_ingestion_error_recovery\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result[\u001b[33m\"\u001b[39;49;00m\u001b[33mrecords_ingested\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert 0 > 0\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestRealTimeIngestion.test_ingestion_deduplication ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:611: in test_ingestion_deduplication\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m ingestion_service.ingest_with_deduplication(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1123: in ingest_with_deduplication\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ingest_batch(deduplicated)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1055: in ingest_batch\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._ingest_batch(table_name, records)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:597: in _ingest_batch\n    \u001b[0mrecord[\u001b[33m\"\u001b[39;49;00m\u001b[33mrequest_payload\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = json.dumps(record[\u001b[33m\"\u001b[39;49;00m\u001b[33mrequest_payload\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   KeyError: 'request_payload'\u001b[0m\n------------------------------ Captured log call ------------------------------\n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:55 ClickHouse is disabled in testing mode - using mock client\n\u001b[35mDEBUG   \u001b[0m app.db.clickhouse:clickhouse.py:12 ClickHouse disabled - Mock execute: \n            CREATE TABLE IF NOT EXISTS synthetic_data_bf7ca79f76094b0183e5bf0580a69e61 (\n                event_id UUID,\n                trace_id UUID,\n                span_id UUID,\n                parent_span_id Nullable(UUID),\n                timestamp_utc DateTime64(3),\n                workload_type String,\n                agent_type String,\n                tool_invocations Array(String),\n                request_payload String,\n                response_payload String,\n                metrics String,\n                corpus_reference_id Nullable(UUID)\n            ) ENGINE = MergeTree()\n            PARTITION BY toYYYYMM(timestamp_utc)\n            ORDER BY (timestamp_utc, trace_id)\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestRealTimeIngestion.test_table_creation_on_demand _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:634: in test_table_creation_on_demand\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(create_table_calls) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert 0 > 0\u001b[0m\n\u001b[1m\u001b[31mE    +  where 0 = len([])\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestRealTimeIngestion.test_parallel_batch_ingestion _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:669: in test_parallel_batch_ingestion\n    \u001b[0mresults = \u001b[94mawait\u001b[39;49;00m asyncio.gather(*tasks)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1055: in ingest_batch\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._ingest_batch(table_name, records)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:597: in _ingest_batch\n    \u001b[0mrecord[\u001b[33m\"\u001b[39;49;00m\u001b[33mrequest_payload\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = json.dumps(record[\u001b[33m\"\u001b[39;49;00m\u001b[33mrequest_payload\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   KeyError: 'request_payload'\u001b[0m\n------------------------------ Captured log call ------------------------------\n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:55 ClickHouse is disabled in testing mode - using mock client\n\u001b[35mDEBUG   \u001b[0m app.db.clickhouse:clickhouse.py:12 ClickHouse disabled - Mock execute: \n            CREATE TABLE IF NOT EXISTS table_0 (\n                event_id UUID,\n                trace_id UUID,\n                span_id UUID,\n                parent_span_id Nullable(UUID),\n                timestamp_utc DateTime64(3),\n                workload_type String,\n                agent_type String,\n                tool_invocations Array(String),\n                request_payload String,\n                response_payload String,\n                metrics String,\n                corpus_reference_id Nullable(UUID)\n            ) ENGINE = MergeTree()\n            PARTITION BY toYYYYMM(timestamp_utc)\n            ORDER BY (timestamp_utc, trace_id)\n        \n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:55 ClickHouse is disabled in testing mode - using mock client\n\u001b[35mDEBUG   \u001b[0m app.db.clickhouse:clickhouse.py:12 ClickHouse disabled - Mock execute: \n            CREATE TABLE IF NOT EXISTS table_1 (\n                event_id UUID,\n                trace_id UUID,\n                span_id UUID,\n                parent_span_id Nullable(UUID),\n                timestamp_utc DateTime64(3),\n                workload_type String,\n                agent_type String,\n                tool_invocations Array(String),\n                request_payload String,\n                response_payload String,\n                metrics String,\n                corpus_reference_id Nullable(UUID)\n            ) ENGINE = MergeTree()\n            PARTITION BY toYYYYMM(timestamp_utc)\n            ORDER BY (timestamp_utc, trace_id)\n        \n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:55 ClickHouse is disabled in testing mode - using mock client\n\u001b[35mDEBUG   \u001b[0m app.db.clickhouse:clickhouse.py:12 ClickHouse disabled - Mock execute: \n            CREATE TABLE IF NOT EXISTS table_2 (\n                event_id UUID,\n                trace_id UUID,\n                span_id UUID,\n                parent_span_id Nullable(UUID),\n                timestamp_utc DateTime64(3),\n                workload_type String,\n                agent_type String,\n                tool_invocations Array(String),\n                request_payload String,\n                response_payload String,\n                metrics String,\n                corpus_reference_id Nullable(UUID)\n            ) ENGINE = MergeTree()\n            PARTITION BY toYYYYMM(timestamp_utc)\n            ORDER BY (timestamp_utc, trace_id)\n        \n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:55 ClickHouse is disabled in testing mode - using mock client\n\u001b[35mDEBUG   \u001b[0m app.db.clickhouse:clickhouse.py:12 ClickHouse disabled - Mock execute: \n            CREATE TABLE IF NOT EXISTS table_3 (\n                event_id UUID,\n                trace_id UUID,\n                span_id UUID,\n                parent_span_id Nullable(UUID),\n                timestamp_utc DateTime64(3),\n                workload_type String,\n                agent_type String,\n                tool_invocations Array(String),\n                request_payload String,\n                response_payload String,\n                metrics String,\n                corpus_reference_id Nullable(UUID)\n            ) ENGINE = MergeTree()\n            PARTITION BY toYYYYMM(timestamp_utc)\n            ORDER BY (timestamp_utc, trace_id)\n        \n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:55 ClickHouse is disabled in testing mode - using mock client\n\u001b[35mDEBUG   \u001b[0m app.db.clickhouse:clickhouse.py:12 ClickHouse disabled - Mock execute: \n            CREATE TABLE IF NOT EXISTS table_4 (\n                event_id UUID,\n                trace_id UUID,\n                span_id UUID,\n                parent_span_id Nullable(UUID),\n                timestamp_utc DateTime64(3),\n                workload_type String,\n                agent_type String,\n                tool_invocations Array(String),\n                request_payload String,\n                response_payload String,\n                metrics String,\n                corpus_reference_id Nullable(UUID)\n            ) ENGINE = MergeTree()\n            PARTITION BY toYYYYMM(timestamp_utc)\n            ORDER BY (timestamp_utc, trace_id)\n        \n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:55 ClickHouse is disabled in testing mode - using mock client\n\u001b[35mDEBUG   \u001b[0m app.db.clickhouse:clickhouse.py:12 ClickHouse disabled - Mock execute: \n            CREATE TABLE IF NOT EXISTS table_5 (\n                event_id UUID,\n                trace_id UUID,\n                span_id UUID,\n                parent_span_id Nullable(UUID),\n                timestamp_utc DateTime64(3),\n                workload_type String,\n                agent_type String,\n                tool_invocations Array(String),\n                request_payload String,\n                response_payload String,\n                metrics String,\n                corpus_reference_id Nullable(UUID)\n            ) ENGINE = MergeTree()\n            PARTITION BY toYYYYMM(timestamp_utc)\n            ORDER BY (timestamp_utc, trace_id)\n        \n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:55 ClickHouse is disabled in testing mode - using mock client\n\u001b[35mDEBUG   \u001b[0m app.db.clickhouse:clickhouse.py:12 ClickHouse disabled - Mock execute: \n            CREATE TABLE IF NOT EXISTS table_6 (\n                event_id UUID,\n                trace_id UUID,\n                span_id UUID,\n                parent_span_id Nullable(UUID),\n                timestamp_utc DateTime64(3),\n                workload_type String,\n                agent_type String,\n                tool_invocations Array(String),\n                request_payload String,\n                response_payload String,\n                metrics String,\n                corpus_reference_id Nullable(UUID)\n            ) ENGINE = MergeTree()\n            PARTITION BY toYYYYMM(timestamp_utc)\n            ORDER BY (timestamp_utc, trace_id)\n        \n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:55 ClickHouse is disabled in testing mode - using mock client\n\u001b[35mDEBUG   \u001b[0m app.db.clickhouse:clickhouse.py:12 ClickHouse disabled - Mock execute: \n            CREATE TABLE IF NOT EXISTS table_7 (\n                event_id UUID,\n                trace_id UUID,\n                span_id UUID,\n                parent_span_id Nullable(UUID),\n                timestamp_utc DateTime64(3),\n                workload_type String,\n                agent_type String,\n                tool_invocations Array(String),\n                request_payload String,\n                response_payload String,\n                metrics String,\n                corpus_reference_id Nullable(UUID)\n            ) ENGINE = MergeTree()\n            PARTITION BY toYYYYMM(timestamp_utc)\n            ORDER BY (timestamp_utc, trace_id)\n        \n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:55 ClickHouse is disabled in testing mode - using mock client\n\u001b[35mDEBUG   \u001b[0m app.db.clickhouse:clickhouse.py:12 ClickHouse disabled - Mock execute: \n            CREATE TABLE IF NOT EXISTS table_8 (\n                event_id UUID,\n                trace_id UUID,\n                span_id UUID,\n                parent_span_id Nullable(UUID),\n                timestamp_utc DateTime64(3),\n                workload_type String,\n                agent_type String,\n                tool_invocations Array(String),\n                request_payload String,\n                response_payload String,\n                metrics String,\n                corpus_reference_id Nullable(UUID)\n            ) ENGINE = MergeTree()\n            PARTITION BY toYYYYMM(timestamp_utc)\n            ORDER BY (timestamp_utc, trace_id)\n        \n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:55 ClickHouse is disabled in testing mode - using mock client\n\u001b[35mDEBUG   \u001b[0m app.db.clickhouse:clickhouse.py:12 ClickHouse disabled - Mock execute: \n            CREATE TABLE IF NOT EXISTS table_9 (\n                event_id UUID,\n                trace_id UUID,\n                span_id UUID,\n                parent_span_id Nullable(UUID),\n                timestamp_utc DateTime64(3),\n                workload_type String,\n                agent_type String,\n                tool_invocations Array(String),\n                request_payload String,\n                response_payload String,\n                metrics String,\n                corpus_reference_id Nullable(UUID)\n            ) ENGINE = MergeTree()\n            PARTITION BY toYYYYMM(timestamp_utc)\n            ORDER BY (timestamp_utc, trace_id)\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestRealTimeIngestion.test_ingestion_with_transformation ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:684: in test_ingestion_with_transformation\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m ingestion_service.ingest_with_transform(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1151: in ingest_with_transform\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ingest_batch(transformed_records)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1055: in ingest_batch\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._ingest_batch(table_name, records)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:597: in _ingest_batch\n    \u001b[0mrecord[\u001b[33m\"\u001b[39;49;00m\u001b[33mrequest_payload\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = json.dumps(record[\u001b[33m\"\u001b[39;49;00m\u001b[33mrequest_payload\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   KeyError: 'request_payload'\u001b[0m\n------------------------------ Captured log call ------------------------------\n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:55 ClickHouse is disabled in testing mode - using mock client\n\u001b[35mDEBUG   \u001b[0m app.db.clickhouse:clickhouse.py:12 ClickHouse disabled - Mock execute: \n            CREATE TABLE IF NOT EXISTS synthetic_data_d7af76ce71494084b60481dbcf92c1fe (\n                event_id UUID,\n                trace_id UUID,\n                span_id UUID,\n                parent_span_id Nullable(UUID),\n                timestamp_utc DateTime64(3),\n                workload_type String,\n                agent_type String,\n                tool_invocations Array(String),\n                request_payload String,\n                response_payload String,\n                metrics String,\n                corpus_reference_id Nullable(UUID)\n            ) ENGINE = MergeTree()\n            PARTITION BY toYYYYMM(timestamp_utc)\n            ORDER BY (timestamp_utc, trace_id)\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestRealTimeIngestion.test_ingestion_circuit_breaker _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:708: in test_ingestion_circuit_breaker\n    \u001b[0m\u001b[94massert\u001b[39;49;00m circuit_breaker.is_open()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert False\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = is_open()\u001b[0m\n\u001b[1m\u001b[31mE    +    where is_open = <app.services.synthetic_data_service.SyntheticDataService.get_circuit_breaker.<locals>.SimpleCircuitBreaker object at 0x000002749FB6B560>.is_open\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestRealTimeIngestion.test_ingestion_progress_tracking ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:724: in test_ingestion_progress_tracking\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ingestion_service.ingest_with_progress(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1215: in ingest_with_progress\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ingest_batch(batch)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1055: in ingest_batch\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._ingest_batch(table_name, records)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:597: in _ingest_batch\n    \u001b[0mrecord[\u001b[33m\"\u001b[39;49;00m\u001b[33mrequest_payload\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = json.dumps(record[\u001b[33m\"\u001b[39;49;00m\u001b[33mrequest_payload\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   KeyError: 'request_payload'\u001b[0m\n------------------------------ Captured log call ------------------------------\n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:55 ClickHouse is disabled in testing mode - using mock client\n\u001b[35mDEBUG   \u001b[0m app.db.clickhouse:clickhouse.py:12 ClickHouse disabled - Mock execute: \n            CREATE TABLE IF NOT EXISTS synthetic_data_3c1857ec185c438993fae20313f30972 (\n                event_id UUID,\n                trace_id UUID,\n                span_id UUID,\n                parent_span_id Nullable(UUID),\n                timestamp_utc DateTime64(3),\n                workload_type String,\n                agent_type String,\n                tool_invocations Array(String),\n                request_payload String,\n                response_payload String,\n                metrics String,\n                corpus_reference_id Nullable(UUID)\n            ) ENGINE = MergeTree()\n            PARTITION BY toYYYYMM(timestamp_utc)\n            ORDER BY (timestamp_utc, trace_id)\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestWebSocketUpdates.test_websocket_connection_management __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:747: in test_websocket_connection_management\n    \u001b[0m\u001b[94massert\u001b[39;49;00m job_id \u001b[95min\u001b[39;49;00m ws_service.active_connections\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'dc5b091e-950e-4f5d-8684-d670e4408a69' in {<AsyncMock id='2699919000016'>: [ConnectionInfo(websocket='dc5b091e-950e-4f5d-8684-d670e4408a69', user_id=<AsyncMock id='2699919000016'>, connected_at=datetime.datetime(2025, 8, 11, 14, 17, 14, 603936, tzinfo=datetime.timezone.utc), last_ping=datetime.datetime(2025, 8, 11, 14, 17, 14, 603936, tzinfo=datetime.timezone.utc), last_pong=None, message_count=0, error_count=1, connection_id='conn_1754921834603')]}\u001b[0m\n\u001b[1m\u001b[31mE    +  where {<AsyncMock id='2699919000016'>: [ConnectionInfo(websocket='dc5b091e-950e-4f5d-8684-d670e4408a69', user_id=<AsyncMock id='2699919000016'>, connected_at=datetime.datetime(2025, 8, 11, 14, 17, 14, 603936, tzinfo=datetime.timezone.utc), last_ping=datetime.datetime(2025, 8, 11, 14, 17, 14, 603936, tzinfo=datetime.timezone.utc), last_pong=None, message_count=0, error_count=1, connection_id='conn_1754921834603')]} = <app.ws_manager.WebSocketManager object at 0x00000274F0C42660>.active_connections\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:14.603 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834603: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.603 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754921834603: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.604 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-221' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 336, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 289, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 343, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 367, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 117, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 136, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 245, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 344, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 246, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 178, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 1671, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 157, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 532, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-221' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestWebSocketUpdates.test_generation_progress_broadcast ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:768: in test_generation_progress_broadcast\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.broadcast_to_job(job_id, progress_update)\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'broadcast_to_job'\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:14.637 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834637: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.637 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754921834637: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.638 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-223' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 336, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 289, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 343, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 367, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 117, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 136, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 245, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 344, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 246, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 178, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 1671, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 157, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 532, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-223' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestWebSocketUpdates.test_batch_completion_notifications ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:788: in test_batch_completion_notifications\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.notify_batch_complete(\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'notify_batch_complete'\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:14.666 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834666: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.666 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754921834666: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.666 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-225' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 336, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 289, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 343, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 367, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 117, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 136, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 245, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 344, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 246, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 178, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 1671, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 157, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 532, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-225' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestWebSocketUpdates.test_error_notification_handling ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:813: in test_error_notification_handling\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.notify_error(job_id, error_data)\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'notify_error'\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:14.693 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834693: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.694 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754921834693: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-227' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n-------------------------- Captured stderr teardown ---------------------------\n2025-08-11 07:17:14.694 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-227' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 336, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 289, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 343, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 367, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 117, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 136, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 245, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 344, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 246, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 178, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 1671, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 157, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 532, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestWebSocketUpdates.test_websocket_reconnection_handling __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:827: in test_websocket_reconnection_handling\n    \u001b[0mws_service.set_job_state(job_id, {\u001b[33m\"\u001b[39;49;00m\u001b[33mprogress\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m50\u001b[39;49;00m})\u001b[90m\u001b[39;49;00m\n    ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'set_job_state'\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:14.719 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834719: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.719 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754921834719: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.721 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-229' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 336, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 289, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 343, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 367, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 117, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 136, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 245, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 344, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 246, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 178, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 1671, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 157, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 532, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-229' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestWebSocketUpdates.test_multiple_client_subscriptions ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:851: in test_multiple_client_subscriptions\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.broadcast_to_job(job_id, update)\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'broadcast_to_job'\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:14.747 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834747: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.747 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834747: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.747 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834747: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.747 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834747: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.748 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834747: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.748 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754921834747: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.748 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-231' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 336, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 289, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 343, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 367, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 117, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 136, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 245, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 344, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 246, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 178, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 1671, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 157, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 532, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-231' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n-------------------------- Captured stderr teardown ---------------------------\n2025-08-11 07:17:14.751 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754921834747: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.751 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754921834747: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.751 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754921834747: 'str' object has no attribute 'client_state'\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestWebSocketUpdates.test_websocket_message_queuing _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:869: in test_websocket_message_queuing\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.broadcast_to_job(\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'broadcast_to_job'\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:14.777 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834777: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.777 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754921834777: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.777 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-237' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 336, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 289, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 343, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 367, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 117, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 136, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 245, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 344, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 246, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 178, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 1671, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 157, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 532, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-237' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestWebSocketUpdates.test_websocket_heartbeat ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:885: in test_websocket_heartbeat\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.start_heartbeat(job_id, interval_seconds=\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'start_heartbeat'\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:14.804 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834804: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.804 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754921834804: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-239' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestWebSocketUpdates.test_generation_completion_notification _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:915: in test_generation_completion_notification\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.notify_completion(job_id, completion_data)\u001b[90m\u001b[39;49;00m\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'notify_completion'\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:14.831 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834830: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.831 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754921834830: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.831 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-241' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 336, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 289, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 343, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 367, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 117, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 136, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 245, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 344, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 246, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 178, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 1671, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 157, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 532, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-241' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestWebSocketUpdates.test_websocket_rate_limiting ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:928: in test_websocket_rate_limiting\n    \u001b[0mws_service.set_rate_limit(job_id, max_messages_per_second=\u001b[94m10\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n    ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'set_rate_limit'\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 07:17:14.857 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754921834857: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.857 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754921834857: 'str' object has no attribute 'client_state'\n2025-08-11 07:17:14.857 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-243' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 336, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 289, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 343, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\main.py\", line 367, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 117, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 136, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 245, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 344, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 246, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\runner.py\", line 178, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 1671, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_hooks.py\", line 512, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pluggy\\_callers.py\", line 121, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\_pytest\\python.py\", line 157, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 532, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-243' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____ TestDataQualityValidation.test_statistical_distribution_validation ______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:972: in test_statistical_distribution_validation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______ TestDataQualityValidation.test_temporal_consistency_validation ________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1000: in test_temporal_consistency_validation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestDataQualityValidation.test_data_completeness_validation _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1016: in test_data_completeness_validation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____ TestDataQualityValidation.test_anomaly_detection_in_generated_data ______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1038: in test_anomaly_detection_in_generated_data\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_with_anomalies(config)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:2038: in generate_with_anomalies\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestDataQualityValidation.test_correlation_preservation ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1049: in test_correlation_preservation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestDataQualityValidation.test_quality_metrics_calculation __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1065: in test_quality_metrics_calculation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestDataQualityValidation.test_data_diversity_validation ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1079: in test_data_diversity_validation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestDataQualityValidation.test_validation_report_generation _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1092: in test_validation_report_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestPerformanceScalability.test_high_throughput_generation __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1119: in test_high_throughput_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m perf_service.generate_parallel(config)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1489: in generate_parallel\n    \u001b[0mresults = \u001b[94mawait\u001b[39;49;00m asyncio.gather(*tasks)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:668: in generate_batch\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'Config' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestPerformanceScalability.test_memory_efficient_streaming __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1147: in test_memory_efficient_streaming\n    \u001b[0m\u001b[94masync\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m record \u001b[95min\u001b[39;49;00m stream:\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1503: in stream_generator\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestPerformanceScalability.test_horizontal_scaling ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1171: in test_horizontal_scaling\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m perf_service.generate_parallel(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1489: in generate_parallel\n    \u001b[0mresults = \u001b[94mawait\u001b[39;49;00m asyncio.gather(*tasks)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:668: in generate_batch\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'Config' object has no attribute 'num_logs'\u001b[0m\n-------------------------- Captured stderr teardown ---------------------------\n2025-08-11 07:17:15.233 | ERROR    | logging:callHandlers:1762 | Task was destroyed but it is pending!\ntask: <Task pending name='Task-266' coro=<TestPerformanceScalability.test_memory_efficient_streaming.<locals>.monitor_memory() running at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\tests\\services\\test_synthetic_data_service_v3.py:1137> wait_for=<Future pending cb=[Task.task_wakeup()]>>\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestPerformanceScalability.test_batch_size_optimization ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1194: in test_batch_size_optimization\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m perf_service.generate_batched(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1520: in generate_batched\n    \u001b[0mbatch = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.generate_batch(config, current_batch_size)\u001b[90m\u001b[39;49;00m\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:668: in generate_batch\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestPerformanceScalability.test_auto_scaling_behavior ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1249: in test_auto_scaling_behavior\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m perf_service.generate_with_auto_scaling(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1566: in generate_with_auto_scaling\n    \u001b[0mbatch = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.generate_batch(config, batch_size)\u001b[90m\u001b[39;49;00m\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:668: in generate_batch\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestErrorRecovery.test_corpus_unavailable_fallback ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1325: in test_corpus_unavailable_fallback\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch.object(recovery_service, \u001b[33m'\u001b[39;49;00m\u001b[33mget_corpus_content\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, side_effect=\u001b[96mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCorpus not found\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)):\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n                      ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <app.services.synthetic_data_service.SyntheticDataService object at 0x000002749FB81DF0> does not have the attribute 'get_corpus_content'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestErrorRecovery.test_clickhouse_connection_recovery ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1355: in test_clickhouse_connection_recovery\n    \u001b[0m\u001b[94massert\u001b[39;49;00m result[\u001b[33m\"\u001b[39;49;00m\u001b[33msuccess\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] == \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert False == True\u001b[0m\n---------------------------- Captured stderr call -----------------------------\nC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\services\\synthetic_data_service.py:587: RuntimeWarning: coroutine 'TestErrorRecovery.test_clickhouse_connection_recovery.<locals>.flaky_connection' was never awaited\n  async with get_clickhouse_client() as client:\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestErrorRecovery.test_generation_checkpoint_recovery ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1379: in test_generation_checkpoint_recovery\n    \u001b[0mresumed_result = \u001b[94mawait\u001b[39;49;00m recovery_service.resume_from_checkpoint(config)\u001b[90m\u001b[39;49;00m\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1646: in resume_from_checkpoint\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestErrorRecovery.test_websocket_disconnect_recovery _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1400: in test_websocket_disconnect_recovery\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m recovery_service.generate_with_ws_updates(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1660: in generate_with_ws_updates\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestErrorRecovery.test_circuit_breaker_operation _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1436: in test_circuit_breaker_operation\n    \u001b[0m\u001b[94massert\u001b[39;49;00m circuit_breaker.state == \u001b[33m\"\u001b[39;49;00m\u001b[33mopen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'closed' == 'open'\u001b[0m\n\u001b[1m\u001b[31mE     \u001b[0m\n\u001b[1m\u001b[31mE     \u001b[0m\u001b[91m- open\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n\u001b[1m\u001b[31mE     \u001b[92m+ closed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestErrorRecovery.test_graceful_degradation _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1513: in test_graceful_degradation\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch.object(recovery_service, \u001b[33m'\u001b[39;49;00m\u001b[33menable_clustering\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, side_effect=\u001b[96mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mClustering unavailable\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)):\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n                      ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <app.services.synthetic_data_service.SyntheticDataService object at 0x00000274A1136240> does not have the attribute 'enable_clustering'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestAdminVisibility.test_generation_job_monitoring ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1543: in test_generation_job_monitoring\n    \u001b[0m\u001b[94massert\u001b[39;49;00m status[\u001b[33m\"\u001b[39;49;00m\u001b[33mstate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] == \u001b[33m\"\u001b[39;49;00m\u001b[33mrunning\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: 'NoneType' object is not subscriptable\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestAdminVisibility.test_audit_log_generation ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1577: in test_audit_log_generation\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m admin_service.generate_with_audit(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1787: in generate_with_audit\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestAdminVisibility.test_alert_configuration _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1614: in test_alert_configuration\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch.object(admin_service, \u001b[33m'\u001b[39;49;00m\u001b[33msend_alert\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m mock_alert:\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n                      ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <app.services.synthetic_data_service.SyntheticDataService object at 0x000002749FB83110> does not have the attribute 'send_alert'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestAdminVisibility.test_job_cancellation_by_admin ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1628: in test_job_cancellation_by_admin\n    \u001b[0madmin_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() got an unexpected keyword argument 'job_id'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestAdminVisibility.test_resource_usage_tracking _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1649: in test_resource_usage_tracking\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m admin_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestAdvancedFeatures.test_anomaly_injection_strategies ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1980: in test_anomaly_injection_strategies\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_with_anomalies(config)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:2038: in generate_with_anomalies\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestAdvancedFeatures.test_cross_correlation_generation ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1998: in test_cross_correlation_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_with_correlations(config)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:2005: in generate_with_correlations\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestAdvancedFeatures.test_geo_distributed_simulation _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:2058: in test_geo_distributed_simulation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_geo_distributed(geo_config)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1950: in generate_geo_distributed\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestAdvancedFeatures.test_adaptive_generation_feedback ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:2081: in test_adaptive_generation_feedback\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_adaptive(config, target_metrics)\u001b[90m\u001b[39;49;00m\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1974: in generate_adaptive\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestAdvancedFeatures.test_multi_model_workload_generation __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:2102: in test_multi_model_workload_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_multi_model(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:936: in generate_multi_model\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestAdvancedFeatures.test_compliance_aware_generation ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:2128: in test_compliance_aware_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_compliant(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:964: in generate_compliant\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestAdvancedFeatures.test_cost_optimized_generation _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:2149: in test_cost_optimized_generation\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_cost_optimized(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:995: in generate_cost_optimized\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestAdvancedFeatures.test_versioned_corpus_generation ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:2170: in test_versioned_corpus_generation\n    \u001b[0mv1_data = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_from_corpus_version(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:1035: in generate_from_corpus_version\n    \u001b[0mrecord = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._generate_single_record(config, \u001b[94mNone\u001b[39;49;00m, i)\u001b[90m\u001b[39;49;00m\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:307: in _generate_single_record\n    \u001b[0mtimestamp = \u001b[96mself\u001b[39;49;00m._generate_timestamp(config, index)\u001b[90m\u001b[39;49;00m\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\synthetic_data_service.py\u001b[0m:352: in _generate_timestamp\n    \u001b[0mhour_offset = (index / config.num_logs) * \u001b[94m24\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n                           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\u001b[0m\n---------------------------- Captured log teardown ----------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestImplementationConsistency.test_synthetic_data_service_has_required_methods _\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_validation.py\u001b[0m:224: in test_synthetic_data_service_has_required_methods\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(service, \u001b[33m'\u001b[39;49;00m\u001b[33mvalidate_workload_distribution\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert False\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = hasattr(<app.services.synthetic_data_service.SyntheticDataService object at 0x00000274A0FE7110>, 'validate_workload_distribution')\u001b[0m\n\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::\u001b[1mTestBaseTool::test_base_tool_execute_without_metadata_attribute\u001b[0m - Failed: DID NOT RAISE <class 'AttributeError'>\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::\u001b[1mTestBaseTool::test_base_tool_exception_types\u001b[0m - assert \"'Custom error'\" == 'Custom error'\n  \n  \u001b[0m\u001b[91m- Custom error\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n  \u001b[92m+ 'Custom error'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n  ? +            +\u001b[90m\u001b[39;49;00m\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::\u001b[1mTestCostAnalyzer::test_cost_analyzer_async_execution\u001b[0m - AssertionError: assert 'Test prompt' in 'Analyzed current costs. Total estimated cost: $0.03'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::\u001b[1mTestCostAnalyzer::test_cost_analyzer_rounding_edge_cases\u001b[0m - AssertionError: Failed for costs [0.005, 0.005, 0.005]\nassert '$0.02' in 'Analyzed current costs. Total estimated cost: $0.01'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_empty_logs\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916314624'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_single_log\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916699568'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_high_latency_values\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916462656'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_zero_latency\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916267584'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_sub_millisecond_latency\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699910739680'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_exception_handling\u001b[0m - assert \"1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916522368'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\" == 'API Error'\n  \n  \u001b[0m\u001b[91m- API Error\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n  \u001b[92m+ 1 validation error for latency_analyzer\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n  \u001b[92m+   Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916522368'>, input_type=Mock]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n  \u001b[92m+     For further information visit https://errors.pydantic.dev/2.10/v/model_type\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_async_execution\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699912347968'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_large_dataset\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699916520160'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_varied_latencies\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940102464'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_partial_failure\u001b[0m - assert 'Prediction service unavailable' in \"1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940115232'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\"\n +  where \"1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940115232'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\" = str(1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940115232'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type)\n +    where 1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940115232'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type = <ExceptionInfo 1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=mod...' id='2699940115232'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type tblen=8>.value\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_edge_case_rounding\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940205904'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_negative_latencies\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940215936'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_mixed_response_formats\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940784432'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_extreme_values\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699906843616'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_concurrent_execution_timing\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='2699940777904'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_agent_message_processing.py::\u001b[1mTestAgentMessageProcessing::test_handle_tool_execution\u001b[0m - TypeError: AgentService.__init__() takes 2 positional arguments but 3 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_agent_message_processing.py::\u001b[1mTestAgentMessageProcessing::test_message_validation\u001b[0m - TypeError: AgentService.__init__() takes 2 positional arguments but 3 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_optimization_fallback_low_quality\u001b[0m - assert 4 > 100\n +  where 4 = len({'response': \"I need more specific information about your Optimize my GPU workload to provide actionable optimization recommendations. Could you provide:\\n\\u2022 Current performance metrics (latency, throughput)\\n\\u2022 Resource constraints (memory, compute)\\n\\u2022 Target improvements (e.g., 20% latency reduction)\\nThis will help me generate specific, measurable optimization strategies.\\n\\n**Quality Issues Detected:**\\n\\U0001f4ca **Specificity Issue**: The response lacked specific details and metrics.\\n\\U0001f3af **Actionability Issue**: The response didn't provide clear action steps.\\n\\U0001f4c8 **Quantification Issue**: Missing numerical values and measurements.\\n\\U0001f504 **Logic Issue**: Circular reasoning detected in the response.\\n\\U0001f4dd **Generic Content**: Found 5 generic phrases.\", 'metadata': {'is_fallback': True, 'failure_reason': 'low_quality', 'content_type': 'optimization', 'agent': 'optimization_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Provide specific metrics and constraints for better recommendations', 'Tip: Include current performance baselines for targeted optimization', \"Tip: Specify measurable goals (e.g., '20% latency reduction')\"], 'recovery_options': [{'action': 'Use Optimization Template', 'description': 'Start with our proven optimization patterns', 'link': '/templates/optimization'}, {'action': 'Run Diagnostics', 'description': 'Analyze your system to identify bottlenecks', 'link': '/tools/diagnostics'}, {'action': 'Schedule Consultation', 'description': 'Get expert help for complex optimizations', 'link': '/support/consultation'}]})\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_data_analysis_fallback_parsing_error\u001b[0m - AssertionError: assert 'system logs' in {'response': 'I encountered an issue processing the data for Analyze system logs. This typically occurs with:\\n\\u2022 Inconsistent data formats\\n\\u2022 Missing required fields\\n\\u2022 Encoding issues\\nCould you verify the data format and provide a sample?', 'metadata': {'is_fallback': True, 'failure_reason': 'parsing_error', 'content_type': 'data_analysis', 'agent': 'data_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Validate JSON structure before submission', 'Tip: Check for special characters in input data', 'Tip: Ensure consistent data types across fields'], 'recovery_options': [{'action': 'Data Validation Tool', 'description': 'Check and fix data format issues', 'link': '/tools/data-validator'}, {'action': 'Sample Analysis', 'description': 'Try with a smaller data sample first', 'link': '/tools/sample-analysis'}, {'action': 'Analysis Templates', 'description': 'Use pre-built analysis patterns', 'link': '/templates/analysis'}]}\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_action_plan_fallback_context_missing\u001b[0m - AssertionError: assert 'deployment plan' in {'response': 'I need more information to provide a valuable response for Create deployment plan. Please provide:\\n\\u2022 Specific details about your use case\\n\\u2022 Current metrics or configuration\\n\\u2022 Desired outcomes or improvements\\n\\u2022 Any constraints or requirements\\nThis will help me generate actionable recommendations.', 'metadata': {'is_fallback': True, 'failure_reason': 'context_missing', 'content_type': 'action_plan', 'agent': 'action_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Include system specifications in your request', 'Tip: Provide current configuration details', 'Tip: Share relevant performance metrics'], 'recovery_options': [{'action': 'Planning Wizard', 'description': 'Guided process for action plan creation', 'link': '/tools/planning-wizard'}, {'action': 'Best Practices', 'description': 'Review proven implementation patterns', 'link': '/docs/best-practices'}, {'action': 'Example Plans', 'description': 'See successful optimization plans', 'link': '/examples/action-plans'}]}\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_report_fallback_validation_failed\u001b[0m - AssertionError: assert 'performance report' in {'response': 'I need more information to provide a valuable response for Generate performance report. Please provide:\\n\\u2022 Specific details about your use case\\n\\u2022 Current metrics or configuration\\n\\u2022 Desired outcomes or improvements\\n\\u2022 Any constraints or requirements\\nThis will help me generate actionable recommendations.', 'metadata': {'is_fallback': True, 'failure_reason': 'validation_failed', 'content_type': 'report', 'agent': 'reporting_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Provide more specific details about your request'], 'recovery_options': [{'action': 'Report Builder', 'description': 'Interactive report generation tool', 'link': '/tools/report-builder'}, {'action': 'Metrics Dashboard', 'description': 'View real-time metrics and trends', 'link': '/dashboard/metrics'}, {'action': 'Export Templates', 'description': 'Download report templates', 'link': '/templates/reports'}]}\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_triage_fallback_timeout\u001b[0m - AssertionError: assert 'system issue' in {'response': 'The analysis for Diagnose system issue is taking longer than expected. You can:\\n\\u2022 Reduce the scope of analysis\\n\\u2022 Process in smaller batches\\n\\u2022 Use our quick optimization templates\\n\\u2022 Schedule for batch processing', 'metadata': {'is_fallback': True, 'failure_reason': 'timeout', 'content_type': 'triage', 'agent': 'triage_agent', 'retry_count': 2, 'can_retry': True}, 'diagnostics': ['Tip: Provide more specific details about your request'], 'recovery_options': [{'action': 'Refine Request', 'description': 'Add more specific details and constraints', 'link': '/help/requests'}, {'action': 'View Examples', 'description': 'See successful request examples', 'link': '/examples'}, {'action': 'Get Help', 'description': 'Contact support for assistance', 'link': '/support'}], 'retry_info': {'attempts': 2, 'max_attempts': 3, 'next_action': 'Consider alternative approach'}}\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_error_message_fallback_llm_error\u001b[0m - AttributeError: 'dict' object has no attribute 'lower'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_fallback_with_circular_reasoning\u001b[0m - AssertionError: assert 'model performance' in {'response': 'Let me provide a more concrete optimization approach for Improve model performance:\\n1. **Measure**: First, profile your current system using tools like [specific profiler]\\n2. **Identify**: Look for bottlenecks in [specific areas]\\n3. **Apply**: Implement specific techniques like [concrete optimization]\\n4. **Verify**: Measure improvements against baseline\\nWould you like me to elaborate on any of these steps?', 'metadata': {'is_fallback': True, 'failure_reason': 'circular_reasoning', 'content_type': 'optimization', 'agent': 'optimization_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Ask for specific optimization techniques', 'Tip: Request quantified recommendations', 'Tip: Focus on measurable improvements'], 'recovery_options': [{'action': 'Use Optimization Template', 'description': 'Start with our proven optimization patterns', 'link': '/templates/optimization'}, {'action': 'Run Diagnostics', 'description': 'Analyze your system to identify bottlenecks', 'link': '/tools/diagnostics'}, {'action': 'Schedule Consultation', 'description': 'Get expert help for complex optimizations', 'link': '/support/consultation'}]}\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_fallback_with_hallucination_risk\u001b[0m - assert 'future trends' in {'response': \"I need more information to provide a valuable response for Analyze future trends. Please provide:\\n\\u2022 Specific details about your use case\\n\\u2022 Current metrics or configuration\\n\\u2022 Desired outcomes or improvements\\n\\u2022 Any constraints or requirements\\nThis will help me generate actionable recommendations.\\n\\n**Quality Issues Detected:**\\n\\U0001f4ca **Specificity Issue**: The response lacked specific details and metrics.\\n\\U0001f3af **Actionability Issue**: The response didn't provide clear action steps.\\n\\U0001f4c8 **Quantification Issue**: Missing numerical values and measurements.\", 'metadata': {'is_fallback': True, 'failure_reason': 'hallucination_risk', 'content_type': 'data_analysis', 'agent': 'data_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Verify all metrics against your actual data', 'Tip: Cross-reference recommendations with documentation', 'Tip: Start with small-scale testing'], 'recovery_options': [{'action': 'Data Validation Tool', 'description': 'Check and fix data format issues', 'link': '/tools/data-validator'}, {'action': 'Sample Analysis', 'description': 'Try with a smaller data sample first', 'link': '/tools/sample-analysis'}, {'action': 'Analysis Templates', 'description': 'Use pre-built analysis patterns', 'link': '/templates/analysis'}]}\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_fallback_with_rate_limit\u001b[0m - AttributeError: 'dict' object has no attribute 'lower'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_fallback_considers_retry_count\u001b[0m - TypeError: object of type 'coroutine' has no len()\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_fallback_includes_diagnostic_tips\u001b[0m - AssertionError: assert 'CSV data' in {'response': 'I encountered an issue processing the data for Process CSV data. This typically occurs with:\\n\\u2022 Inconsistent data formats\\n\\u2022 Missing required fields\\n\\u2022 Encoding issues\\nCould you verify the data format and provide a sample?', 'metadata': {'is_fallback': True, 'failure_reason': 'parsing_error', 'content_type': 'data_analysis', 'agent': 'data_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Validate JSON structure before submission', 'Tip: Check for special characters in input data', 'Tip: Ensure consistent data types across fields'], 'recovery_options': [{'action': 'Data Validation Tool', 'description': 'Check and fix data format issues', 'link': '/tools/data-validator'}, {'action': 'Sample Analysis', 'description': 'Try with a smaller data sample first', 'link': '/tools/sample-analysis'}, {'action': 'Analysis Templates', 'description': 'Use pre-built analysis patterns', 'link': '/templates/analysis'}]}\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_fallback_with_previous_responses\u001b[0m - assert 'database queries' in {'response': \"The optimization analysis for Optimize database queries requires additional context. To provide value-driven recommendations, I need:\\n\\u2022 Baseline performance data\\n\\u2022 System architecture details\\n\\u2022 Specific bottlenecks you're experiencing\\nWith this information, I can suggest targeted optimizations with expected improvements.\", 'metadata': {'is_fallback': True, 'failure_reason': 'low_quality', 'content_type': 'optimization', 'agent': 'optimization_agent', 'retry_count': 1, 'can_retry': True}, 'diagnostics': ['Tip: Provide specific metrics and constraints for better recommendations', 'Tip: Include current performance baselines for targeted optimization', \"Tip: Specify measurable goals (e.g., '20% latency reduction')\"], 'recovery_options': [{'action': 'Use Optimization Template', 'description': 'Start with our proven optimization patterns', 'link': '/templates/optimization'}, {'action': 'Run Diagnostics', 'description': 'Analyze your system to identify bottlenecks', 'link': '/tools/diagnostics'}, {'action': 'Schedule Consultation', 'description': 'Get expert help for complex optimizations', 'link': '/support/consultation'}], 'retry_info': {'attempts': 1, 'max_attempts': 3, 'next_action': 'Retry with more context'}}\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_format_response_with_placeholders\u001b[0m - AttributeError: 'FallbackResponseService' object has no attribute '_format_response'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_get_recovery_suggestions\u001b[0m - TypeError: FallbackResponseService._get_recovery_suggestions() takes 2 positional arguments but 3 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_fallback_response_quality\u001b[0m - assert 4 >= 50\n +  where 4 = len({'response': 'I need more specific information about your Test request to provide actionable optimization recommendations. Could you provide:\\n\\u2022 Current performance metrics (latency, throughput)\\n\\u2022 Resource constraints (memory, compute)\\n\\u2022 Target improvements (e.g., 20% latency reduction)\\nThis will help me generate specific, measurable optimization strategies.', 'metadata': {'is_fallback': True, 'failure_reason': 'low_quality', 'content_type': 'optimization', 'agent': 'test_agent', 'retry_count': 0, 'can_retry': True}, 'diagnostics': ['Tip: Provide specific metrics and constraints for better recommendations', 'Tip: Include current performance baselines for targeted optimization', \"Tip: Specify measurable goals (e.g., '20% latency reduction')\"], 'recovery_options': [{'action': 'Use Optimization Template', 'description': 'Start with our proven optimization patterns', 'link': '/templates/optimization'}, {'action': 'Run Diagnostics', 'description': 'Analyze your system to identify bottlenecks', 'link': '/tools/diagnostics'}, {'action': 'Schedule Consultation', 'description': 'Get expert help for complex optimizations', 'link': '/support/consultation'}]})\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_llm_cache_service.py::\u001b[1mtest_llm_cache_service_initialization\u001b[0m - TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_llm_cache_service.py::\u001b[1mtest_cache_set_and_get\u001b[0m - TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_llm_cache_service.py::\u001b[1mtest_cache_expiration\u001b[0m - TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_llm_cache_service.py::\u001b[1mtest_cache_size_limit\u001b[0m - TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_llm_cache_service.py::\u001b[1mtest_cache_stats\u001b[0m - TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_message_handlers.py::\u001b[1mtest_handle_start_agent\u001b[0m - assert ('supervisor' in \"object mock can't be used in 'await' expression\" or 'agent' in \"object mock can't be used in 'await' expression\")\n +  where \"object mock can't be used in 'await' expression\" = <built-in method lower of str object at 0x00000274A10A4C30>()\n +    where <built-in method lower of str object at 0x00000274A10A4C30> = \"object Mock can't be used in 'await' expression\".lower\n +      where \"object Mock can't be used in 'await' expression\" = str(TypeError(\"object Mock can't be used in 'await' expression\"))\n +  and   \"object mock can't be used in 'await' expression\" = <built-in method lower of str object at 0x00000274A10A4C30>()\n +    where <built-in method lower of str object at 0x00000274A10A4C30> = \"object Mock can't be used in 'await' expression\".lower\n +      where \"object Mock can't be used in 'await' expression\" = str(TypeError(\"object Mock can't be used in 'await' expression\"))\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_validate_high_quality_optimization_content\u001b[0m - AssertionError: assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.39999999999999997, actionability_score=0.5, quantification_score=1.0, relevance_score=0.5, completeness_score=0.2, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=77, sentence_count=12, numeric_values_count=0, specific_terms_count=0, overall_score=0.5700000000000001, quality_level=<QualityLevel.ACCEPTABLE: 'acceptable'>, issues=['Contains 3 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.']}, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_validate_data_analysis_with_metrics\u001b[0m - AssertionError: assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.6, actionability_score=0.3, quantification_score=0.9999999999999999, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=59, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.6449999999999999, quality_level=<QualityLevel.ACCEPTABLE: 'acceptable'>, issues=[], suggestions=['Include clear action steps or commands']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Provide step-by-step actionable instructions with specific commands or code.']}, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_validate_action_plan_completeness\u001b[0m - AssertionError: assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.4, actionability_score=0.45, quantification_score=0.5499999999999999, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=89, sentence_count=3, numeric_values_count=0, specific_terms_count=0, overall_score=0.41250000000000003, quality_level=<QualityLevel.POOR: 'poor'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add verification steps and success criteria']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.']}, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_validate_with_strict_mode\u001b[0m - assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.0, completeness_score=0.0, novelty_score=0.0, clarity_score=0.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=0, sentence_count=0, numeric_values_count=0, specific_terms_count=0, overall_score=0.0, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[\"Validation error: name 'metrics' is not defined\"], suggestions=[]), retry_suggested=False, retry_prompt_adjustments=None, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_validate_error_message_clarity\u001b[0m - assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.0, completeness_score=0.0, novelty_score=0.0, clarity_score=0.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=0, sentence_count=0, numeric_values_count=0, specific_terms_count=0, overall_score=0.0, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[\"Validation error: name 'metrics' is not defined\"], suggestions=[]), retry_suggested=False, retry_prompt_adjustments=None, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_validate_report_redundancy\u001b[0m - AssertionError: assert 0.4 > 0.5\n +  where 0.4 = QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.6, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=1, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.4, word_count=53, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.35, quality_level=<QualityLevel.POOR: 'poor'>, issues=['Contains 1 generic phrases', 'Contains 5 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Reduce redundant information']).redundancy_ratio\n +    where QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.6, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=1, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.4, word_count=53, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.35, quality_level=<QualityLevel.POOR: 'poor'>, issues=['Contains 1 generic phrases', 'Contains 5 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Reduce redundant information']) = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.6, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=1, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.4, word_count=53, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.35, quality_level=<QualityLevel.POOR: 'poor'>, issues=['Contains 1 generic phrases', 'Contains 5 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Reduce redundant information']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.', 'Include numerical values for all claims. Show before/after metrics with percentages.']}, fallback_response=None).metrics\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_domain_specific_term_recognition\u001b[0m - AssertionError: assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.6, actionability_score=0.55, quantification_score=0.9, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=44, sentence_count=7, numeric_values_count=0, specific_terms_count=0, overall_score=0.5925, quality_level=<QualityLevel.ACCEPTABLE: 'acceptable'>, issues=[], suggestions=[]), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': []}, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_caching_mechanism\u001b[0m - AssertionError: assert 'quality:general:c1402699b07df03b830d11ee43df782f' in {}\n +  where {} = <app.services.quality_gate_service.QualityGateService object at 0x00000274A100ED20>.validation_cache\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_retry_suggestions_for_failed_validation\u001b[0m - AssertionError: assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=9, sentence_count=2, numeric_values_count=0, specific_terms_count=0, overall_score=0.125, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=['Contains 2 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Include before/after performance metrics']), retry_suggested=False, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.', 'Include numerical values for all claims. Show before/after metrics with percentages.']}, fallback_response=None).retry_suggested\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_hallucination_risk_detection\u001b[0m - AssertionError: assert 0.2 > 0.5\n +  where 0.2 = QualityMetrics(specificity_score=0.05, actionability_score=0.0, quantification_score=0.15, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=41, sentence_count=4, numeric_values_count=0, specific_terms_count=0, overall_score=0.28, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes']).hallucination_risk\n +    where QualityMetrics(specificity_score=0.05, actionability_score=0.0, quantification_score=0.15, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=41, sentence_count=4, numeric_values_count=0, specific_terms_count=0, overall_score=0.28, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes']) = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.05, actionability_score=0.0, quantification_score=0.15, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=41, sentence_count=4, numeric_values_count=0, specific_terms_count=0, overall_score=0.28, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes']), retry_suggested=False, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.', 'Include numerical values for all claims. Show before/after metrics with percentages.']}, fallback_response=None).metrics\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_triage_content_validation\u001b[0m - assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.0, completeness_score=0.0, novelty_score=0.0, clarity_score=0.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=0, sentence_count=0, numeric_values_count=0, specific_terms_count=0, overall_score=0.0, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[\"Validation error: name 'metrics' is not defined\"], suggestions=[]), retry_suggested=False, retry_prompt_adjustments=None, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_quality_level_classification\u001b[0m - AttributeError: 'QualityGateService' object has no attribute '_classify_quality_level'. Did you mean: '_determine_quality_level'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityMonitoringServiceInitialization::test_initialization_with_defaults\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'metrics_store'. Did you mean: 'metrics_buffer'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityMonitoringServiceInitialization::test_initialization_with_custom_config\u001b[0m - TypeError: QualityMonitoringService.__init__() got an unexpected keyword argument 'config'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityMonitoringServiceInitialization::test_initialization_with_metrics_collector\u001b[0m - AttributeError: <module 'app.services.quality_monitoring_service' from 'C:\\\\Users\\\\antho\\\\OneDrive\\\\Desktop\\\\Netra\\\\netra-core-generation-1\\\\app\\\\services\\\\quality_monitoring_service.py'> does not have the attribute 'MetricsCollector'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsCollection::test_collect_response_quality_metrics\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'collect_response_metrics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsCollection::test_collect_system_metrics\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'collect_system_metrics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsCollection::test_collect_error_metrics\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'collect_error_metrics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsCollection::test_batch_metrics_collection\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'collect_batch_metrics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityThresholds::test_set_quality_thresholds\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'set_thresholds'. Did you mean: '_check_thresholds'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityThresholds::test_validate_against_thresholds\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'set_thresholds'. Did you mean: '_check_thresholds'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityThresholds::test_dynamic_threshold_adjustment\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'calculate_dynamic_threshold'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestAlerting::test_trigger_alert_on_threshold_breach\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'set_thresholds'. Did you mean: '_check_thresholds'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestAlerting::test_alert_rate_limiting\u001b[0m - AttributeError: <app.services.quality_monitoring_service.QualityMonitoringService object at 0x00000274A1172630> does not have the attribute 'send_alert'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestAlerting::test_alert_escalation\u001b[0m - AttributeError: <app.services.quality_monitoring_service.QualityMonitoringService object at 0x00000274A10C5040> does not have the attribute 'escalate_alert'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsAggregation::test_aggregate_metrics_by_time_window\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'store_metric'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsAggregation::test_calculate_statistics\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'calculate_statistics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsAggregation::test_trend_analysis\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'analyze_trend'. Did you mean: '_analyze_trends'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityReporting::test_generate_quality_report\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'store_metric'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityReporting::test_export_metrics_to_json\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'store_metric'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityReporting::test_generate_sla_compliance_report\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'check_sla_compliance'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestAnomalyDetection::test_detect_anomalies_zscore\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'detect_anomalies'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestAnomalyDetection::test_detect_anomalies_iqr\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'detect_anomalies'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestAnomalyDetection::test_real_time_anomaly_detection\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'configure_anomaly_detector'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestPerformanceMonitoring::test_monitor_response_times\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'record_response_time'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestPerformanceMonitoring::test_monitor_throughput\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'record_request'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestIntegration::test_integration_with_agent_service\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'collect_agent_metrics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestIntegration::test_integration_with_database\u001b[0m - AttributeError: <module 'app.services.database' (namespace) from ['C:\\\\Users\\\\antho\\\\OneDrive\\\\Desktop\\\\Netra\\\\netra-core-generation-1\\\\app\\\\services\\\\database']> does not have the attribute 'metrics_repository'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_schema_validation_service.py::\u001b[1mTestSchemaValidationService::test_validate_schema\u001b[0m - AttributeError: __aenter__\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_security_service.py::\u001b[1mtest_encrypt_and_decrypt\u001b[0m - AttributeError: 'SecurityService' object has no attribute 'encrypt'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_state_persistence.py::\u001b[1mTestStatePersistence::test_save_agent_state\u001b[0m - TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_state_persistence.py::\u001b[1mTestStatePersistence::test_restore_agent_state\u001b[0m - TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_state_persistence.py::\u001b[1mTestStatePersistence::test_cleanup_old_states\u001b[0m - TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_state_persistence.py::\u001b[1mTestStatePersistence::test_state_versioning\u001b[0m - TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_state_persistence.py::\u001b[1mTestStatePersistence::test_concurrent_state_updates\u001b[0m - TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_supply_catalog_service.py::\u001b[1mTestSupplyCatalogService::test_get_all_options\u001b[0m - AssertionError: assert <MagicMock name='gpt-4.name' id='2699940775456'> == 'gpt-4'\n +  where <MagicMock name='gpt-4.name' id='2699940775456'> = <MagicMock name='gpt-4' id='2699940152336'>.name\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_supply_catalog_service.py::\u001b[1mTestSupplyCatalogService::test_get_option_by_id\u001b[0m - AssertionError: assert <MagicMock name='gpt-4.name' id='2699940829600'> == 'gpt-4'\n +  where <MagicMock name='gpt-4.name' id='2699940829600'> = <MagicMock name='gpt-4' id='2699942156320'>.name\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_supply_catalog_service.py::\u001b[1mTestSupplyCatalogService::test_get_option_by_name\u001b[0m - AssertionError: assert <MagicMock name='gpt-4.name' id='2699940163616'> == 'gpt-4'\n +  where <MagicMock name='gpt-4.name' id='2699940163616'> = <MagicMock name='gpt-4' id='2699942152144'>.name\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_creation_with_clickhouse_table\u001b[0m - TypeError: 'domain' is an invalid keyword argument for Corpus\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_status_transitions\u001b[0m - AttributeError: 'CorpusService' object has no attribute 'is_valid_transition'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_content_upload_batch\u001b[0m - AttributeError: 'CorpusService' object has no attribute 'upload_corpus_content'. Did you mean: '_copy_corpus_content'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_validation\u001b[0m - AttributeError: 'CorpusService' object has no attribute 'validate_corpus_record'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_availability_check\u001b[0m - AttributeError: 'CorpusService' object has no attribute 'check_corpus_availability'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_fallback_to_default\u001b[0m - AttributeError: <module 'app.services.corpus_service' from 'C:\\\\Users\\\\antho\\\\OneDrive\\\\Desktop\\\\Netra\\\\netra-core-generation-1\\\\app\\\\services\\\\corpus_service.py'> does not have the attribute 'get_default_corpus'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_caching_mechanism\u001b[0m - AttributeError: <app.services.corpus_service.CorpusService object at 0x00000274A100A990> does not have the attribute '_fetch_corpus_content'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_deletion_cascade\u001b[0m - AssertionError: Expected 'execute' to have been called.\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_metadata_tracking\u001b[0m - AttributeError: 'CorpusService' object has no attribute 'create_corpus_metadata'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_concurrent_access\u001b[0m - assert False\n +  where False = all(<generator object TestCorpusManagement.test_corpus_concurrent_access.<locals>.<genexpr> at 0x00000274A11B12A0>)\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_workload_distribution_generation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_temporal_pattern_generation\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_tool_invocation_patterns\u001b[0m - assert datetime.datetime(2025, 8, 11, 14, 17, 10, 487585) <= datetime.datetime(2025, 8, 11, 14, 17, 10, 213188)\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_error_scenario_generation\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_domain_specific_generation\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_statistical_distribution_generation\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_custom_tool_catalog_generation\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_incremental_generation\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_generation_with_corpus_sampling\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_batch_ingestion_to_clickhouse\u001b[0m - KeyError: 'request_payload'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_ingestion_error_recovery\u001b[0m - assert 0 > 0\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_ingestion_deduplication\u001b[0m - KeyError: 'request_payload'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_table_creation_on_demand\u001b[0m - assert 0 > 0\n +  where 0 = len([])\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_parallel_batch_ingestion\u001b[0m - KeyError: 'request_payload'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_ingestion_with_transformation\u001b[0m - KeyError: 'request_payload'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_ingestion_circuit_breaker\u001b[0m - assert False\n +  where False = is_open()\n +    where is_open = <app.services.synthetic_data_service.SyntheticDataService.get_circuit_breaker.<locals>.SimpleCircuitBreaker object at 0x000002749FB6B560>.is_open\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_ingestion_progress_tracking\u001b[0m - KeyError: 'request_payload'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_websocket_connection_management\u001b[0m - AssertionError: assert 'dc5b091e-950e-4f5d-8684-d670e4408a69' in {<AsyncMock id='2699919000016'>: [ConnectionInfo(websocket='dc5b091e-950e-4f5d-8684-d670e4408a69', user_id=<AsyncMock id='2699919000016'>, connected_at=datetime.datetime(2025, 8, 11, 14, 17, 14, 603936, tzinfo=datetime.timezone.utc), last_ping=datetime.datetime(2025, 8, 11, 14, 17, 14, 603936, tzinfo=datetime.timezone.utc), last_pong=None, message_count=0, error_count=1, connection_id='conn_1754921834603')]}\n +  where {<AsyncMock id='2699919000016'>: [ConnectionInfo(websocket='dc5b091e-950e-4f5d-8684-d670e4408a69', user_id=<AsyncMock id='2699919000016'>, connected_at=datetime.datetime(2025, 8, 11, 14, 17, 14, 603936, tzinfo=datetime.timezone.utc), last_ping=datetime.datetime(2025, 8, 11, 14, 17, 14, 603936, tzinfo=datetime.timezone.utc), last_pong=None, message_count=0, error_count=1, connection_id='conn_1754921834603')]} = <app.ws_manager.WebSocketManager object at 0x00000274F0C42660>.active_connections\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_generation_progress_broadcast\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'broadcast_to_job'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_batch_completion_notifications\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'notify_batch_complete'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_error_notification_handling\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'notify_error'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_websocket_reconnection_handling\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'set_job_state'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_multiple_client_subscriptions\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'broadcast_to_job'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_websocket_message_queuing\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'broadcast_to_job'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_websocket_heartbeat\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'start_heartbeat'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_generation_completion_notification\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'notify_completion'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_websocket_rate_limiting\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'set_rate_limit'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_statistical_distribution_validation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_temporal_consistency_validation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_data_completeness_validation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_anomaly_detection_in_generated_data\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_correlation_preservation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_quality_metrics_calculation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_data_diversity_validation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_validation_report_generation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_high_throughput_generation\u001b[0m - AttributeError: 'Config' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_memory_efficient_streaming\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_horizontal_scaling\u001b[0m - AttributeError: 'Config' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_batch_size_optimization\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_auto_scaling_behavior\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_corpus_unavailable_fallback\u001b[0m - AttributeError: <app.services.synthetic_data_service.SyntheticDataService object at 0x000002749FB81DF0> does not have the attribute 'get_corpus_content'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_clickhouse_connection_recovery\u001b[0m - assert False == True\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_generation_checkpoint_recovery\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_websocket_disconnect_recovery\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_circuit_breaker_operation\u001b[0m - AssertionError: assert 'closed' == 'open'\n  \n  \u001b[0m\u001b[91m- open\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n  \u001b[92m+ closed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_graceful_degradation\u001b[0m - AttributeError: <app.services.synthetic_data_service.SyntheticDataService object at 0x00000274A1136240> does not have the attribute 'enable_clustering'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_generation_job_monitoring\u001b[0m - TypeError: 'NoneType' object is not subscriptable\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_audit_log_generation\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_alert_configuration\u001b[0m - AttributeError: <app.services.synthetic_data_service.SyntheticDataService object at 0x000002749FB83110> does not have the attribute 'send_alert'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_job_cancellation_by_admin\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() got an unexpected keyword argument 'job_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_resource_usage_tracking\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_anomaly_injection_strategies\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_cross_correlation_generation\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_geo_distributed_simulation\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_adaptive_generation_feedback\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_multi_model_workload_generation\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_compliance_aware_generation\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_cost_optimized_generation\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_versioned_corpus_generation\u001b[0m - AttributeError: 'GenerationConfig' object has no attribute 'num_logs'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_validation.py::\u001b[1mTestImplementationConsistency::test_synthetic_data_service_has_required_methods\u001b[0m - AssertionError: assert False\n +  where False = hasattr(<app.services.synthetic_data_service.SyntheticDataService object at 0x00000274A0FE7110>, 'validate_workload_distribution')\n\u001b[31mERROR\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::\u001b[1mTestBaseTool::test_base_tool_execute_wrapper\u001b[0m - RuntimeError: There is no current event loop in thread 'MainThread'.\n\u001b[31mERROR\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_basic_functionality\u001b[0m - RuntimeError: There is no current event loop in thread 'MainThread'.\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_agent_message_processing.py::\u001b[1mTestAgentMessageProcessing::test_process_user_message\u001b[0m - RuntimeError: There is no current event loop in thread 'MainThread'.\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_clickhouse_service.py::\u001b[1mTestClickHouseConnection::test_client_initialization\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_clickhouse_service.py::\u001b[1mTestClickHouseConnection::test_basic_query_execution\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_clickhouse_service.py::\u001b[1mTestClickHouseConnection::test_query_with_parameters\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_clickhouse_service.py::\u001b[1mTestBasicOperations::test_show_tables\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestUnitOfWork::test_uow_initialization\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestUnitOfWork::test_uow_transaction_commit\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestUnitOfWork::test_uow_transaction_rollback\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestUnitOfWork::test_uow_nested_transactions\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestUnitOfWork::test_uow_concurrent_access\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_create\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_bulk_create\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_get_by_id\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_get_many\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_update\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_delete\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_soft_delete\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_pagination\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestMessageRepository::test_get_messages_by_thread\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestMessageRepository::test_get_messages_with_pagination\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestMessageRepository::test_get_latest_messages\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestThreadRepository::test_get_threads_by_user\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestThreadRepository::test_get_active_threads\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestThreadRepository::test_archive_thread\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestRunRepository::test_create_run_with_tools\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestRunRepository::test_update_run_status\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestRunRepository::test_get_active_runs\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestReferenceRepository::test_create_reference_with_metadata\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestReferenceRepository::test_get_references_by_message\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestReferenceRepository::test_search_references\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_complete_generation_workflow\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_multi_tenant_generation\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_real_time_streaming_pipeline\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_failure_recovery_integration\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_cross_component_validation\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_performance_under_load\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_data_consistency_verification\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_monitoring_integration\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_security_and_access_control\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31mERROR\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_cleanup_and_retention\u001b[0m - AttributeError: 'FixtureDef' object has no attribute 'unittest'\n\u001b[31m================= \u001b[31m\u001b[1m160 failed\u001b[0m, \u001b[32m331 passed\u001b[0m, \u001b[31m\u001b[1m42 errors\u001b[0m\u001b[31m in 17.55s\u001b[0m\u001b[31m =================\u001b[0m\n================================================================================\n[FAIL] TESTS FAILED with exit code 1 after 23.21s\n================================================================================\n\n"
    },
    "frontend": {
      "status": "skipped",
      "duration": 0,
      "exit_code": null,
      "output": ""
    },
    "overall": {
      "status": "failed",
      "start_time": 1754921821.1447148,
      "end_time": 1754921845.213894
    }
  },
  "summary": {
    "total_duration": 24.06718897819519,
    "overall_passed": false,
    "exit_code": 1
  }
}