{
  "runs": [
    {
      "timestamp": "2025-08-16T20:21:25.184468",
      "level": "smoke",
      "results": {
        "backend": {
          "status": "passed",
          "duration": 5.516597270965576,
          "exit_code": 0,
          "output": "Loaded test environment from C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\.env.test\n================================================================================\nNETRA AI PLATFORM - BACKEND TEST RUNNER\n================================================================================\nTest Configuration:\n  Category: smoke\n  Parallel: disabled\n  Coverage: disabled\n  Fail Fast: enabled\n  Environment: testing\n\nRunning command:\n  pytest app/tests/routes/test_health_route.py app/tests/core/test_error_handling.py::TestNetraExceptions::test_configuration_error app/tests/core/test_config_manager.py::TestConfigManager::test_initialization app/tests/services/test_security_service.py::test_encrypt_and_decrypt tests/test_system_startup.py::TestSystemStartup::test_configuration_loading -v -x --maxfail=1 --tb=short --asyncio-mode=auto --color=yes --strict-markers --disable-warnings -p no:warnings -m not real_services\n================================================================================\n\u001b[1m============================= test session starts =============================\u001b[0m\nplatform win32 -- Python 3.12.4, pytest-8.4.1, pluggy-1.6.0 -- C:\\Users\\antho\\miniconda3\\python.exe\ncachedir: .pytest_cache\nmetadata: {'Python': '3.12.4', 'Platform': 'Windows-11-10.0.26100-SP0', 'Packages': {'pytest': '8.4.1', 'pluggy': '1.6.0'}, 'Plugins': {'anyio': '4.9.0', 'Faker': '37.4.2', 'langsmith': '0.4.10', 'asyncio': '0.21.1', 'cov': '6.2.1', 'html': '4.1.1', 'json-report': '1.5.0', 'metadata': '3.1.1', 'mock': '3.14.1', 'timeout': '2.4.0', 'xdist': '3.8.0', 'typeguard': '4.4.4'}}\nrootdir: C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, Faker-37.4.2, langsmith-0.4.10, asyncio-0.21.1, cov-6.2.1, html-4.1.1, json-report-1.5.0, metadata-3.1.1, mock-3.14.1, timeout-2.4.0, xdist-3.8.0, typeguard-4.4.4\nasyncio: mode=Mode.AUTO\n\u001b[1mcollecting ... \u001b[0mcollected 7 items\n\napp\\tests\\routes\\test_health_route.py::test_basic_import \u001b[32mPASSED\u001b[0m\u001b[32m          [ 14%]\u001b[0m\napp\\tests\\routes\\test_health_route.py::test_health_endpoint_direct \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\napp\\tests\\routes\\test_health_route.py::test_live_endpoint \u001b[32mPASSED\u001b[0m\u001b[32m         [ 42%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_configuration_error \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_initialization \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\napp\\tests\\services\\test_security_service.py::test_encrypt_and_decrypt \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\napp::TestSystemStartup::test_configuration_loading \u001b[32mPASSED\u001b[0m\u001b[32m                [100%]\u001b[0m\n\n\u001b[32m============================== \u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 0.28s\u001b[0m\u001b[32m ==============================\u001b[0m\n================================================================================\n[PASS] ALL TESTS PASSED in 4.72s\n================================================================================\n\n--- Logging error in Loguru Handler #1 ---\nRecord was: {'elapsed': datetime.timedelta(seconds=3, microseconds=237322), 'exception': None, 'extra': {}, 'file': (name='__init__.py', path='C:\\\\Users\\\\antho\\\\miniconda3\\\\Lib\\\\logging\\\\__init__.py'), 'function': 'handle', 'level': (name='INFO', no=20, icon='\\u2139\\ufe0f'), 'line': 1028, 'message': 'Multiprocessing resources cleaned up', 'module': '__init__', 'name': 'logging', 'process': (id=34616, name='MainProcess'), 'thread': (id=20932, name='MainThread'), 'time': datetime(2025, 8, 16, 20, 21, 24, 640633, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=61200), 'Pacific Daylight Time'))}\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\loguru\\_handler.py\", line 315, in _queued_writer\n    self._sink.write(message)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\loguru\\_simple_sinks.py\", line 16, in write\n    self._stream.write(message)\nValueError: I/O operation on closed file.\n--- End of logging error ---\n",
          "test_counts": {
            "total": 7,
            "passed": 7,
            "failed": 0,
            "skipped": 0,
            "errors": 0,
            "test_files": 8
          },
          "coverage": null,
          "test_details": []
        },
        "frontend": {
          "status": "skipped",
          "duration": 0,
          "exit_code": null,
          "output": "",
          "test_counts": {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "errors": 0
          },
          "coverage": null
        },
        "e2e": {
          "status": "pending",
          "duration": 0,
          "exit_code": null,
          "output": "",
          "test_counts": {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "errors": 0
          },
          "coverage": null
        },
        "overall": {
          "status": "passed",
          "start_time": 1755400879.6642416,
          "end_time": 1755400885.1831055
        }
      },
      "summary": {
        "total": 7,
        "passed": 7,
        "failed": 0,
        "skipped": 0,
        "errors": 0
      }
    },
    {
      "timestamp": "2025-08-16T20:23:21.732973",
      "level": "real_services",
      "results": {
        "backend": {
          "status": "failed",
          "duration": 10.578988313674927,
          "exit_code": 1,
          "output": "Loaded test environment from C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\.env.test\n================================================================================\nNETRA AI PLATFORM - BACKEND TEST RUNNER\n================================================================================\nTest Configuration:\n  Category: all\n  Parallel: disabled\n  Coverage: disabled\n  Fail Fast: enabled\n  Environment: testing\n\nRunning command:\n  pytest app/tests tests integration_tests -vv -x --maxfail=1 --tb=short --asyncio-mode=auto --color=yes --strict-markers --disable-warnings -p no:warnings -m real_services\n================================================================================\n\u001b[1m============================= test session starts =============================\u001b[0m\nplatform win32 -- Python 3.12.4, pytest-8.4.1, pluggy-1.6.0 -- C:\\Users\\antho\\miniconda3\\python.exe\ncachedir: .pytest_cache\nmetadata: {'Python': '3.12.4', 'Platform': 'Windows-11-10.0.26100-SP0', 'Packages': {'pytest': '8.4.1', 'pluggy': '1.6.0'}, 'Plugins': {'anyio': '4.9.0', 'Faker': '37.4.2', 'langsmith': '0.4.10', 'asyncio': '0.21.1', 'cov': '6.2.1', 'html': '4.1.1', 'json-report': '1.5.0', 'metadata': '3.1.1', 'mock': '3.14.1', 'timeout': '2.4.0', 'xdist': '3.8.0', 'typeguard': '4.4.4'}}\nrootdir: C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, Faker-37.4.2, langsmith-0.4.10, asyncio-0.21.1, cov-6.2.1, html-4.1.1, json-report-1.5.0, metadata-3.1.1, mock-3.14.1, timeout-2.4.0, xdist-3.8.0, typeguard-4.4.4\nasyncio: mode=Mode.AUTO\n\u001b[1mcollecting ... \u001b[0mcollected 4513 items / 1 error / 1 skipped\n\n=================================== ERRORS ====================================\n\u001b[31m\u001b[1m______________ ERROR collecting tests/test_real_data_services.py ______________\u001b[0m\n'real_data' not found in `markers` configuration option\n\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n\u001b[31mERROR\u001b[0m app\\tests\\test_real_data_services.py - Failed: 'real_data' not found in `markers` configuration option\n\u001b[31m!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\n\u001b[31m========================= \u001b[33m1 skipped\u001b[0m, \u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 5.05s\u001b[0m\u001b[31m =========================\u001b[0m\n================================================================================\n[FAIL] TESTS FAILED with exit code 1 after 9.61s\n================================================================================\n\n--- Logging error in Loguru Handler #7 ---\nRecord was: {'elapsed': datetime.timedelta(seconds=8, microseconds=171028), 'exception': None, 'extra': {}, 'file': (name='__init__.py', path='C:\\\\Users\\\\antho\\\\miniconda3\\\\Lib\\\\logging\\\\__init__.py'), 'function': 'handle', 'level': (name='INFO', no=20, icon='\\u2139\\ufe0f'), 'line': 1028, 'message': 'Multiprocessing resources cleaned up', 'module': '__init__', 'name': 'logging', 'process': (id=41416, name='MainProcess'), 'thread': (id=1760, name='MainThread'), 'time': datetime(2025, 8, 16, 20, 23, 21, 11313, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=61200), 'Pacific Daylight Time'))}\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\loguru\\_handler.py\", line 315, in _queued_writer\n    self._sink.write(message)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\loguru\\_simple_sinks.py\", line 16, in write\n    self._stream.write(message)\nValueError: I/O operation on closed file.\n--- End of logging error ---\n",
          "test_counts": {
            "total": 2,
            "passed": 0,
            "failed": 0,
            "skipped": 1,
            "errors": 1,
            "test_files": 0
          },
          "coverage": null,
          "test_details": []
        },
        "frontend": {
          "status": "skipped",
          "duration": 0,
          "exit_code": null,
          "output": "",
          "test_counts": {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "errors": 0
          },
          "coverage": null
        },
        "e2e": {
          "status": "pending",
          "duration": 0,
          "exit_code": null,
          "output": "",
          "test_counts": {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "errors": 0
          },
          "coverage": null
        },
        "overall": {
          "status": "failed",
          "start_time": 1755400991.150426,
          "end_time": 1755401001.7309391
        }
      },
      "summary": {
        "total": 2,
        "passed": 0,
        "failed": 0,
        "skipped": 1,
        "errors": 1
      }
    }
  ],
  "flaky_tests": {},
  "failure_patterns": {},
  "performance_trends": []
}