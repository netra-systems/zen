{
  "timestamp": "2025-08-11T05:50:12.546378",
  "test_level": "unit",
  "configuration": {
    "description": "Unit tests for isolated components (1-2 minutes)",
    "purpose": "Development validation, component testing",
    "backend_args": [
      "--category",
      "unit",
      "-v"
    ],
    "frontend_args": [
      "--testPathPattern=unit"
    ],
    "timeout": 120,
    "run_coverage": false,
    "run_both": true
  },
  "results": {
    "backend": {
      "status": "failed",
      "duration": 36.79616093635559,
      "exit_code": 1,
      "output": "================================================================================\nNETRA AI PLATFORM - BACKEND TEST RUNNER\n================================================================================\nTest Configuration:\n  Category: unit\n  Parallel: disabled\n  Coverage: disabled\n  Fail Fast: disabled\n  Environment: testing\n\nRunning command:\n  pytest app/tests/services app/tests/core -vv --tb=short --asyncio-mode=auto --color=yes --strict-markers --disable-warnings -p no:warnings\n================================================================================\n\u001b[1m============================= test session starts =============================\u001b[0m\nplatform win32 -- Python 3.12.4, pytest-8.3.2, pluggy-1.5.0 -- C:\\Users\\antho\\miniconda3\\python.exe\ncachedir: .pytest_cache\nmetadata: {'Python': '3.12.4', 'Platform': 'Windows-11-10.0.26100-SP0', 'Packages': {'pytest': '8.3.2', 'pluggy': '1.5.0'}, 'Plugins': {'anyio': '4.9.0', 'Faker': '37.4.2', 'langsmith': '0.4.10', 'asyncio': '1.1.0', 'cov': '6.2.1', 'html': '4.1.1', 'json-report': '1.5.0', 'metadata': '3.1.1', 'mock': '3.14.1', 'timeout': '2.4.0', 'xdist': '3.8.0', 'typeguard': '4.4.4'}}\nrootdir: C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, Faker-37.4.2, langsmith-0.4.10, asyncio-1.1.0, cov-6.2.1, html-4.1.1, json-report-1.5.0, metadata-3.1.1, mock-3.14.1, timeout-2.4.0, xdist-3.8.0, typeguard-4.4.4\nasyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=function, asyncio_default_test_loop_scope=function\n\u001b[1mcollecting ... \u001b[0mcollected 519 items\n\napp\\tests\\services\\agents\\test_sub_agent.py::test_agent_node_is_coroutine <- ..\\v2\\app\\tests\\services\\agents\\test_sub_agent.py \u001b[31mFAILED\u001b[0m\u001b[31m [  0%]\u001b[0m\napp\\tests\\services\\agents\\test_supervisor_service.py::test_supervisor_end_to_end <- ..\\v2\\app\\tests\\services\\agents\\test_supervisor_service.py \u001b[31mERROR\u001b[0m\u001b[31m [  0%]\u001b[0m\napp\\tests\\services\\agents\\test_tools.py::test_tool_dispatcher <- ..\\v2\\app\\tests\\services\\agents\\test_tools.py \u001b[31mFAILED\u001b[0m\u001b[31m [  0%]\u001b[0m\napp\\tests\\services\\agents\\test_tools.py::test_tool_dispatcher_tool_not_found <- ..\\v2\\app\\tests\\services\\agents\\test_tools.py \u001b[31mFAILED\u001b[0m\u001b[31m [  0%]\u001b[0m\napp\\tests\\services\\agents\\test_tools.py::test_tool_dispatcher_tool_error <- ..\\v2\\app\\tests\\services\\agents\\test_tools.py \u001b[31mFAILED\u001b[0m\u001b[31m [  0%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\test_tool_builder.py::test_tool_builder_and_dispatcher <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\test_tool_builder.py \u001b[31mFAILED\u001b[0m\u001b[31m [  1%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_advanced_optimization_for_core_function.py::test_advanced_optimization_for_core_function_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_advanced_optimization_for_core_function.py \u001b[32mPASSED\u001b[0m\u001b[31m [  1%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestToolMetadata::test_tool_metadata_creation_basic <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  1%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestToolMetadata::test_tool_metadata_creation_full <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  1%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestToolMetadata::test_tool_metadata_validation_missing_required <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  1%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestToolMetadata::test_tool_metadata_dict_conversion <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  2%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestToolMetadata::test_tool_metadata_json_serialization <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  2%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestToolMetadata::test_tool_metadata_edge_cases <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  2%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_instantiation <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  2%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_get_metadata <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  2%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_execute_wrapper <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  3%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_execute_failure <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  3%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_concrete_tool_run_method <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  3%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_with_llm_name <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  3%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_multiple_executions <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  3%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_concurrent_execution <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  4%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_execute_without_metadata_attribute <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[31mFAILED\u001b[0m\u001b[31m [  4%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_inheritance_chain <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  4%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_with_complex_kwargs <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  4%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_exception_types <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[31mFAILED\u001b[0m\u001b[31m [  4%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::TestBaseTool::test_base_tool_async_delay <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py \u001b[32mPASSED\u001b[0m\u001b[31m [  5%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_basic_functionality <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py \u001b[32mPASSED\u001b[0m\u001b[31m [  5%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_empty_logs <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py \u001b[32mPASSED\u001b[0m\u001b[31m [  5%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_single_log <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py \u001b[32mPASSED\u001b[0m\u001b[31m [  5%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_large_costs <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py \u001b[32mPASSED\u001b[0m\u001b[31m [  5%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_zero_costs <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py \u001b[32mPASSED\u001b[0m\u001b[31m [  5%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_fractional_cents <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py \u001b[32mPASSED\u001b[0m\u001b[31m [  6%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_exception_handling <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py \u001b[32mPASSED\u001b[0m\u001b[31m [  6%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_async_execution <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [  6%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_concurrent_logs <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py \u001b[32mPASSED\u001b[0m\u001b[31m [  6%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_negative_costs <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py \u001b[32mPASSED\u001b[0m\u001b[31m [  6%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_mixed_model_types <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py \u001b[32mPASSED\u001b[0m\u001b[31m [  7%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_partial_failure <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py \u001b[32mPASSED\u001b[0m\u001b[31m [  7%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::TestCostAnalyzer::test_cost_analyzer_rounding_edge_cases <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [  7%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_reduction_quality_preservation.py::test_cost_reduction_quality_preservation_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_reduction_quality_preservation.py \u001b[32mPASSED\u001b[0m\u001b[31m [  7%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_simulation_for_increased_usage.py::test_cost_simulation_for_increased_usage_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_simulation_for_increased_usage.py \u001b[32mPASSED\u001b[0m\u001b[31m [  7%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_kv_cache_optimization_audit.py::test_kv_cache_optimization_audit_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_kv_cache_optimization_audit.py \u001b[32mPASSED\u001b[0m\u001b[31m [  8%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_basic_functionality <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_empty_logs <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_single_log <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_high_latency_values <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_zero_latency <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_sub_millisecond_latency <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_exception_handling <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_async_execution <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_large_dataset <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_varied_latencies <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_partial_failure <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_edge_case_rounding <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_negative_latencies <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_mixed_response_formats <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_extreme_values <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::TestLatencyAnalyzer::test_latency_analyzer_concurrent_execution_timing <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 11%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_multi_objective_optimization.py::test_multi_objective_optimization_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_multi_objective_optimization.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 11%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_new_model_effectiveness_analysis.py::test_new_model_effectiveness_analysis_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_new_model_effectiveness_analysis.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 11%]\u001b[0m\napp\\tests\\services\\apex_optimizer_agent\\tools\\test_tool_latency_optimization.py::test_tool_latency_optimization_tool <- ..\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_tool_latency_optimization.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 11%]\u001b[0m\napp\\tests\\services\\test_agent_message_processing.py::TestAgentMessageProcessing::test_process_user_message <- ..\\v2\\app\\tests\\services\\test_agent_message_processing.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 11%]\u001b[0m\napp\\tests\\services\\test_agent_message_processing.py::TestAgentMessageProcessing::test_handle_tool_execution <- ..\\v2\\app\\tests\\services\\test_agent_message_processing.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 12%]\u001b[0m\napp\\tests\\services\\test_agent_message_processing.py::TestAgentMessageProcessing::test_message_validation <- ..\\v2\\app\\tests\\services\\test_agent_message_processing.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 12%]\u001b[0m\napp\\tests\\services\\test_agent_service.py::test_run_agent <- ..\\v2\\app\\tests\\services\\test_agent_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 12%]\u001b[0m\napp\\tests\\services\\test_agent_service_advanced.py::test_agent_service_initialization <- ..\\v2\\app\\tests\\services\\test_agent_service_advanced.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 12%]\u001b[0m\napp\\tests\\services\\test_agent_service_advanced.py::test_agent_service_process_request <- ..\\v2\\app\\tests\\services\\test_agent_service_advanced.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 12%]\u001b[0m\napp\\tests\\services\\test_agent_service_advanced.py::test_agent_service_websocket_message_handling <- ..\\v2\\app\\tests\\services\\test_agent_service_advanced.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 13%]\u001b[0m\napp\\tests\\services\\test_clickhouse_service.py::TestClickHouseConnection::test_client_initialization <- ..\\v2\\app\\tests\\services\\test_clickhouse_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 13%]\u001b[0m\napp\\tests\\services\\test_clickhouse_service.py::TestClickHouseConnection::test_list_corpus_tables <- ..\\v2\\app\\tests\\services\\test_clickhouse_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 13%]\u001b[0m\napp\\tests\\services\\test_clickhouse_service.py::TestClickHouseConnection::test_basic_query_execution <- ..\\v2\\app\\tests\\services\\test_clickhouse_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 13%]\u001b[0m\napp\\tests\\services\\test_clickhouse_service.py::TestClickHouseConnection::test_query_with_parameters <- ..\\v2\\app\\tests\\services\\test_clickhouse_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 13%]\u001b[0m\napp\\tests\\services\\test_clickhouse_service.py::TestBasicOperations::test_show_tables <- ..\\v2\\app\\tests\\services\\test_clickhouse_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 14%]\u001b[0m\napp\\tests\\services\\test_corpus_service.py::TestCorpusService::test_corpus_status_enum <- ..\\v2\\app\\tests\\services\\test_corpus_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 14%]\u001b[0m\napp\\tests\\services\\test_corpus_service.py::TestCorpusService::test_corpus_schema <- ..\\v2\\app\\tests\\services\\test_corpus_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 14%]\u001b[0m\napp\\tests\\services\\test_corpus_service.py::TestCorpusService::test_corpus_create_schema <- ..\\v2\\app\\tests\\services\\test_corpus_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 14%]\u001b[0m\napp\\tests\\services\\test_corpus_service.py::TestCorpusService::test_corpus_service_import <- ..\\v2\\app\\tests\\services\\test_corpus_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 14%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestUnitOfWork::test_uow_initialization <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 15%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestUnitOfWork::test_uow_transaction_commit <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 15%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestUnitOfWork::test_uow_transaction_rollback <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 15%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestUnitOfWork::test_uow_nested_transactions <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 15%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestUnitOfWork::test_uow_concurrent_access <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 15%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_create <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 15%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_bulk_create <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 16%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_get_by_id <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 16%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_get_many <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 16%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_update <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 16%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_delete <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 16%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_soft_delete <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 17%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestBaseRepository::test_repository_pagination <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 17%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestMessageRepository::test_get_messages_by_thread <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 17%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestMessageRepository::test_get_messages_with_pagination <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 17%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestMessageRepository::test_get_latest_messages <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 17%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestThreadRepository::test_get_threads_by_user <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 18%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestThreadRepository::test_get_active_threads <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 18%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestThreadRepository::test_archive_thread <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 18%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestRunRepository::test_create_run_with_tools <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 18%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestRunRepository::test_update_run_status <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 18%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestRunRepository::test_get_active_runs <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 19%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestReferenceRepository::test_create_reference_with_metadata <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 19%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestReferenceRepository::test_get_references_by_message <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 19%]\u001b[0m\napp\\tests\\services\\test_database_repositories.py::TestReferenceRepository::test_search_references <- ..\\v2\\app\\tests\\services\\test_database_repositories.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 19%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_process_demo_chat_new_session <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 19%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_process_demo_chat_existing_session <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 20%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_get_industry_templates_valid <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 20%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_get_industry_templates_invalid <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 20%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_calculate_roi_financial <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 20%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_calculate_roi_different_industries <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 20%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_generate_synthetic_metrics <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 21%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_generate_report <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 21%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_generate_report_session_not_found <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 21%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_get_session_status <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 21%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_get_session_status_not_found <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 21%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_submit_feedback <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 21%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_track_demo_interaction <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 22%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_get_analytics_summary <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 22%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_generate_demo_response <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 22%]\u001b[0m\napp\\tests\\services\\test_demo_service.py::TestDemoService::test_error_handling_redis_failure <- ..\\v2\\app\\tests\\services\\test_demo_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 22%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_optimization_fallback_low_quality <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 22%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_data_analysis_fallback_parsing_error <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_action_plan_fallback_context_missing <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_report_fallback_validation_failed <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_triage_fallback_timeout <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_error_message_fallback_llm_error <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_fallback_with_circular_reasoning <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 24%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_fallback_with_hallucination_risk <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 24%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_fallback_with_rate_limit <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 24%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_fallback_considers_retry_count <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 24%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_fallback_includes_diagnostic_tips <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 24%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_generate_fallback_with_previous_responses <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 25%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_format_response_with_placeholders <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 25%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_get_diagnostic_tips_for_failure_reason <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 25%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_get_recovery_suggestions <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 25%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_fallback_response_quality <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 25%]\u001b[0m\napp\\tests\\services\\test_fallback_response_service.py::TestFallbackResponseService::test_fallback_service_initialization <- ..\\v2\\app\\tests\\services\\test_fallback_response_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 26%]\u001b[0m\napp\\tests\\services\\test_generation_service.py::test_update_job_status <- ..\\v2\\app\\tests\\services\\test_generation_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 26%]\u001b[0m\napp\\tests\\services\\test_generation_service.py::test_get_corpus_from_clickhouse <- ..\\v2\\app\\tests\\services\\test_generation_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 26%]\u001b[0m\napp\\tests\\services\\test_generation_service.py::test_save_corpus_to_clickhouse <- ..\\v2\\app\\tests\\services\\test_generation_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 26%]\u001b[0m\napp\\tests\\services\\test_job_store_service.py::TestJobStore::test_job_store_initialization <- ..\\v2\\app\\tests\\services\\test_job_store_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 26%]\u001b[0m\napp\\tests\\services\\test_job_store_service.py::TestJobStore::test_set_and_get_job <- ..\\v2\\app\\tests\\services\\test_job_store_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 26%]\u001b[0m\napp\\tests\\services\\test_job_store_service.py::TestJobStore::test_update_job_status <- ..\\v2\\app\\tests\\services\\test_job_store_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 27%]\u001b[0m\napp\\tests\\services\\test_job_store_service.py::TestJobStore::test_nonexistent_job <- ..\\v2\\app\\tests\\services\\test_job_store_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 27%]\u001b[0m\napp\\tests\\services\\test_job_store_service.py::TestJobStore::test_global_job_store <- ..\\v2\\app\\tests\\services\\test_job_store_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 27%]\u001b[0m\napp\\tests\\services\\test_key_manager.py::test_load_from_settings_success <- ..\\v2\\app\\tests\\services\\test_key_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 27%]\u001b[0m\napp\\tests\\services\\test_key_manager.py::test_load_from_settings_jwt_key_too_short <- ..\\v2\\app\\tests\\services\\test_key_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 27%]\u001b[0m\napp\\tests\\services\\test_llm_cache_service.py::test_llm_cache_service_initialization <- ..\\v2\\app\\tests\\services\\test_llm_cache_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\napp\\tests\\services\\test_llm_cache_service.py::test_cache_set_and_get <- ..\\v2\\app\\tests\\services\\test_llm_cache_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\napp\\tests\\services\\test_llm_cache_service.py::test_cache_expiration <- ..\\v2\\app\\tests\\services\\test_llm_cache_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\napp\\tests\\services\\test_llm_cache_service.py::test_cache_size_limit <- ..\\v2\\app\\tests\\services\\test_llm_cache_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\napp\\tests\\services\\test_llm_cache_service.py::test_cache_stats <- ..\\v2\\app\\tests\\services\\test_llm_cache_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\napp\\tests\\services\\test_message_handlers.py::test_message_handler_service_initialization <- ..\\v2\\app\\tests\\services\\test_message_handlers.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 29%]\u001b[0m\napp\\tests\\services\\test_message_handlers.py::test_handle_start_agent <- ..\\v2\\app\\tests\\services\\test_message_handlers.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 29%]\u001b[0m\napp\\tests\\services\\test_message_handlers.py::test_websocket_schema_imports <- ..\\v2\\app\\tests\\services\\test_message_handlers.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 29%]\u001b[0m\napp\\tests\\services\\test_overall_supervisor.py::test_overall_supervisor_workflow <- ..\\v2\\app\\tests\\services\\test_overall_supervisor.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 29%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestPermissionServiceConstants::test_role_hierarchy_ordering \u001b[32mPASSED\u001b[0m\u001b[31m [ 29%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestPermissionServiceConstants::test_role_permissions_inheritance \u001b[32mPASSED\u001b[0m\u001b[31m [ 30%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestPermissionServiceConstants::test_critical_permissions_restricted \u001b[32mPASSED\u001b[0m\u001b[31m [ 30%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestDetectDeveloperStatus::test_detect_developer_with_dev_mode_env \u001b[32mPASSED\u001b[0m\u001b[31m [ 30%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestDetectDeveloperStatus::test_detect_developer_with_netra_email \u001b[32mPASSED\u001b[0m\u001b[31m [ 30%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestDetectDeveloperStatus::test_detect_developer_with_dev_environment \u001b[32mPASSED\u001b[0m\u001b[31m [ 30%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestDetectDeveloperStatus::test_detect_developer_priority_order \u001b[32mPASSED\u001b[0m\u001b[31m [ 31%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestDetectDeveloperStatus::test_detect_developer_with_none_email \u001b[32mPASSED\u001b[0m\u001b[31m [ 31%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestUpdateUserRole::test_update_user_role_auto_elevate_to_developer \u001b[32mPASSED\u001b[0m\u001b[31m [ 31%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestUpdateUserRole::test_update_user_role_no_elevation_for_admin \u001b[32mPASSED\u001b[0m\u001b[31m [ 31%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestUpdateUserRole::test_update_user_role_no_elevation_for_super_admin \u001b[32mPASSED\u001b[0m\u001b[31m [ 31%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestUpdateUserRole::test_update_user_role_skip_developer_check \u001b[32mPASSED\u001b[0m\u001b[31m [ 31%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestUpdateUserRole::test_update_user_role_already_developer \u001b[32mPASSED\u001b[0m\u001b[31m [ 32%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestUpdateUserRole::test_update_user_role_power_user_elevation \u001b[32mPASSED\u001b[0m\u001b[31m [ 32%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestCheckPermission::test_check_permission_standard_user \u001b[32mPASSED\u001b[0m\u001b[31m [ 32%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestCheckPermission::test_check_permission_super_admin \u001b[32mPASSED\u001b[0m\u001b[31m [ 32%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestCheckPermission::test_check_permission_developer \u001b[32mPASSED\u001b[0m\u001b[31m [ 32%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestCheckPermission::test_check_permission_invalid_role \u001b[32mPASSED\u001b[0m\u001b[31m [ 33%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestCheckPermission::test_check_permission_none_role \u001b[32mPASSED\u001b[0m\u001b[31m [ 33%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestGetUserPermissions::test_get_user_permissions_standard_user \u001b[32mPASSED\u001b[0m\u001b[31m [ 33%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestGetUserPermissions::test_get_user_permissions_super_admin \u001b[32mPASSED\u001b[0m\u001b[31m [ 33%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestGetUserPermissions::test_get_user_permissions_invalid_role \u001b[32mPASSED\u001b[0m\u001b[31m [ 33%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestAdminChecks::test_is_admin_or_higher \u001b[32mPASSED\u001b[0m\u001b[31m [ 34%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestAdminChecks::test_is_developer_or_higher \u001b[32mPASSED\u001b[0m\u001b[31m [ 34%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestPermissionGroups::test_has_any_permission \u001b[32mPASSED\u001b[0m\u001b[31m [ 34%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestPermissionGroups::test_has_all_permissions \u001b[32mPASSED\u001b[0m\u001b[31m [ 34%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestSecurityEdgeCases::test_sql_injection_in_permission_check \u001b[32mPASSED\u001b[0m\u001b[31m [ 34%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestSecurityEdgeCases::test_case_sensitivity_in_roles \u001b[32mPASSED\u001b[0m\u001b[31m [ 35%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestSecurityEdgeCases::test_permission_escalation_attempt \u001b[32mPASSED\u001b[0m\u001b[31m [ 35%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestIntegrationScenarios::test_new_user_onboarding_flow \u001b[32mPASSED\u001b[0m\u001b[31m [ 35%]\u001b[0m\napp\\tests\\services\\test_permission_service.py::TestIntegrationScenarios::test_production_environment_security \u001b[32mPASSED\u001b[0m\u001b[31m [ 35%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_high_quality_optimization_content <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_low_quality_generic_content <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 36%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_detect_circular_reasoning <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 36%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_data_analysis_with_metrics <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 36%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_action_plan_completeness <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 36%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_with_strict_mode <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 36%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_error_message_clarity <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 36%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_validate_report_redundancy <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_domain_specific_term_recognition <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_caching_mechanism <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_retry_suggestions_for_failed_validation <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_hallucination_risk_detection <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_triage_content_validation <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_context_aware_validation <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 38%]\u001b[0m\napp\\tests\\services\\test_quality_gate_service.py::TestQualityGateService::test_quality_level_classification <- ..\\v2\\app\\tests\\services\\test_quality_gate_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityMonitoringServiceInitialization::test_initialization_with_defaults \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityMonitoringServiceInitialization::test_initialization_with_custom_config \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityMonitoringServiceInitialization::test_initialization_with_metrics_collector \u001b[31mFAILED\u001b[0m\u001b[31m [ 39%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsCollection::test_collect_response_quality_metrics \u001b[31mFAILED\u001b[0m\u001b[31m [ 39%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsCollection::test_collect_system_metrics \u001b[31mFAILED\u001b[0m\u001b[31m [ 39%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsCollection::test_collect_error_metrics \u001b[31mFAILED\u001b[0m\u001b[31m [ 39%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsCollection::test_batch_metrics_collection \u001b[31mFAILED\u001b[0m\u001b[31m [ 39%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityThresholds::test_set_quality_thresholds \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityThresholds::test_validate_against_thresholds \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityThresholds::test_dynamic_threshold_adjustment \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestAlerting::test_trigger_alert_on_threshold_breach \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestAlerting::test_alert_rate_limiting \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestAlerting::test_alert_escalation \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsAggregation::test_aggregate_metrics_by_time_window \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsAggregation::test_calculate_statistics \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestMetricsAggregation::test_trend_analysis \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityReporting::test_generate_quality_report \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityReporting::test_export_metrics_to_json \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestQualityReporting::test_generate_sla_compliance_report \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestAnomalyDetection::test_detect_anomalies_zscore \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestAnomalyDetection::test_detect_anomalies_iqr \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestAnomalyDetection::test_real_time_anomaly_detection \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestPerformanceMonitoring::test_monitor_response_times \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestPerformanceMonitoring::test_monitor_throughput \u001b[31mFAILED\u001b[0m\u001b[31m [ 43%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestIntegration::test_integration_with_agent_service \u001b[31mFAILED\u001b[0m\u001b[31m [ 43%]\u001b[0m\napp\\tests\\services\\test_quality_monitoring_service.py::TestIntegration::test_integration_with_database \u001b[31mFAILED\u001b[0m\u001b[31m [ 43%]\u001b[0m\napp\\tests\\services\\test_schema_validation_service.py::TestSchemaValidationService::test_validate_schema <- ..\\v2\\app\\tests\\services\\test_schema_validation_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 43%]\u001b[0m\napp\\tests\\services\\test_schema_validation_service.py::TestSchemaValidationService::test_schema_service_import <- ..\\v2\\app\\tests\\services\\test_schema_validation_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 43%]\u001b[0m\napp\\tests\\services\\test_schema_validation_service.py::TestSchemaValidationService::test_user_data_schema <- ..\\v2\\app\\tests\\services\\test_schema_validation_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 44%]\u001b[0m\napp\\tests\\services\\test_schema_validation_service.py::TestSchemaValidationService::test_invalid_user_data <- ..\\v2\\app\\tests\\services\\test_schema_validation_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 44%]\u001b[0m\napp\\tests\\services\\test_schema_validation_service.py::TestSchemaValidationService::test_websocket_message_schema <- ..\\v2\\app\\tests\\services\\test_schema_validation_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 44%]\u001b[0m\napp\\tests\\services\\test_schema_validation_service.py::TestSchemaValidationService::test_agent_message_schema <- ..\\v2\\app\\tests\\services\\test_schema_validation_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 44%]\u001b[0m\napp\\tests\\services\\test_security_service.py::test_encrypt_and_decrypt <- ..\\v2\\app\\tests\\services\\test_security_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 44%]\u001b[0m\napp\\tests\\services\\test_security_service.py::test_create_and_validate_access_token <- ..\\v2\\app\\tests\\services\\test_security_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 45%]\u001b[0m\napp\\tests\\services\\test_security_service.py::test_invalid_token <- ..\\v2\\app\\tests\\services\\test_security_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 45%]\u001b[0m\napp\\tests\\services\\test_state_persistence.py::TestStatePersistence::test_save_agent_state <- ..\\v2\\app\\tests\\services\\test_state_persistence.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 45%]\u001b[0m\napp\\tests\\services\\test_state_persistence.py::TestStatePersistence::test_restore_agent_state <- ..\\v2\\app\\tests\\services\\test_state_persistence.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 45%]\u001b[0m\napp\\tests\\services\\test_state_persistence.py::TestStatePersistence::test_cleanup_old_states <- ..\\v2\\app\\tests\\services\\test_state_persistence.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 45%]\u001b[0m\napp\\tests\\services\\test_state_persistence.py::TestStatePersistence::test_state_versioning <- ..\\v2\\app\\tests\\services\\test_state_persistence.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 46%]\u001b[0m\napp\\tests\\services\\test_state_persistence.py::TestStatePersistence::test_concurrent_state_updates <- ..\\v2\\app\\tests\\services\\test_state_persistence.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 46%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_get_all_options <- ..\\v2\\app\\tests\\services\\test_supply_catalog_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 46%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_get_option_by_id <- ..\\v2\\app\\tests\\services\\test_supply_catalog_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 46%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_get_option_by_name <- ..\\v2\\app\\tests\\services\\test_supply_catalog_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 46%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_create_option <- ..\\v2\\app\\tests\\services\\test_supply_catalog_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 47%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_update_option <- ..\\v2\\app\\tests\\services\\test_supply_catalog_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 47%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_delete_option <- ..\\v2\\app\\tests\\services\\test_supply_catalog_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 47%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_delete_option_not_found <- ..\\v2\\app\\tests\\services\\test_supply_catalog_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 47%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_autofill_catalog_empty <- ..\\v2\\app\\tests\\services\\test_supply_catalog_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 47%]\u001b[0m\napp\\tests\\services\\test_supply_catalog_service.py::TestSupplyCatalogService::test_autofill_catalog_existing_data <- ..\\v2\\app\\tests\\services\\test_supply_catalog_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 47%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service.py::TestSyntheticDataService::test_service_initialization <- ..\\v2\\app\\tests\\services\\test_synthetic_data_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 48%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service.py::TestSyntheticDataService::test_workload_categories_enum <- ..\\v2\\app\\tests\\services\\test_synthetic_data_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 48%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service.py::TestSyntheticDataService::test_generation_status_enum <- ..\\v2\\app\\tests\\services\\test_synthetic_data_service.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 48%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_creation_with_clickhouse_table \u001b[31mFAILED\u001b[0m\u001b[31m [ 48%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_status_transitions \u001b[31mFAILED\u001b[0m\u001b[31m [ 48%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_content_upload_batch \u001b[31mFAILED\u001b[0m\u001b[31m [ 49%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 49%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_availability_check \u001b[31mFAILED\u001b[0m\u001b[31m [ 49%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_fallback_to_default \u001b[31mFAILED\u001b[0m\u001b[31m [ 49%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_caching_mechanism \u001b[31mFAILED\u001b[0m\u001b[31m [ 49%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_deletion_cascade \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_metadata_tracking \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestCorpusManagement::test_corpus_concurrent_access \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_workload_distribution_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_temporal_pattern_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_tool_invocation_patterns \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_error_scenario_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_trace_hierarchy_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_domain_specific_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_statistical_distribution_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_custom_tool_catalog_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_incremental_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataGenerationEngine::test_generation_with_corpus_sampling \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_batch_ingestion_to_clickhouse \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_streaming_ingestion_with_backpressure \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_ingestion_error_recovery \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_ingestion_deduplication \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_table_creation_on_demand \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_ingestion_metrics_tracking \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_parallel_batch_ingestion \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_ingestion_with_transformation \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_ingestion_circuit_breaker \u001b[31mFAILED\u001b[0m\u001b[31m [ 54%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestRealTimeIngestion::test_ingestion_progress_tracking \u001b[31mFAILED\u001b[0m\u001b[31m [ 54%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_websocket_connection_management \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-709' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 54%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_generation_progress_broadcast \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-713' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 54%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_batch_completion_notifications \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-717' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 54%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_error_notification_handling \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-721' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 55%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_websocket_reconnection_handling \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-725' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 55%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_multiple_client_subscriptions \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-729' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\nTask exception was never retrieved\nfuture: <Task finished name='Task-731' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\nTask exception was never retrieved\nfuture: <Task finished name='Task-733' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 55%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_websocket_message_queuing \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-737' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 55%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_websocket_heartbeat \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-741' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 55%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_generation_completion_notification \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-745' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 56%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestWebSocketUpdates::test_websocket_rate_limiting \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-749' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 56%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_schema_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_statistical_distribution_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_referential_integrity_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_temporal_consistency_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_data_completeness_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_anomaly_detection_in_generated_data \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_correlation_preservation \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_quality_metrics_calculation \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_data_diversity_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestDataQualityValidation::test_validation_report_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_high_throughput_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_memory_efficient_streaming \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_horizontal_scaling \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_batch_size_optimization \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_connection_pooling_efficiency \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_cache_effectiveness \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_auto_scaling_behavior \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_resource_limit_handling \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_query_optimization \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestPerformanceScalability::test_burst_load_handling \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_corpus_unavailable_fallback \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_clickhouse_connection_recovery \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_generation_checkpoint_recovery \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_websocket_disconnect_recovery \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_memory_overflow_handling \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_circuit_breaker_operation \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_dead_letter_queue_processing \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_transaction_rollback \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_idempotent_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestErrorRecovery::test_graceful_degradation \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_generation_job_monitoring \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_detailed_metrics_dashboard \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_corpus_usage_analytics \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_audit_log_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_performance_profiling \u001b[31mFAILED\u001b[0m\u001b[31m [ 63%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_alert_configuration \u001b[31mFAILED\u001b[0m\u001b[31m [ 63%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_job_cancellation_by_admin \u001b[31mFAILED\u001b[0m\u001b[31m [ 63%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_resource_usage_tracking \u001b[31mFAILED\u001b[0m\u001b[31m [ 63%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_admin_diagnostic_tools \u001b[31mFAILED\u001b[0m\u001b[31m [ 63%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdminVisibility::test_batch_job_management \u001b[31mFAILED\u001b[0m\u001b[31m [ 63%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_complete_generation_workflow \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_multi_tenant_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_real_time_streaming_pipeline \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_failure_recovery_integration \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_cross_component_validation \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nTask exception was never retrieved\nfuture: <Task finished name='Task-883' coro=<TestIntegration.test_real_time_streaming_pipeline.<locals>.ws_listener() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\tests\\services\\test_synthetic_data_service_v3.py:1759> exception=AttributeError(\"'WebSocketManager' object has no attribute 'listen'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\tests\\services\\test_synthetic_data_service_v3.py\", line 1760, in ws_listener\n    async for message in full_stack[\"websocket\"].listen(job_id):\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'WebSocketManager' object has no attribute 'listen'\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 64%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_performance_under_load \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_data_consistency_verification \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_monitoring_integration \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_security_and_access_control \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestIntegration::test_cleanup_and_retention \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_ml_driven_pattern_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 66%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_anomaly_injection_strategies \u001b[31mFAILED\u001b[0m\u001b[31m [ 66%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_cross_correlation_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 66%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_temporal_event_sequences \u001b[31mFAILED\u001b[0m\u001b[31m [ 66%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_geo_distributed_simulation \u001b[31mFAILED\u001b[0m\u001b[31m [ 66%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_adaptive_generation_feedback \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_multi_model_workload_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_compliance_aware_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_cost_optimized_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_service_v3.py::TestAdvancedFeatures::test_versioned_corpus_generation \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_spec_version_is_3 \u001b[32mPASSED\u001b[0m\u001b[31m [ 68%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_enhanced_admin_visibility_section_exists \u001b[32mPASSED\u001b[0m\u001b[31m [ 68%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_data_agent_integration_section_exists \u001b[32mPASSED\u001b[0m\u001b[31m [ 68%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_log_structure_coherence_section_exists \u001b[32mPASSED\u001b[0m\u001b[31m [ 68%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_comprehensive_testing_framework_defined \u001b[32mPASSED\u001b[0m\u001b[31m [ 68%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_version_3_improvements_documented \u001b[32mPASSED\u001b[0m\u001b[31m [ 68%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_admin_prompts_categories_defined \u001b[32mPASSED\u001b[0m\u001b[31m [ 69%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_clustering_algorithms_defined \u001b[32mPASSED\u001b[0m\u001b[31m [ 69%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_alerting_rules_defined \u001b[32mPASSED\u001b[0m\u001b[31m [ 69%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestSpecificationValidation::test_migration_guide_present \u001b[32mPASSED\u001b[0m\u001b[31m [ 69%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_workload_categories_match_spec \u001b[32mPASSED\u001b[0m\u001b[31m [ 69%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_generation_status_enum_matches_spec \u001b[32mPASSED\u001b[0m\u001b[31m [ 70%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_corpus_service_exists \u001b[32mPASSED\u001b[0m\u001b[31m [ 70%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_websocket_manager_exists \u001b[32mPASSED\u001b[0m\u001b[31m [ 70%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_synthetic_data_service_has_required_methods \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_clickhouse_integration_configured \u001b[32mPASSED\u001b[0m\u001b[31m [ 70%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_agent_tools_available \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_data_clustering_tool_exists \u001b[32mPASSED\u001b[0m\u001b[31m [ 71%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_test_file_created \u001b[32mPASSED\u001b[0m\u001b[31m [ 71%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestImplementationConsistency::test_spec_xml_is_valid \u001b[32mPASSED\u001b[0m\u001b[31m [ 71%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_corpus_lifecycle_states \u001b[32mPASSED\u001b[0m\u001b[31m [ 71%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_workload_distribution_calculation \u001b[32mPASSED\u001b[0m\u001b[31m [ 72%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_batch_size_optimization_range \u001b[32mPASSED\u001b[0m\u001b[31m [ 72%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_websocket_message_types \u001b[32mPASSED\u001b[0m\u001b[31m [ 72%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_quality_metrics_calculation \u001b[32mPASSED\u001b[0m\u001b[31m [ 72%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_circuit_breaker_configuration \u001b[32mPASSED\u001b[0m\u001b[31m [ 72%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_ingestion_rate_targets \u001b[32mPASSED\u001b[0m\u001b[31m [ 73%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_temporal_pattern_types \u001b[32mPASSED\u001b[0m\u001b[31m [ 73%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_tool_catalog_structure \u001b[32mPASSED\u001b[0m\u001b[31m [ 73%]\u001b[0m\napp\\tests\\services\\test_synthetic_data_validation.py::TestKeyFeatureImplementation::test_admin_dashboard_update_frequency \u001b[32mPASSED\u001b[0m\u001b[31m [ 73%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::test_create_thread <- ..\\v2\\app\\tests\\services\\test_thread_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::test_get_thread_history <- ..\\v2\\app\\tests\\services\\test_thread_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::test_update_thread_metadata <- ..\\v2\\app\\tests\\services\\test_thread_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::test_delete_thread <- ..\\v2\\app\\tests\\services\\test_thread_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::test_thread_pagination <- ..\\v2\\app\\tests\\services\\test_thread_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\napp\\tests\\services\\test_thread_service.py::TestThreadService::test_thread_search <- ..\\v2\\app\\tests\\services\\test_thread_service.py \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_tool_registration <- ..\\v2\\app\\tests\\services\\test_tool_dispatcher.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 74%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_tool_execution_with_validation <- ..\\v2\\app\\tests\\services\\test_tool_dispatcher.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 75%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_tool_chain_execution <- ..\\v2\\app\\tests\\services\\test_tool_dispatcher.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 75%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_tool_error_handling <- ..\\v2\\app\\tests\\services\\test_tool_dispatcher.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 75%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_unknown_tool_handling <- ..\\v2\\app\\tests\\services\\test_tool_dispatcher.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 75%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_concurrent_tool_execution <- ..\\v2\\app\\tests\\services\\test_tool_dispatcher.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 75%]\u001b[0m\napp\\tests\\services\\test_tool_dispatcher.py::TestToolDispatcher::test_tool_metadata_tracking <- ..\\v2\\app\\tests\\services\\test_tool_dispatcher.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 76%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_create_user \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\n(trapped) error reading bcrypt version\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\passlib\\handlers\\bcrypt.py\", line 620, in _load_backend_mixin\n    version = _bcrypt.__about__.__version__\n              ^^^^^^^^^^^^^^^^^\nAttributeError: module 'bcrypt' has no attribute '__about__'\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 76%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_get_user_by_email \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_get_user_by_email_not_found \u001b[32mPASSED\u001b[0m\u001b[31m [ 76%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_update_user \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_delete_user \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_get_user_by_id \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_get_multiple_users \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_password_hashing_security \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_create_user_with_duplicate_email \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\napp\\tests\\services\\test_user_service.py::TestUserService::test_user_service_singleton \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestSecretManager::test_initialization <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestSecretManager::test_load_from_environment <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestSecretManager::test_secret_manager_client_creation_success <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestSecretManager::test_secret_manager_client_creation_failure <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestSecretManager::test_load_from_secret_manager_success <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestSecretManager::test_load_secrets_fallback_to_env <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 79%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_initialization <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 79%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_validate_valid_config <- ..\\v2\\app\\tests\\core\\test_config_manager.py \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nLLM configuration warnings: LLM 'default' is missing API key, LLM 'analysis' is missing API key, LLM 'triage' is missing API key, LLM 'data' is missing API key, LLM 'optimizations_core' is missing API key, LLM 'actions_to_meet_goals' is missing API key, LLM 'reporting' is missing API key\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 79%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_validate_database_config_missing_url <- ..\\v2\\app\\tests\\core\\test_config_manager.py \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration validation failed: Database configuration errors: Database URL is not configured\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 79%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_validate_database_config_invalid_url <- ..\\v2\\app\\tests\\core\\test_config_manager.py \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration validation failed: Database configuration errors: Database URL must be a PostgreSQL connection string\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 79%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_validate_auth_config_missing_jwt_secret <- ..\\v2\\app\\tests\\core\\test_config_manager.py \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration validation failed: Authentication configuration errors: JWT secret key is not configured\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 80%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_validate_auth_config_production_dev_secret <- ..\\v2\\app\\tests\\core\\test_config_manager.py \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration validation failed: Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured\n\u001b[31mFAILED\u001b[0m\u001b[31m                                                                   [ 80%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_get_validation_report_success <- ..\\v2\\app\\tests\\core\\test_config_manager.py \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nLLM configuration warnings: LLM 'default' is missing API key, LLM 'analysis' is missing API key, LLM 'triage' is missing API key, LLM 'data' is missing API key, LLM 'optimizations_core' is missing API key, LLM 'actions_to_meet_goals' is missing API key, LLM 'reporting' is missing API key\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 80%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigValidator::test_get_validation_report_failure <- ..\\v2\\app\\tests\\core\\test_config_manager.py \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration validation failed: Database configuration errors: Database URL is not configured\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 80%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_initialization <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 80%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_get_environment_development <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 81%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_get_environment_testing <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 81%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_get_environment_production <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 81%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_create_base_config_development <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 81%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_create_base_config_unknown_defaults_to_dev <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 81%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_load_configuration_success <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 82%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_load_configuration_validation_failure <- ..\\v2\\app\\tests\\core\\test_config_manager.py \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration loading failed: Validation failed\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 82%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_get_config_caching <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 82%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigManager::test_reload_config <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 82%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigurationFunctions::test_get_config <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 82%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigurationFunctions::test_reload_config <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 83%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigurationIntegration::test_full_configuration_flow <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 83%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigurationIntegration::test_testing_configuration <- ..\\v2\\app\\tests\\core\\test_config_manager.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 83%]\u001b[0m\napp\\tests\\core\\test_config_manager.py::TestConfigurationIntegration::test_configuration_error_handling <- ..\\v2\\app\\tests\\core\\test_config_manager.py \n\u001b[1m-------------------------------- live log call --------------------------------\u001b[0m\nConfiguration loading failed: Test error\n\u001b[32mPASSED\u001b[0m\u001b[31m                                                                   [ 83%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorCodes::test_error_code_values <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 83%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorCodes::test_error_severity_values <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 84%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorDetails::test_error_details_creation <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 84%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorDetails::test_error_details_with_context <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 84%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorDetails::test_error_details_serialization <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 84%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_netra_exception_basic <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 84%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_netra_exception_with_code <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 84%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_netra_exception_to_dict <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 85%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_configuration_error <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 85%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_validation_error_with_errors <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 85%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_authentication_error <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 85%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_authorization_error <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 85%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_token_expired_error <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 86%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_database_error <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 86%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_record_not_found_error <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 86%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_service_error <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 86%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_llm_request_error <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 86%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestNetraExceptions::test_websocket_error <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 87%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandler::test_handle_netra_exception <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 87%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandler::test_handle_pydantic_validation_error <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 87%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandler::test_handle_sqlalchemy_integrity_error <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 87%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandler::test_handle_http_exception <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 87%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandler::test_handle_unknown_exception <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 88%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandler::test_get_http_status_code_mapping <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 88%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_trace_id_context <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 88%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_request_id_context <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 88%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_user_id_context <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 88%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_custom_context <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 89%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_get_all_context <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 89%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_clear_context <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 89%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_error_context_manager <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 89%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorContext::test_get_enriched_error_context <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 89%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandlerFunctions::test_handle_exception_function <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 89%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandlerFunctions::test_get_http_status_code_function <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 90%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandlerFunctions::test_netra_exception_handler <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 90%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandlerFunctions::test_validation_exception_handler <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 90%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandlerFunctions::test_http_exception_handler <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 90%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorHandlerFunctions::test_general_exception_handler <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 90%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorResponseModel::test_error_response_creation <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 91%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorResponseModel::test_error_response_with_details <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 91%]\u001b[0m\napp\\tests\\core\\test_error_handling.py::TestErrorResponseModel::test_error_response_serialization <- ..\\v2\\app\\tests\\core\\test_error_handling.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 91%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseServiceMixin::test_initialization <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 91%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseServiceMixin::test_update_metrics_success <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 91%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseServiceMixin::test_update_metrics_failure <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 92%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseServiceMixin::test_update_metrics_average_calculation <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 92%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseServiceMixin::test_create_background_task <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 92%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseServiceMixin::test_cancel_background_tasks <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 92%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_initialization <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 92%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_initialize_success <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 93%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_initialize_failure <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 93%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_initialize_idempotent <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 93%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_shutdown <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 93%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_health_check_healthy <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 93%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_health_check_with_dependencies <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 94%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestBaseService::test_health_check_exception <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 94%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestDatabaseService::test_initialization <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 94%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestDatabaseService::test_set_session_factory <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 94%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestDatabaseService::test_get_db_session_no_factory <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 94%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestDatabaseService::test_get_db_session_success <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 94%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestDatabaseService::test_get_db_session_exception <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 95%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_create_entity <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 95%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_get_by_id_found <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 95%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_get_by_id_not_found <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 95%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_update_entity_success <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 95%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_update_entity_not_found <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 96%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_delete_entity_success <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 96%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_delete_entity_not_found <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 96%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_exists_true <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 96%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestCRUDService::test_exists_false <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 96%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestAsyncTaskService::test_initialization <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestAsyncTaskService::test_start_background_tasks <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestAsyncTaskService::test_start_background_tasks_idempotent <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestAsyncTaskService::test_stop_background_tasks <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_initialization <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_register_service <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 98%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_get_service_not_found <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 98%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_get_all_services <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 98%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_initialize_all <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 98%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_shutdown_all <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 98%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_health_check_all <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 99%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceRegistry::test_health_check_all_with_exception <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 99%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceModels::test_service_health_creation <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 99%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestServiceModels::test_service_metrics_creation <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 99%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestGlobalServiceRegistry::test_global_registry_exists <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [ 99%]\u001b[0m\napp\\tests\\core\\test_service_interfaces.py::TestGlobalServiceRegistry::test_global_registry_operations <- ..\\v2\\app\\tests\\core\\test_service_interfaces.py \u001b[32mPASSED\u001b[0m\u001b[31m [100%]\u001b[0m\n\n=================================== ERRORS ====================================\n\u001b[31m\u001b[1m________________ ERROR at setup of test_supervisor_end_to_end _________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\agents\\test_supervisor_service.py\u001b[0m:17: in supervisor\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SupervisorAgent.__init__() missing 1 required positional argument: 'tool_dispatcher'\u001b[0m\n================================== FAILURES ===================================\n\u001b[31m\u001b[1m________________________ test_agent_node_is_coroutine _________________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\agents\\test_sub_agent.py\u001b[0m:13: in test_agent_node_is_coroutine\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: Can't instantiate abstract class ConcreteSubAgent without an implementation for abstract method 'execute'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________________________ test_tool_dispatcher _____________________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\agents\\test_tools.py\u001b[0m:15: in test_tool_dispatcher\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert None == 3\u001b[0m\n\u001b[1m\u001b[31mE    +  where None = ToolResult(tool_input=ToolInput(tool_name='mock_tool', args=[], kwargs={'a': 1, 'b': 2}), status=<ToolStatus.ERROR: 'error'>, message=\"BaseTool.arun() missing 1 required positional argument: 'tool_input'\", payload=None, start_time=1754916584.7837372, end_time=None).payload\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________________ test_tool_dispatcher_tool_not_found _____________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\agents\\test_tools.py\u001b[0m:22: in test_tool_dispatcher_tool_not_found\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert \"Tool 'non_existent_tool' not found\" in 'Tool non_existent_tool not found'\u001b[0m\n\u001b[1m\u001b[31mE    +  where 'Tool non_existent_tool not found' = ToolResult(tool_input=ToolInput(tool_name='non_existent_tool', args=[], kwargs={'a': 1, 'b': 2}), status=<ToolStatus.ERROR: 'error'>, message='Tool non_existent_tool not found', payload=None, start_time=1754916584.799587, end_time=None).message\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________________ test_tool_dispatcher_tool_error _______________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\agents\\test_tools.py\u001b[0m:29: in test_tool_dispatcher_tool_error\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert 'Error dispatching tool' in \"BaseTool.arun() missing 1 required positional argument: 'tool_input'\"\u001b[0m\n\u001b[1m\u001b[31mE    +  where \"BaseTool.arun() missing 1 required positional argument: 'tool_input'\" = ToolResult(tool_input=ToolInput(tool_name='mock_tool', args=[], kwargs={'a': 1, 'b': '2'}), status=<ToolStatus.ERROR: 'error'>, message=\"BaseTool.arun() missing 1 required positional argument: 'tool_input'\", payload=None, start_time=1754916584.821934, end_time=None).message\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________________ test_tool_builder_and_dispatcher _______________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\test_tool_builder.py\u001b[0m:40: in test_tool_builder_and_dispatcher\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert <ToolStatus.ERROR: 'error'> == <ToolStatus.SUCCESS: 'success'>\u001b[0m\n\u001b[1m\u001b[31mE     \u001b[0m\n\u001b[1m\u001b[31mE     - success\u001b[0m\n\u001b[1m\u001b[31mE     + error\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______ TestBaseTool.test_base_tool_execute_without_metadata_attribute ________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py\u001b[0m:282: in test_base_tool_execute_without_metadata_attribute\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   Failed: DID NOT RAISE <class 'AttributeError'>\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestBaseTool.test_base_tool_exception_types _________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py\u001b[0m:336: in test_base_tool_exception_types\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert \"'Custom error'\" == 'Custom error'\u001b[0m\n\u001b[1m\u001b[31mE     \u001b[0m\n\u001b[1m\u001b[31mE     - Custom error\u001b[0m\n\u001b[1m\u001b[31mE     + 'Custom error'\u001b[0m\n\u001b[1m\u001b[31mE     ? +            +\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestCostAnalyzer.test_cost_analyzer_async_execution _____________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py\u001b[0m:170: in test_cost_analyzer_async_execution\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'Test prompt' in 'Analyzed current costs. Total estimated cost: $0.03'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestCostAnalyzer.test_cost_analyzer_rounding_edge_cases ___________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py\u001b[0m:268: in test_cost_analyzer_rounding_edge_cases\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: Failed for costs [0.005, 0.005, 0.005]\u001b[0m\n\u001b[1m\u001b[31mE   assert '$0.02' in 'Analyzed current costs. Total estimated cost: $0.01'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestLatencyAnalyzer.test_latency_analyzer_basic_functionality ________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:78: in test_latency_analyzer_basic_functionality\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116475808'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\nC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py:78: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n\u001b[31m\u001b[1m____________ TestLatencyAnalyzer.test_latency_analyzer_empty_logs _____________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:96: in test_latency_analyzer_empty_logs\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116645664'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestLatencyAnalyzer.test_latency_analyzer_single_log _____________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:107: in test_latency_analyzer_single_log\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116655888'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestLatencyAnalyzer.test_latency_analyzer_high_latency_values ________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:121: in test_latency_analyzer_high_latency_values\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116375584'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestLatencyAnalyzer.test_latency_analyzer_zero_latency ____________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:130: in test_latency_analyzer_zero_latency\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848115300496'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______ TestLatencyAnalyzer.test_latency_analyzer_sub_millisecond_latency ______\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:143: in test_latency_analyzer_sub_millisecond_latency\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116819584'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestLatencyAnalyzer.test_latency_analyzer_exception_handling _________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:155: in test_latency_analyzer_exception_handling\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert \"1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116818816'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\" == 'API Error'\u001b[0m\n\u001b[1m\u001b[31mE     \u001b[0m\n\u001b[1m\u001b[31mE     - API Error\u001b[0m\n\u001b[1m\u001b[31mE     + 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     +   Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116818816'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE     +     For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestLatencyAnalyzer.test_latency_analyzer_async_execution __________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:169: in test_latency_analyzer_async_execution\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848134636064'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestLatencyAnalyzer.test_latency_analyzer_large_dataset ___________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:188: in test_latency_analyzer_large_dataset\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848134647248'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestLatencyAnalyzer.test_latency_analyzer_varied_latencies __________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:211: in test_latency_analyzer_varied_latencies\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138677328'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestLatencyAnalyzer.test_latency_analyzer_partial_failure __________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:229: in test_latency_analyzer_partial_failure\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert 'Prediction service unavailable' in \"1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138905312'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\"\u001b[0m\n\u001b[1m\u001b[31mE    +  where \"1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138905312'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\" = str(1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138905312'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type)\u001b[0m\n\u001b[1m\u001b[31mE    +    where 1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138905312'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type = <ExceptionInfo 1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=mod...' id='1848138905312'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type tblen=8>.value\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestLatencyAnalyzer.test_latency_analyzer_edge_case_rounding _________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:247: in test_latency_analyzer_edge_case_rounding\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138977728'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestLatencyAnalyzer.test_latency_analyzer_negative_latencies _________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:260: in test_latency_analyzer_negative_latencies\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138900272'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______ TestLatencyAnalyzer.test_latency_analyzer_mixed_response_formats _______\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:277: in test_latency_analyzer_mixed_response_formats\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848107828768'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestLatencyAnalyzer.test_latency_analyzer_extreme_values ___________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:288: in test_latency_analyzer_extreme_values\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848107840960'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____ TestLatencyAnalyzer.test_latency_analyzer_concurrent_execution_timing ____\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py\u001b[0m:306: in test_latency_analyzer_concurrent_execution_timing\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\u001b[0m:189: in warning_emitting_wrapper\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrapped(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:1013: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.run(tool_input, callbacks=callbacks)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:883: in run\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m error_to_raise\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:845: in run\n    \u001b[0mtool_args, tool_kwargs = \u001b[96mself\u001b[39;49;00m._to_args_and_kwargs(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:760: in _to_args_and_kwargs\n    \u001b[0mtool_input = \u001b[96mself\u001b[39;49;00m._parse_input(tool_input, tool_call_id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\langchain_core\\tools\\base.py\u001b[0m:667: in _parse_input\n    \u001b[0mresult = input_args.model_validate(tool_input)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\u001b[0m\n\u001b[1m\u001b[31mE     Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848139108368'>, input_type=Mock]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/model_type\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestAgentMessageProcessing.test_process_user_message _____________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_agent_message_processing.py\u001b[0m:18: in test_process_user_message\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: AgentService.__init__() takes 2 positional arguments but 3 were given\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestAgentMessageProcessing.test_handle_tool_execution ____________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_agent_message_processing.py\u001b[0m:43: in test_handle_tool_execution\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: AgentService.__init__() takes 2 positional arguments but 3 were given\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestAgentMessageProcessing.test_message_validation ______________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_agent_message_processing.py\u001b[0m:69: in test_message_validation\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: AgentService.__init__() takes 2 positional arguments but 3 were given\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________________________ test_run_agent ________________________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_agent_service.py\u001b[0m:26: in test_run_agent\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for AnalysisRequest\u001b[0m\n\u001b[1m\u001b[31mE   request_model\u001b[0m\n\u001b[1m\u001b[31mE     Field required [type=missing, input_value={'settings': Settings(deb...'))], constraints=None)}, input_type=dict]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/missing\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestClickHouseConnection.test_client_initialization _____________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_clickhouse_service.py\u001b[0m:28: in test_client_initialization\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'MockClickHouseDatabase' object has no attribute 'fetch'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:37 ClickHouse is disabled in testing mode - using mock client\n\u001b[31m\u001b[1m______________ TestClickHouseConnection.test_list_corpus_tables _______________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_clickhouse_service.py\u001b[0m:34: in test_list_corpus_tables\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\clickhouse_service.py\u001b[0m:5: in list_corpus_tables\n    \u001b[0mtables = \u001b[94mawait\u001b[39;49;00m client.fetch(\u001b[33m\"\u001b[39;49;00m\u001b[33mSHOW TABLES LIKE \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mnetra_content_corpus_\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'MockClickHouseDatabase' object has no attribute 'fetch'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n------------------------------ Captured log call ------------------------------\n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:37 ClickHouse is disabled in testing mode - using mock client\n\u001b[31m\u001b[1m_____________ TestClickHouseConnection.test_basic_query_execution _____________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_clickhouse_service.py\u001b[0m:43: in test_basic_query_execution\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'MockClickHouseDatabase' object has no attribute 'fetch'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:37 ClickHouse is disabled in testing mode - using mock client\n\u001b[31m\u001b[1m_____________ TestClickHouseConnection.test_query_with_parameters _____________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_clickhouse_service.py\u001b[0m:51: in test_query_with_parameters\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'MockClickHouseDatabase' object has no attribute 'fetch'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_clickhouse_service.py\u001b[0m:56: in test_query_with_parameters\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'MockClickHouseDatabase' object has no attribute 'fetch'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:37 ClickHouse is disabled in testing mode - using mock client\n\u001b[31m\u001b[1m____________________ TestBasicOperations.test_show_tables _____________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_clickhouse_service.py\u001b[0m:69: in test_show_tables\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'MockClickHouseDatabase' object has no attribute 'fetch'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_clickhouse_service.py\u001b[0m:74: in test_show_tables\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'MockClickHouseDatabase' object has no attribute 'fetch'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[32mINFO    \u001b[0m app.db.clickhouse:clickhouse.py:37 ClickHouse is disabled in testing mode - using mock client\n\u001b[31m\u001b[1m____________________ TestCorpusService.test_corpus_schema _____________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_corpus_service.py\u001b[0m:32: in test_corpus_schema\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 4 validation errors for Corpus\u001b[0m\n\u001b[1m\u001b[31mE   status\u001b[0m\n\u001b[1m\u001b[31mE     Field required [type=missing, input_value={'id': 'test-corpus-123',... corpus for validation'}, input_type=dict]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/missing\u001b[0m\n\u001b[1m\u001b[31mE   created_by_id\u001b[0m\n\u001b[1m\u001b[31mE     Field required [type=missing, input_value={'id': 'test-corpus-123',... corpus for validation'}, input_type=dict]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/missing\u001b[0m\n\u001b[1m\u001b[31mE   created_at\u001b[0m\n\u001b[1m\u001b[31mE     Field required [type=missing, input_value={'id': 'test-corpus-123',... corpus for validation'}, input_type=dict]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/missing\u001b[0m\n\u001b[1m\u001b[31mE   updated_at\u001b[0m\n\u001b[1m\u001b[31mE     Field required [type=missing, input_value={'id': 'test-corpus-123',... corpus for validation'}, input_type=dict]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/missing\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestUnitOfWork.test_uow_transaction_commit __________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:57: in test_uow_transaction_commit\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:47.385 | ERROR    | app.services.database.unit_of_work:__aexit__:46 | UnitOfWork rolled back due to exception: 'dict' object has no attribute 'rollback'\n\u001b[31m\u001b[1m________________ TestUnitOfWork.test_uow_transaction_rollback _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:81: in test_uow_transaction_rollback\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:47.418 | ERROR    | app.services.database.unit_of_work:__aexit__:46 | UnitOfWork rolled back due to exception: 'dict' object has no attribute 'rollback'\n\u001b[31m\u001b[1m_________________ TestUnitOfWork.test_uow_nested_transactions _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:98: in test_uow_nested_transactions\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:47.447 | ERROR    | app.services.database.unit_of_work:__aexit__:46 | UnitOfWork rolled back due to exception: 'dict' object has no attribute 'rollback'\n\u001b[31m\u001b[1m__________________ TestUnitOfWork.test_uow_concurrent_access __________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:137: in test_uow_concurrent_access\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:126: in create_thread\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:47.480 | ERROR    | app.services.database.unit_of_work:__aexit__:46 | UnitOfWork rolled back due to exception: 'dict' object has no attribute 'rollback'\n2025-08-11 05:49:47.482 | ERROR    | app.services.database.unit_of_work:__aexit__:46 | UnitOfWork rolled back due to exception: 'dict' object has no attribute 'rollback'\n\u001b[31m\u001b[1m__________________ TestBaseRepository.test_repository_create __________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:159: in test_repository_create\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________ TestBaseRepository.test_repository_bulk_create ________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:173: in test_repository_bulk_create\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'ThreadRepository' object has no attribute 'bulk_create'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestBaseRepository.test_repository_get_by_id _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:180: in test_repository_get_by_id\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestBaseRepository.test_repository_get_many _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:196: in test_repository_get_many\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________________ TestBaseRepository.test_repository_update __________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:210: in test_repository_update\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________________ TestBaseRepository.test_repository_delete __________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:225: in test_repository_delete\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________ TestBaseRepository.test_repository_soft_delete ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:239: in test_repository_soft_delete\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestBaseRepository.test_repository_pagination ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:263: in test_repository_pagination\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestMessageRepository.test_get_messages_by_thread ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:286: in test_get_messages_by_thread\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestMessageRepository.test_get_messages_with_pagination ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:306: in test_get_messages_with_pagination\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________ TestMessageRepository.test_get_latest_messages ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:331: in test_get_latest_messages\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestThreadRepository.test_get_threads_by_user ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:364: in test_get_threads_by_user\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestThreadRepository.test_get_active_threads _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:379: in test_get_active_threads\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________________ TestThreadRepository.test_archive_thread ___________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:401: in test_archive_thread\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestRunRepository.test_create_run_with_tools _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:418: in test_create_run_with_tools\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________________ TestRunRepository.test_update_run_status ___________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:437: in test_update_run_status\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________________ TestRunRepository.test_get_active_runs ____________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:460: in test_get_active_runs\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestReferenceRepository.test_create_reference_with_metadata _________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:488: in test_create_reference_with_metadata\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestReferenceRepository.test_get_references_by_message ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:511: in test_get_references_by_message\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________ TestReferenceRepository.test_search_references ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:31: in create\n    \u001b[0mdb.add(entity)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'add'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_database_repositories.py\u001b[0m:535: in test_search_references\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\database\\base_repository.py\u001b[0m:46: in create\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m db.rollback()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'dict' object has no attribute 'rollback'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_optimization_fallback_low_quality _\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:60: in test_generate_optimization_fallback_low_quality\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: object of type 'coroutine' has no len()\u001b[0m\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_data_analysis_fallback_parsing_error _\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:82: in test_generate_data_analysis_fallback_parsing_error\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: argument of type 'coroutine' is not iterable\u001b[0m\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_action_plan_fallback_context_missing _\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:100: in test_generate_action_plan_fallback_context_missing\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: argument of type 'coroutine' is not iterable\u001b[0m\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_report_fallback_validation_failed _\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:118: in test_generate_report_fallback_validation_failed\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: argument of type 'coroutine' is not iterable\u001b[0m\n\u001b[31m\u001b[1m______ TestFallbackResponseService.test_generate_triage_fallback_timeout ______\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:136: in test_generate_triage_fallback_timeout\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: argument of type 'coroutine' is not iterable\u001b[0m\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_error_message_fallback_llm_error __\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:155: in test_generate_error_message_fallback_llm_error\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'coroutine' object has no attribute 'lower'\u001b[0m\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_fallback_with_circular_reasoning __\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:173: in test_generate_fallback_with_circular_reasoning\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: argument of type 'coroutine' is not iterable\u001b[0m\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_fallback_with_hallucination_risk __\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:202: in test_generate_fallback_with_hallucination_risk\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: argument of type 'coroutine' is not iterable\u001b[0m\n\u001b[31m\u001b[1m_____ TestFallbackResponseService.test_generate_fallback_with_rate_limit ______\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:220: in test_generate_fallback_with_rate_limit\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:220: in <genexpr>\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'coroutine' object has no attribute 'lower'\u001b[0m\n\u001b[31m\u001b[1m__ TestFallbackResponseService.test_generate_fallback_considers_retry_count ___\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:243: in test_generate_fallback_considers_retry_count\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: object of type 'coroutine' has no len()\u001b[0m\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_fallback_includes_diagnostic_tips _\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:264: in test_generate_fallback_includes_diagnostic_tips\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: argument of type 'coroutine' is not iterable\u001b[0m\n\u001b[31m\u001b[1m_ TestFallbackResponseService.test_generate_fallback_with_previous_responses __\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:284: in test_generate_fallback_with_previous_responses\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: argument of type 'coroutine' is not iterable\u001b[0m\n---------------------------- Captured stderr call -----------------------------\nC:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\stash.py:86: RuntimeWarning: coroutine 'FallbackResponseService.generate_fallback' was never awaited\n  def get(self, key: StashKey[T], default: D) -> T | D:\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n\u001b[31m\u001b[1m_____ TestFallbackResponseService.test_format_response_with_placeholders ______\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:292: in test_format_response_with_placeholders\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'FallbackResponseService' object has no attribute '_format_response'\u001b[0m\n\u001b[31m\u001b[1m__________ TestFallbackResponseService.test_get_recovery_suggestions __________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:313: in test_get_recovery_suggestions\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: FallbackResponseService._get_recovery_suggestions() takes 2 positional arguments but 3 were given\u001b[0m\n\u001b[31m\u001b[1m_________ TestFallbackResponseService.test_fallback_response_quality __________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_fallback_response_service.py\u001b[0m:347: in test_fallback_response_quality\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: object of type 'coroutine' has no len()\u001b[0m\n\u001b[31m\u001b[1m___________________________ test_update_job_status ____________________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_generation_service.py\u001b[0m:16: in test_update_job_status\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\generation_service.py\u001b[0m:34: in update_job_status\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m manager.broadcast(json.dumps({\u001b[33m\"\u001b[39;49;00m\u001b[33mjob_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: job_id, \u001b[33m\"\u001b[39;49;00m\u001b[33mstatus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: status, **kwargs}))\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\ws_manager.py\u001b[0m:336: in broadcast\n    \u001b[0mmessage[\u001b[33m\"\u001b[39;49;00m\u001b[33mtimestamp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = time.time()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: 'str' object does not support item assignment\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________________ test_save_corpus_to_clickhouse ________________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_generation_service.py\u001b[0m:65: in test_save_corpus_to_clickhouse\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\generation_service.py\u001b[0m:96: in save_corpus_to_clickhouse\n    \u001b[0mrecord = ContentCorpus(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for ContentCorpus\u001b[0m\n\u001b[1m\u001b[31mE   created_at\u001b[0m\n\u001b[1m\u001b[31mE     Field required [type=missing, input_value={'workload_type': 'test_t...432f-a9c7-36a256b5d458'}, input_type=dict]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/missing\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:48.508 | ERROR    | app.services.generation_service:save_corpus_to_clickhouse:115 | Failed to save corpus to ClickHouse table test_corpus\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 113, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 132, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 241, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 341, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 642, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n    handle._run()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_generation_service.py\", line 65, in test_save_corpus_to_clickhouse\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\services\\generation_service.py\", line 96, in save_corpus_to_clickhouse\n    record = ContentCorpus(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydantic\\main.py\", line 253, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\npydantic_core._pydantic_core.ValidationError: 1 validation error for ContentCorpus\ncreated_at\n  Field required [type=missing, input_value={'workload_type': 'test_t...432f-a9c7-36a256b5d458'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\services\\generation_service.py:119: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited\n  db.disconnect()\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n\u001b[31m\u001b[1m____________________ test_llm_cache_service_initialization ____________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_llm_cache_service.py\u001b[0m:8: in test_llm_cache_service_initialization\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________________________ test_cache_set_and_get ____________________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_llm_cache_service.py\u001b[0m:15: in test_cache_set_and_get\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________________________ test_cache_expiration ____________________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_llm_cache_service.py\u001b[0m:29: in test_cache_expiration\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________________________ test_cache_size_limit ____________________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_llm_cache_service.py\u001b[0m:43: in test_cache_size_limit\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________________________ test_cache_stats _______________________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_llm_cache_service.py\u001b[0m:54: in test_cache_stats\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________________________ test_handle_start_agent ___________________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_message_handlers.py\u001b[0m:38: in test_handle_start_agent\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\message_handlers.py\u001b[0m:43: in handle_start_agent\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.thread_service.create_message(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: object Mock can't be used in 'await' expression\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_message_handlers.py\u001b[0m:43: in test_handle_start_agent\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert ('supervisor' in \"object mock can't be used in 'await' expression\" or 'agent' in \"object mock can't be used in 'await' expression\")\u001b[0m\n\u001b[1m\u001b[31mE    +  where \"object mock can't be used in 'await' expression\" = <built-in method lower of str object at 0x000001AE4EE0EBB0>()\u001b[0m\n\u001b[1m\u001b[31mE    +    where <built-in method lower of str object at 0x000001AE4EE0EBB0> = \"object Mock can't be used in 'await' expression\".lower\u001b[0m\n\u001b[1m\u001b[31mE    +      where \"object Mock can't be used in 'await' expression\" = str(TypeError(\"object Mock can't be used in 'await' expression\"))\u001b[0m\n\u001b[1m\u001b[31mE    +  and   \"object mock can't be used in 'await' expression\" = <built-in method lower of str object at 0x000001AE4EE0EBB0>()\u001b[0m\n\u001b[1m\u001b[31mE    +    where <built-in method lower of str object at 0x000001AE4EE0EBB0> = \"object Mock can't be used in 'await' expression\".lower\u001b[0m\n\u001b[1m\u001b[31mE    +      where \"object Mock can't be used in 'await' expression\" = str(TypeError(\"object Mock can't be used in 'await' expression\"))\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___ TestQualityGateService.test_validate_high_quality_optimization_content ____\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_quality_gate_service.py\u001b[0m:58: in test_validate_high_quality_optimization_content\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.39999999999999997, actionability_score=0.5, quantification_score=1.0, relevance_score=0.5, completeness_score=0.2, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=77, sentence_count=12, numeric_values_count=0, specific_terms_count=0, overall_score=0.5700000000000001, quality_level=<QualityLevel.ACCEPTABLE: 'acceptable'>, issues=['Contains 3 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.']}, fallback_response=None).passed\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______ TestQualityGateService.test_validate_data_analysis_with_metrics _______\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_quality_gate_service.py\u001b[0m:129: in test_validate_data_analysis_with_metrics\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.6, actionability_score=0.3, quantification_score=0.9999999999999999, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=59, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.6449999999999999, quality_level=<QualityLevel.ACCEPTABLE: 'acceptable'>, issues=[], suggestions=['Include clear action steps or commands']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Provide step-by-step actionable instructions with specific commands or code.']}, fallback_response=None).passed\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestQualityGateService.test_validate_action_plan_completeness ________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_quality_gate_service.py\u001b[0m:161: in test_validate_action_plan_completeness\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.4, actionability_score=0.45, quantification_score=0.5499999999999999, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=89, sentence_count=3, numeric_values_count=0, specific_terms_count=0, overall_score=0.41250000000000003, quality_level=<QualityLevel.POOR: 'poor'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add verification steps and success criteria']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.']}, fallback_response=None).passed\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestQualityGateService.test_validate_with_strict_mode ____________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_quality_gate_service.py\u001b[0m:190: in test_validate_with_strict_mode\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.0, completeness_score=0.0, novelty_score=0.0, clarity_score=0.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=0, sentence_count=0, numeric_values_count=0, specific_terms_count=0, overall_score=0.0, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[\"Validation error: name 'metrics' is not defined\"], suggestions=[]), retry_suggested=False, retry_prompt_adjustments=None, fallback_response=None).passed\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:48.980 | ERROR    | app.services.quality_gate_service:validate_content:273 | Error validating content: name 'metrics' is not defined\n\u001b[31m\u001b[1m_________ TestQualityGateService.test_validate_error_message_clarity __________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_quality_gate_service.py\u001b[0m:216: in test_validate_error_message_clarity\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.0, completeness_score=0.0, novelty_score=0.0, clarity_score=0.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=0, sentence_count=0, numeric_values_count=0, specific_terms_count=0, overall_score=0.0, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[\"Validation error: name 'metrics' is not defined\"], suggestions=[]), retry_suggested=False, retry_prompt_adjustments=None, fallback_response=None).passed\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:49.002 | ERROR    | app.services.quality_gate_service:validate_content:273 | Error validating content: name 'metrics' is not defined\n\u001b[31m\u001b[1m___________ TestQualityGateService.test_validate_report_redundancy ____________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_quality_gate_service.py\u001b[0m:247: in test_validate_report_redundancy\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 0.4 > 0.5\u001b[0m\n\u001b[1m\u001b[31mE    +  where 0.4 = QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.6, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=1, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.4, word_count=53, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.35, quality_level=<QualityLevel.POOR: 'poor'>, issues=['Contains 1 generic phrases', 'Contains 5 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Reduce redundant information']).redundancy_ratio\u001b[0m\n\u001b[1m\u001b[31mE    +    where QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.6, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=1, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.4, word_count=53, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.35, quality_level=<QualityLevel.POOR: 'poor'>, issues=['Contains 1 generic phrases', 'Contains 5 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Reduce redundant information']) = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.6, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=1, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.4, word_count=53, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.35, quality_level=<QualityLevel.POOR: 'poor'>, issues=['Contains 1 generic phrases', 'Contains 5 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Reduce redundant information']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.', 'Include numerical values for all claims. Show before/after metrics with percentages.']}, fallback_response=None).metrics\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestQualityGateService.test_domain_specific_term_recognition _________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_quality_gate_service.py\u001b[0m:265: in test_domain_specific_term_recognition\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.6, actionability_score=0.55, quantification_score=0.9, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=44, sentence_count=7, numeric_values_count=0, specific_terms_count=0, overall_score=0.5925, quality_level=<QualityLevel.ACCEPTABLE: 'acceptable'>, issues=[], suggestions=[]), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': []}, fallback_response=None).passed\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestQualityGateService.test_caching_mechanism ________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_quality_gate_service.py\u001b[0m:288: in test_caching_mechanism\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'quality:general:c1402699b07df03b830d11ee43df782f' in {}\u001b[0m\n\u001b[1m\u001b[31mE    +  where {} = <app.services.quality_gate_service.QualityGateService object at 0x000001AE4ECB8E30>.validation_cache\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:49.059 | ERROR    | app.services.quality_gate_service:validate_content:273 | Error validating content: name 'metrics' is not defined\n2025-08-11 05:49:49.059 | ERROR    | app.services.quality_gate_service:validate_content:273 | Error validating content: name 'metrics' is not defined\n\u001b[31m\u001b[1m_____ TestQualityGateService.test_retry_suggestions_for_failed_validation _____\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_quality_gate_service.py\u001b[0m:301: in test_retry_suggestions_for_failed_validation\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=9, sentence_count=2, numeric_values_count=0, specific_terms_count=0, overall_score=0.125, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=['Contains 2 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Include before/after performance metrics']), retry_suggested=False, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.', 'Include numerical values for all claims. Show before/after metrics with percentages.']}, fallback_response=None).retry_suggested\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestQualityGateService.test_hallucination_risk_detection ___________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_quality_gate_service.py\u001b[0m:323: in test_hallucination_risk_detection\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 0.2 > 0.5\u001b[0m\n\u001b[1m\u001b[31mE    +  where 0.2 = QualityMetrics(specificity_score=0.05, actionability_score=0.0, quantification_score=0.15, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=41, sentence_count=4, numeric_values_count=0, specific_terms_count=0, overall_score=0.28, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes']).hallucination_risk\u001b[0m\n\u001b[1m\u001b[31mE    +    where QualityMetrics(specificity_score=0.05, actionability_score=0.0, quantification_score=0.15, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=41, sentence_count=4, numeric_values_count=0, specific_terms_count=0, overall_score=0.28, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes']) = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.05, actionability_score=0.0, quantification_score=0.15, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=41, sentence_count=4, numeric_values_count=0, specific_terms_count=0, overall_score=0.28, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes']), retry_suggested=False, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.', 'Include numerical values for all claims. Show before/after metrics with percentages.']}, fallback_response=None).metrics\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestQualityGateService.test_triage_content_validation ____________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_quality_gate_service.py\u001b[0m:348: in test_triage_content_validation\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert False is True\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.0, completeness_score=0.0, novelty_score=0.0, clarity_score=0.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=0, sentence_count=0, numeric_values_count=0, specific_terms_count=0, overall_score=0.0, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[\"Validation error: name 'metrics' is not defined\"], suggestions=[]), retry_suggested=False, retry_prompt_adjustments=None, fallback_response=None).passed\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:49.111 | ERROR    | app.services.quality_gate_service:validate_content:273 | Error validating content: name 'metrics' is not defined\n\u001b[31m\u001b[1m__________ TestQualityGateService.test_quality_level_classification ___________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_quality_gate_service.py\u001b[0m:393: in test_quality_level_classification\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityGateService' object has no attribute '_classify_quality_level'. Did you mean: '_determine_quality_level'?\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestQualityMonitoringServiceInitialization.test_initialization_with_defaults _\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:44: in test_initialization_with_defaults\n    \u001b[0m\u001b[94massert\u001b[39;49;00m service.metrics_store \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'metrics_store'. Did you mean: 'metrics_buffer'?\u001b[0m\n\u001b[31m\u001b[1m_ TestQualityMonitoringServiceInitialization.test_initialization_with_custom_config _\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:55: in test_initialization_with_custom_config\n    \u001b[0mservice = QualityMonitoringService(config=config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: QualityMonitoringService.__init__() got an unexpected keyword argument 'config'\u001b[0m\n\u001b[31m\u001b[1m_ TestQualityMonitoringServiceInitialization.test_initialization_with_metrics_collector _\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1387: in patched\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.decoration_helper(patched,\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\contextlib.py\u001b[0m:137: in __enter__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mnext\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.gen)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1369: in decoration_helper\n    \u001b[0marg = exit_stack.enter_context(patching)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\contextlib.py\u001b[0m:526: in enter_context\n    \u001b[0mresult = _enter(cm)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <module 'app.services.quality_monitoring_service' from 'C:\\\\Users\\\\antho\\\\OneDrive\\\\Desktop\\\\Netra\\\\netra-core-generation-1\\\\app\\\\services\\\\quality_monitoring_service.py'> does not have the attribute 'MetricsCollector'\u001b[0m\n\u001b[31m\u001b[1m_________ TestMetricsCollection.test_collect_response_quality_metrics _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:86: in test_collect_response_quality_metrics\n    \u001b[0mmetrics = \u001b[94mawait\u001b[39;49;00m service.collect_response_metrics(response)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'collect_response_metrics'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestMetricsCollection.test_collect_system_metrics ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:103: in test_collect_system_metrics\n    \u001b[0mmetrics = \u001b[94mawait\u001b[39;49;00m service.collect_system_metrics()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'collect_system_metrics'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestMetricsCollection.test_collect_error_metrics _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:121: in test_collect_error_metrics\n    \u001b[0mmetrics = \u001b[94mawait\u001b[39;49;00m service.collect_error_metrics(error)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'collect_error_metrics'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestMetricsCollection.test_batch_metrics_collection _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:138: in test_batch_metrics_collection\n    \u001b[0mbatch_metrics = \u001b[94mawait\u001b[39;49;00m service.collect_batch_metrics(responses)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'collect_batch_metrics'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestQualityThresholds.test_set_quality_thresholds ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:158: in test_set_quality_thresholds\n    \u001b[0mservice.set_thresholds(thresholds)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'set_thresholds'. Did you mean: '_check_thresholds'?\u001b[0m\n\u001b[31m\u001b[1m___________ TestQualityThresholds.test_validate_against_thresholds ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:167: in test_validate_against_thresholds\n    \u001b[0mservice.set_thresholds({\u001b[33m\"\u001b[39;49;00m\u001b[33mmin_confidence\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m0.8\u001b[39;49;00m})\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'set_thresholds'. Did you mean: '_check_thresholds'?\u001b[0m\n\u001b[31m\u001b[1m___________ TestQualityThresholds.test_dynamic_threshold_adjustment ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:185: in test_dynamic_threshold_adjustment\n    \u001b[0mnew_threshold = service.calculate_dynamic_threshold(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'calculate_dynamic_threshold'\u001b[0m\n\u001b[31m\u001b[1m_____________ TestAlerting.test_trigger_alert_on_threshold_breach _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:201: in test_trigger_alert_on_threshold_breach\n    \u001b[0mservice.set_thresholds({\u001b[33m\"\u001b[39;49;00m\u001b[33mmin_confidence\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m0.8\u001b[39;49;00m})\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'set_thresholds'. Did you mean: '_check_thresholds'?\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________________ TestAlerting.test_alert_rate_limiting ____________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:218: in test_alert_rate_limiting\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch.object(service, \u001b[33m'\u001b[39;49;00m\u001b[33msend_alert\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, new_callable=AsyncMock) \u001b[94mas\u001b[39;49;00m mock_alert:\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <app.services.quality_monitoring_service.QualityMonitoringService object at 0x000001AE4EC28560> does not have the attribute 'send_alert'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________________ TestAlerting.test_alert_escalation ______________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:238: in test_alert_escalation\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch.object(service, \u001b[33m'\u001b[39;49;00m\u001b[33mescalate_alert\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, new_callable=AsyncMock) \u001b[94mas\u001b[39;49;00m mock_escalate:\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <app.services.quality_monitoring_service.QualityMonitoringService object at 0x000001AE4EC0F8F0> does not have the attribute 'escalate_alert'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestMetricsAggregation.test_aggregate_metrics_by_time_window _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:260: in test_aggregate_metrics_by_time_window\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m service.store_metric(metric)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'store_metric'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestMetricsAggregation.test_calculate_statistics _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:279: in test_calculate_statistics\n    \u001b[0mstats = \u001b[94mawait\u001b[39;49;00m service.calculate_statistics(values)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'calculate_statistics'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestMetricsAggregation.test_trend_analysis __________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:301: in test_trend_analysis\n    \u001b[0mtrend = \u001b[94mawait\u001b[39;49;00m service.analyze_trend(timestamps, values)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'analyze_trend'. Did you mean: '_analyze_trends'?\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestQualityReporting.test_generate_quality_report ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:317: in test_generate_quality_report\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m service.store_metric({\u001b[33m\"\u001b[39;49;00m\u001b[33mconfidence\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m0.9\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mlatency\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m1.2\u001b[39;49;00m})\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'store_metric'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestQualityReporting.test_export_metrics_to_json _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:340: in test_export_metrics_to_json\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m service.store_metric(metric)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'store_metric'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestQualityReporting.test_generate_sla_compliance_report ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:365: in test_generate_sla_compliance_report\n    \u001b[0mreport = \u001b[94mawait\u001b[39;49;00m service.check_sla_compliance(sla_targets, actual_metrics)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'check_sla_compliance'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestAnomalyDetection.test_detect_anomalies_zscore ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:384: in test_detect_anomalies_zscore\n    \u001b[0manomalies = \u001b[94mawait\u001b[39;49;00m service.detect_anomalies(values, method=\u001b[33m\"\u001b[39;49;00m\u001b[33mzscore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, threshold=\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'detect_anomalies'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________ TestAnomalyDetection.test_detect_anomalies_iqr ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:397: in test_detect_anomalies_iqr\n    \u001b[0manomalies = \u001b[94mawait\u001b[39;49;00m service.detect_anomalies(values, method=\u001b[33m\"\u001b[39;49;00m\u001b[33miqr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'detect_anomalies'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestAnomalyDetection.test_real_time_anomaly_detection ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:408: in test_real_time_anomaly_detection\n    \u001b[0mservice.configure_anomaly_detector(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'configure_anomaly_detector'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestPerformanceMonitoring.test_monitor_response_times ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:441: in test_monitor_response_times\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m service.record_response_time(rt)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'record_response_time'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestPerformanceMonitoring.test_monitor_throughput ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:457: in test_monitor_throughput\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m service.record_request()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'record_request'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestIntegration.test_integration_with_agent_service _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:482: in test_integration_with_agent_service\n    \u001b[0mmetrics = \u001b[94mawait\u001b[39;49;00m service.collect_agent_metrics(mock_agent_instance)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'QualityMonitoringService' object has no attribute 'collect_agent_metrics'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________ TestIntegration.test_integration_with_database ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_quality_monitoring_service.py\u001b[0m:493: in test_integration_with_database\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch(\u001b[33m'\u001b[39;49;00m\u001b[33mapp.services.database.metrics_repository\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m mock_repo:\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <module 'app.services.database' (namespace) from ['C:\\\\Users\\\\antho\\\\OneDrive\\\\Desktop\\\\Netra\\\\netra-core-generation-1\\\\app\\\\services\\\\database']> does not have the attribute 'metrics_repository'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestSchemaValidationService.test_validate_schema _______________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_schema_validation_service.py\u001b[0m:24: in test_validate_schema\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:660: in __getattr__\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(name)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: __aenter__\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________________________ test_encrypt_and_decrypt ___________________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_security_service.py\u001b[0m:15: in test_encrypt_and_decrypt\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SecurityService' object has no attribute 'encrypt'\u001b[0m\n\u001b[31m\u001b[1m_________________ TestStatePersistence.test_save_agent_state __________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_state_persistence.py\u001b[0m:16: in test_save_agent_state\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestStatePersistence.test_restore_agent_state ________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_state_persistence.py\u001b[0m:53: in test_restore_agent_state\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestStatePersistence.test_cleanup_old_states _________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_state_persistence.py\u001b[0m:67: in test_cleanup_old_states\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestStatePersistence.test_state_versioning __________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_state_persistence.py\u001b[0m:89: in test_state_versioning\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestStatePersistence.test_concurrent_state_updates ______________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_state_persistence.py\u001b[0m:102: in test_concurrent_state_updates\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestSupplyCatalogService.test_get_all_options ________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_supply_catalog_service.py\u001b[0m:37: in test_get_all_options\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert <MagicMock name='gpt-4.name' id='1848101313744'> == 'gpt-4'\u001b[0m\n\u001b[1m\u001b[31mE    +  where <MagicMock name='gpt-4.name' id='1848101313744'> = <MagicMock name='gpt-4' id='1848116206464'>.name\u001b[0m\n\u001b[31m\u001b[1m_______________ TestSupplyCatalogService.test_get_option_by_id ________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_supply_catalog_service.py\u001b[0m:49: in test_get_option_by_id\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert <MagicMock name='gpt-4.name' id='1848115301840'> == 'gpt-4'\u001b[0m\n\u001b[1m\u001b[31mE    +  where <MagicMock name='gpt-4.name' id='1848115301840'> = <MagicMock name='gpt-4' id='1848110259360'>.name\u001b[0m\n\u001b[31m\u001b[1m______________ TestSupplyCatalogService.test_get_option_by_name _______________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_supply_catalog_service.py\u001b[0m:60: in test_get_option_by_name\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert <MagicMock name='gpt-4.name' id='1848138895232'> == 'gpt-4'\u001b[0m\n\u001b[1m\u001b[31mE    +  where <MagicMock name='gpt-4.name' id='1848138895232'> = <MagicMock name='gpt-4' id='1848134639856'>.name\u001b[0m\n\u001b[31m\u001b[1m_______ TestCorpusManagement.test_corpus_creation_with_clickhouse_table _______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:92: in test_corpus_creation_with_clickhouse_table\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m corpus_service.create_corpus(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\corpus_service.py\u001b[0m:69: in create_corpus\n    \u001b[0mdb_corpus = models.Corpus(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m<string>\u001b[0m:4: in __init__\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\sqlalchemy\\orm\\state.py\u001b[0m:571: in _initialize_instance\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m util.safe_reraise():\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\u001b[0m:224: in __exit__\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m exc_value.with_traceback(exc_tb)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\sqlalchemy\\orm\\state.py\u001b[0m:569: in _initialize_instance\n    \u001b[0mmanager.original_init(*mixed[\u001b[94m1\u001b[39;49;00m:], **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\sqlalchemy\\orm\\decl_base.py\u001b[0m:2179: in _declarative_constructor\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mTypeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: 'domain' is an invalid keyword argument for Corpus\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestCorpusManagement.test_corpus_status_transitions _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:115: in test_corpus_status_transitions\n    \u001b[0m\u001b[94massert\u001b[39;49;00m corpus_service.is_valid_transition(from_status, to_status)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'CorpusService' object has no attribute 'is_valid_transition'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestCorpusManagement.test_corpus_content_upload_batch ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:132: in test_corpus_content_upload_batch\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m corpus_service.upload_corpus_content(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'CorpusService' object has no attribute 'upload_corpus_content'. Did you mean: '_copy_corpus_content'?\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestCorpusManagement.test_corpus_validation _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:156: in test_corpus_validation\n    \u001b[0m\u001b[94massert\u001b[39;49;00m corpus_service.validate_corpus_record(valid_record) == \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'CorpusService' object has no attribute 'validate_corpus_record'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestCorpusManagement.test_corpus_availability_check _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:168: in test_corpus_availability_check\n    \u001b[0mis_available, record_count = \u001b[94mawait\u001b[39;49;00m corpus_service.check_corpus_availability(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'CorpusService' object has no attribute 'check_corpus_availability'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestCorpusManagement.test_corpus_fallback_to_default _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:178: in test_corpus_fallback_to_default\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch(\u001b[33m'\u001b[39;49;00m\u001b[33mapp.services.corpus_service.get_default_corpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m mock_default:\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <module 'app.services.corpus_service' from 'C:\\\\Users\\\\antho\\\\OneDrive\\\\Desktop\\\\Netra\\\\netra-core-generation-1\\\\app\\\\services\\\\corpus_service.py'> does not have the attribute 'get_default_corpus'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestCorpusManagement.test_corpus_caching_mechanism ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:195: in test_corpus_caching_mechanism\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch.object(corpus_service, \u001b[33m'\u001b[39;49;00m\u001b[33m_fetch_corpus_content\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m mock_fetch:\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <app.services.corpus_service.CorpusService object at 0x000001AE4EE32870> does not have the attribute '_fetch_corpus_content'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestCorpusManagement.test_corpus_deletion_cascade ______________\u001b[0m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:913: in assert_called\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAssertionError\u001b[39;49;00m(msg)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: Expected 'execute' to have been called.\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:214: in test_corpus_deletion_cascade\n    \u001b[0mmock_clickhouse_client.execute.assert_called()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: Expected 'execute' to have been called.\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:51.790 | ERROR    | app.core.unified_logging:_log:313 | Failed to delete corpus 2c15fc88-1b3c-4ca4-8119-b563f857dd3e: 'str' object does not support item assignment\n\u001b[31m\u001b[1m_____________ TestCorpusManagement.test_corpus_metadata_tracking ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:221: in test_corpus_metadata_tracking\n    \u001b[0mmetadata = corpus_service.create_corpus_metadata(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'CorpusService' object has no attribute 'create_corpus_metadata'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestCorpusManagement.test_corpus_concurrent_access ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:247: in test_corpus_concurrent_access\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[95mnot\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(r, \u001b[96mException\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m r \u001b[95min\u001b[39;49;00m results)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   assert False\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = all(<generator object TestCorpusManagement.test_corpus_concurrent_access.<locals>.<genexpr> at 0x000001AE4EBA2880>)\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______ TestDataGenerationEngine.test_workload_distribution_generation ________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:276: in test_workload_distribution_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestDataGenerationEngine.test_temporal_pattern_generation __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:300: in test_temporal_pattern_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_with_temporal_patterns(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_temporal_patterns'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestDataGenerationEngine.test_tool_invocation_patterns ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:313: in test_tool_invocation_patterns\n    \u001b[0mpatterns = \u001b[94mawait\u001b[39;49;00m generation_service.generate_tool_invocations(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_tool_invocations'. Did you mean: '_generate_tool_invocations'?\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestDataGenerationEngine.test_error_scenario_generation ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:335: in test_error_scenario_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_with_errors(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_errors'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestDataGenerationEngine.test_trace_hierarchy_generation ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:349: in test_trace_hierarchy_generation\n    \u001b[0mtraces = \u001b[94mawait\u001b[39;49;00m generation_service.generate_trace_hierarchies(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_trace_hierarchies'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestDataGenerationEngine.test_domain_specific_generation ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:379: in test_domain_specific_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_domain_specific(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_domain_specific'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______ TestDataGenerationEngine.test_statistical_distribution_generation ______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:400: in test_statistical_distribution_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_with_distribution(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_distribution'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestDataGenerationEngine.test_custom_tool_catalog_generation _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:424: in test_custom_tool_catalog_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_with_custom_tools(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_custom_tools'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestDataGenerationEngine.test_incremental_generation _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:447: in test_incremental_generation\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m generation_service.generate_incremental(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_incremental'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestDataGenerationEngine.test_generation_with_corpus_sampling ________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:468: in test_generation_with_corpus_sampling\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m generation_service.generate_from_corpus(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_from_corpus'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestRealTimeIngestion.test_batch_ingestion_to_clickhouse ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:500: in test_batch_ingestion_to_clickhouse\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m ingestion_service.ingest_batch(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'ingest_batch'. Did you mean: '_ingest_batch'?\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______ TestRealTimeIngestion.test_streaming_ingestion_with_backpressure _______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:519: in test_streaming_ingestion_with_backpressure\n    \u001b[0mingestion_metrics = \u001b[94mawait\u001b[39;49;00m ingestion_service.ingest_stream(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'ingest_stream'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestRealTimeIngestion.test_ingestion_error_recovery _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:546: in test_ingestion_error_recovery\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m ingestion_service.ingest_with_retry(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'ingest_with_retry'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestRealTimeIngestion.test_ingestion_deduplication ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:567: in test_ingestion_deduplication\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m ingestion_service.ingest_with_deduplication(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'ingest_with_deduplication'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestRealTimeIngestion.test_table_creation_on_demand _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:583: in test_table_creation_on_demand\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ingestion_service.ensure_table_exists(table_name)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'ensure_table_exists'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestRealTimeIngestion.test_ingestion_metrics_tracking ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:600: in test_ingestion_metrics_tracking\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ingestion_service.track_ingestion(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'track_ingestion'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestRealTimeIngestion.test_parallel_batch_ingestion _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:621: in test_parallel_batch_ingestion\n    \u001b[0mingestion_service.ingest_batch(batch, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtable_\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mi\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'ingest_batch'. Did you mean: '_ingest_batch'?\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestRealTimeIngestion.test_ingestion_with_transformation ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:640: in test_ingestion_with_transformation\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m ingestion_service.ingest_with_transform(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'ingest_with_transform'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestRealTimeIngestion.test_ingestion_circuit_breaker _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:651: in test_ingestion_circuit_breaker\n    \u001b[0mcircuit_breaker = ingestion_service.get_circuit_breaker(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'get_circuit_breaker'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestRealTimeIngestion.test_ingestion_progress_tracking ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:680: in test_ingestion_progress_tracking\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ingestion_service.ingest_with_progress(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'ingest_with_progress'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestWebSocketUpdates.test_websocket_connection_management __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:714: in test_websocket_connection_management\n    \u001b[0m\u001b[94massert\u001b[39;49;00m job_id \u001b[95min\u001b[39;49;00m ws_service.active_connections\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'aa99b8ff-9f29-43e4-8028-ac3d55bcd949' in {<AsyncMock id='1848157113664'>: [ConnectionInfo(websocket='aa99b8ff-9f29-43e4-8028-ac3d55bcd949', user_id=<AsyncMock id='1848157113664'>, connected_at=datetime.datetime(2025, 8, 11, 12, 49, 53, 491824, tzinfo=datetime.timezone.utc), last_ping=datetime.datetime(2025, 8, 11, 12, 49, 53, 491824, tzinfo=datetime.timezone.utc), last_pong=None, message_count=0, error_count=1, connection_id='conn_1754916593491')]}\u001b[0m\n\u001b[1m\u001b[31mE    +  where {<AsyncMock id='1848157113664'>: [ConnectionInfo(websocket='aa99b8ff-9f29-43e4-8028-ac3d55bcd949', user_id=<AsyncMock id='1848157113664'>, connected_at=datetime.datetime(2025, 8, 11, 12, 49, 53, 491824, tzinfo=datetime.timezone.utc), last_ping=datetime.datetime(2025, 8, 11, 12, 49, 53, 491824, tzinfo=datetime.timezone.utc), last_pong=None, message_count=0, error_count=1, connection_id='conn_1754916593491')]} = <app.ws_manager.WebSocketManager object at 0x000001AE1EA18B30>.active_connections\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:53.491 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916593491: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.491 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754916593491: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.491 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-709' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 113, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 132, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 241, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 341, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 642, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-709' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31m\u001b[1m___________ TestWebSocketUpdates.test_generation_progress_broadcast ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:735: in test_generation_progress_broadcast\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.broadcast_to_job(job_id, progress_update)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'broadcast_to_job'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:53.571 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916593571: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.571 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754916593571: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.571 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-713' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 113, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 132, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 241, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 341, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 642, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-713' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31m\u001b[1m__________ TestWebSocketUpdates.test_batch_completion_notifications ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:755: in test_batch_completion_notifications\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.notify_batch_complete(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'notify_batch_complete'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:53.651 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916593635: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.651 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754916593635: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.651 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-717' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 113, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 132, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 241, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 341, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 642, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-717' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31m\u001b[1m____________ TestWebSocketUpdates.test_error_notification_handling ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:780: in test_error_notification_handling\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.notify_error(job_id, error_data)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'notify_error'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:53.731 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916593731: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.731 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754916593731: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.731 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-721' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 113, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 132, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 241, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 341, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 642, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-721' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31m\u001b[1m__________ TestWebSocketUpdates.test_websocket_reconnection_handling __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:794: in test_websocket_reconnection_handling\n    \u001b[0mws_service.set_job_state(job_id, {\u001b[33m\"\u001b[39;49;00m\u001b[33mprogress\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m50\u001b[39;49;00m})\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'set_job_state'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:53.819 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916593819: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.825 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754916593819: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.825 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-725' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 113, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 132, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 241, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 341, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 642, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-725' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31m\u001b[1m___________ TestWebSocketUpdates.test_multiple_client_subscriptions ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:818: in test_multiple_client_subscriptions\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.broadcast_to_job(job_id, update)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'broadcast_to_job'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:53.906 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916593906: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.907 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916593906: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.908 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916593908: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.908 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916593908: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.909 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916593909: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.909 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754916593906: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.910 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-729' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 113, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 132, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 241, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 341, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 642, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.910 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754916593908: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.910 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-731' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 113, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 132, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 241, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 341, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 642, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.910 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754916593909: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:53.910 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-733' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 113, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 132, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 241, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 341, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 642, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-729' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-731' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-733' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31m\u001b[1m_____________ TestWebSocketUpdates.test_websocket_message_queuing _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:836: in test_websocket_message_queuing\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.broadcast_to_job(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'broadcast_to_job'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:54.003 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916593987: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:54.003 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754916593987: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:54.003 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-737' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 113, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 132, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 241, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 341, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 642, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-737' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31m\u001b[1m________________ TestWebSocketUpdates.test_websocket_heartbeat ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:852: in test_websocket_heartbeat\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.start_heartbeat(job_id, interval_seconds=\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'start_heartbeat'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:54.075 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916594075: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:54.076 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754916594075: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:54.076 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-741' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 113, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 132, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 241, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 341, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 642, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-741' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31m\u001b[1m________ TestWebSocketUpdates.test_generation_completion_notification _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:882: in test_generation_completion_notification\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m ws_service.notify_completion(job_id, completion_data)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'notify_completion'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:54.153 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916594153: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:54.153 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754916594153: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:54.153 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-745' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 113, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 132, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 241, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 341, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 642, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-745' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31m\u001b[1m______________ TestWebSocketUpdates.test_websocket_rate_limiting ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:895: in test_websocket_rate_limiting\n    \u001b[0mws_service.set_rate_limit(job_id, max_messages_per_second=\u001b[94m10\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'WebSocketManager' object has no attribute 'set_rate_limit'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:49:54.249 | ERROR    | app.ws_manager:_send_to_connection:207 | Unexpected error sending to conn_1754916594249: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:54.249 | ERROR    | app.ws_manager:_heartbeat_loop:252 | Heartbeat error for conn_1754916594249: 'str' object has no attribute 'client_state'\n2025-08-11 05:49:54.249 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-749' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 113, in pytest_runtest_protocol\n    runtestprotocol(item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 132, in runtestprotocol\n    reports.append(call_and_report(item, \"call\", log))\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 241, in call_and_report\n    call = CallInfo.from_call(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 341, in from_call\n    result: TResult | None = func()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 642, in inner\n    _loop.run_until_complete(task)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 674, in run_until_complete\n    self.run_forever()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1972, in _run_once\n    handle = self._ready.popleft()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\nAttributeError: 'str' object has no attribute 'client_state'\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m asyncio:base_events.py:1821 Task exception was never retrieved\nfuture: <Task finished name='Task-749' coro=<WebSocketManager._heartbeat_loop() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py:230> exception=AttributeError(\"'str' object has no attribute 'client_state'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 256, in _heartbeat_loop\n    await self.disconnect(conn_info.user_id, conn_info.websocket)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\ws_manager.py\", line 125, in disconnect\n    if websocket.client_state == WebSocketState.CONNECTED:\n       ^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'client_state'\n\u001b[31m\u001b[1m______________ TestDataQualityValidation.test_schema_validation _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:937: in test_schema_validation\n    \u001b[0m\u001b[94massert\u001b[39;49;00m validation_service.validate_schema(valid_record) == \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'validate_schema'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____ TestDataQualityValidation.test_statistical_distribution_validation ______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:943: in test_statistical_distribution_validation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______ TestDataQualityValidation.test_referential_integrity_validation _______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:960: in test_referential_integrity_validation\n    \u001b[0mtraces = \u001b[94mawait\u001b[39;49;00m validation_service.generate_trace_hierarchies(num_traces=\u001b[94m10\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_trace_hierarchies'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______ TestDataQualityValidation.test_temporal_consistency_validation ________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:971: in test_temporal_consistency_validation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestDataQualityValidation.test_data_completeness_validation _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:987: in test_data_completeness_validation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____ TestDataQualityValidation.test_anomaly_detection_in_generated_data ______\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1009: in test_anomaly_detection_in_generated_data\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_with_anomalies(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_anomalies'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestDataQualityValidation.test_correlation_preservation ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1020: in test_correlation_preservation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestDataQualityValidation.test_quality_metrics_calculation __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1036: in test_quality_metrics_calculation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestDataQualityValidation.test_data_diversity_validation ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1050: in test_data_diversity_validation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestDataQualityValidation.test_validation_report_generation _________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1063: in test_validation_report_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m validation_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestPerformanceScalability.test_high_throughput_generation __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1094: in test_high_throughput_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m perf_service.generate_parallel(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_parallel'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________ TestPerformanceScalability.test_memory_efficient_streaming __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1117: in test_memory_efficient_streaming\n    \u001b[0mstream = perf_service.generate_stream(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_stream'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestPerformanceScalability.test_horizontal_scaling ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1146: in test_horizontal_scaling\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m perf_service.generate_parallel(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_parallel'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestPerformanceScalability.test_batch_size_optimization ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1169: in test_batch_size_optimization\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m perf_service.generate_batched(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_batched'. Did you mean: '_generate_batches'?\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________ TestPerformanceScalability.test_connection_pooling_efficiency ________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1183: in test_connection_pooling_efficiency\n    \u001b[0mpool_metrics = \u001b[94mawait\u001b[39;49;00m perf_service.test_connection_pool(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'test_connection_pool'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestPerformanceScalability.test_cache_effectiveness _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1198: in test_cache_effectiveness\n    \u001b[0mcorpus1 = \u001b[94mawait\u001b[39;49;00m perf_service.get_corpus_cached(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_corpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'get_corpus_cached'. Did you mean: 'corpus_cache'?\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestPerformanceScalability.test_auto_scaling_behavior ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1224: in test_auto_scaling_behavior\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m perf_service.generate_with_auto_scaling(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_auto_scaling'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestPerformanceScalability.test_resource_limit_handling ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1245: in test_resource_limit_handling\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m perf_service.generate_with_limits(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_limits'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestPerformanceScalability.test_query_optimization ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1255: in test_query_optimization\n    \u001b[0munoptimized_time = \u001b[94mawait\u001b[39;49;00m perf_service.benchmark_query(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'benchmark_query'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestPerformanceScalability.test_burst_load_handling _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1284: in test_burst_load_handling\n    \u001b[0mnormal_result = \u001b[94mawait\u001b[39;49;00m perf_service.generate_with_pattern(normal_config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_pattern'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestErrorRecovery.test_corpus_unavailable_fallback ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1304: in test_corpus_unavailable_fallback\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch.object(recovery_service, \u001b[33m'\u001b[39;49;00m\u001b[33mget_corpus_content\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, side_effect=\u001b[96mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCorpus not found\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)):\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <app.services.synthetic_data_service.SyntheticDataService object at 0x000001AE4EC1DD00> does not have the attribute 'get_corpus_content'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestErrorRecovery.test_clickhouse_connection_recovery ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1328: in test_clickhouse_connection_recovery\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m recovery_service.ingest_with_retry(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'ingest_with_retry'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestErrorRecovery.test_generation_checkpoint_recovery ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1341: in test_generation_checkpoint_recovery\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch.object(recovery_service, \u001b[33m'\u001b[39;49;00m\u001b[33mgenerate_batch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m mock_gen:\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <app.services.synthetic_data_service.SyntheticDataService object at 0x000001AE4EC986E0> does not have the attribute 'generate_batch'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestErrorRecovery.test_websocket_disconnect_recovery _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1379: in test_websocket_disconnect_recovery\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m recovery_service.generate_with_ws_updates(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_ws_updates'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________ TestErrorRecovery.test_memory_overflow_handling _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1395: in test_memory_overflow_handling\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m recovery_service.generate_with_memory_limit(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_memory_limit'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestErrorRecovery.test_circuit_breaker_operation _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1405: in test_circuit_breaker_operation\n    \u001b[0mcircuit_breaker = recovery_service.get_circuit_breaker()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'get_circuit_breaker'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestErrorRecovery.test_dead_letter_queue_processing _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1433: in test_dead_letter_queue_processing\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m recovery_service.process_with_dlq(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'process_with_dlq'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestErrorRecovery.test_transaction_rollback _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1460: in test_transaction_rollback\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m recovery_service.query_records()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'query_records'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestErrorRecovery.test_idempotent_generation _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1473: in test_idempotent_generation\n    \u001b[0mresult1 = \u001b[94mawait\u001b[39;49;00m recovery_service.generate_idempotent(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_idempotent'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestErrorRecovery.test_graceful_degradation _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1492: in test_graceful_degradation\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m patch.object(recovery_service, \u001b[33m'\u001b[39;49;00m\u001b[33menable_clustering\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, side_effect=\u001b[96mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mClustering unavailable\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)):\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1458: in __enter__\n    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\unittest\\mock.py\u001b[0m:1431: in get_original\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: <app.services.synthetic_data_service.SyntheticDataService object at 0x000001AE4EC9B0B0> does not have the attribute 'enable_clustering'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestAdminVisibility.test_generation_job_monitoring ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1516: in test_generation_job_monitoring\n    \u001b[0madmin_service.generate_monitored(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_monitored'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestAdminVisibility.test_detailed_metrics_dashboard _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1535: in test_detailed_metrics_dashboard\n    \u001b[0mmetrics = \u001b[94mawait\u001b[39;49;00m admin_service.get_generation_metrics(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'get_generation_metrics'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________ TestAdminVisibility.test_corpus_usage_analytics _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1548: in test_corpus_usage_analytics\n    \u001b[0manalytics = \u001b[94mawait\u001b[39;49;00m admin_service.get_corpus_analytics()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'get_corpus_analytics'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestAdminVisibility.test_audit_log_generation ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1560: in test_audit_log_generation\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m admin_service.generate_with_audit(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_audit'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________ TestAdminVisibility.test_performance_profiling ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1576: in test_performance_profiling\n    \u001b[0mprofile = \u001b[94mawait\u001b[39;49;00m admin_service.profile_generation(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'profile_generation'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestAdminVisibility.test_alert_configuration _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1594: in test_alert_configuration\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m admin_service.configure_alerts(alert_config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'configure_alerts'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestAdminVisibility.test_job_cancellation_by_admin ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1611: in test_job_cancellation_by_admin\n    \u001b[0madmin_service.generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() got an unexpected keyword argument 'job_id'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestAdminVisibility.test_resource_usage_tracking _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1630: in test_resource_usage_tracking\n    \u001b[0mresource_tracker = \u001b[94mawait\u001b[39;49;00m admin_service.start_resource_tracking()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'start_resource_tracking'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________ TestAdminVisibility.test_admin_diagnostic_tools _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1646: in test_admin_diagnostic_tools\n    \u001b[0mdiagnostics = \u001b[94mawait\u001b[39;49;00m admin_service.run_diagnostics()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'run_diagnostics'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestAdminVisibility.test_batch_job_management ________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1660: in test_batch_job_management\n    \u001b[0mjob_id = \u001b[94mawait\u001b[39;49;00m admin_service.schedule_generation(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'schedule_generation'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestIntegration.test_complete_generation_workflow ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1696: in test_complete_generation_workflow\n    \u001b[0mcorpus = \u001b[94mawait\u001b[39;49;00m full_stack[\u001b[33m\"\u001b[39;49;00m\u001b[33mcorpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].create_corpus(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: CorpusService.create_corpus() missing 1 required positional argument: 'corpus_data'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestIntegration.test_multi_tenant_generation _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1737: in test_multi_tenant_generation\n    \u001b[0mjob = \u001b[94mawait\u001b[39;49;00m full_stack[\u001b[33m\"\u001b[39;49;00m\u001b[33mgeneration\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].generate_for_tenant(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_for_tenant'. Did you mean: '_generate_content'?\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestIntegration.test_real_time_streaming_pipeline ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1768: in test_real_time_streaming_pipeline\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m full_stack[\u001b[33m\"\u001b[39;49;00m\u001b[33mgeneration\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].generate_streaming(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_streaming'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestIntegration.test_failure_recovery_integration ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1797: in test_failure_recovery_integration\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m full_stack[\u001b[33m\"\u001b[39;49;00m\u001b[33mgeneration\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].generate_with_recovery(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_recovery'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_______________ TestIntegration.test_cross_component_validation _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1809: in test_cross_component_validation\n    \u001b[0mgeneration_result = \u001b[94mawait\u001b[39;49;00m full_stack[\u001b[33m\"\u001b[39;49;00m\u001b[33mgeneration\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].generate_synthetic_data(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n-------------------------- Captured stderr teardown ---------------------------\n2025-08-11 05:49:57.299 | ERROR    | logging:callHandlers:1762 | Task exception was never retrieved\nfuture: <Task finished name='Task-883' coro=<TestIntegration.test_real_time_streaming_pipeline.<locals>.ws_listener() done, defined at C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\tests\\services\\test_synthetic_data_service_v3.py:1759> exception=AttributeError(\"'WebSocketManager' object has no attribute 'listen'\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 463, in <module>\n    main()\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 457, in main\n    exit_code = run_tests(pytest_args, args, isolation_manager)\n  File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\scripts\\test_backend.py\", line 277, in run_tests\n    exit_code = pytest.main(pytest_args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\__init__.py\", line 175, in main\n    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 330, in pytest_cmdline_main\n    return wrap_session(config, _main)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 283, in wrap_session\n    session.exitstatus = doit(config, session) or 0\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 337, in _main\n    config.hook.pytest_runtestloop(session=session)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\main.py\", line 362, in pytest_runtestloop\n    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 182, in _multicall\n    return outcome.get_result()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 115, in pytest_runtest_protocol\n    return True\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 143, in runtestprotocol\n    return reports\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 249, in call_and_report\n    return report\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 351, in from_call\n    return cls(\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 182, in _multicall\n    return outcome.get_result()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\runner.py\", line 184, in pytest_runtest_call\n    raise\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 426, in runtest\n    super().runtest()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\pluggy\\_callers.py\", line 182, in _multicall\n    return outcome.get_result()\n  File \"C:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\site-packages\\pytest_asyncio\\plugin.py\", line 649, in inner\n    raise\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 687, in run_until_complete\n    return future.result()\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 649, in run_forever\n    sys.set_asyncgen_hooks(*old_agen_hooks)\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1988, in _run_once\n    handle = None  # Needed to break cycles when an exception occurs.\n  File \"C:\\Users\\antho\\miniconda3\\Lib\\asyncio\\events.py\", line 103, in _run\n    self = None  # Needed to break cycles when an exception occurs.\n> File \"C:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\app\\tests\\services\\test_synthetic_data_service_v3.py\", line 1760, in ws_listener\n    async for message in full_stack[\"websocket\"].listen(job_id):\nAttributeError: 'WebSocketManager' object has no attribute 'listen'\n\u001b[31m\u001b[1m_________________ TestIntegration.test_performance_under_load _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1831: in test_performance_under_load\n    \u001b[0mresults = \u001b[94mawait\u001b[39;49;00m asyncio.gather(*[run_job(i) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(concurrent_jobs)])\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1826: in run_job\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[94mawait\u001b[39;49;00m full_stack[\u001b[33m\"\u001b[39;49;00m\u001b[33mgeneration\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestIntegration.test_data_consistency_verification ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1846: in test_data_consistency_verification\n    \u001b[0mgeneration_result = \u001b[94mawait\u001b[39;49;00m full_stack[\u001b[33m\"\u001b[39;49;00m\u001b[33mgeneration\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() got an unexpected keyword argument 'job_id'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestIntegration.test_monitoring_integration _________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1869: in test_monitoring_integration\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m full_stack[\u001b[33m\"\u001b[39;49;00m\u001b[33mgeneration\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].enable_monitoring()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'enable_monitoring'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________ TestIntegration.test_security_and_access_control _______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1887: in test_security_and_access_control\n    \u001b[0mrestricted_corpus = \u001b[94mawait\u001b[39;49;00m full_stack[\u001b[33m\"\u001b[39;49;00m\u001b[33mcorpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].create_corpus(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: CorpusService.create_corpus() missing 1 required positional argument: 'corpus_data'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_________________ TestIntegration.test_cleanup_and_retention __________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1912: in test_cleanup_and_retention\n    \u001b[0m\u001b[94mawait\u001b[39;49;00m full_stack[\u001b[33m\"\u001b[39;49;00m\u001b[33mgeneration\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].generate_synthetic_data(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: SyntheticDataService.generate_synthetic_data() got an unexpected keyword argument 'job_id'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestAdvancedFeatures.test_ml_driven_pattern_generation ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1944: in test_ml_driven_pattern_generation\n    \u001b[0mtraining_data = \u001b[94mawait\u001b[39;49;00m advanced_service.load_production_patterns()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'load_production_patterns'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestAdvancedFeatures.test_anomaly_injection_strategies ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1978: in test_anomaly_injection_strategies\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_with_anomalies(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_anomalies'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestAdvancedFeatures.test_cross_correlation_generation ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:1996: in test_cross_correlation_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_with_correlations(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_correlations'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestAdvancedFeatures.test_temporal_event_sequences ______________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:2021: in test_temporal_event_sequences\n    \u001b[0msequences = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_event_sequences(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_event_sequences'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestAdvancedFeatures.test_geo_distributed_simulation _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:2056: in test_geo_distributed_simulation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_geo_distributed(geo_config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_geo_distributed'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________ TestAdvancedFeatures.test_adaptive_generation_feedback ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:2079: in test_adaptive_generation_feedback\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_adaptive(config, target_metrics)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_adaptive'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________ TestAdvancedFeatures.test_multi_model_workload_generation __________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:2100: in test_multi_model_workload_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_multi_model(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_multi_model'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestAdvancedFeatures.test_compliance_aware_generation ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:2126: in test_compliance_aware_generation\n    \u001b[0mrecords = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_compliant(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_compliant'. Did you mean: '_generate_content'?\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________ TestAdvancedFeatures.test_cost_optimized_generation _____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:2147: in test_cost_optimized_generation\n    \u001b[0mresult = \u001b[94mawait\u001b[39;49;00m advanced_service.generate_cost_optimized(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'generate_cost_optimized'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________ TestAdvancedFeatures.test_versioned_corpus_generation ____________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_service_v3.py\u001b[0m:2160: in test_versioned_corpus_generation\n    \u001b[0mv1_corpus = \u001b[94mawait\u001b[39;49;00m advanced_service.create_corpus_version(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'SyntheticDataService' object has no attribute 'create_corpus_version'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_ TestImplementationConsistency.test_synthetic_data_service_has_required_methods _\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_validation.py\u001b[0m:223: in test_synthetic_data_service_has_required_methods\n    \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(service, \u001b[33m'\u001b[39;49;00m\u001b[33mgenerate_batch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert False\u001b[0m\n\u001b[1m\u001b[31mE    +  where False = hasattr(<app.services.synthetic_data_service.SyntheticDataService object at 0x000001AE4C562AB0>, 'generate_batch')\u001b[0m\n\u001b[31m\u001b[1m__________ TestImplementationConsistency.test_agent_tools_available ___________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_synthetic_data_validation.py\u001b[0m:237: in test_agent_tools_available\n    \u001b[0m\u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mapp\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96magents\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96madmin_tool_dispatcher\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m AdminToolDispatcher\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   ModuleNotFoundError: No module named 'app.agents.admin_tool_dispatcher'\u001b[0m\n\u001b[31m\u001b[1m____________________ TestThreadService.test_create_thread _____________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_thread_service.py\u001b[0m:17: in test_create_thread\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: ThreadService() takes no arguments\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________________ TestThreadService.test_get_thread_history __________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_thread_service.py\u001b[0m:41: in test_get_thread_history\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: ThreadService() takes no arguments\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m________________ TestThreadService.test_update_thread_metadata ________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_thread_service.py\u001b[0m:59: in test_update_thread_metadata\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: ThreadService() takes no arguments\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________________ TestThreadService.test_delete_thread _____________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_thread_service.py\u001b[0m:77: in test_delete_thread\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: ThreadService() takes no arguments\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m__________________ TestThreadService.test_thread_pagination ___________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_thread_service.py\u001b[0m:93: in test_thread_pagination\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: ThreadService() takes no arguments\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m____________________ TestThreadService.test_thread_search _____________________\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\services\\test_thread_service.py\u001b[0m:109: in test_thread_search\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: ThreadService() takes no arguments\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________________ TestUserService.test_get_user_by_email ____________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_user_service.py\u001b[0m:103: in test_get_user_by_email\n    \u001b[0m\u001b[94massert\u001b[39;49;00m found_user.username == sample_user.username\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'User' object has no attribute 'username'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________________ TestUserService.test_update_user _______________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_user_service.py\u001b[0m:139: in test_update_user\n    \u001b[0mupdate_data = UserUpdate(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   pydantic_core._pydantic_core.ValidationError: 1 validation error for UserUpdate\u001b[0m\n\u001b[1m\u001b[31mE   email\u001b[0m\n\u001b[1m\u001b[31mE     Field required [type=missing, input_value={'full_name': 'Updated Na...sername': 'updateduser'}, input_type=dict]\u001b[0m\n\u001b[1m\u001b[31mE       For further information visit https://errors.pydantic.dev/2.11/v/missing\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m______________________ TestUserService.test_delete_user _______________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_user_service.py\u001b[0m:180: in test_delete_user\n    \u001b[0mdeleted_user = \u001b[94mawait\u001b[39;49;00m crud_user.remove(db=mock_db_session, \u001b[96mid\u001b[39;49;00m=sample_user.id)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\base.py\u001b[0m:76: in remove\n    \u001b[0mobj = \u001b[94mawait\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.get(db, \u001b[96mid\u001b[39;49;00m=\u001b[96mid\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp\\services\\base.py\u001b[0m:43: in get\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m result.scalars().first()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'coroutine' object has no attribute 'first'\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____________________ TestUserService.test_get_user_by_id _____________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_user_service.py\u001b[0m:205: in test_get_user_by_id\n    \u001b[0m\u001b[94massert\u001b[39;49;00m found_user.id == sample_user.id\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert <Mock name='mock.execute().scalars().first().id' id='1848134676896'> == 'bd186e0b-4604-4e8f-8830-ee652ef3549c'\u001b[0m\n\u001b[1m\u001b[31mE    +  where <Mock name='mock.execute().scalars().first().id' id='1848134676896'> = <Mock name='mock.execute().scalars().first()' id='1848134669264'>.id\u001b[0m\n\u001b[1m\u001b[31mE    +  and   'bd186e0b-4604-4e8f-8830-ee652ef3549c' = <app.db.models_postgres.User object at 0x000001AE4D691D00>.id\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m___________________ TestUserService.test_get_multiple_users ___________________\u001b[0m\n\u001b[1m\u001b[31mapp\\tests\\services\\test_user_service.py\u001b[0m:219: in test_get_multiple_users\n    \u001b[0mUser(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m<string>\u001b[0m:4: in __init__\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\sqlalchemy\\orm\\state.py\u001b[0m:571: in _initialize_instance\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m util.safe_reraise():\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\u001b[0m:224: in __exit__\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m exc_value.with_traceback(exc_tb)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\sqlalchemy\\orm\\state.py\u001b[0m:569: in _initialize_instance\n    \u001b[0mmanager.original_init(*mixed[\u001b[94m1\u001b[39;49;00m:], **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m..\\..\\..\\..\\miniconda3\\Lib\\site-packages\\sqlalchemy\\orm\\decl_base.py\u001b[0m:2179: in _declarative_constructor\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mTypeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: 'username' is an invalid keyword argument for User\u001b[0m\n----------------------------- Captured log setup ------------------------------\n\u001b[35mDEBUG   \u001b[0m asyncio:selector_events.py:64 Using selector: SelectSelector\n\u001b[31m\u001b[1m_____ TestConfigValidator.test_validate_auth_config_production_dev_secret _____\u001b[0m\n\u001b[1m\u001b[31mC:\\Users\\antho\\OneDrive\\Desktop\\Netra\\netra-core-generation-1\\v2\\app\\tests\\core\\test_config_manager.py\u001b[0m:159: in test_validate_auth_config_production_dev_secret\n    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: assert 'Development JWT secret key cannot be used in production' in 'Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured'\u001b[0m\n\u001b[1m\u001b[31mE    +  where 'Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured' = str(ConfigurationValidationError('Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured'))\u001b[0m\n\u001b[1m\u001b[31mE    +    where ConfigurationValidationError('Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured') = <ExceptionInfo ConfigurationValidationError('Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured') tblen=3>.value\u001b[0m\n---------------------------- Captured stderr call -----------------------------\n2025-08-11 05:50:08.319 | ERROR    | logging:callHandlers:1762 | Configuration validation failed: Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured\n------------------------------ Captured log call ------------------------------\n\u001b[1m\u001b[31mERROR   \u001b[0m app.core.config_validator:config_validator.py:34 Configuration validation failed: Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured\n\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\agents\\test_sub_agent.py::\u001b[1mtest_agent_node_is_coroutine\u001b[0m - TypeError: Can't instantiate abstract class ConcreteSubAgent without an implementation for abstract method 'execute'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\agents\\test_tools.py::\u001b[1mtest_tool_dispatcher\u001b[0m - assert None == 3\n +  where None = ToolResult(tool_input=ToolInput(tool_name='mock_tool', args=[], kwargs={'a': 1, 'b': 2}), status=<ToolStatus.ERROR: 'error'>, message=\"BaseTool.arun() missing 1 required positional argument: 'tool_input'\", payload=None, start_time=1754916584.7837372, end_time=None).payload\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\agents\\test_tools.py::\u001b[1mtest_tool_dispatcher_tool_not_found\u001b[0m - assert \"Tool 'non_existent_tool' not found\" in 'Tool non_existent_tool not found'\n +  where 'Tool non_existent_tool not found' = ToolResult(tool_input=ToolInput(tool_name='non_existent_tool', args=[], kwargs={'a': 1, 'b': 2}), status=<ToolStatus.ERROR: 'error'>, message='Tool non_existent_tool not found', payload=None, start_time=1754916584.799587, end_time=None).message\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\agents\\test_tools.py::\u001b[1mtest_tool_dispatcher_tool_error\u001b[0m - assert 'Error dispatching tool' in \"BaseTool.arun() missing 1 required positional argument: 'tool_input'\"\n +  where \"BaseTool.arun() missing 1 required positional argument: 'tool_input'\" = ToolResult(tool_input=ToolInput(tool_name='mock_tool', args=[], kwargs={'a': 1, 'b': '2'}), status=<ToolStatus.ERROR: 'error'>, message=\"BaseTool.arun() missing 1 required positional argument: 'tool_input'\", payload=None, start_time=1754916584.821934, end_time=None).message\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\test_tool_builder.py::\u001b[1mtest_tool_builder_and_dispatcher\u001b[0m - AssertionError: assert <ToolStatus.ERROR: 'error'> == <ToolStatus.SUCCESS: 'success'>\n  \n  - success\n  + error\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::\u001b[1mTestBaseTool::test_base_tool_execute_without_metadata_attribute\u001b[0m - Failed: DID NOT RAISE <class 'AttributeError'>\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_base.py::\u001b[1mTestBaseTool::test_base_tool_exception_types\u001b[0m - assert \"'Custom error'\" == 'Custom error'\n  \n  - Custom error\n  + 'Custom error'\n  ? +            +\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::\u001b[1mTestCostAnalyzer::test_cost_analyzer_async_execution\u001b[0m - AssertionError: assert 'Test prompt' in 'Analyzed current costs. Total estimated cost: $0.03'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_cost_analyzer.py::\u001b[1mTestCostAnalyzer::test_cost_analyzer_rounding_edge_cases\u001b[0m - AssertionError: Failed for costs [0.005, 0.005, 0.005]\nassert '$0.02' in 'Analyzed current costs. Total estimated cost: $0.01'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_basic_functionality\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116475808'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_empty_logs\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116645664'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_single_log\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116655888'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_high_latency_values\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116375584'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_zero_latency\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848115300496'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_sub_millisecond_latency\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116819584'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_exception_handling\u001b[0m - assert \"1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116818816'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\" == 'API Error'\n  \n  - API Error\n  + 1 validation error for latency_analyzer\n  +   Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848116818816'>, input_type=Mock]\n  +     For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_async_execution\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848134636064'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_large_dataset\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848134647248'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_varied_latencies\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138677328'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_partial_failure\u001b[0m - assert 'Prediction service unavailable' in \"1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138905312'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\"\n +  where \"1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138905312'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\" = str(1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138905312'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type)\n +    where 1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138905312'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type = <ExceptionInfo 1 validation error for latency_analyzer\\n  Input should be a valid dictionary or instance of latency_analyzer [type=mod...' id='1848138905312'>, input_type=Mock]\\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type tblen=8>.value\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_edge_case_rounding\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138977728'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_negative_latencies\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848138900272'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_mixed_response_formats\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848107828768'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_extreme_values\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848107840960'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\apex_optimizer_agent\\tools\\test_latency_analyzer.py::\u001b[1mTestLatencyAnalyzer::test_latency_analyzer_concurrent_execution_timing\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for latency_analyzer\n  Input should be a valid dictionary or instance of latency_analyzer [type=model_type, input_value=<Mock spec='ToolContext' id='1848139108368'>, input_type=Mock]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_agent_message_processing.py::\u001b[1mTestAgentMessageProcessing::test_process_user_message\u001b[0m - TypeError: AgentService.__init__() takes 2 positional arguments but 3 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_agent_message_processing.py::\u001b[1mTestAgentMessageProcessing::test_handle_tool_execution\u001b[0m - TypeError: AgentService.__init__() takes 2 positional arguments but 3 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_agent_message_processing.py::\u001b[1mTestAgentMessageProcessing::test_message_validation\u001b[0m - TypeError: AgentService.__init__() takes 2 positional arguments but 3 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_agent_service.py::\u001b[1mtest_run_agent\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for AnalysisRequest\nrequest_model\n  Field required [type=missing, input_value={'settings': Settings(deb...'))], constraints=None)}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_clickhouse_service.py::\u001b[1mTestClickHouseConnection::test_client_initialization\u001b[0m - AttributeError: 'MockClickHouseDatabase' object has no attribute 'fetch'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_clickhouse_service.py::\u001b[1mTestClickHouseConnection::test_list_corpus_tables\u001b[0m - AttributeError: 'MockClickHouseDatabase' object has no attribute 'fetch'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_clickhouse_service.py::\u001b[1mTestClickHouseConnection::test_basic_query_execution\u001b[0m - AttributeError: 'MockClickHouseDatabase' object has no attribute 'fetch'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_clickhouse_service.py::\u001b[1mTestClickHouseConnection::test_query_with_parameters\u001b[0m - AttributeError: 'MockClickHouseDatabase' object has no attribute 'fetch'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_clickhouse_service.py::\u001b[1mTestBasicOperations::test_show_tables\u001b[0m - AttributeError: 'MockClickHouseDatabase' object has no attribute 'fetch'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_corpus_service.py::\u001b[1mTestCorpusService::test_corpus_schema\u001b[0m - pydantic_core._pydantic_core.ValidationError: 4 validation errors for Corpus\nstatus\n  Field required [type=missing, input_value={'id': 'test-corpus-123',... corpus for validation'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ncreated_by_id\n  Field required [type=missing, input_value={'id': 'test-corpus-123',... corpus for validation'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ncreated_at\n  Field required [type=missing, input_value={'id': 'test-corpus-123',... corpus for validation'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nupdated_at\n  Field required [type=missing, input_value={'id': 'test-corpus-123',... corpus for validation'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestUnitOfWork::test_uow_transaction_commit\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestUnitOfWork::test_uow_transaction_rollback\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestUnitOfWork::test_uow_nested_transactions\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestUnitOfWork::test_uow_concurrent_access\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_create\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_bulk_create\u001b[0m - AttributeError: 'ThreadRepository' object has no attribute 'bulk_create'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_get_by_id\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_get_many\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_update\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_delete\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_soft_delete\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestBaseRepository::test_repository_pagination\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestMessageRepository::test_get_messages_by_thread\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestMessageRepository::test_get_messages_with_pagination\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestMessageRepository::test_get_latest_messages\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestThreadRepository::test_get_threads_by_user\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestThreadRepository::test_get_active_threads\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestThreadRepository::test_archive_thread\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestRunRepository::test_create_run_with_tools\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestRunRepository::test_update_run_status\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestRunRepository::test_get_active_runs\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestReferenceRepository::test_create_reference_with_metadata\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestReferenceRepository::test_get_references_by_message\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_database_repositories.py::\u001b[1mTestReferenceRepository::test_search_references\u001b[0m - AttributeError: 'dict' object has no attribute 'rollback'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_optimization_fallback_low_quality\u001b[0m - TypeError: object of type 'coroutine' has no len()\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_data_analysis_fallback_parsing_error\u001b[0m - TypeError: argument of type 'coroutine' is not iterable\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_action_plan_fallback_context_missing\u001b[0m - TypeError: argument of type 'coroutine' is not iterable\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_report_fallback_validation_failed\u001b[0m - TypeError: argument of type 'coroutine' is not iterable\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_triage_fallback_timeout\u001b[0m - TypeError: argument of type 'coroutine' is not iterable\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_error_message_fallback_llm_error\u001b[0m - AttributeError: 'coroutine' object has no attribute 'lower'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_fallback_with_circular_reasoning\u001b[0m - TypeError: argument of type 'coroutine' is not iterable\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_fallback_with_hallucination_risk\u001b[0m - TypeError: argument of type 'coroutine' is not iterable\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_fallback_with_rate_limit\u001b[0m - AttributeError: 'coroutine' object has no attribute 'lower'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_fallback_considers_retry_count\u001b[0m - TypeError: object of type 'coroutine' has no len()\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_fallback_includes_diagnostic_tips\u001b[0m - TypeError: argument of type 'coroutine' is not iterable\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_generate_fallback_with_previous_responses\u001b[0m - TypeError: argument of type 'coroutine' is not iterable\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_format_response_with_placeholders\u001b[0m - AttributeError: 'FallbackResponseService' object has no attribute '_format_response'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_get_recovery_suggestions\u001b[0m - TypeError: FallbackResponseService._get_recovery_suggestions() takes 2 positional arguments but 3 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_fallback_response_service.py::\u001b[1mTestFallbackResponseService::test_fallback_response_quality\u001b[0m - TypeError: object of type 'coroutine' has no len()\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_generation_service.py::\u001b[1mtest_update_job_status\u001b[0m - TypeError: 'str' object does not support item assignment\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_generation_service.py::\u001b[1mtest_save_corpus_to_clickhouse\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for ContentCorpus\ncreated_at\n  Field required [type=missing, input_value={'workload_type': 'test_t...432f-a9c7-36a256b5d458'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_llm_cache_service.py::\u001b[1mtest_llm_cache_service_initialization\u001b[0m - TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_llm_cache_service.py::\u001b[1mtest_cache_set_and_get\u001b[0m - TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_llm_cache_service.py::\u001b[1mtest_cache_expiration\u001b[0m - TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_llm_cache_service.py::\u001b[1mtest_cache_size_limit\u001b[0m - TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_llm_cache_service.py::\u001b[1mtest_cache_stats\u001b[0m - TypeError: LLMCacheService.__init__() got an unexpected keyword argument 'ttl'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_message_handlers.py::\u001b[1mtest_handle_start_agent\u001b[0m - assert ('supervisor' in \"object mock can't be used in 'await' expression\" or 'agent' in \"object mock can't be used in 'await' expression\")\n +  where \"object mock can't be used in 'await' expression\" = <built-in method lower of str object at 0x000001AE4EE0EBB0>()\n +    where <built-in method lower of str object at 0x000001AE4EE0EBB0> = \"object Mock can't be used in 'await' expression\".lower\n +      where \"object Mock can't be used in 'await' expression\" = str(TypeError(\"object Mock can't be used in 'await' expression\"))\n +  and   \"object mock can't be used in 'await' expression\" = <built-in method lower of str object at 0x000001AE4EE0EBB0>()\n +    where <built-in method lower of str object at 0x000001AE4EE0EBB0> = \"object Mock can't be used in 'await' expression\".lower\n +      where \"object Mock can't be used in 'await' expression\" = str(TypeError(\"object Mock can't be used in 'await' expression\"))\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_validate_high_quality_optimization_content\u001b[0m - AssertionError: assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.39999999999999997, actionability_score=0.5, quantification_score=1.0, relevance_score=0.5, completeness_score=0.2, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=77, sentence_count=12, numeric_values_count=0, specific_terms_count=0, overall_score=0.5700000000000001, quality_level=<QualityLevel.ACCEPTABLE: 'acceptable'>, issues=['Contains 3 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.']}, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_validate_data_analysis_with_metrics\u001b[0m - AssertionError: assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.6, actionability_score=0.3, quantification_score=0.9999999999999999, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=59, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.6449999999999999, quality_level=<QualityLevel.ACCEPTABLE: 'acceptable'>, issues=[], suggestions=['Include clear action steps or commands']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Provide step-by-step actionable instructions with specific commands or code.']}, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_validate_action_plan_completeness\u001b[0m - AssertionError: assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.4, actionability_score=0.45, quantification_score=0.5499999999999999, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=89, sentence_count=3, numeric_values_count=0, specific_terms_count=0, overall_score=0.41250000000000003, quality_level=<QualityLevel.POOR: 'poor'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add verification steps and success criteria']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.']}, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_validate_with_strict_mode\u001b[0m - assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.0, completeness_score=0.0, novelty_score=0.0, clarity_score=0.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=0, sentence_count=0, numeric_values_count=0, specific_terms_count=0, overall_score=0.0, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[\"Validation error: name 'metrics' is not defined\"], suggestions=[]), retry_suggested=False, retry_prompt_adjustments=None, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_validate_error_message_clarity\u001b[0m - assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.0, completeness_score=0.0, novelty_score=0.0, clarity_score=0.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=0, sentence_count=0, numeric_values_count=0, specific_terms_count=0, overall_score=0.0, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[\"Validation error: name 'metrics' is not defined\"], suggestions=[]), retry_suggested=False, retry_prompt_adjustments=None, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_validate_report_redundancy\u001b[0m - AssertionError: assert 0.4 > 0.5\n +  where 0.4 = QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.6, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=1, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.4, word_count=53, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.35, quality_level=<QualityLevel.POOR: 'poor'>, issues=['Contains 1 generic phrases', 'Contains 5 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Reduce redundant information']).redundancy_ratio\n +    where QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.6, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=1, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.4, word_count=53, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.35, quality_level=<QualityLevel.POOR: 'poor'>, issues=['Contains 1 generic phrases', 'Contains 5 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Reduce redundant information']) = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.6, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=1, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.4, word_count=53, sentence_count=6, numeric_values_count=0, specific_terms_count=0, overall_score=0.35, quality_level=<QualityLevel.POOR: 'poor'>, issues=['Contains 1 generic phrases', 'Contains 5 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Reduce redundant information']), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.', 'Include numerical values for all claims. Show before/after metrics with percentages.']}, fallback_response=None).metrics\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_domain_specific_term_recognition\u001b[0m - AssertionError: assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.6, actionability_score=0.55, quantification_score=0.9, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=44, sentence_count=7, numeric_values_count=0, specific_terms_count=0, overall_score=0.5925, quality_level=<QualityLevel.ACCEPTABLE: 'acceptable'>, issues=[], suggestions=[]), retry_suggested=True, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': []}, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_caching_mechanism\u001b[0m - AssertionError: assert 'quality:general:c1402699b07df03b830d11ee43df782f' in {}\n +  where {} = <app.services.quality_gate_service.QualityGateService object at 0x000001AE4ECB8E30>.validation_cache\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_retry_suggestions_for_failed_validation\u001b[0m - AssertionError: assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=9, sentence_count=2, numeric_values_count=0, specific_terms_count=0, overall_score=0.125, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=['Contains 2 vague optimization terms without specifics'], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes', 'Include before/after performance metrics']), retry_suggested=False, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.', 'Include numerical values for all claims. Show before/after metrics with percentages.']}, fallback_response=None).retry_suggested\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_hallucination_risk_detection\u001b[0m - AssertionError: assert 0.2 > 0.5\n +  where 0.2 = QualityMetrics(specificity_score=0.05, actionability_score=0.0, quantification_score=0.15, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=41, sentence_count=4, numeric_values_count=0, specific_terms_count=0, overall_score=0.28, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes']).hallucination_risk\n +    where QualityMetrics(specificity_score=0.05, actionability_score=0.0, quantification_score=0.15, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=41, sentence_count=4, numeric_values_count=0, specific_terms_count=0, overall_score=0.28, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes']) = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.05, actionability_score=0.0, quantification_score=0.15, relevance_score=0.5, completeness_score=0.0, novelty_score=0.5, clarity_score=1.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.2, redundancy_ratio=0.0, word_count=41, sentence_count=4, numeric_values_count=0, specific_terms_count=0, overall_score=0.28, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[], suggestions=['Add specific metrics, parameters, or configuration values', 'Include clear action steps or commands', 'Add numerical values and measurable outcomes']), retry_suggested=False, retry_prompt_adjustments={'temperature': 0.3, 'additional_instructions': ['Be extremely specific. Include exact parameter values, configuration settings, and metrics.', 'Provide step-by-step actionable instructions with specific commands or code.', 'Include numerical values for all claims. Show before/after metrics with percentages.']}, fallback_response=None).metrics\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_triage_content_validation\u001b[0m - assert False is True\n +  where False = ValidationResult(passed=False, metrics=QualityMetrics(specificity_score=0.0, actionability_score=0.0, quantification_score=0.0, relevance_score=0.0, completeness_score=0.0, novelty_score=0.0, clarity_score=0.0, generic_phrase_count=0, circular_reasoning_detected=False, hallucination_risk=0.0, redundancy_ratio=0.0, word_count=0, sentence_count=0, numeric_values_count=0, specific_terms_count=0, overall_score=0.0, quality_level=<QualityLevel.UNACCEPTABLE: 'unacceptable'>, issues=[\"Validation error: name 'metrics' is not defined\"], suggestions=[]), retry_suggested=False, retry_prompt_adjustments=None, fallback_response=None).passed\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_gate_service.py::\u001b[1mTestQualityGateService::test_quality_level_classification\u001b[0m - AttributeError: 'QualityGateService' object has no attribute '_classify_quality_level'. Did you mean: '_determine_quality_level'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityMonitoringServiceInitialization::test_initialization_with_defaults\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'metrics_store'. Did you mean: 'metrics_buffer'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityMonitoringServiceInitialization::test_initialization_with_custom_config\u001b[0m - TypeError: QualityMonitoringService.__init__() got an unexpected keyword argument 'config'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityMonitoringServiceInitialization::test_initialization_with_metrics_collector\u001b[0m - AttributeError: <module 'app.services.quality_monitoring_service' from 'C:\\\\Users\\\\antho\\\\OneDrive\\\\Desktop\\\\Netra\\\\netra-core-generation-1\\\\app\\\\services\\\\quality_monitoring_service.py'> does not have the attribute 'MetricsCollector'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsCollection::test_collect_response_quality_metrics\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'collect_response_metrics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsCollection::test_collect_system_metrics\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'collect_system_metrics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsCollection::test_collect_error_metrics\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'collect_error_metrics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsCollection::test_batch_metrics_collection\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'collect_batch_metrics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityThresholds::test_set_quality_thresholds\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'set_thresholds'. Did you mean: '_check_thresholds'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityThresholds::test_validate_against_thresholds\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'set_thresholds'. Did you mean: '_check_thresholds'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityThresholds::test_dynamic_threshold_adjustment\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'calculate_dynamic_threshold'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestAlerting::test_trigger_alert_on_threshold_breach\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'set_thresholds'. Did you mean: '_check_thresholds'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestAlerting::test_alert_rate_limiting\u001b[0m - AttributeError: <app.services.quality_monitoring_service.QualityMonitoringService object at 0x000001AE4EC28560> does not have the attribute 'send_alert'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestAlerting::test_alert_escalation\u001b[0m - AttributeError: <app.services.quality_monitoring_service.QualityMonitoringService object at 0x000001AE4EC0F8F0> does not have the attribute 'escalate_alert'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsAggregation::test_aggregate_metrics_by_time_window\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'store_metric'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsAggregation::test_calculate_statistics\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'calculate_statistics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestMetricsAggregation::test_trend_analysis\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'analyze_trend'. Did you mean: '_analyze_trends'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityReporting::test_generate_quality_report\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'store_metric'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityReporting::test_export_metrics_to_json\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'store_metric'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestQualityReporting::test_generate_sla_compliance_report\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'check_sla_compliance'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestAnomalyDetection::test_detect_anomalies_zscore\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'detect_anomalies'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestAnomalyDetection::test_detect_anomalies_iqr\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'detect_anomalies'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestAnomalyDetection::test_real_time_anomaly_detection\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'configure_anomaly_detector'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestPerformanceMonitoring::test_monitor_response_times\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'record_response_time'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestPerformanceMonitoring::test_monitor_throughput\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'record_request'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestIntegration::test_integration_with_agent_service\u001b[0m - AttributeError: 'QualityMonitoringService' object has no attribute 'collect_agent_metrics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_quality_monitoring_service.py::\u001b[1mTestIntegration::test_integration_with_database\u001b[0m - AttributeError: <module 'app.services.database' (namespace) from ['C:\\\\Users\\\\antho\\\\OneDrive\\\\Desktop\\\\Netra\\\\netra-core-generation-1\\\\app\\\\services\\\\database']> does not have the attribute 'metrics_repository'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_schema_validation_service.py::\u001b[1mTestSchemaValidationService::test_validate_schema\u001b[0m - AttributeError: __aenter__\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_security_service.py::\u001b[1mtest_encrypt_and_decrypt\u001b[0m - AttributeError: 'SecurityService' object has no attribute 'encrypt'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_state_persistence.py::\u001b[1mTestStatePersistence::test_save_agent_state\u001b[0m - TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_state_persistence.py::\u001b[1mTestStatePersistence::test_restore_agent_state\u001b[0m - TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_state_persistence.py::\u001b[1mTestStatePersistence::test_cleanup_old_states\u001b[0m - TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_state_persistence.py::\u001b[1mTestStatePersistence::test_state_versioning\u001b[0m - TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_state_persistence.py::\u001b[1mTestStatePersistence::test_concurrent_state_updates\u001b[0m - TypeError: StatePersistenceService.__init__() takes 1 positional argument but 2 were given\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_supply_catalog_service.py::\u001b[1mTestSupplyCatalogService::test_get_all_options\u001b[0m - AssertionError: assert <MagicMock name='gpt-4.name' id='1848101313744'> == 'gpt-4'\n +  where <MagicMock name='gpt-4.name' id='1848101313744'> = <MagicMock name='gpt-4' id='1848116206464'>.name\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_supply_catalog_service.py::\u001b[1mTestSupplyCatalogService::test_get_option_by_id\u001b[0m - AssertionError: assert <MagicMock name='gpt-4.name' id='1848115301840'> == 'gpt-4'\n +  where <MagicMock name='gpt-4.name' id='1848115301840'> = <MagicMock name='gpt-4' id='1848110259360'>.name\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_supply_catalog_service.py::\u001b[1mTestSupplyCatalogService::test_get_option_by_name\u001b[0m - AssertionError: assert <MagicMock name='gpt-4.name' id='1848138895232'> == 'gpt-4'\n +  where <MagicMock name='gpt-4.name' id='1848138895232'> = <MagicMock name='gpt-4' id='1848134639856'>.name\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_creation_with_clickhouse_table\u001b[0m - TypeError: 'domain' is an invalid keyword argument for Corpus\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_status_transitions\u001b[0m - AttributeError: 'CorpusService' object has no attribute 'is_valid_transition'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_content_upload_batch\u001b[0m - AttributeError: 'CorpusService' object has no attribute 'upload_corpus_content'. Did you mean: '_copy_corpus_content'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_validation\u001b[0m - AttributeError: 'CorpusService' object has no attribute 'validate_corpus_record'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_availability_check\u001b[0m - AttributeError: 'CorpusService' object has no attribute 'check_corpus_availability'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_fallback_to_default\u001b[0m - AttributeError: <module 'app.services.corpus_service' from 'C:\\\\Users\\\\antho\\\\OneDrive\\\\Desktop\\\\Netra\\\\netra-core-generation-1\\\\app\\\\services\\\\corpus_service.py'> does not have the attribute 'get_default_corpus'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_caching_mechanism\u001b[0m - AttributeError: <app.services.corpus_service.CorpusService object at 0x000001AE4EE32870> does not have the attribute '_fetch_corpus_content'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_deletion_cascade\u001b[0m - AssertionError: Expected 'execute' to have been called.\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_metadata_tracking\u001b[0m - AttributeError: 'CorpusService' object has no attribute 'create_corpus_metadata'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestCorpusManagement::test_corpus_concurrent_access\u001b[0m - assert False\n +  where False = all(<generator object TestCorpusManagement.test_corpus_concurrent_access.<locals>.<genexpr> at 0x000001AE4EBA2880>)\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_workload_distribution_generation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_temporal_pattern_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_temporal_patterns'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_tool_invocation_patterns\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_tool_invocations'. Did you mean: '_generate_tool_invocations'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_error_scenario_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_errors'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_trace_hierarchy_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_trace_hierarchies'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_domain_specific_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_domain_specific'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_statistical_distribution_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_distribution'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_custom_tool_catalog_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_custom_tools'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_incremental_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_incremental'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataGenerationEngine::test_generation_with_corpus_sampling\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_from_corpus'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_batch_ingestion_to_clickhouse\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'ingest_batch'. Did you mean: '_ingest_batch'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_streaming_ingestion_with_backpressure\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'ingest_stream'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_ingestion_error_recovery\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'ingest_with_retry'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_ingestion_deduplication\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'ingest_with_deduplication'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_table_creation_on_demand\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'ensure_table_exists'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_ingestion_metrics_tracking\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'track_ingestion'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_parallel_batch_ingestion\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'ingest_batch'. Did you mean: '_ingest_batch'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_ingestion_with_transformation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'ingest_with_transform'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_ingestion_circuit_breaker\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'get_circuit_breaker'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestRealTimeIngestion::test_ingestion_progress_tracking\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'ingest_with_progress'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_websocket_connection_management\u001b[0m - AssertionError: assert 'aa99b8ff-9f29-43e4-8028-ac3d55bcd949' in {<AsyncMock id='1848157113664'>: [ConnectionInfo(websocket='aa99b8ff-9f29-43e4-8028-ac3d55bcd949', user_id=<AsyncMock id='1848157113664'>, connected_at=datetime.datetime(2025, 8, 11, 12, 49, 53, 491824, tzinfo=datetime.timezone.utc), last_ping=datetime.datetime(2025, 8, 11, 12, 49, 53, 491824, tzinfo=datetime.timezone.utc), last_pong=None, message_count=0, error_count=1, connection_id='conn_1754916593491')]}\n +  where {<AsyncMock id='1848157113664'>: [ConnectionInfo(websocket='aa99b8ff-9f29-43e4-8028-ac3d55bcd949', user_id=<AsyncMock id='1848157113664'>, connected_at=datetime.datetime(2025, 8, 11, 12, 49, 53, 491824, tzinfo=datetime.timezone.utc), last_ping=datetime.datetime(2025, 8, 11, 12, 49, 53, 491824, tzinfo=datetime.timezone.utc), last_pong=None, message_count=0, error_count=1, connection_id='conn_1754916593491')]} = <app.ws_manager.WebSocketManager object at 0x000001AE1EA18B30>.active_connections\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_generation_progress_broadcast\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'broadcast_to_job'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_batch_completion_notifications\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'notify_batch_complete'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_error_notification_handling\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'notify_error'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_websocket_reconnection_handling\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'set_job_state'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_multiple_client_subscriptions\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'broadcast_to_job'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_websocket_message_queuing\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'broadcast_to_job'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_websocket_heartbeat\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'start_heartbeat'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_generation_completion_notification\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'notify_completion'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestWebSocketUpdates::test_websocket_rate_limiting\u001b[0m - AttributeError: 'WebSocketManager' object has no attribute 'set_rate_limit'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_schema_validation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'validate_schema'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_statistical_distribution_validation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_referential_integrity_validation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_trace_hierarchies'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_temporal_consistency_validation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_data_completeness_validation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_anomaly_detection_in_generated_data\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_anomalies'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_correlation_preservation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_quality_metrics_calculation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_data_diversity_validation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestDataQualityValidation::test_validation_report_generation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_high_throughput_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_parallel'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_memory_efficient_streaming\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_stream'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_horizontal_scaling\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_parallel'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_batch_size_optimization\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_batched'. Did you mean: '_generate_batches'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_connection_pooling_efficiency\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'test_connection_pool'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_cache_effectiveness\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'get_corpus_cached'. Did you mean: 'corpus_cache'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_auto_scaling_behavior\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_auto_scaling'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_resource_limit_handling\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_limits'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_query_optimization\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'benchmark_query'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestPerformanceScalability::test_burst_load_handling\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_pattern'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_corpus_unavailable_fallback\u001b[0m - AttributeError: <app.services.synthetic_data_service.SyntheticDataService object at 0x000001AE4EC1DD00> does not have the attribute 'get_corpus_content'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_clickhouse_connection_recovery\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'ingest_with_retry'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_generation_checkpoint_recovery\u001b[0m - AttributeError: <app.services.synthetic_data_service.SyntheticDataService object at 0x000001AE4EC986E0> does not have the attribute 'generate_batch'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_websocket_disconnect_recovery\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_ws_updates'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_memory_overflow_handling\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_memory_limit'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_circuit_breaker_operation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'get_circuit_breaker'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_dead_letter_queue_processing\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'process_with_dlq'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_transaction_rollback\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'query_records'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_idempotent_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_idempotent'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestErrorRecovery::test_graceful_degradation\u001b[0m - AttributeError: <app.services.synthetic_data_service.SyntheticDataService object at 0x000001AE4EC9B0B0> does not have the attribute 'enable_clustering'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_generation_job_monitoring\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_monitored'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_detailed_metrics_dashboard\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'get_generation_metrics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_corpus_usage_analytics\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'get_corpus_analytics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_audit_log_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_audit'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_performance_profiling\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'profile_generation'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_alert_configuration\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'configure_alerts'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_job_cancellation_by_admin\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() got an unexpected keyword argument 'job_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_resource_usage_tracking\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'start_resource_tracking'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_admin_diagnostic_tools\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'run_diagnostics'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdminVisibility::test_batch_job_management\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'schedule_generation'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_complete_generation_workflow\u001b[0m - TypeError: CorpusService.create_corpus() missing 1 required positional argument: 'corpus_data'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_multi_tenant_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_for_tenant'. Did you mean: '_generate_content'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_real_time_streaming_pipeline\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_streaming'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_failure_recovery_integration\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_recovery'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_cross_component_validation\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_performance_under_load\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() missing 2 required positional arguments: 'config' and 'user_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_data_consistency_verification\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() got an unexpected keyword argument 'job_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_monitoring_integration\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'enable_monitoring'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_security_and_access_control\u001b[0m - TypeError: CorpusService.create_corpus() missing 1 required positional argument: 'corpus_data'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestIntegration::test_cleanup_and_retention\u001b[0m - TypeError: SyntheticDataService.generate_synthetic_data() got an unexpected keyword argument 'job_id'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_ml_driven_pattern_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'load_production_patterns'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_anomaly_injection_strategies\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_anomalies'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_cross_correlation_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_with_correlations'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_temporal_event_sequences\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_event_sequences'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_geo_distributed_simulation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_geo_distributed'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_adaptive_generation_feedback\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_adaptive'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_multi_model_workload_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_multi_model'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_compliance_aware_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_compliant'. Did you mean: '_generate_content'?\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_cost_optimized_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'generate_cost_optimized'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_service_v3.py::\u001b[1mTestAdvancedFeatures::test_versioned_corpus_generation\u001b[0m - AttributeError: 'SyntheticDataService' object has no attribute 'create_corpus_version'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_validation.py::\u001b[1mTestImplementationConsistency::test_synthetic_data_service_has_required_methods\u001b[0m - AssertionError: assert False\n +  where False = hasattr(<app.services.synthetic_data_service.SyntheticDataService object at 0x000001AE4C562AB0>, 'generate_batch')\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_synthetic_data_validation.py::\u001b[1mTestImplementationConsistency::test_agent_tools_available\u001b[0m - ModuleNotFoundError: No module named 'app.agents.admin_tool_dispatcher'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_thread_service.py::\u001b[1mTestThreadService::test_create_thread\u001b[0m - TypeError: ThreadService() takes no arguments\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_thread_service.py::\u001b[1mTestThreadService::test_get_thread_history\u001b[0m - TypeError: ThreadService() takes no arguments\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_thread_service.py::\u001b[1mTestThreadService::test_update_thread_metadata\u001b[0m - TypeError: ThreadService() takes no arguments\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_thread_service.py::\u001b[1mTestThreadService::test_delete_thread\u001b[0m - TypeError: ThreadService() takes no arguments\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_thread_service.py::\u001b[1mTestThreadService::test_thread_pagination\u001b[0m - TypeError: ThreadService() takes no arguments\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_thread_service.py::\u001b[1mTestThreadService::test_thread_search\u001b[0m - TypeError: ThreadService() takes no arguments\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_user_service.py::\u001b[1mTestUserService::test_get_user_by_email\u001b[0m - AttributeError: 'User' object has no attribute 'username'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_user_service.py::\u001b[1mTestUserService::test_update_user\u001b[0m - pydantic_core._pydantic_core.ValidationError: 1 validation error for UserUpdate\nemail\n  Field required [type=missing, input_value={'full_name': 'Updated Na...sername': 'updateduser'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_user_service.py::\u001b[1mTestUserService::test_delete_user\u001b[0m - AttributeError: 'coroutine' object has no attribute 'first'\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_user_service.py::\u001b[1mTestUserService::test_get_user_by_id\u001b[0m - AssertionError: assert <Mock name='mock.execute().scalars().first().id' id='1848134676896'> == 'bd186e0b-4604-4e8f-8830-ee652ef3549c'\n +  where <Mock name='mock.execute().scalars().first().id' id='1848134676896'> = <Mock name='mock.execute().scalars().first()' id='1848134669264'>.id\n +  and   'bd186e0b-4604-4e8f-8830-ee652ef3549c' = <app.db.models_postgres.User object at 0x000001AE4D691D00>.id\n\u001b[31mFAILED\u001b[0m app\\tests\\services\\test_user_service.py::\u001b[1mTestUserService::test_get_multiple_users\u001b[0m - TypeError: 'username' is an invalid keyword argument for User\n\u001b[31mFAILED\u001b[0m app\\tests\\core\\test_config_manager.py::\u001b[1mTestConfigValidator::test_validate_auth_config_production_dev_secret\u001b[0m - AssertionError: assert 'Development JWT secret key cannot be used in production' in 'Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured'\n +  where 'Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured' = str(ConfigurationValidationError('Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured'))\n +    where ConfigurationValidationError('Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured') = <ExceptionInfo ConfigurationValidationError('Authentication configuration errors: JWT secret key appears to be a development/test key - not suitable for production, OAuth client ID is not configured, OAuth client secret is not configured') tblen=3>.value\n\u001b[31mERROR\u001b[0m app\\tests\\services\\agents\\test_supervisor_service.py::\u001b[1mtest_supervisor_end_to_end\u001b[0m - TypeError: SupervisorAgent.__init__() missing 1 required positional argument: 'tool_dispatcher'\n\u001b[31m================== \u001b[31m\u001b[1m244 failed\u001b[0m, \u001b[32m274 passed\u001b[0m, \u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 28.44s\u001b[0m\u001b[31m ==================\u001b[0m\n================================================================================\n[FAIL] TESTS FAILED with exit code 1 after 34.62s\n================================================================================\n\nC:\\Users\\antho\\miniconda3\\Lib\\pathlib.py:404: RuntimeWarning: coroutine 'FallbackResponseService.generate_fallback' was never awaited\n  parsed = [sys.intern(str(x)) for x in rel.split(sep) if x and x != '.']\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\nC:\\Users\\antho\\AppData\\Roaming\\Python\\Python312\\site-packages\\_pytest\\config\\compat.py:49: RuntimeWarning: coroutine 'FallbackResponseService.generate_fallback' was never awaited\n  def __getattr__(self, key: str) -> pluggy.HookCaller:\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
    },
    "frontend": {
      "status": "failed",
      "duration": 0.12996125221252441,
      "exit_code": 2,
      "output": "\nusage: test_frontend.py [-h]\n                        [--category {unit,integration,components,hooks,store,websocket,auth,e2e,smoke}]\n                        [--keyword KEYWORD] [--e2e] [--cypress-open] [--watch]\n                        [--coverage] [--update-snapshots] [--lint] [--fix]\n                        [--type-check] [--build] [--check-deps]\n                        [--install-deps] [--verbose] [--isolation]\n                        [tests ...]\ntest_frontend.py: error: unrecognized arguments: --testPathPattern=unit\n"
    },
    "overall": {
      "status": "failed",
      "start_time": 1754916575.5986605,
      "end_time": 1754916612.5463789
    }
  },
  "summary": {
    "total_duration": 36.926122188568115,
    "overall_passed": false,
    "exit_code": 2
  }
}