[pytest]
markers =
    performance: Performance and SLA validation tests
    slow: Slow tests that may take longer to complete  
    unit: Unit tests for isolated components
    integration: Integration tests for component interaction
    smoke: Quick smoke tests for basic functionality
    critical: Critical path tests that protect revenue
    golden_path: Golden path user flow tests
    mission_critical: Mission critical functionality tests
    real_services: Tests requiring real external services
    mock_only: Tests using only mocks
    agents: Agent-specific tests
    real_e2e: End-to-end tests with real services
    real_llm: Tests that use real LLM services
    e2e: End-to-end tests
    websocket: WebSocket-related tests
    database: Database-related tests
    auth: Authentication-related tests
    oauth: OAuth flow tests
    security: Security validation tests  
    api: API endpoint tests
    frontend: Frontend component tests
    rate_limiting: Rate limiting and DDoS protection tests
    stress: Stress tests with high load or concurrency
    spike_testing: Spike testing and recovery validation tests
    benchmark: Performance benchmark tests
    isolation: Isolation and multi-tenancy tests
    cpu_isolation: CPU isolation tests
    filesystem_isolation: File system isolation tests
    memory_isolation: Memory isolation tests
    multi_tenant_isolation: Multi-tenant isolation tests
    network_isolation: Network isolation tests
    resource_isolation: Resource isolation tests
    soak: Long-duration soak testing for stability validation
    resilience: Resilience and recovery validation tests
    redis: Redis-dependent tests requiring real Redis connection
    staging: Staging environment specific tests
    dev: Development environment specific tests
    comprehensive: Comprehensive system-wide integration tests
    coverage_validation: Code coverage validation tests
    real_data: Tests requiring real data generation and processing
    real_database: Tests requiring real database connections
    real_quality: Tests requiring real quality validation services
    flaky: Tests that may fail intermittently
    bad_test: Tests marked as consistently failing
    l2_integration: Level 2 integration tests
    l3_integration: Level 3 integration tests
    l4_integration: Level 4 integration tests
    l3: Level 3 tests
    L3: Level 3 tests (uppercase)
    l2: Level 2 tests
    ssot: Single Source of Truth compliance tests
    database_ssot: Database Manager SSOT compliance tests
    auth_ssot: Authentication SSOT compliance tests
    websocket_ssot: WebSocket Manager SSOT compliance tests
    level_4: Level 4 tests
    timeout: Tests with timeout configuration
    observability: Observability and monitoring tests
    alerting: Alert and notification tests
    custom_metrics: Custom metrics tests
    dashboard: Dashboard functionality tests
    tracing: Distributed tracing tests
    initialization: System initialization tests
    health_monitoring: Health check tests
    disaster_recovery: Disaster recovery tests
    enterprise: Enterprise-specific tests
    critical_path: Critical path validation tests
    production: Production environment tests requiring real infrastructure
    sla: Service Level Agreement validation tests
    skip_if_no_real_llm: Skip tests when real LLM is not available
    expected_failure: Tests that are expected to fail initially (for bug reproduction)
    reproduction: Tests that reproduce specific bugs or issues

testpaths = 
    tests
    netra_backend/tests
    integration_tests

python_files = test_*.py
python_classes = Test*
python_functions = test_*

addopts = 
    -ra
    --strict-markers
    --disable-warnings
    --tb=short
    --asyncio-mode=auto
    --timeout=120
    -m "not staging"

asyncio_default_fixture_loop_scope = function

filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::UserWarning