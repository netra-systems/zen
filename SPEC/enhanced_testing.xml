<?xml version='1.0' encoding='utf-8'?>
<specification>
  <metadata>
    <name>Enhanced Testing and Reporting Specification</name>
    <type>testing</type>
    <version>4.0</version>
    <last_updated>2025-08-12</last_updated>
    <description>
            Comprehensive specification for unified test reporting, intelligent metrics,
            and modern testing practices for the Netra AI Platform.
        </description>
    <last_edited>2025-08-21T08:47:28.447554</last_edited>
    <legacy_status is_legacy="true" identified_date="2025-08-21T08:47:28.447554">
      <reasons>
        <reason>Content contains: REPLACED BY</reason>
        <reason>Content contains: archived</reason>
        <reason>Content contains: old</reason>
      </reasons>
    </legacy_status>
  </metadata>
  <overview>
    <goals>
      <goal>Unified test reporting with comprehensive metrics</goal>
      <goal>Intelligent test categorization and analysis</goal>
      <goal>Historical tracking and trend analysis</goal>
      <goal>Machine and human-readable reports</goal>
      <goal>Automated cleanup and organization</goal>
      <goal>97% overall test coverage target</goal>
    </goals>
  </overview>
  <enhanced_test_reporter>
    <description>
            Advanced test reporting system providing comprehensive metrics,
            intelligent analysis, and organized report management.
        </description>
    <location>scripts/enhanced_test_reporter.py</location>
    <features>
      <feature name="comprehensive_metrics">
        <description>Track detailed test metrics beyond basic pass/fail</description>
        <metrics>
          <metric>Total test files count</metric>
          <metric>Test execution duration per test</metric>
          <metric>Test speed percentiles (P50, P95, P99)</metric>
          <metric>Category-wise breakdown</metric>
          <metric>Coverage per component</metric>
          <metric>Flaky test detection</metric>
          <metric>Performance trends</metric>
        </metrics>
      </feature>
      <feature name="intelligent_categorization">
        <description>Automatically categorize tests based on path and content</description>
        <categories>
          <category>Unit Tests</category>
          <category>Integration Tests</category>
          <category>End-to-End Tests</category>
          <category>Smoke Tests</category>
          <category>Performance Tests</category>
          <category>Security Tests</category>
          <category>API Tests</category>
          <category>UI Tests</category>
          <category>Database Tests</category>
          <category>WebSocket Tests</category>
          <category>Authentication Tests</category>
          <category>Agent Tests</category>
          <category>LLM Tests</category>
        </categories>
      </feature>
      <feature name="change_detection">
        <description>Track changes between test runs</description>
        <tracking>
          <change>New failures since last run</change>
          <change>Fixed tests since last run</change>
          <change>Test count changes</change>
          <change>Coverage changes</change>
          <change>Performance changes</change>
          <change>Flaky test patterns</change>
        </tracking>
      </feature>
      <feature name="visual_reporting">
        <description>Enhanced markdown reports with visual elements</description>
        <elements>
          <element>Progress bars for test status</element>
          <element>Trend icons for metrics</element>
          <element>Color-coded status badges</element>
          <element>Category breakdown tables</element>
          <element>Performance distribution charts</element>
          <element>Historical trend graphs</element>
        </elements>
      </feature>
    </features>
    <report_structure>
      <section order="1" name="executive_summary">
        <content>High-level metrics and overall status</content>
        <includes>
          <item>Total test files and tests</item>
          <item>Pass rate with trend</item>
          <item>Coverage with trend</item>
          <item>Duration with trend</item>
        </includes>
      </section>
      <section order="2" name="test_results">
        <content>Detailed test results breakdown</content>
        <includes>
          <item>Summary statistics with visual bars</item>
          <item>Component breakdown table</item>
          <item>Category distribution</item>
        </includes>
      </section>
      <section order="3" name="change_detection">
        <content>Changes since last run</content>
        <includes>
          <item>New failures list</item>
          <item>Fixed tests list</item>
          <item>Flaky tests identified</item>
        </includes>
      </section>
      <section order="4" name="performance_analysis">
        <content>Test performance metrics</content>
        <includes>
          <item>Speed distribution (percentiles)</item>
          <item>Top 5 slowest tests</item>
          <item>Performance recommendations</item>
        </includes>
      </section>
      <section order="5" name="failure_analysis">
        <content>Detailed failure breakdown</content>
        <includes>
          <item>Failures by category</item>
          <item>Common failure patterns</item>
          <item>Priority recommendations</item>
        </includes>
      </section>
      <section order="6" name="historical_trends">
        <content>Historical data and trends</content>
        <includes>
          <item>Last 5 runs comparison</item>
          <item>Coverage trend</item>
          <item>Performance trend</item>
        </includes>
      </section>
      <section order="7" name="recommendations">
        <content>Actionable improvement recommendations</content>
        <includes>
          <item>Coverage improvements needed</item>
          <item>Performance optimizations</item>
          <item>Flaky test fixes</item>
          <item>Critical failures to address</item>
        </includes>
      </section>
    </report_structure>
  </enhanced_test_reporter>
  <report_organization>
    <description>
            Organized directory structure for test reports and artifacts
        </description>
    <directory_structure>
      <directory name="test_reports" purpose="Root directory for all test artifacts">
        <subdirectory name="latest" purpose="Current test reports (one per level)">
          <file>smoke_report.md</file>
          <file>unit_report.md</file>
          <file>integration_report.md</file>
          <file>comprehensive_report.md</file>
          <file>critical_report.md</file>
        </subdirectory>
        <subdirectory name="history" purpose="Archived historical reports">
          <naming_convention>{level}_report_{YYYYMMDD}_{HHMMSS}.md</naming_convention>
          <retention_policy>7 days default, configurable</retention_policy>
        </subdirectory>
        <subdirectory name="metrics" purpose="JSON metrics and statistics">
          <file>test_history.json - Historical run data</file>
          <file>{level}_metrics.json - Per-level metrics</file>
          <file>coverage_trends.json - Coverage tracking</file>
          <file>performance_trends.json - Performance metrics</file>
        </subdirectory>
        <subdirectory name="analysis" purpose="Detailed analysis reports">
          <file>failure_analysis.json - Failure patterns</file>
          <file>flaky_tests.json - Flaky test tracking</file>
          <file>slow_tests.json - Performance bottlenecks</file>
          <file>coverage_gaps.json - Coverage analysis</file>
        </subdirectory>
      </directory>
    </directory_structure>
    <cleanup_policy>
      <description>Automated cleanup and organization</description>
      <script>scripts/cleanup_test_reports.py</script>
      <rules>
        <rule>Delete JSON report files (replaced by metrics/)</rule>
        <rule>Archive MD reports older than retention period</rule>
        <rule>Move misplaced files to correct directories</rule>
        <rule>Remove temporary test artifacts</rule>
        <rule>Clean __pycache__ and .pytest_cache</rule>
      </rules>
      <schedule>Run after every 10th test execution</schedule>
    </cleanup_policy>
  </report_organization>
  <test_runner_enhancements>
    <description>
            Improvements to test_runner.py for better reporting
        </description>
    <integration>
      <feature>Automatic enhanced reporter detection</feature>
      <feature>Fallback to standard reporting if enhanced unavailable</feature>
      <feature>Periodic cleanup execution (10% chance)</feature>
      <feature>Test file counting for all components</feature>
    </integration>
    <metrics_collection>
      <metric>Test file counts per component</metric>
      <metric>Individual test timing (when available)</metric>
      <metric>Category distribution</metric>
      <metric>Failure pattern detection</metric>
      <metric>Coverage per module</metric>
    </metrics_collection>
  </test_runner_enhancements>
  <machine_readable_formats>
    <description>
            Structured data formats for CI/CD integration
        </description>
    <json_metrics>
      <location>test_reports/metrics/{level}_metrics.json</location>
      <schema>
                {
                    "timestamp": "ISO-8601",
                    "level": "string",
                    "metrics": {
                        "total_files": "integer",
                        "total_tests": "integer",
                        "passed": "integer",
                        "failed": "integer",
                        "skipped": "integer",
                        "errors": "integer",
                        "coverage": "float|null",
                        "duration": "float"
                    },
                    "categories": {
                        "[category]": {
                            "count": "integer",
                            "passed": "integer",
                            "failed": "integer"
                        }
                    },
                    "failures": ["array of test names"],
                    "slow_tests": ["array of test objects"]
                }
            </schema>
    </json_metrics>
    <historical_data>
      <location>test_reports/metrics/test_history.json</location>
      <content>
        <item>Run history with timestamps</item>
        <item>Flaky test tracking</item>
        <item>Failure patterns</item>
        <item>Performance trends</item>
      </content>
      <retention>Last 100 runs</retention>
    </historical_data>
  </machine_readable_formats>
  <best_practices>
    <practice category="reporting">
      <name>Single Source of Truth</name>
      <description>Latest reports always in latest/ directory</description>
      <rationale>Easy to find current status without searching</rationale>
    </practice>
    <practice category="organization">
      <name>Automatic Cleanup</name>
      <description>Regular cleanup prevents clutter</description>
      <implementation>10% chance after each run or manual execution</implementation>
    </practice>
    <practice category="metrics">
      <name>Comprehensive Tracking</name>
      <description>Track more than just pass/fail</description>
      <metrics>Duration, categories, trends, patterns</metrics>
    </practice>
    <practice category="analysis">
      <name>Historical Comparison</name>
      <description>Always compare with previous runs</description>
      <benefits>Detect regressions early, track improvements</benefits>
    </practice>
    <practice category="visualization">
      <name>Visual Elements</name>
      <description>Use visual aids in reports</description>
      <elements>Progress bars, trend icons, color coding</elements>
    </practice>
  </best_practices>
  <commands>
    <command name="run_tests_with_enhanced_reporting">
      <description>Run tests with enhanced reporting</description>
      <usage>python test_runner.py --level {level}</usage>
      <note>Enhanced reporter activates automatically if available</note>
    </command>
    <command name="cleanup_test_reports">
      <description>Clean up and organize test reports</description>
      <usage>python scripts/cleanup_test_reports.py --keep-days 7</usage>
      <options>
        <option>--dry-run: Preview changes without executing</option>
        <option>--keep-days N: Number of days to keep history</option>
      </options>
    </command>
    <command name="view_test_metrics">
      <description>View test metrics and trends</description>
      <usage>cat test_reports/metrics/test_history.json | python -m json.tool</usage>
    </command>
    <command name="generate_custom_report">
      <description>Generate custom test report</description>
      <usage>python scripts/enhanced_test_reporter.py --level {level}</usage>
    </command>
  </commands>
  <troubleshooting>
    <issue>
      <problem>Unicode encoding errors in reports</problem>
      <solution>Replace emoji with ASCII indicators ([OK], [ERROR], etc.)</solution>
      <prevention>Use encoding='utf-8' for all file operations</prevention>
    </issue>
    <issue>
      <problem>Enhanced reporter not loading</problem>
      <solution>Check scripts/enhanced_test_reporter.py exists</solution>
      <fallback>Standard reporting continues to work</fallback>
    </issue>
    <issue>
      <problem>Reports directory cluttered</problem>
      <solution>Run cleanup_test_reports.py</solution>
      <automation>Automatic cleanup every 10 runs</automation>
    </issue>
    <issue>
      <problem>Missing test file counts</problem>
      <solution>Update test_runner.py to latest version</solution>
      <check>Ensure _parse_test_counts includes file counting</check>
    </issue>
  </troubleshooting>
  <cross_references>
    <reference>SPEC/testing.xml - Core testing specification</reference>
    <reference>SPEC/coverage_requirements.xml - Coverage targets</reference>
    <reference>SPEC/learnings.xml - Historical fixes and patterns</reference>
    <reference>test_runner.py - Main test execution</reference>
    <reference>scripts/enhanced_test_reporter.py - Enhanced reporting</reference>
    <reference>scripts/cleanup_test_reports.py - Report cleanup</reference>
  </cross_references>
</specification>