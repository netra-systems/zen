<?xml version="1.0" encoding="UTF-8"?>
<spec>
  <meta>
    <title>Unified Test Runner Specification</title>
    <category>testing</category>
    <created>2025-08-18</created>
    <cross_references>
      <ref>CLAUDE.md#unified-test-runner</ref>
      <ref>testing.xml</ref>
      <ref>learnings/testing.xml</ref>
    </cross_references>
  </meta>

  <test_runner>
    <overview>
      SINGLE AUTHORITATIVE TEST RUNNER - Do not create alternatives.
      The test_runner.py is the only approved method for running tests.
    </overview>

    <test_levels>
      <level name="integration" time="3-5min" default="true">
        <purpose>DEFAULT - Feature validation</purpose>
        <command>python test_runner.py --level integration --no-coverage --fast-fail</command>
        <when>After any feature change</when>
      </level>
      
      <level name="unit" time="1-2min">
        <purpose>Component testing</purpose>
        <command>python test_runner.py --level unit</command>
        <when>During development</when>
      </level>
      
      <level name="smoke" time="&lt;30s">
        <purpose>Pre-commit validation</purpose>
        <command>python test_runner.py --level smoke</command>
        <when>Before every commit</when>
      </level>
      
      <level name="agents" time="2-3min">
        <purpose>Agent testing</purpose>
        <command>python test_runner.py --level agents</command>
        <when>After agent changes</when>
      </level>
      
      <level name="critical" time="1-2min">
        <purpose>Essential paths</purpose>
        <command>python test_runner.py --level critical</command>
        <when>Quick validation</when>
      </level>
      
      <level name="real_e2e" time="15-20min" critical="true">
        <purpose>CRITICAL - Real LLM testing</purpose>
        <command>python test_runner.py --level real_e2e --real-llm</command>
        <when>Before major releases</when>
      </level>
      
      <level name="comprehensive" time="30-45min">
        <purpose>Full validation</purpose>
        <command>python test_runner.py --level comprehensive</command>
        <when>Before production deploy</when>
      </level>
    </test_levels>

    <comprehensive_categories>
      <category name="comprehensive-backend" time="10-15min">
        <purpose>Full backend validation</purpose>
        <command>python test_runner.py --level comprehensive-backend</command>
      </category>
      
      <category name="comprehensive-frontend" time="10-15min">
        <purpose>Full frontend validation</purpose>
        <command>python test_runner.py --level comprehensive-frontend</command>
      </category>
      
      <category name="comprehensive-core" time="10-15min">
        <purpose>Core components deep test</purpose>
        <command>python test_runner.py --level comprehensive-core</command>
      </category>
      
      <category name="comprehensive-agents" time="10-15min">
        <purpose>Multi-agent system validation</purpose>
        <command>python test_runner.py --level comprehensive-agents</command>
      </category>
      
      <category name="comprehensive-websocket" time="10-15min">
        <purpose>WebSocket deep validation</purpose>
        <command>python test_runner.py --level comprehensive-websocket</command>
      </category>
      
      <category name="comprehensive-database" time="10-15min">
        <purpose>Database operations validation</purpose>
        <command>python test_runner.py --level comprehensive-database</command>
      </category>
      
      <category name="comprehensive-api" time="10-15min">
        <purpose>API endpoints validation</purpose>
        <command>python test_runner.py --level comprehensive-api</command>
      </category>
    </comprehensive_categories>

    <real_llm_testing>
      <critical>Always test with real LLMs before releases to catch integration issues</critical>
      
      <commands>
        <command purpose="Integration tests with real LLM (DEFAULT for releases)">
          python test_runner.py --level integration --real-llm
        </command>
        
        <command purpose="Specific model testing">
          python test_runner.py --level integration --real-llm --llm-model gemini-2.5-flash
        </command>
        
        <command purpose="Full E2E with real services">
          python test_runner.py --level real_e2e --real-llm --llm-timeout 60
        </command>
        
        <command purpose="Agent tests with real LLM (MANDATORY for agent changes)">
          python test_runner.py --level agents --real-llm
        </command>
      </commands>
    </real_llm_testing>

    <speed_optimizations>
      <option name="CI Mode" safe="true">
        <command>python test_runner.py --level unit --ci</command>
      </option>
      
      <option name="No Warnings">
        <command>python test_runner.py --level unit --no-warnings</command>
      </option>
      
      <option name="No Coverage">
        <command>python test_runner.py --level unit --no-coverage</command>
      </option>
      
      <option name="Fast Fail">
        <command>python test_runner.py --level unit --fast-fail</command>
      </option>
      
      <option name="Speed Mode" warning="May skip slow tests">
        <command>python test_runner.py --level unit --speed</command>
      </option>
    </speed_optimizations>

    <component_selection>
      <command purpose="Backend only">python test_runner.py --level unit --backend-only</command>
      <command purpose="Frontend only">python test_runner.py --level unit --frontend-only</command>
    </component_selection>

    <test_discovery>
      <command purpose="List all tests">python test_runner.py --list</command>
      <command purpose="List with JSON format">python test_runner.py --list --list-format json</command>
      <command purpose="List with Markdown format">python test_runner.py --list --list-format markdown</command>
      <command purpose="List specific category">python test_runner.py --list --list-category unit</command>
      <command purpose="Show failing tests">python test_runner.py --show-failing</command>
      <command purpose="Run only failing tests">python test_runner.py --run-failing</command>
      <command purpose="Clear failing tests log">python test_runner.py --clear-failing</command>
    </test_discovery>

    <staging_testing>
      <command purpose="Test against staging">python test_runner.py --level integration --staging</command>
      <command purpose="Override staging URLs">
        python test_runner.py --staging --staging-url https://staging.example.com --staging-api-url https://api.staging.example.com
      </command>
    </staging_testing>

    <cicd_integration>
      <command purpose="CI mode with JSON output">
        python test_runner.py --level unit --ci --output results.json --report-format json
      </command>
      <command purpose="Generate coverage report">
        python test_runner.py --level comprehensive --coverage-output coverage.xml
      </command>
    </cicd_integration>

    <parallelism>
      <command purpose="Sequential execution">python test_runner.py --level unit --real-llm --parallel 1</command>
      <command purpose="Auto-detect parallelism">python test_runner.py --level unit --real-llm --parallel auto</command>
    </parallelism>
  </test_runner>
</spec>