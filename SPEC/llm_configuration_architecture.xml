<?xml version="1.0" encoding="UTF-8"?>
<spec>
  <name>LLM Configuration Architecture</name>
  <version>1.0.0</version>
  <status>active</status>
  <created>2025-08-26</created>
  <category>architecture</category>
  
  <overview>
    <summary>
      Centralized LLM model configuration and management system ensuring consistent model 
      usage across the entire codebase with GEMINI_2_5_FLASH as the default model.
    </summary>
    
    <business_value>
      <segment>Platform/Internal</segment>
      <goal>Cost optimization and standardization</goal>
      <impact>Reduces LLM costs by 90% through efficient model selection</impact>
      <revenue_impact>Direct cost reduction increases margins</revenue_impact>
    </business_value>
  </overview>
  
  <architecture>
    <principle name="Single Source of Truth">
      All LLM model references MUST go through the centralized configuration module.
      No hardcoded model strings are allowed anywhere in the codebase.
    </principle>
    
    <principle name="Default Model Standardization">
      GEMINI_2_5_FLASH is the default model for all environments (test, development, production).
      This provides the best cost-performance ratio.
    </principle>
    
    <principle name="Deprecation Path">
      Legacy models (GPT-4, GPT-3.5-turbo, Claude) are marked as deprecated.
      The system provides migration helpers for smooth transition.
    </principle>
  </architecture>
  
  <implementation>
    <component name="LLMModel Enum" path="netra_backend/app/llm/llm_defaults.py">
      <description>
        Central enum defining all allowed LLM models with their configurations.
        This is the ONLY place where model names should be defined.
      </description>
      <models>
        <model name="GEMINI_2_5_FLASH" status="default">
          Default for all tests and development. Most cost-effective option.
        </model>
        <model name="GEMINI_2_5_PRO" status="advanced">
          For advanced scenarios requiring higher capability.
        </model>
        <model name="GPT_4" status="deprecated">
          Legacy OpenAI model - migrate to GEMINI.
        </model>
        <model name="GPT_3_5_TURBO" status="deprecated">
          Legacy OpenAI model - migrate to GEMINI.
        </model>
        <model name="CLAUDE_3_OPUS" status="deprecated">
          Legacy Anthropic model - migrate to GEMINI.
        </model>
        <model name="MOCK" status="testing">
          Mock model for unit tests.
        </model>
      </models>
    </component>
    
    <component name="LLMConfig Class" path="netra_backend/app/llm/llm_defaults.py">
      <description>
        Centralized configuration for LLM usage including costs, performance 
        characteristics, and validation helpers.
      </description>
      <features>
        <feature>Model cost tracking (per 1k tokens)</feature>
        <feature>Performance characteristics (max_tokens, temperature, timeouts)</feature>
        <feature>Model validation and deprecation warnings</feature>
        <feature>API key environment variable mapping</feature>
      </features>
    </component>
    
    <component name="Migration Tools" path="scripts/">
      <tool name="migrate_llm_models.py">
        Automated migration script to update all hardcoded LLM references 
        to use the centralized configuration.
      </tool>
      <tool name="llm_compliance_report.py">
        Compliance validation and reporting tool to ensure all code 
        follows the centralized configuration.
      </tool>
    </component>
  </implementation>
  
  <usage_patterns>
    <pattern name="Import and Use">
      <code language="python">
from netra_backend.app.llm.llm_defaults import LLMModel, LLMConfig

# Get default model
default_model = LLMModel.get_default()
model_name = default_model.value  # "gemini-2.5-flash"

# Get model configuration
config = LLMConfig.get_model_config(default_model)
# Returns: {
#   "model_name": "gemini-2.5-flash",
#   "max_tokens": 8192,
#   "temperature": 0.0,
#   "cost_per_1k_tokens": 0.001,
#   ...
# }
      </code>
    </pattern>
    
    <pattern name="Test Configuration">
      <code language="python">
# In tests, always use the test default
test_model = LLMModel.get_test_default()  # Always returns GEMINI_2_5_FLASH
      </code>
    </pattern>
    
    <pattern name="Environment Variables">
      <code language="python">
# Get the correct API key environment variable
api_key_var = LLMConfig.get_api_key_env_var(LLMModel.GEMINI_2_5_FLASH)
# Returns: "GOOGLE_API_KEY"
      </code>
    </pattern>
  </usage_patterns>
  
  <compliance_rules>
    <rule id="no-hardcoded-models" severity="critical">
      <description>
        No hardcoded model strings (e.g., "gpt-4", "claude-3") are allowed.
        All model references must use the LLMModel enum.
      </description>
      <validation>Run: python scripts/llm_compliance_report.py</validation>
    </rule>
    
    <rule id="default-to-gemini" severity="high">
      <description>
        All new code must default to GEMINI_2_5_FLASH unless there's a 
        specific documented requirement for a different model.
      </description>
    </rule>
    
    <rule id="no-openai-key-requirement" severity="high">
      <description>
        Tests must not require OPENAI_API_KEY. Use GOOGLE_API_KEY instead.
      </description>
    </rule>
    
    <rule id="import-from-defaults" severity="medium">
      <description>
        All LLM configuration must be imported from llm_defaults.py.
        Do not create local model configurations.
      </description>
    </rule>
  </compliance_rules>
  
  <cost_analysis>
    <comparison base="gpt-4">
      <model name="GEMINI_2_5_FLASH" cost_ratio="0.033">
        30x cheaper than GPT-4, suitable for 95% of use cases.
      </model>
      <model name="GEMINI_2_5_PRO" cost_ratio="0.33">
        3x cheaper than GPT-4, for advanced scenarios.
      </model>
      <model name="GPT_3_5_TURBO" cost_ratio="0.067">
        15x cheaper than GPT-4, but GEMINI_2_5_FLASH is better and cheaper.
      </model>
    </comparison>
    
    <monthly_savings>
      Estimated 90% reduction in LLM costs by standardizing on GEMINI_2_5_FLASH
      for testing and development environments.
    </monthly_savings>
  </cost_analysis>
  
  <migration_status>
    <phase name="Phase 1" status="completed">
      Created centralized configuration (llm_defaults.py)
    </phase>
    <phase name="Phase 2" status="completed">
      Migrated all test files to use centralized config (186 files updated)
    </phase>
    <phase name="Phase 3" status="completed">
      Validated compliance - 0 violations remaining
    </phase>
    <phase name="Phase 4" status="pending">
      Update CI/CD to use GOOGLE_API_KEY instead of OPENAI_API_KEY
    </phase>
    <phase name="Phase 5" status="pending">
      Update production configuration to use new defaults
    </phase>
  </migration_status>
  
  <testing>
    <validation_scripts>
      <script>python scripts/llm_compliance_report.py</script>
      <script>python scripts/migrate_llm_models.py --validate-only</script>
    </validation_scripts>
    
    <expected_output>
      [SUCCESS] FULLY COMPLIANT - No violations found!
      Default model is correctly set to: gemini-2.5-flash
    </expected_output>
  </testing>
  
  <learnings>
    <learning date="2025-08-26">
      Successfully migrated 186 files from hardcoded model strings to 
      centralized configuration. The migration was automated using AST 
      parsing and regex replacement, ensuring no manual errors.
    </learning>
    
    <learning date="2025-08-26">
      GEMINI_2_5_FLASH provides equivalent or better performance compared 
      to GPT-3.5-turbo at 50% of the cost, making it ideal for testing.
    </learning>
    
    <learning date="2025-08-26">
      Centralized configuration eliminates the risk of accidentally using 
      expensive models in tests and provides a single point for cost tracking.
    </learning>
  </learnings>
</spec>