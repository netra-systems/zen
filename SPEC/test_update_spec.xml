<?xml version="1.0" encoding="UTF-8"?>
<test_update_specification>
  <metadata>
    <version>1.0</version>
    <created>2025-08-10</created>
    <purpose>Automated test suite management, refreshing, and continuous improvement</purpose>
    <scope>Comprehensive testing strategy with 97% coverage target and self-healing capabilities</scope>
    <usage>
      User can simply run: python scripts/test_updater.py --execute-spec
      Or reference: "Run test_update_spec.xml" to trigger automated test improvements
    </usage>
  </metadata>

  <coverage_goals>
    <goal id="CG1">
      <name>Overall Coverage Target</name>
      <target>97%</target>
      <current_baseline>
        <backend>70%</backend>
        <frontend>60%</frontend>
      </current_baseline>
      <milestones>
        <milestone week="1">75% backend, 70% frontend</milestone>
        <milestone week="2">80% backend, 80% frontend</milestone>
        <milestone week="3">85% backend, 85% frontend</milestone>
        <milestone week="4">90% backend, 90% frontend</milestone>
        <milestone week="6">95% backend, 95% frontend</milestone>
        <milestone week="8">97% overall coverage</milestone>
      </milestones>
    </goal>
    
    <goal id="CG2">
      <name>Critical Path Coverage</name>
      <target>100%</target>
      <critical_paths>
        - Authentication and authorization flows
        - WebSocket connection lifecycle
        - Agent orchestration pipeline
        - Database transactions and rollbacks
        - Payment processing (if applicable)
        - Data privacy and security operations
      </critical_paths>
    </goal>
    
    <goal id="CG3">
      <name>Edge Case Coverage</name>
      <target>95%</target>
      <focus_areas>
        - Error handling paths
        - Timeout scenarios
        - Concurrent request handling
        - Resource exhaustion cases
        - Network failure recovery
        - Data validation boundaries
      </focus_areas>
    </goal>
  </coverage_goals>

  <automated_test_processes>
    <process id="ATP1">
      <name>Test Discovery and Generation</name>
      <description>Automatically identify untested code and generate test cases</description>
      <execution_steps>
        <step order="1">
          <action>Scan codebase for functions/methods without tests</action>
          <command>python scripts/test_discovery.py --find-untested</command>
          <output>reports/untested_code.json</output>
        </step>
        <step order="2">
          <action>Analyze code complexity and prioritize test generation</action>
          <command>python scripts/test_priority.py --analyze-complexity</command>
          <output>reports/test_priorities.json</output>
        </step>
        <step order="3">
          <action>Generate test templates using AI assistance</action>
          <command>python scripts/test_generator.py --create-templates</command>
          <output>generated_tests/</output>
        </step>
        <step order="4">
          <action>Validate generated tests</action>
          <command>python scripts/test_validator.py --check-generated</command>
          <output>reports/test_validation.json</output>
        </step>
        <step order="5">
          <action>Integrate validated tests into test suite</action>
          <command>python scripts/test_integrator.py --merge-tests</command>
        </step>
        <step order="6">
          <action>Add AI metadata headers to generated test files</action>
          <command>python scripts/metadata_header_generator.py --batch generated_tests/</command>
          <metadata_info>
            - Mark as AI-generated content
            - Include generation context and parameters
            - Track coverage improvements
            - Link to parent code files being tested
          </metadata_info>
        </step>
      </execution_steps>
    </process>
    
    <process id="ATP2">
      <name>Test Refresh and Update</name>
      <description>Update existing tests to match code changes and modern patterns</description>
      <execution_steps>
        <step order="1">
          <action>Detect outdated test patterns</action>
          <command>python scripts/test_pattern_analyzer.py --find-outdated</command>
          <triggers>
            - Tests using deprecated APIs
            - Tests with outdated mocking patterns
            - Tests not following current conventions
            - Tests with hardcoded values
          </triggers>
        </step>
        <step order="2">
          <action>Update test implementations</action>
          <command>python scripts/test_modernizer.py --update-patterns</command>
          <updates>
            - Replace mockResolvedValueOnce with mockImplementationOnce
            - Add proper WebSocketProvider wrappers
            - Update to use latest testing utilities
            - Convert to async/await patterns
          </updates>
        </step>
        <step order="3">
          <action>Enhance test assertions</action>
          <command>python scripts/test_assertion_enhancer.py --improve</command>
          <enhancements>
            - Add more specific assertions
            - Check for side effects
            - Validate error messages
            - Verify state transitions
          </enhancements>
        </step>
        <step order="4">
          <action>Update test data and fixtures</action>
          <command>python scripts/test_fixture_updater.py --refresh</command>
          <updates>
            - Generate realistic test data
            - Update mock responses
            - Refresh database fixtures
            - Modernize factory patterns
          </updates>
        </step>
        <step order="5">
          <action>Update metadata headers for modified tests</action>
          <command>python scripts/metadata_header_updater.py --update-modified</command>
          <tracking>
            - Record modernization changes
            - Document pattern updates
            - Track performance improvements
            - Note breaking changes
          </tracking>
        </step>
      </execution_steps>
    </process>
    
    <process id="ATP3">
      <name>Legacy Test Cleanup</name>
      <description>Identify and remove or update legacy tests</description>
      <execution_steps>
        <step order="1">
          <action>Identify legacy test patterns</action>
          <command>python scripts/test_legacy_scanner.py --scan</command>
          <patterns>
            - Skipped tests without justification
            - Tests for removed features
            - Duplicate test scenarios
            - Tests with sleep() or arbitrary waits
            - Tests depending on external services
            - Flaky tests with inconsistent results
          </patterns>
        </step>
        <step order="2">
          <action>Categorize legacy tests</action>
          <command>python scripts/test_categorizer.py --classify-legacy</command>
          <categories>
            - REMOVE: Tests for deleted code
            - UPDATE: Tests needing modernization
            - FIX: Broken but needed tests
            - REPLACE: Tests to be rewritten
            - KEEP: Valid legacy tests to preserve
          </categories>
        </step>
        <step order="3">
          <action>Process legacy tests based on category</action>
          <command>python scripts/test_legacy_processor.py --execute</command>
          <actions>
            - Remove obsolete tests
            - Update test implementations
            - Fix broken assertions
            - Replace with modern equivalents
            - Document preserved legacy tests
          </actions>
        </step>
        <step order="4">
          <action>Verify test suite integrity</action>
          <command>python test_runner.py --mode comprehensive</command>
        </step>
      </execution_steps>
    </process>
    
    <process id="ATP4">
      <name>Coverage Gap Analysis</name>
      <description>Continuously identify and fill coverage gaps</description>
      <execution_steps>
        <step order="1">
          <action>Generate detailed coverage reports</action>
          <command>python scripts/coverage_analyzer.py --detailed</command>
          <reports>
            - Line coverage by module
            - Branch coverage analysis
            - Path coverage metrics
            - Mutation testing results
          </reports>
        </step>
        <step order="2">
          <action>Identify critical uncovered code</action>
          <command>python scripts/coverage_gap_finder.py --priority</command>
          <prioritization>
            - Security-related code
            - Financial transactions
            - Data mutations
            - Error handlers
            - Integration points
          </prioritization>
        </step>
        <step order="3">
          <action>Generate coverage improvement plan</action>
          <command>python scripts/coverage_planner.py --create-plan</command>
          <output>reports/coverage_improvement_plan.md</output>
        </step>
        <step order="4">
          <action>Track coverage trends</action>
          <command>python scripts/coverage_tracker.py --update-metrics</command>
          <metrics>
            - Coverage delta per commit
            - Coverage by feature area
            - Test execution time trends
            - Flakiness indicators
          </metrics>
        </step>
      </execution_steps>
    </process>
    
    <process id="ATP5">
      <name>Test Performance Optimization</name>
      <description>Improve test execution speed and reliability</description>
      <execution_steps>
        <step order="1">
          <action>Profile test execution times</action>
          <command>python scripts/test_profiler.py --analyze-performance</command>
          <metrics>
            - Individual test durations
            - Setup/teardown overhead
            - Database transaction times
            - Network call latencies
          </metrics>
        </step>
        <step order="2">
          <action>Optimize slow tests</action>
          <command>python scripts/test_optimizer.py --speed-up</command>
          <optimizations>
            - Parallelize independent tests
            - Use in-memory databases
            - Mock expensive operations
            - Implement test data caching
            - Reduce fixture complexity
          </optimizations>
        </step>
        <step order="3">
          <action>Implement test sharding</action>
          <command>python scripts/test_sharder.py --configure</command>
          <strategy>
            - Group by execution time
            - Separate by resource usage
            - Isolate flaky tests
            - Balance across workers
          </strategy>
        </step>
        <step order="4">
          <action>Monitor test reliability</action>
          <command>python scripts/test_reliability_monitor.py --track</command>
          <tracking>
            - Flaky test detection
            - Failure pattern analysis
            - Environment sensitivity
            - Resource leak detection
          </tracking>
        </step>
      </execution_steps>
    </process>
  </automated_test_processes>

  <test_types_enhancement>
    <test_type id="TT1">
      <name>Unit Tests</name>
      <coverage_target>98%</coverage_target>
      <enhancement_strategies>
        <strategy>Parameterized testing for edge cases</strategy>
        <strategy>Property-based testing with Hypothesis</strategy>
        <strategy>Mutation testing to verify test quality</strategy>
        <strategy>Snapshot testing for complex outputs</strategy>
      </enhancement_strategies>
      <automation_commands>
        <command>python scripts/enhance_unit_tests.py --add-edge-cases</command>
        <command>python scripts/enhance_unit_tests.py --add-property-tests</command>
        <command>python scripts/mutation_testing.py --verify-quality</command>
      </automation_commands>
    </test_type>
    
    <test_type id="TT2">
      <name>Integration Tests</name>
      <coverage_target>95%</coverage_target>
      <enhancement_strategies>
        <strategy>Contract testing between services</strategy>
        <strategy>Database transaction testing</strategy>
        <strategy>API response validation</strategy>
        <strategy>WebSocket event flow testing</strategy>
      </enhancement_strategies>
      <automation_commands>
        <command>python scripts/enhance_integration_tests.py --add-contracts</command>
        <command>python scripts/enhance_integration_tests.py --test-transactions</command>
        <command>python scripts/enhance_integration_tests.py --validate-apis</command>
      </automation_commands>
    </test_type>
    
    <test_type id="TT3">
      <name>End-to-End Tests</name>
      <coverage_target>90%</coverage_target>
      <enhancement_strategies>
        <strategy>User journey mapping</strategy>
        <strategy>Cross-browser testing</strategy>
        <strategy>Mobile responsiveness testing</strategy>
        <strategy>Performance testing under load</strategy>
      </enhancement_strategies>
      <automation_commands>
        <command>python scripts/enhance_e2e_tests.py --map-journeys</command>
        <command>python scripts/enhance_e2e_tests.py --add-browsers</command>
        <command>python scripts/enhance_e2e_tests.py --test-mobile</command>
        <command>python scripts/load_testing.py --simulate-users</command>
      </automation_commands>
    </test_type>
    
    <test_type id="TT4">
      <name>Security Tests</name>
      <coverage_target>100%</coverage_target>
      <enhancement_strategies>
        <strategy>SQL injection testing</strategy>
        <strategy>XSS vulnerability scanning</strategy>
        <strategy>Authentication bypass attempts</strategy>
        <strategy>Rate limiting verification</strategy>
        <strategy>OWASP Top 10 coverage</strategy>
      </enhancement_strategies>
      <automation_commands>
        <command>python scripts/security_tests.py --scan-vulnerabilities</command>
        <command>python scripts/security_tests.py --test-auth</command>
        <command>python scripts/security_tests.py --verify-rate-limits</command>
        <command>python scripts/security_tests.py --owasp-check</command>
      </automation_commands>
    </test_type>
    
    <test_type id="TT5">
      <name>Performance Tests</name>
      <coverage_target>95%</coverage_target>
      <enhancement_strategies>
        <strategy>Response time benchmarking</strategy>
        <strategy>Memory leak detection</strategy>
        <strategy>Concurrent user simulation</strategy>
        <strategy>Database query optimization</strategy>
        <strategy>CDN and caching effectiveness</strategy>
      </enhancement_strategies>
      <automation_commands>
        <command>python scripts/performance_tests.py --benchmark</command>
        <command>python scripts/performance_tests.py --detect-leaks</command>
        <command>python scripts/performance_tests.py --stress-test</command>
        <command>python scripts/performance_tests.py --profile-queries</command>
      </automation_commands>
    </test_type>
  </test_types_enhancement>

  <ultra_thinking_capabilities>
    <capability id="UTC1">
      <name>Intelligent Test Analysis Engine</name>
      <description>Advanced cognitive processing for comprehensive test understanding</description>
      <features>
        <feature>Deep code semantic analysis to understand intent, not just structure</feature>
        <feature>Cross-module dependency mapping for impact analysis</feature>
        <feature>Historical failure pattern recognition with ML models</feature>
        <feature>Business logic extraction from code and comments</feature>
        <feature>Test quality scoring based on multiple dimensions</feature>
        <feature>Automatic test categorization and prioritization</feature>
      </features>
      <implementation>
        <step>Parse AST and build semantic model of codebase</step>
        <step>Extract business rules and invariants</step>
        <step>Identify critical paths through control flow analysis</step>
        <step>Build dependency graph with transitive closure</step>
        <step>Train ML model on historical test results</step>
        <step>Generate intelligent test recommendations</step>
      </implementation>
      <automation_script>scripts/intelligent_test_analyzer.py</automation_script>
    </capability>
    
    <capability id="UTC2">
      <name>Autonomous Test Reviewer</name>
      <description>Self-directed test review and improvement without user prompts</description>
      <features>
        <feature>Continuous background test quality assessment</feature>
        <feature>Proactive identification of test gaps</feature>
        <feature>Automatic test enhancement suggestions</feature>
        <feature>Self-initiated test refactoring</feature>
        <feature>Intelligent test deduplication</feature>
        <feature>Coverage goal tracking with auto-remediation</feature>
      </features>
      <triggers>
        <trigger>Scheduled review cycles (hourly/daily/weekly)</trigger>
        <trigger>Code commit detection</trigger>
        <trigger>Coverage drop below threshold</trigger>
        <trigger>Test failure pattern detected</trigger>
        <trigger>Performance degradation observed</trigger>
      </triggers>
      <automation_script>scripts/test_autonomous_review.py</automation_script>
    </capability>
    
    <capability id="UTC3">
      <name>Test Generation Intelligence</name>
      <description>Ultra-smart test generation using deep learning and program synthesis</description>
      <features>
        <feature>Property-based test generation using formal methods</feature>
        <feature>Fuzzing-guided test case creation</feature>
        <feature>Symbolic execution for path coverage</feature>
        <feature>Contract-based test derivation</feature>
        <feature>Example-based test synthesis from documentation</feature>
        <feature>Mutation-guided test improvement</feature>
      </features>
      <strategies>
        <strategy>Analyze function signatures and generate type-based tests</strategy>
        <strategy>Extract invariants and generate property tests</strategy>
        <strategy>Use concolic execution for path exploration</strategy>
        <strategy>Generate tests from JSDoc/docstring examples</strategy>
        <strategy>Create regression tests from bug reports</strategy>
        <strategy>Synthesize tests from API specifications</strategy>
      </strategies>
      <automation_script>scripts/ultra_test_generator.py</automation_script>
    </capability>
    
    <capability id="UTC4">
      <name>Self-Healing Test Framework</name>
      <description>Tests that automatically fix themselves when code changes</description>
      <features>
        <feature>Automatic selector updates when UI changes</feature>
        <feature>API contract migration when endpoints change</feature>
        <feature>Mock data regeneration on schema changes</feature>
        <feature>Test assertion relaxation for acceptable variations</feature>
        <feature>Automatic timeout adjustments based on performance</feature>
        <feature>Self-correcting test dependencies</feature>
      </features>
      <healing_strategies>
        <strategy>Use multiple selector strategies with fallbacks</strategy>
        <strategy>Implement smart waits with condition polling</strategy>
        <strategy>Generate mocks from actual API responses</strategy>
        <strategy>Use fuzzy matching for text assertions</strategy>
        <strategy>Implement retry logic with exponential backoff</strategy>
        <strategy>Auto-update test data from production samples</strategy>
      </healing_strategies>
      <automation_script>scripts/self_healing_framework.py</automation_script>
    </capability>
    
    <capability id="UTC5">
      <name>Predictive Test Optimization</name>
      <description>ML-powered test optimization and prediction</description>
      <features>
        <feature>Predict which tests will fail based on code changes</feature>
        <feature>Optimize test execution order for faster feedback</feature>
        <feature>Identify redundant tests using coverage analysis</feature>
        <feature>Predict test execution time for better scheduling</feature>
        <feature>Detect test coupling and suggest decoupling</feature>
        <feature>Recommend test parallelization strategies</feature>
      </features>
      <models>
        <model>Random Forest for failure prediction</model>
        <model>Neural network for execution time estimation</model>
        <model>Clustering for test similarity detection</model>
        <model>Graph algorithms for dependency analysis</model>
        <model>Reinforcement learning for test scheduling</model>
      </models>
      <automation_script>scripts/predictive_optimizer.py</automation_script>
    </capability>
  </ultra_thinking_capabilities>

  <ai_agent_file_metadata_tracking>
    <metadata>
      <name>AI Agent File Modification Tracking System</name>
      <version>1.0</version>
      <purpose>Create comprehensive audit trail for all AI-assisted code modifications</purpose>
      <description>Every file modified by an AI agent includes detailed metadata header for change tracking, accountability, and rollback capabilities</description>
      <implementation_date>2025-08-10</implementation_date>
    </metadata>

    <tracking_requirement id="AAFMT1">
      <name>Mandatory Metadata Header</name>
      <description>All files modified by AI agents must include a metadata header at the top</description>
      <enforcement>Automatic via pre-commit hooks and agent tooling</enforcement>
      <format>
        <location>Top of file after shebang/encoding declarations</location>
        <style>Language-appropriate comment format</style>
        <structure>
          /*
           * AI AGENT MODIFICATION METADATA
           * ================================
           * Timestamp: ISO-8601 format with timezone
           * Agent: Model name and version
           * Context: Task description or prompt summary
           * Git State: Branch, commit hash, dirty status
           * Change Type: Feature/Fix/Refactor/Test/Docs
           * Risk Level: Low/Medium/High/Critical
           * Review Status: Pending/Reviewed/Approved
           * Session ID: Unique identifier for tracking
           * ================================
           */
        </structure>
      </format>
    </tracking_requirement>

    <metadata_fields>
      <field id="MF1">
        <name>Timestamp</name>
        <format>ISO-8601 with timezone (e.g., 2025-08-10T14:30:00Z)</format>
        <required>true</required>
        <example>2025-08-10T14:30:00-07:00</example>
      </field>
      
      <field id="MF2">
        <name>Agent Information</name>
        <components>
          <component>Model Name (e.g., Claude, GPT-4)</component>
          <component>Model Version (e.g., claude-opus-4-1-20250805)</component>
          <component>Tool Version (e.g., claude-code v1.2.3)</component>
        </components>
        <required>true</required>
        <example>Claude Opus 4.1 (claude-opus-4-1-20250805) via claude-code v1.2.3</example>
      </field>
      
      <field id="MF3">
        <name>Context Information</name>
        <components>
          <component>Task Description (first 200 chars of prompt)</component>
          <component>Execution Plan Summary</component>
          <component>Parent Task ID (if part of larger workflow)</component>
          <component>User Intent Category</component>
        </components>
        <required>true</required>
        <example>Task: Add dark mode toggle to settings | Plan: Implement UI component, add state management, update styles</example>
      </field>
      
      <field id="MF4">
        <name>Git State</name>
        <components>
          <component>Current Branch</component>
          <component>Base Commit Hash (8 chars)</component>
          <component>Working Directory Status (clean/dirty)</component>
          <component>Uncommitted Changes Count</component>
        </components>
        <required>true</required>
        <example>Branch: feature/dark-mode | Commit: a1b2c3d4 | Status: dirty (3 uncommitted)</example>
      </field>
      
      <field id="MF5">
        <name>Change Classification</name>
        <components>
          <component>Change Type (Feature/Bugfix/Refactor/Test/Documentation/Performance/Security)</component>
          <component>Impact Scope (File/Module/Component/System)</component>
          <component>Breaking Change Flag (true/false)</component>
          <component>Dependencies Modified (true/false)</component>
        </components>
        <required>true</required>
        <example>Type: Feature | Scope: Component | Breaking: false | Dependencies: true</example>
      </field>
      
      <field id="MF6">
        <name>Risk Assessment</name>
        <levels>
          <level name="Low">Cosmetic changes, documentation, simple refactors</level>
          <level name="Medium">New features, non-critical bug fixes</level>
          <level name="High">Core logic changes, API modifications</level>
          <level name="Critical">Security changes, payment logic, authentication</level>
        </levels>
        <required>true</required>
        <auto_detection>Based on file path, change type, and keywords</auto_detection>
      </field>
      
      <field id="MF7">
        <name>Review Tracking</name>
        <components>
          <component>Review Status (Pending/InReview/Reviewed/Approved/Rejected)</component>
          <component>Reviewer (if reviewed)</component>
          <component>Review Comments</component>
          <component>Auto-Review Score (0-100)</component>
        </components>
        <required>true</required>
        <example>Status: Pending | Auto-Score: 85/100</example>
      </field>
      
      <field id="MF8">
        <name>Session Tracking</name>
        <components>
          <component>Session ID (UUID)</component>
          <component>Conversation ID</component>
          <component>Request Sequence Number</component>
          <component>Total Files Modified in Session</component>
        </components>
        <required>true</required>
        <example>Session: 550e8400-e29b-41d4-a716-446655440000 | Conv: chat_123 | Seq: 5 | Files: 12</example>
      </field>
      
      <field id="MF9">
        <name>Quality Metrics</name>
        <components>
          <component>Lines Added/Modified/Deleted</component>
          <component>Complexity Delta (before/after)</component>
          <component>Test Coverage Delta</component>
          <component>Linting Issues Introduced</component>
        </components>
        <required>false</required>
        <example>+45/-12 lines | Complexity: 12→10 | Coverage: 75%→80% | Lint: 0 issues</example>
      </field>
      
      <field id="MF10">
        <name>Rollback Information</name>
        <components>
          <component>Previous Version Hash</component>
          <component>Rollback Command</component>
          <component>Backup Location</component>
          <component>Recovery Instructions</component>
        </components>
        <required>true</required>
        <example>Previous: abc123de | Rollback: git checkout abc123de -- path/to/file.py</example>
      </field>
    </metadata_fields>

    <implementation_strategy>
      <strategy id="IS1">
        <name>Automatic Header Injection</name>
        <description>Modify AI agent tools to automatically prepend metadata headers</description>
        <implementation_steps>
          <step>Hook into Edit, Write, and MultiEdit tools</step>
          <step>Gather metadata before file modification</step>
          <step>Prepend formatted header to file content</step>
          <step>Preserve existing headers for history</step>
          <step>Update header on subsequent modifications</step>
        </implementation_steps>
        <code_location>app/tools/file_operations.py</code_location>
      </strategy>
      
      <strategy id="IS2">
        <name>Git Integration</name>
        <description>Integrate with git hooks for metadata enforcement</description>
        <git_hooks>
          <hook name="pre-commit">
            <action>Verify metadata headers exist in modified files</action>
            <action>Validate metadata format and completeness</action>
            <action>Block commit if metadata missing or invalid</action>
          </hook>
          <hook name="post-commit">
            <action>Update metadata with commit hash</action>
            <action>Archive metadata to audit log</action>
          </hook>
        </git_hooks>
        <script>scripts/git_hooks/metadata_validator.py</script>
      </strategy>
      
      <strategy id="IS3">
        <name>Metadata Database</name>
        <description>Central database for tracking all AI modifications</description>
        <schema>
          <table name="ai_modifications">
            <column>id (UUID PRIMARY KEY)</column>
            <column>file_path (TEXT)</column>
            <column>timestamp (TIMESTAMP WITH TIMEZONE)</column>
            <column>agent_info (JSONB)</column>
            <column>git_state (JSONB)</column>
            <column>change_metadata (JSONB)</column>
            <column>session_id (UUID)</column>
            <column>user_id (UUID)</column>
          </table>
        </schema>
        <features>
          <feature>Query modifications by agent, user, or time range</feature>
          <feature>Track modification patterns and trends</feature>
          <feature>Generate audit reports</feature>
          <feature>Support rollback operations</feature>
        </features>
      </strategy>
      
      <strategy id="IS4">
        <name>Intelligent Metadata Extraction</name>
        <description>Automatically extract context from prompts and conversations</description>
        <extraction_methods>
          <method>NLP analysis of user prompts</method>
          <method>Task classification using ML models</method>
          <method>Dependency graph analysis</method>
          <method>Impact assessment algorithms</method>
        </extraction_methods>
        <script>scripts/metadata_extractor.py</script>
      </strategy>
    </implementation_strategy>

    <header_formats>
      <format language="python">
        <template><![CDATA[
# AI AGENT MODIFICATION METADATA
# ================================
# Timestamp: {timestamp}
# Agent: {agent_name} {agent_version}
# Context: {task_description}
# Git: {git_branch} | {git_commit} | {git_status}
# Change: {change_type} | Scope: {scope} | Risk: {risk_level}
# Session: {session_id} | Seq: {sequence_number}
# Review: {review_status} | Score: {auto_score}
# ================================
        ]]></template>
      </format>
      
      <format language="javascript">
        <template><![CDATA[
/**
 * AI AGENT MODIFICATION METADATA
 * ================================
 * Timestamp: {timestamp}
 * Agent: {agent_name} {agent_version}
 * Context: {task_description}
 * Git: {git_branch} | {git_commit} | {git_status}
 * Change: {change_type} | Scope: {scope} | Risk: {risk_level}
 * Session: {session_id} | Seq: {sequence_number}
 * Review: {review_status} | Score: {auto_score}
 * ================================
 */
        ]]></template>
      </format>
      
      <format language="typescript">
        <template><![CDATA[
/**
 * AI AGENT MODIFICATION METADATA
 * ================================
 * @timestamp {timestamp}
 * @agent {agent_name} {agent_version}
 * @context {task_description}
 * @git {git_branch} | {git_commit} | {git_status}
 * @change {change_type} | Scope: {scope} | Risk: {risk_level}
 * @session {session_id} | Seq: {sequence_number}
 * @review {review_status} | Score: {auto_score}
 * ================================
 */
        ]]></template>
      </format>
      
      <format language="html">
        <template><![CDATA[
<!--
  AI AGENT MODIFICATION METADATA
  ================================
  Timestamp: {timestamp}
  Agent: {agent_name} {agent_version}
  Context: {task_description}
  Git: {git_branch} | {git_commit} | {git_status}
  Change: {change_type} | Scope: {scope} | Risk: {risk_level}
  Session: {session_id} | Seq: {sequence_number}
  Review: {review_status} | Score: {auto_score}
  ================================
-->
        ]]></template>
      </format>
      
      <format language="css">
        <template><![CDATA[
/*
 * AI AGENT MODIFICATION METADATA
 * ================================
 * Timestamp: {timestamp}
 * Agent: {agent_name} {agent_version}
 * Context: {task_description}
 * Git: {git_branch} | {git_commit} | {git_status}
 * Change: {change_type} | Scope: {scope} | Risk: {risk_level}
 * Session: {session_id} | Seq: {sequence_number}
 * Review: {review_status} | Score: {auto_score}
 * ================================
 */
        ]]></template>
      </format>
      
      <format language="sql">
        <template><![CDATA[
-- AI AGENT MODIFICATION METADATA
-- ================================
-- Timestamp: {timestamp}
-- Agent: {agent_name} {agent_version}
-- Context: {task_description}
-- Git: {git_branch} | {git_commit} | {git_status}
-- Change: {change_type} | Scope: {scope} | Risk: {risk_level}
-- Session: {session_id} | Seq: {sequence_number}
-- Review: {review_status} | Score: {auto_score}
-- ================================
        ]]></template>
      </format>
      
      <format language="yaml">
        <template><![CDATA[
# AI AGENT MODIFICATION METADATA
# ================================
# metadata:
#   timestamp: {timestamp}
#   agent: {agent_name} {agent_version}
#   context: {task_description}
#   git:
#     branch: {git_branch}
#     commit: {git_commit}
#     status: {git_status}
#   change:
#     type: {change_type}
#     scope: {scope}
#     risk: {risk_level}
#   session:
#     id: {session_id}
#     sequence: {sequence_number}
#   review:
#     status: {review_status}
#     score: {auto_score}
# ================================
        ]]></template>
      </format>
      
      <format language="xml">
        <template><![CDATA[
<!--
  AI AGENT MODIFICATION METADATA
  ================================
  <metadata>
    <timestamp>{timestamp}</timestamp>
    <agent>{agent_name} {agent_version}</agent>
    <context>{task_description}</context>
    <git>
      <branch>{git_branch}</branch>
      <commit>{git_commit}</commit>
      <status>{git_status}</status>
    </git>
    <change>
      <type>{change_type}</type>
      <scope>{scope}</scope>
      <risk>{risk_level}</risk>
    </change>
    <session>
      <id>{session_id}</id>
      <sequence>{sequence_number}</sequence>
    </session>
    <review>
      <status>{review_status}</status>
      <score>{auto_score}</score>
    </review>
  </metadata>
  ================================
-->
        ]]></template>
      </format>
    </header_formats>

    <automation_tools>
      <tool id="AT1">
        <name>Metadata Header Generator</name>
        <description>Automatically generate metadata headers for files</description>
        <command>python scripts/metadata_header_generator.py --file {file_path}</command>
        <features>
          <feature>Auto-detect file language</feature>
          <feature>Extract git information</feature>
          <feature>Gather session context</feature>
          <feature>Format appropriate header</feature>
        </features>
      </tool>
      
      <tool id="AT2">
        <name>Metadata Validator</name>
        <description>Validate metadata headers in modified files</description>
        <command>python scripts/metadata_validator.py --validate {file_path}</command>
        <validations>
          <validation>Check header presence</validation>
          <validation>Verify required fields</validation>
          <validation>Validate timestamp format</validation>
          <validation>Confirm git state accuracy</validation>
        </validations>
      </tool>
      
      <tool id="AT3">
        <name>Metadata Audit Reporter</name>
        <description>Generate audit reports from metadata</description>
        <command>python scripts/metadata_audit_reporter.py --generate-report</command>
        <reports>
          <report>Daily modification summary</report>
          <report>Agent activity analysis</report>
          <report>Risk assessment overview</report>
          <report>Review status tracking</report>
        </reports>
      </tool>
      
      <tool id="AT4">
        <name>Metadata History Viewer</name>
        <description>View modification history for files</description>
        <command>python scripts/metadata_history.py --file {file_path}</command>
        <features>
          <feature>Show all modifications chronologically</feature>
          <feature>Filter by agent or user</feature>
          <feature>Display change diffs</feature>
          <feature>Export history to JSON/CSV</feature>
        </features>
      </tool>
      
      <tool id="AT5">
        <name>Rollback Manager</name>
        <description>Rollback AI modifications using metadata</description>
        <command>python scripts/rollback_manager.py --rollback {session_id}</command>
        <capabilities>
          <capability>Rollback single file</capability>
          <capability>Rollback entire session</capability>
          <capability>Selective rollback by criteria</capability>
          <capability>Preview rollback changes</capability>
        </capabilities>
      </tool>
    </automation_tools>

    <monitoring_and_compliance>
      <compliance id="MC1">
        <name>Metadata Coverage</name>
        <metric>Percentage of AI-modified files with valid metadata</metric>
        <target>100%</target>
        <enforcement>Block commits without metadata</enforcement>
      </compliance>
      
      <compliance id="MC2">
        <name>Review Completion</name>
        <metric>Percentage of AI modifications reviewed</metric>
        <target>100% for high/critical risk changes</target>
        <alert_threshold>Any critical change unreviewed for >1 hour</alert_threshold>
      </compliance>
      
      <compliance id="MC3">
        <name>Audit Trail Integrity</name>
        <metric>Complete chain of modifications traceable</metric>
        <verification>Daily audit trail validation</verification>
        <backup>Redundant storage of metadata</backup>
      </compliance>
      
      <compliance id="MC4">
        <name>Session Tracking</name>
        <metric>All modifications linked to sessions</metric>
        <requirement>No orphaned modifications</requirement>
        <validation>Session ID validation on every change</validation>
      </compliance>
    </monitoring_and_compliance>

    <integration_with_test_automation>
      <integration id="ITA1">
        <name>Test Generation Metadata</name>
        <description>Include metadata in AI-generated tests</description>
        <additional_fields>
          <field>Test Coverage Delta</field>
          <field>Test Type (unit/integration/e2e)</field>
          <field>Tested Components</field>
          <field>Expected Behavior Description</field>
        </additional_fields>
      </integration>
      
      <integration id="ITA2">
        <name>Test Update Tracking</name>
        <description>Track when tests are modified by AI</description>
        <tracking>
          <track>Reason for test modification</track>
          <track>Previous test behavior</track>
          <track>New assertions added</track>
          <track>Performance impact</track>
        </tracking>
      </integration>
      
      <integration id="ITA3">
        <name>Coverage Impact Analysis</name>
        <description>Link code changes to coverage changes</description>
        <analysis>
          <analyze>Coverage before modification</analyze>
          <analyze>Coverage after modification</analyze>
          <analyze>New lines requiring tests</analyze>
          <analyze>Test recommendations</analyze>
        </analysis>
      </integration>
    </integration_with_test_automation>

    <benefits>
      <benefit id="B1">
        <name>Complete Audit Trail</name>
        <description>Every AI modification is tracked and traceable</description>
        <value>Regulatory compliance and accountability</value>
      </benefit>
      
      <benefit id="B2">
        <name>Quick Rollback Capability</name>
        <description>Instantly revert problematic changes</description>
        <value>Reduced risk and faster recovery</value>
      </benefit>
      
      <benefit id="B3">
        <name>Quality Insights</name>
        <description>Analyze AI agent performance and patterns</description>
        <value>Continuous improvement of AI assistance</value>
      </benefit>
      
      <benefit id="B4">
        <name>Security and Compliance</name>
        <description>Track all changes for security audits</description>
        <value>Meet regulatory requirements</value>
      </benefit>
      
      <benefit id="B5">
        <name>Team Collaboration</name>
        <description>Clear visibility of AI-assisted changes</description>
        <value>Better code review and knowledge sharing</value>
      </benefit>
    </benefits>

    <rollout_plan>
      <phase id="RP1">
        <name>Phase 1: Core Implementation</name>
        <timeline>Week 1-2</timeline>
        <tasks>
          <task>Implement metadata header generator</task>
          <task>Modify AI agent tools to inject headers</task>
          <task>Create basic validation scripts</task>
          <task>Set up metadata database</task>
        </tasks>
      </phase>
      
      <phase id="RP2">
        <name>Phase 2: Integration</name>
        <timeline>Week 3-4</timeline>
        <tasks>
          <task>Integrate with git hooks</task>
          <task>Connect to test automation</task>
          <task>Implement audit reporting</task>
          <task>Add rollback capabilities</task>
        </tasks>
      </phase>
      
      <phase id="RP3">
        <name>Phase 3: Enforcement</name>
        <timeline>Week 5-6</timeline>
        <tasks>
          <task>Enable mandatory metadata validation</task>
          <task>Block commits without metadata</task>
          <task>Set up monitoring alerts</task>
          <task>Train team on new requirements</task>
        </tasks>
      </phase>
      
      <phase id="RP4">
        <name>Phase 4: Optimization</name>
        <timeline>Week 7-8</timeline>
        <tasks>
          <task>Optimize metadata extraction</task>
          <task>Enhance auto-review scoring</task>
          <task>Implement advanced analytics</task>
          <task>Create compliance dashboards</task>
        </tasks>
      </phase>
    </rollout_plan>

    <execution_commands>
      <command id="EC1">
        <name>Enable Metadata Tracking</name>
        <description>Turn on automatic metadata tracking for all AI modifications</description>
        <primary_command>python scripts/enable_metadata_tracking.py --activate</primary_command>
        <what_it_does>
          <action>Installs git hooks for metadata validation</action>
          <action>Configures AI agent tools to inject headers</action>
          <action>Sets up metadata database</action>
          <action>Enables monitoring and alerts</action>
        </what_it_does>
      </command>
      
      <command id="EC2">
        <name>Generate Metadata Report</name>
        <description>Create comprehensive report of AI modifications</description>
        <primary_command>python scripts/metadata_report.py --generate</primary_command>
        <options>
          <option>--date-range: Specify time period</option>
          <option>--agent: Filter by specific agent</option>
          <option>--risk-level: Filter by risk level</option>
          <option>--format: Output format (html/pdf/json)</option>
        </options>
      </command>
      
      <command id="EC3">
        <name>Validate Project Metadata</name>
        <description>Check all files for proper metadata headers</description>
        <primary_command>python scripts/validate_all_metadata.py --scan</primary_command>
        <actions>
          <action>Scan all project files</action>
          <action>Identify files missing metadata</action>
          <action>Validate existing metadata</action>
          <action>Generate compliance report</action>
        </actions>
      </command>
    </execution_commands>
  </ai_agent_file_metadata_tracking>

  <continuous_improvement>
    <improvement id="CI1">
      <name>AI-Powered Test Generation</name>
      <description>Use AI to automatically generate comprehensive test cases</description>
      <implementation>
        <step>Analyze code structure and complexity</step>
        <step>Generate test scenarios based on code paths</step>
        <step>Create edge case tests using boundary analysis</step>
        <step>Validate generated tests with mutation testing</step>
        <step>Integrate high-quality tests into suite</step>
      </implementation>
      <automation_script>scripts/ai_test_generator.py</automation_script>
    </improvement>
    
    <improvement id="CI2">
      <name>Self-Healing Tests</name>
      <description>Automatically fix broken tests when code changes</description>
      <implementation>
        <step>Detect test failures after code changes</step>
        <step>Analyze failure patterns and root causes</step>
        <step>Generate fix proposals</step>
        <step>Apply fixes and validate</step>
        <step>Create pull request with fixes</step>
      </implementation>
      <automation_script>scripts/self_healing_tests.py</automation_script>
    </improvement>
    
    <improvement id="CI3">
      <name>Test Impact Analysis</name>
      <description>Run only affected tests based on code changes</description>
      <implementation>
        <step>Build dependency graph of code and tests</step>
        <step>Detect changed files in commit</step>
        <step>Identify affected test suites</step>
        <step>Execute minimal test set</step>
        <step>Report coverage impact</step>
      </implementation>
      <automation_script>scripts/test_impact_analyzer.py</automation_script>
    </improvement>
    
    <improvement id="CI4">
      <name>Predictive Test Failure</name>
      <description>Predict which tests are likely to fail based on code changes</description>
      <implementation>
        <step>Train ML model on historical test results</step>
        <step>Analyze code change patterns</step>
        <step>Predict failure probability</step>
        <step>Prioritize risky tests</step>
        <step>Alert developers proactively</step>
      </implementation>
      <automation_script>scripts/predictive_test_failure.py</automation_script>
    </improvement>
    
    <improvement id="CI5">
      <name>Test Documentation Generation</name>
      <description>Automatically generate and maintain test documentation</description>
      <implementation>
        <step>Extract test descriptions and purposes</step>
        <step>Document test coverage areas</step>
        <step>Generate test execution guides</step>
        <step>Create test maintenance runbooks</step>
        <step>Update documentation on changes</step>
      </implementation>
      <automation_script>scripts/test_doc_generator.py</automation_script>
    </improvement>
  </continuous_improvement>

  <automated_test_management>
    <management id="ATM1">
      <name>Continuous Test Review</name>
      <description>Autonomous test review without user intervention</description>
      <execution_mode>background</execution_mode>
      <triggers>
        <trigger>Every code commit</trigger>
        <trigger>Hourly background scan</trigger>
        <trigger>Coverage threshold breach</trigger>
        <trigger>Test failure patterns</trigger>
      </triggers>
      <actions>
        <action>Analyze test quality and coverage</action>
        <action>Generate missing unit tests</action>
        <action>Create integration test scenarios</action>
        <action>Update E2E test flows</action>
        <action>Remove redundant tests</action>
        <action>Fix flaky tests automatically</action>
      </actions>
      <command>python scripts/test_autonomous_review.py --auto</command>
    </management>
    
    <management id="ATM2">
      <name>Legacy Test Cleanup Manager</name>
      <description>Intelligent cleanup of outdated and problematic tests</description>
      <cleanup_criteria>
        <criterion>Tests for deleted code (auto-remove)</criterion>
        <criterion>Tests with >3 skip decorators (investigate)</criterion>
        <criterion>Tests failing >50% of runs (fix or remove)</criterion>
        <criterion>Tests with execution time >30s (optimize)</criterion>
        <criterion>Tests with no assertions (enhance or remove)</criterion>
        <criterion>Tests using deprecated patterns (modernize)</criterion>
      </cleanup_criteria>
      <cleanup_process>
        <step>Scan entire test suite for legacy patterns</step>
        <step>Categorize tests by cleanup priority</step>
        <step>Apply automated fixes where possible</step>
        <step>Generate manual fix recommendations</step>
        <step>Track cleanup progress</step>
      </cleanup_process>
      <command>python scripts/legacy_test_cleaner.py --aggressive</command>
    </management>
    
    <management id="ATM3">
      <name>Test Categorization Engine</name>
      <description>Automatically categorize and organize tests</description>
      <categories>
        <category name="smoke" criteria="Critical path, &lt;1s execution"/>
        <category name="unit" criteria="Single function/class testing"/>
        <category name="integration" criteria="Multi-component interaction"/>
        <category name="e2e" criteria="Full user journey"/>
        <category name="performance" criteria="Speed/load testing"/>
        <category name="security" criteria="Security validation"/>
        <category name="regression" criteria="Bug fix verification"/>
      </categories>
      <organization>
        <action>Auto-tag tests based on content analysis</action>
        <action>Reorganize test files by category</action>
        <action>Update test runner configurations</action>
        <action>Create category-specific pipelines</action>
      </organization>
      <command>python scripts/test_categorizer.py --reorganize</command>
    </management>
    
    <management id="ATM4">
      <name>Coverage Goal Enforcer</name>
      <description>Aggressively pursue and maintain 97% coverage target</description>
      <enforcement_rules>
        <rule>Block merges if coverage drops below 95%</rule>
        <rule>Auto-generate tests for uncovered code</rule>
        <rule>Daily coverage improvement tasks</rule>
        <rule>Weekly coverage sprints for problem areas</rule>
        <rule>Monthly coverage competitions between teams</rule>
      </enforcement_rules>
      <strategies>
        <strategy>Focus on high-value code paths first</strategy>
        <strategy>Use mutation testing to verify test quality</strategy>
        <strategy>Generate parameterized tests for edge cases</strategy>
        <strategy>Create property-based tests for algorithms</strategy>
        <strategy>Implement contract tests for APIs</strategy>
      </strategies>
      <command>python scripts/coverage_enforcer.py --target 97</command>
    </management>
  </automated_test_management>

  <execution_workflow>
    <workflow id="EW1">
      <name>Daily Test Update Cycle</name>
      <schedule>0 2 * * *</schedule>
      <steps>
        <step>Run ultra-thinking analysis on codebase</step>
        <step>Execute autonomous test review</step>
        <step>Run coverage gap analysis</step>
        <step>Generate missing tests for critical paths</step>
        <step>Add AI metadata headers to all generated files</step>
        <step>Update outdated test patterns</step>
        <step>Clean up legacy tests</step>
        <step>Update metadata for all modified files</step>
        <step>Execute comprehensive test suite</step>
        <step>Generate coverage report</step>
        <step>Generate AI modification audit report</step>
        <step>Create improvement tasks</step>
      </steps>
      <command>python scripts/test_updater.py --daily-cycle --with-metadata</command>
    </workflow>
    
    <workflow id="EW2">
      <name>Weekly Test Optimization</name>
      <schedule>0 3 * * 1</schedule>
      <steps>
        <step>Profile test performance</step>
        <step>Identify slow tests</step>
        <step>Apply optimization strategies</step>
        <step>Clean up legacy tests</step>
        <step>Update test fixtures</step>
        <step>Validate improvements</step>
      </steps>
      <command>python scripts/test_updater.py --weekly-optimization</command>
    </workflow>
    
    <workflow id="EW3">
      <name>Monthly Test Audit</name>
      <schedule>0 4 1 * *</schedule>
      <steps>
        <step>Comprehensive coverage analysis</step>
        <step>Test quality assessment</step>
        <step>Security test verification</step>
        <step>Performance baseline update</step>
        <step>Test debt evaluation</step>
        <step>Generate executive report</step>
      </steps>
      <command>python scripts/test_updater.py --monthly-audit</command>
    </workflow>
  </execution_workflow>

  <monitoring_and_alerts>
    <metric id="MA1">
      <name>Coverage Drop Alert</name>
      <threshold>Coverage decreases by more than 2%</threshold>
      <action>Block merge and notify team</action>
    </metric>
    
    <metric id="MA2">
      <name>Test Execution Time Alert</name>
      <threshold>Test suite takes longer than 30 minutes</threshold>
      <action>Trigger optimization workflow</action>
    </metric>
    
    <metric id="MA3">
      <name>Flaky Test Detection</name>
      <threshold>Test fails intermittently 3 times in 24 hours</threshold>
      <action>Quarantine test and create fix task</action>
    </metric>
    
    <metric id="MA4">
      <name>New Code Coverage</name>
      <threshold>New code has less than 95% coverage</threshold>
      <action>Request additional tests before merge</action>
    </metric>
    
    <metric id="MA5">
      <name>Test Debt Accumulation</name>
      <threshold>More than 10 skipped tests</threshold>
      <action>Schedule cleanup sprint</action>
    </metric>
  </monitoring_and_alerts>

  <reporting>
    <report id="R1">
      <name>Test Coverage Dashboard</name>
      <frequency>Real-time</frequency>
      <location>reports/coverage/dashboard.html</location>
      <contents>
        - Overall coverage percentage
        - Coverage by module
        - Coverage trends over time
        - Uncovered critical paths
        - Recent coverage changes
      </contents>
    </report>
    
    <report id="R2">
      <name>Test Health Report</name>
      <frequency>Daily</frequency>
      <location>reports/test_health.md</location>
      <contents>
        - Test suite execution time
        - Flaky test list
        - Failed test analysis
        - Performance metrics
        - Improvement recommendations
      </contents>
    </report>
    
    <report id="R3">
      <name>Test Update Summary</name>
      <frequency>Weekly</frequency>
      <location>reports/test_update_summary.md</location>
      <contents>
        - Tests added/updated/removed
        - Coverage improvements
        - Performance optimizations
        - Legacy test cleanup
        - Upcoming improvements
      </contents>
    </report>
    
    <report id="R4">
      <name>Executive Test Report</name>
      <frequency>Monthly</frequency>
      <location>reports/executive_test_report.pdf</location>
      <contents>
        - Coverage achievement vs goals
        - Test reliability metrics
        - Quality indicators
        - Risk assessment
        - Resource recommendations
      </contents>
    </report>
  </reporting>

  <integration_points>
    <integration id="IP1">
      <name>CI/CD Pipeline</name>
      <description>Automatic test updates in continuous integration</description>
      <hooks>
        - Pre-commit: Quick test validation
        - Pre-merge: Comprehensive testing
        - Post-merge: Coverage tracking
        - Nightly: Test optimization
      </hooks>
    </integration>
    
    <integration id="IP2">
      <name>Code Review</name>
      <description>Automated test quality checks in pull requests</description>
      <checks>
        - Coverage requirements met
        - Test naming conventions
        - Assertion quality
        - Mock usage patterns
        - Performance impact
      </checks>
    </integration>
    
    <integration id="IP3">
      <name>IDE Integration</name>
      <description>Real-time test feedback in development environment</description>
      <features>
        - Live coverage indicators
        - Test generation suggestions
        - Quick fix proposals
        - Performance warnings
      </features>
    </integration>
    
    <integration id="IP4">
      <name>Monitoring Systems</name>
      <description>Production correlation with test coverage</description>
      <correlations>
        - Bug origin vs test coverage
        - Performance issues vs test scenarios
        - User errors vs E2E coverage
        - Security incidents vs security tests
      </correlations>
    </integration>
  </integration_points>

  <master_execution_commands>
    <command id="MEC1">
      <name>Run Complete Test Update Specification</name>
      <description>Single command to execute all test improvements</description>
      <usage>User can say: "run test_update_spec.xml"</usage>
      <primary_command>python scripts/test_updater.py --execute-spec</primary_command>
      <alternative_commands>
        <command>python scripts/test_autonomous_review.py --full-analysis</command>
        <command>python scripts/test_updater.py --ultra-think</command>
        <command>make test-update-spec</command>
      </alternative_commands>
      <what_it_does>
        <action>Analyzes entire codebase with ultra-thinking capabilities</action>
        <action>Identifies all test gaps and quality issues</action>
        <action>Generates missing tests automatically</action>
        <action>Updates and modernizes existing tests</action>
        <action>Removes redundant and legacy tests</action>
        <action>Optimizes test performance</action>
        <action>Ensures 97% coverage goal achievement</action>
        <action>Creates comprehensive reports</action>
        <action>Sets up continuous improvement</action>
      </what_it_does>
    </command>
    
    <command id="MEC2">
      <name>Quick Test Refresh</name>
      <description>Fast test update for immediate improvements</description>
      <usage>User can say: "refresh tests" or "update tests"</usage>
      <primary_command>python scripts/test_autonomous_review.py --quick</primary_command>
      <execution_time>&lt; 5 minutes</execution_time>
      <actions>
        <action>Quick coverage analysis</action>
        <action>Generate tests for top 10 uncovered files</action>
        <action>Fix obvious test issues</action>
        <action>Update deprecated patterns</action>
      </actions>
    </command>
    
    <command id="MEC3">
      <name>Add Specific Test Types</name>
      <description>Generate specific test categories on demand</description>
      <usage>User doesn't need to specify "add unit tests" or "add e2e tests"</usage>
      <intelligent_detection>System automatically determines what tests are needed</intelligent_detection>
      <primary_command>python scripts/test_autonomous_review.py --smart-generate</primary_command>
      <auto_detection>
        <detect>Missing unit tests for services</detect>
        <detect>Missing integration tests for APIs</detect>
        <detect>Missing E2E tests for user flows</detect>
        <detect>Missing performance tests for critical paths</detect>
        <detect>Missing security tests for auth flows</detect>
      </auto_detection>
    </command>
  </master_execution_commands>

  <quick_start_guide>
    <step order="1">
      <title>One-Command Setup and Execution</title>
      <command>python scripts/test_updater.py --execute-spec</command>
      <description>Automatically sets up, analyzes, and improves entire test suite to 97% coverage</description>
      <note>This is all you need! The system handles everything else automatically.</note>
    </step>
    
    <alternative_simple_commands>
      <command>
        <trigger>User says: "run test_update_spec.xml"</trigger>
        <execution>python scripts/test_updater.py --execute-spec</execution>
      </command>
      <command>
        <trigger>User says: "improve tests"</trigger>
        <execution>python scripts/test_autonomous_review.py --auto</execution>
      </command>
      <command>
        <trigger>User says: "update test coverage"</trigger>
        <execution>python scripts/test_updater.py --execute-spec</execution>
      </command>
      <command>
        <trigger>User says: "clean up tests"</trigger>
        <execution>python scripts/legacy_test_cleaner.py --aggressive</execution>
      </command>
    </alternative_simple_commands>
    
    <optional_advanced_steps>
      <step order="2">
        <title>Schedule Continuous Automation (Optional)</title>
        <command>python scripts/test_updater.py --schedule-automation</command>
        <description>Set up automated test improvement cycles</description>
      </step>
      
      <step order="3">
        <title>Monitor Progress (Optional)</title>
        <command>python scripts/test_updater.py --monitor</command>
        <description>Track coverage improvements toward 97% goal</description>
      </step>
    </optional_advanced_steps>
  </quick_start_guide>

  <success_criteria>
    <criterion id="SC1">
      <name>Coverage Goal Achievement</name>
      <target>97% overall test coverage</target>
      <timeline>8 weeks from implementation</timeline>
    </criterion>
    
    <criterion id="SC2">
      <name>Test Execution Speed</name>
      <target>Full suite runs in under 15 minutes</target>
      <current>~30 minutes</current>
    </criterion>
    
    <criterion id="SC3">
      <name>Test Reliability</name>
      <target>Less than 1% flaky tests</target>
      <measurement>Consistent pass rate over 99%</measurement>
    </criterion>
    
    <criterion id="SC4">
      <name>Automated Test Generation</name>
      <target>80% of new code automatically tested</target>
      <measurement>Generated tests vs manual tests ratio</measurement>
    </criterion>
    
    <criterion id="SC5">
      <name>Legacy Test Elimination</name>
      <target>Zero skipped tests without justification</target>
      <measurement>Complete legacy test cleanup</measurement>
    </criterion>
    
    <criterion id="SC6">
      <name>Critical Path Coverage</name>
      <target>100% coverage of business-critical paths</target>
      <measurement>No untested critical functionality</measurement>
    </criterion>
  </success_criteria>

  <cross_references>
    <reference id="XR1">
      <specification>SPEC/code_changes.xml</specification>
      <relationship>Extends test coverage requirements from 70%/60% to 97%</relationship>
      <integration>Code changes must trigger test update automation</integration>
    </reference>
    
    <reference id="XR2">
      <specification>SPEC/LEGACY_CODE_CLEANUP.xml</specification>
      <relationship>Provides test cleanup procedures for legacy code removal</relationship>
      <integration>Legacy test cleanup is part of overall code cleanup strategy</integration>
    </reference>
    
    <reference id="XR3">
      <specification>CLAUDE.md</specification>
      <relationship>Documents test update commands for AI agents</relationship>
      <integration>Agents can execute "run test_update_spec.xml" directly</integration>
    </reference>
    
    <reference id="XR4">
      <specification>SPEC/conventions.xml</specification>
      <relationship>Follows project testing conventions</relationship>
      <integration>Generated tests comply with established patterns</integration>
    </reference>
    
    <reference id="XR5">
      <specification>SPEC/ai_slop_prevention_spec.xml</specification>
      <relationship>Ensures generated tests avoid AI anti-patterns</relationship>
      <integration>Test generation follows quality guidelines</integration>
    </reference>
    
    <reference id="XR6">
      <specification>AI Agent File Metadata Tracking</specification>
      <relationship>Provides comprehensive audit trail for all AI-modified files including tests</relationship>
      <integration>All AI-generated or modified tests include metadata headers for tracking</integration>
      <benefits>
        - Complete traceability of AI-generated tests
        - Rollback capability for problematic test changes
        - Quality metrics tracking for test improvements
        - Audit compliance for regulatory requirements
      </benefits>
    </reference>
  </cross_references>
</test_update_specification>