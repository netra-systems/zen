<?xml version="1.0" encoding="UTF-8"?>
<specification>
    <metadata>
        <name>Testing Specification</name>
        <type>testing</type>
        <version>2.1</version>
        <last_updated>2025-08-11</last_updated>
    </metadata>
    
    <overview>
        <description>
            Comprehensive testing strategy for the Netra AI Optimization Platform.
            Targets 97% code coverage with focus on E2E tests, quality gates, and real LLM validation.
        </description>
        <goals>
            <goal>Maintain 97% overall test coverage</goal>
            <goal>E2E tests with real LLM calls for validation</goal>
            <goal>Quality gate integration for output validation</goal>
            <goal>Synthetic data generation for consistent testing</goal>
            <goal>Fast feedback loops with quick test mode</goal>
        </goals>
    </overview>
    
    <test_categories>
        <category name="unit">
            <description>Fast, isolated tests for individual components</description>
            <location>app/tests/services/*, app/tests/core/*</location>
            <coverage_target>98%</coverage_target>
        </category>
        
        <category name="integration">
            <description>Tests for component interactions</description>
            <location>app/tests/integration/*</location>
            <coverage_target>95%</coverage_target>
        </category>
        
        <category name="e2e">
            <description>End-to-end tests with real services</description>
            <location>app/tests/agents/test_*_e2e_*.py</location>
            <coverage_target>90%</coverage_target>
        </category>
        
        <category name="critical">
            <description>Business-critical path tests</description>
            <location>app/tests/*_critical.py</location>
            <coverage_target>100%</coverage_target>
        </category>
    </test_categories>
    
    <unified_test_runner>
        <description>Single entry point for all Netra AI Platform testing with automatic report generation</description>
        <path>test_runner.py</path>
        
        <test_levels>
            <level name="smoke" recommended="pre_commit">
                <description>Quick smoke tests for basic functionality (< 30 seconds)</description>
                <purpose>Pre-commit validation, basic health checks</purpose>
                <command>python test_runner.py --level smoke</command>
                <timeout>30</timeout>
                <coverage>false</coverage>
                <components>backend+frontend</components>
            </level>
            
            <level name="unit">
                <description>Unit tests for isolated components (1-2 minutes)</description>
                <purpose>Development validation, component testing</purpose>
                <command>python test_runner.py --level unit</command>
                <timeout>120</timeout>
                <coverage>false</coverage>
                <components>backend+frontend</components>
            </level>
            
            <level name="integration">
                <description>Integration tests for component interaction (3-5 minutes)</description>
                <purpose>Feature validation, API testing</purpose>
                <command>python test_runner.py --level integration</command>
                <timeout>300</timeout>
                <coverage>false</coverage>
                <components>backend+frontend</components>
            </level>
            
            <level name="comprehensive">
                <description>Full test suite with coverage (10-15 minutes)</description>
                <purpose>Pre-release validation, full system testing</purpose>
                <command>python test_runner.py --level comprehensive</command>
                <timeout>900</timeout>
                <coverage>true</coverage>
                <components>backend+frontend</components>
            </level>
            
            <level name="critical">
                <description>Critical path tests only (1-2 minutes)</description>
                <purpose>Essential functionality verification</purpose>
                <command>python test_runner.py --level critical</command>
                <timeout>120</timeout>
                <coverage>false</coverage>
                <components>backend_only</components>
            </level>
        </test_levels>
        
        <fallback_runners>
            <runner name="simple_fallback">
                <description>Simple fallback for when main runner has issues</description>
                <command>python test_runner.py --simple</command>
                <recommended_for>Troubleshooting test infrastructure issues</recommended_for>
            </runner>
            
            <runner name="direct_pytest">
                <description>Direct pytest for specific test debugging</description>
                <command>python -m pytest app/tests/test_simple_health.py -v</command>
                <recommended_for>Individual test file debugging</recommended_for>
            </runner>
        </fallback_runners>
        
        <report_generation>
            <description>All tests automatically save reports to test_reports/ folder</description>
            <formats>
                <format>JSON (machine readable)</format>
                <format>Markdown (human readable)</format>
                <format>Latest report (overwrites for quick access)</format>
            </formats>
            <location>test_reports/</location>
            <naming_convention>test_report_{level}_{timestamp}.{format}</naming_convention>
        </report_generation>
    </unified_test_runner>
    
    <working_test_files>
        <test_file name="test_simple_health.py">
            <path>app/tests/test_simple_health.py</path>
            <description>Basic tests that always pass, good for validation</description>
            <test_count>5</test_count>
            <reliability>100%</reliability>
        </test_file>
        
        <test_file name="test_working_health.py">
            <path>app/tests/test_working_health.py</path>
            <description>Tests app imports and configuration without hanging</description>
            <test_count>5</test_count>
            <reliability>100%</reliability>
        </test_file>
    </working_test_files>
    
    <e2e_real_llm_tests>
        <description>
            Comprehensive E2E tests making real LLM calls for the 9 example prompts.
            Each prompt tested with 10 unique variations for total of 90 tests.
        </description>
        
        <test_file>app/tests/agents/test_example_prompts_e2e_real.py</test_file>
        
        <features>
            <feature>Real LLM calls through complete agent workflow</feature>
            <feature>Synthetic data generation for consistent context</feature>
            <feature>Quality gate validation for all responses</feature>
            <feature>10 unique variations per example prompt</feature>
            <feature>Full sub-agent orchestration testing</feature>
            <feature>Corpus generation when needed</feature>
            <feature>Performance metrics collection</feature>
        </features>
        
        <example_prompts_tested>
            <prompt id="1" category="cost_optimization">Cost reduction with quality preservation</prompt>
            <prompt id="2" category="latency_optimization">3x latency improvement without cost increase</prompt>
            <prompt id="3" category="capacity_planning">50% usage increase impact analysis</prompt>
            <prompt id="4" category="function_optimization">Advanced function optimization methods</prompt>
            <prompt id="5" category="model_selection">New model effectiveness evaluation</prompt>
            <prompt id="6" category="audit">KV cache audit for optimizations</prompt>
            <prompt id="7" category="multi_objective">20% cost reduction + 2x latency improvement</prompt>
            <prompt id="8" category="tool_migration">GPT-5 migration recommendations</prompt>
            <prompt id="9" category="rollback_analysis">Upgrade worth analysis and rollback</prompt>
        </example_prompts_tested>
        
        <variation_types>
            <variation num="0">Original prompt unchanged</variation>
            <variation num="1">Add budget context</variation>
            <variation num="2">Mark as urgent (24 hour deadline)</variation>
            <variation num="3">Include GPU infrastructure details</variation>
            <variation num="4">Change to team perspective (I -> our)</variation>
            <variation num="5">Add regional context</variation>
            <variation num="6">Include error rate constraints</variation>
            <variation num="7">Use CAPS for urgency</variation>
            <variation num="8">Frame as follow-up</variation>
            <variation num="9">Include GPU count information</variation>
        </variation_types>
        
        <synthetic_data_contexts>
            <context type="cost_optimization">
                <field>current_costs (daily/monthly/per_request)</field>
                <field>feature latencies and requirements</field>
                <field>models in use</field>
                <field>daily request volume</field>
            </context>
            <context type="latency_optimization">
                <field>current latency percentiles (p50/p95/p99)</field>
                <field>target improvement factor</field>
                <field>infrastructure specs (GPU type/count/memory)</field>
            </context>
            <context type="capacity_planning">
                <field>current usage metrics (RPS/daily requests)</field>
                <field>expected growth percentage</field>
                <field>rate limits per model</field>
                <field>cost per token metrics</field>
            </context>
            <context type="function_optimization">
                <field>function performance metrics</field>
                <field>identified bottlenecks</field>
                <field>available optimization methods</field>
            </context>
            <context type="model_selection">
                <field>current model configuration</field>
                <field>candidate models for evaluation</field>
                <field>evaluation criteria and thresholds</field>
                <field>workload characteristics</field>
            </context>
            <context type="audit">
                <field>KV cache instance count</field>
                <field>cache configurations (size/hit rate/TTL)</field>
                <field>optimization opportunities</field>
            </context>
            <context type="multi_objective">
                <field>multiple optimization objectives</field>
                <field>constraints (quality/error rate/budget)</field>
                <field>current system state</field>
            </context>
            <context type="tool_migration">
                <field>agent tool inventory</field>
                <field>migration criteria</field>
                <field>verbosity options</field>
            </context>
            <context type="rollback_analysis">
                <field>upgrade timestamp</field>
                <field>before/after metrics comparison</field>
                <field>affected endpoint count</field>
            </context>
        </synthetic_data_contexts>
        
        <quality_validation>
            <description>All responses validated through QualityGateService</description>
            <acceptance_levels>
                <level>EXCELLENT</level>
                <level>GOOD</level>
                <level>ACCEPTABLE</level>
            </acceptance_levels>
            <content_types>
                <type>OPTIMIZATION</type>
                <type>DATA_ANALYSIS</type>
                <type>ACTION_PLAN</type>
            </content_types>
            <validation_criteria>
                <criterion>Specificity score >= 0.6</criterion>
                <criterion>Actionability score >= 0.6</criterion>
                <criterion>No circular reasoning</criterion>
                <criterion>Minimal generic phrases</criterion>
                <criterion>Quantification where appropriate</criterion>
            </validation_criteria>
        </quality_validation>
    </e2e_real_llm_tests>
    
    <unified_commands>
        <command name="pre_commit" description="Quick pre-commit validation">
            python test_runner.py --level smoke
        </command>
        <command name="development" description="Unit tests during development">
            python test_runner.py --level unit
        </command>
        <command name="feature_validation" description="Integration testing">
            python test_runner.py --level integration
        </command>
        <command name="pre_release" description="Full comprehensive testing">
            python test_runner.py --level comprehensive
        </command>
        <command name="critical_only" description="Essential functionality only">
            python test_runner.py --level critical
        </command>
        <command name="backend_only" description="Backend tests only">
            python test_runner.py --level unit --backend-only
        </command>
        <command name="frontend_only" description="Frontend tests only">
            python test_runner.py --level unit --frontend-only
        </command>
        <command name="simple_fallback" description="When main runner has issues">
            python test_runner.py --simple
        </command>
        <command name="e2e_real" description="Real LLM E2E tests (direct pytest)">
            pytest app/tests/agents/test_example_prompts_e2e_real.py -v -s
        </command>
    </unified_commands>
    
    <troubleshooting>
        <issue>
            <problem>Import test failures</problem>
            <solution>Update app/tests/test_internal_imports.py and test_external_imports.py</solution>
        </issue>
        <issue>
            <problem>WebSocket test failures</problem>
            <solution>Ensure WebSocketProvider wraps test components</solution>
        </issue>
        <issue>
            <problem>ClickHouse test errors</problem>
            <solution>Check SPEC/clickhouse.xml for array handling patterns</solution>
        </issue>
        <issue>
            <problem>E2E tests timeout</problem>
            <solution>Increase pytest timeout: pytest --timeout=300</solution>
        </issue>
        <issue>
            <problem>LLM API rate limits</problem>
            <solution>Add delays between tests or use retry logic</solution>
        </issue>
    </troubleshooting>
    
    <best_practices>
        <practice>Always run quick tests before commits</practice>
        <practice>Use mocks for unit tests, real services for E2E</practice>
        <practice>Maintain test isolation - no shared state</practice>
        <practice>Update tests when modifying functionality</practice>
        <practice>Document non-obvious test behaviors</practice>
        <practice>Use fixtures for common test setup</practice>
        <practice>Validate both success and failure paths</practice>
        <practice>Test edge cases and boundary conditions</practice>
    </best_practices>
    
    <coverage_requirements>
        <requirement>
            <target>97%</target>
            <scope>Overall codebase</scope>
            <excludes>__pycache__, migrations, test files</excludes>
        </requirement>
        <requirement>
            <target>100%</target>
            <scope>Critical business logic</scope>
            <includes>supervisor agent, auth, websocket handlers</includes>
        </requirement>
        <requirement>
            <target>90%</target>
            <scope>API endpoints</scope>
            <includes>all routes/*</includes>
        </requirement>
    </coverage_requirements>
</specification>