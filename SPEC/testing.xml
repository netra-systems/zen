<?xml version="1.0" encoding="UTF-8"?>
<specification>
    <metadata>
        <name>Testing Specification</name>
        <type>testing</type>
        <version>3.2</version>
        <last_updated>2025-08-16</last_updated>
        <changes>
            <change version="3.0" date="2025-08-11">
                - Coverage generation for unit level and above
                - Restructured report format with test counts at top
                - Removed per-run .md/.json files
                - Added history folder for archiving
                - Enhanced latest report structure
            </change>
            <change version="3.1" date="2025-08-11">
                - Added real LLM testing capabilities
                - Introduced --real-llm option for test runner
                - Added dedicated test environments configuration
                - Defined seed data management for real LLM tests
            </change>
            <change version="3.2" date="2025-08-16">
                - Added frontend parallel testing architecture
                - Introduced test suite organization with priorities and weights
                - Created custom Jest configuration for parallel execution
                - Added intelligent test suite runner with resource management
                - Defined 11 test suites with optimized parallel execution strategy
            </change>
        </changes>
    </metadata>
    
    <overview>
        <description>
            Comprehensive testing strategy for the Netra AI Optimization Platform.
            Targets 97% code coverage with focus on E2E tests, quality gates, and real LLM validation.
            Includes automatic bad test detection to identify and manage consistently failing tests.
        </description>
        <goals>
            <goal>Maintain 97% overall test coverage</goal>
            <goal>Fixing tests must fix the actual spirit of the test, Fixing the SUT (System under test) and if needed the test iteself.</goal>
            <goal>Tests that attempt to test SUT that doesn't actually exist must be removed</goal>
            <goal>E2E tests with real LLM calls for validation</goal>
            <goal>Quality gate integration for output validation</goal>
            <goal>Synthetic data generation for consistent testing</goal>
            <goal>Fast feedback loops with quick test mode</goal>
            <goal>When fixing tests align tests with latest code. Update tests as needed to test real latest code.</goal>
            <goal>Automatically detect and report bad tests (consistently failing or high failure rate)</goal>
            <goal>Track test failure patterns to identify tests needing deletion or major refactoring</goal>
        </goals>
    </overview>
    
    <real_test_requirements>
        <description>
            CRITICAL: Tests must test REAL functionality, not mocks of their own creation.
            Based on frontend test paradox analysis (2025-08-19).
        </description>
        
        <mandatory_rules>
            <rule id="no-mock-components">
                Tests MUST NOT include their own mock component implementations.
                Test the real component or don't test at all.
            </rule>
            
            <rule id="real-child-components">
                Integration tests MUST use real child components, not mock divs.
                Mocking all children defeats the purpose of integration testing.
            </rule>
            
            <rule id="test-size-limits">
                Test files MUST follow same 300-line limit as production code.
                Test functions MUST follow same 8-line limit as production code.
            </rule>
            
            <rule id="minimal-mocking">
                Mock ONLY truly unavailable resources.
                WebSocket, state management, and UI components should be real.
            </rule>
            
            <rule id="mock-justification-required">
                ALL mocks MUST be justified with explicit documentation.
                A mock without justification is a violation.
                Use @mock_justified decorator or inline comment with rationale.
                Example for unit test: @mock_justified("Because of the scope of this being a unit test of {insert more info}, the 
                   api {more info} is not part of the SUT and therefore acceptable to be mocked.")
                Example for E2E tests: @mock_justified("This subsystem is very far from SUT yet required for one small sub item")
            </rule>
            
            <rule id="fix-sut-not-tests">
                When tests fail, fix the System Under Test first.
                Only modify tests if the requirements genuinely changed.
            </rule>
        </mandatory_rules>
        
        <test_pyramid>
            <level name="unit" percentage="20">
                Test individual functions with minimal mocking
            </level>
            <level name="integration" percentage="60">
                Test real component interactions with real child components
            </level>
            <level name="e2e" percentage="20">
                Test complete user flows with real backend
            </level>
        </test_pyramid>
        
        <anti_patterns>
            <anti_pattern>Creating mock components inside test files</anti_pattern>
            <anti_pattern>Mocking all child components in integration tests</anti_pattern>
            <anti_pattern>Using jest.fn() for everything</anti_pattern>
            <anti_pattern>Modifying tests to pass instead of fixing bugs</anti_pattern>
            <anti_pattern>Tests longer than 300 lines</anti_pattern>
            <anti_pattern>Test functions longer than 8 lines</anti_pattern>
            <anti_pattern>Mocks without explicit justification</anti_pattern>
            <anti_pattern>Using @patch or Mock() without @mock_justified decorator</anti_pattern>
        </anti_patterns>
    </real_test_requirements>
    
    <test_categories>
        <category name="unit">
            <description>Fast, isolated tests for individual components</description>
            <location>app/tests/services/*, app/tests/core/*</location>
            <coverage_target>98%</coverage_target>
        </category>
        
        <category name="integration">
            <description>Tests for component interactions</description>
            <location>app/tests/integration/*</location>
            <coverage_target>95%</coverage_target>
        </category>
        
        <category name="e2e">
            <description>End-to-end tests with real services</description>
            <location>app/tests/agents/test_*_e2e_*.py</location>
            <coverage_target>90%</coverage_target>
        </category>
        
        <category name="critical">
            <description>Business-critical path tests</description>
            <location>app/tests/*_critical.py</location>
            <coverage_target>100%</coverage_target>
        </category>
        
        <category name="tdd">
            <description>Test-driven development tests for features in development</description>
            <location>Tests marked with @tdd_test decorator</location>
            <coverage_target>Skipped until feature enabled</coverage_target>
            <note>Enables TDD workflow while maintaining 100% CI/CD pass rate</note>
        </category>
        
        <category name="feature_flagged">
            <description>Tests controlled by feature flags based on development status</description>
            <location>Tests marked with @feature_flag decorator</location>
            <coverage_target>Based on feature status</coverage_target>
            <note>Provides clear visibility of feature readiness and dependencies</note>
        </category>
    </test_categories>
    
    <unified_test_runner>
        <description>Single entry point for all Netra AI Platform testing with automatic report generation</description>
        <path>test_runner.py</path>
        
        <github_actions_integration>
            <description>Test runner is integrated with GitHub Actions workflows for CI/CD</description>
            <reference>
                <workflow>.github/workflows/test-runner.yml</workflow>
                <description>Main test execution workflow</description>
            </reference>
            <reference>
                <workflow>.github/workflows/test-standard-template.yml</workflow>
                <description>Template for creating new test workflows</description>
            </reference>
            <reference>
                <workflow>.github/workflows/test-runner-validation.yml</workflow>
                <description>Validates test runner exit code propagation</description>
            </reference>
            <reference>
                <documentation>.github/workflows/WORKFLOW_FIX_SUMMARY.md</documentation>
                <description>Complete guide to workflow fixes and best practices</description>
            </reference>
            <exit_code_handling>
                <rule>Test runner returns non-zero exit codes on failure</rule>
                <rule>Workflows must use || exit $? to propagate failures</rule>
                <rule>Never use continue-on-error in test steps</rule>
            </exit_code_handling>
        </github_actions_integration>
        
        <test_levels>
            <level name="smoke" recommended="pre_commit">
                <description>Quick smoke tests for basic functionality (< 30 seconds)</description>
                <purpose>Pre-commit validation, basic health checks</purpose>
                <command>python test_runner.py --level smoke</command>
                <timeout>30</timeout>
                <coverage>false</coverage>
                <components>backend+frontend</components>
                <ci_flags>--no-coverage --fast-fail --ci --no-warnings</ci_flags>
            </level>
            
            <level name="unit">
                <description>Unit tests for isolated components (1-2 minutes)</description>
                <purpose>Development validation, component testing</purpose>
                <command>python test_runner.py --level unit</command>
                <timeout>120</timeout>
                <coverage>true</coverage>
                <components>backend+frontend</components>
                <note>Coverage summary generated for unit level and above</note>
                <ci_flags>--no-coverage --fast-fail --ci --no-warnings</ci_flags>
            </level>
            
            <level name="integration">
                <description>Integration tests for component interaction (3-5 minutes)</description>
                <purpose>Feature validation, API testing</purpose>
                <command>python test_runner.py --level integration</command>
                <timeout>300</timeout>
                <coverage>true</coverage>
                <components>backend+frontend</components>
                <note>Coverage summary generated for unit level and above</note>
                <ci_flags>--no-coverage --fast-fail --ci --no-warnings</ci_flags>
            </level>
            
            <level name="comprehensive">
                <description>Full test suite with coverage (30-45 minutes)</description>
                <purpose>Pre-release validation, full system testing</purpose>
                <command>python test_runner.py --level comprehensive</command>
                <timeout>2700</timeout>
                <coverage>true</coverage>
                <components>backend+frontend</components>
                <note>Complete validation when thoroughness is essential. Use modular comprehensive levels for faster focused testing</note>
            </level>
            
            <level name="comprehensive-backend">
                <description>Comprehensive backend tests only (15-20 minutes)</description>
                <purpose>Full backend validation without frontend</purpose>
                <command>python test_runner.py --level comprehensive-backend</command>
                <timeout>1200</timeout>
                <coverage>true</coverage>
                <components>backend_only</components>
                <note>Optimal for backend-focused development cycles</note>
            </level>
            
            <level name="comprehensive-frontend">
                <description>Comprehensive frontend tests only (10-15 minutes)</description>
                <purpose>Full frontend validation without backend</purpose>
                <command>python test_runner.py --level comprehensive-frontend</command>
                <timeout>900</timeout>
                <coverage>true</coverage>
                <components>frontend_only</components>
                <note>Optimal for UI/UX development cycles</note>
            </level>
            
            <level name="comprehensive-core">
                <description>Core functionality comprehensive tests (10-15 minutes)</description>
                <purpose>Deep validation of core components only</purpose>
                <command>python test_runner.py --level comprehensive-core</command>
                <timeout>900</timeout>
                <coverage>true</coverage>
                <components>backend_only</components>
                <note>Validates fundamental system components thoroughly</note>
            </level>
            
            <level name="comprehensive-agents">
                <description>Agent system comprehensive tests (10-15 minutes)</description>
                <purpose>Deep validation of multi-agent system</purpose>
                <command>python test_runner.py --level comprehensive-agents</command>
                <timeout>900</timeout>
                <coverage>true</coverage>
                <components>backend_only</components>
                <note>Ensures agent coordination and responses meet quality standards</note>
            </level>
            
            <level name="comprehensive-websocket">
                <description>WebSocket comprehensive tests (5-10 minutes)</description>
                <purpose>Deep validation of WebSocket functionality</purpose>
                <command>python test_runner.py --level comprehensive-websocket</command>
                <timeout>600</timeout>
                <coverage>true</coverage>
                <components>backend_only</components>
                <note>Validates real-time communication thoroughly</note>
            </level>
            
            <level name="comprehensive-database">
                <description>Database comprehensive tests (10-15 minutes)</description>
                <purpose>Deep validation of all database operations</purpose>
                <command>python test_runner.py --level comprehensive-database</command>
                <timeout>900</timeout>
                <coverage>true</coverage>
                <components>backend_only</components>
                <note>Ensures data integrity and repository patterns work correctly</note>
            </level>
            
            <level name="comprehensive-api">
                <description>API comprehensive tests (10-15 minutes)</description>
                <purpose>Deep validation of all API endpoints</purpose>
                <command>python test_runner.py --level comprehensive-api</command>
                <timeout>900</timeout>
                <coverage>true</coverage>
                <components>backend_only</components>
                <note>Validates all API routes and response contracts</note>
            </level>
            
            <level name="critical">
                <description>Critical path tests only (1-2 minutes)</description>
                <purpose>Essential functionality verification</purpose>
                <command>python test_runner.py --level critical</command>
                <timeout>120</timeout>
                <coverage>true</coverage>
                <components>backend_only</components>
                <note>Coverage summary generated for unit level and above</note>
            </level>
        </test_levels>
        
        <fallback_runners>
            <runner name="simple_fallback">
                <description>Simple fallback for when main runner has issues</description>
                <command>python test_runner.py --simple</command>
                <recommended_for>Troubleshooting test infrastructure issues</recommended_for>
            </runner>
            
            <runner name="direct_pytest">
                <description>Direct pytest for specific test debugging</description>
                <command>python -m pytest app/tests/test_simple_health.py -v</command>
                <recommended_for>Individual test file debugging</recommended_for>
            </runner>
        </fallback_runners>
        
        <report_generation>
            <description>Unified test report generation with history tracking</description>
            
            <latest_report>
                <description>Always-current report that overwrites on each run</description>
                <location>test_reports/latest_{level}_report.md</location>
                <structure>
                    <section order="1">Test Summary (total/passed/failed/skipped counts)</section>
                    <section order="2">Coverage Summary (if level >= unit)</section>
                    <section order="3">Environment Variables and Configuration</section>
                    <section order="4">Test Output Details</section>
                    <section order="5">Error Summary (if any failures)</section>
                </structure>
                <required_metrics>
                    <metric>Total test count</metric>
                    <metric>Passed test count</metric>
                    <metric>Failed test count</metric>
                    <metric>Skipped test count</metric>
                    <metric>Test duration</metric>
                    <metric>Coverage percentage (if applicable)</metric>
                </required_metrics>
            </latest_report>
            
            <history_tracking>
                <description>Previous test reports archived for trend analysis</description>
                <location>test_reports/history/</location>
                <naming>test_report_{level}_{timestamp}.md</naming>
                <process>
                    <step>When running tests, check if latest report exists</step>
                    <step>If exists, move to history folder with timestamp</step>
                    <step>Generate new latest report in main folder</step>
                </process>
            </history_tracking>
            
            <deprecated_formats>
                <format status="removed">Per-run JSON reports (test_report_{level}_{timestamp}.json)</format>
                <format status="removed">Per-run MD reports (test_report_{level}_{timestamp}.md)</format>
                <reason>Eliminated redundancy - history folder serves archival needs</reason>
            </deprecated_formats>
            
            <coverage_requirements>
                <requirement>Coverage summary MUST be generated for unit level and above</requirement>
                <requirement>Coverage data included in report header for visibility</requirement>
                <requirement>Branch coverage and line coverage both reported</requirement>
            </coverage_requirements>
        </report_generation>
    </unified_test_runner>
    
    <frontend_parallel_testing>
        <description>
            Frontend test suite organization for optimal parallel execution.
            Tests are categorized into suites with priority levels and resource weights
            to maximize parallelization efficiency while preventing resource contention.
        </description>
        
        <configuration_files>
            <file name="jest.config.suites.cjs">
                <description>Jest configuration with parallel test suite projects</description>
                <location>frontend/jest.config.suites.cjs</location>
            </file>
            <file name="test-suite-runner.js">
                <description>Intelligent test suite runner with priority-based execution</description>
                <location>frontend/test-suite-runner.js</location>
            </file>
            <file name="jest-suite-reporter.js">
                <description>Custom reporter for test suite summary and metrics</description>
                <location>frontend/jest-suite-reporter.js</location>
            </file>
        </configuration_files>
        
        <test_suites>
            <suite name="components" priority="1" weight="0.8" timeout="5">
                <description>Component unit tests - fast and isolated</description>
                <pattern>**/__tests__/components/**/!(*.integration|*.e2e).test.[jt]s?(x)</pattern>
            </suite>
            
            <suite name="chat" priority="2" weight="0.8" timeout="8">
                <description>Chat-related component tests</description>
                <pattern>**/__tests__/components/chat/**/*.test.[jt]s?(x)</pattern>
                <pattern>**/__tests__/chat/**/*.test.[jt]s?(x)</pattern>
            </suite>
            
            <suite name="hooks" priority="1" weight="0.6" timeout="5">
                <description>React hooks unit tests</description>
                <pattern>**/__tests__/hooks/**/*.test.[jt]s?(x)</pattern>
            </suite>
            
            <suite name="auth" priority="2" weight="0.4" timeout="8">
                <description>Authentication flow tests</description>
                <pattern>**/__tests__/auth/**/*.test.[jt]s?(x)</pattern>
            </suite>
            
            <suite name="integration-basic" priority="3" weight="0.5" timeout="15">
                <description>Basic integration tests</description>
                <pattern>**/__tests__/integration/basic-*.test.[jt]s?(x)</pattern>
            </suite>
            
            <suite name="integration-advanced" priority="3" weight="0.6" timeout="20">
                <description>Advanced integration scenarios</description>
                <pattern>**/__tests__/integration/advanced-integration/**/*.test.[jt]s?(x)</pattern>
            </suite>
            
            <suite name="integration-comprehensive" priority="4" weight="0.8" timeout="25">
                <description>Comprehensive integration tests - heaviest</description>
                <pattern>**/__tests__/integration/comprehensive/**/*.test.[jt]s?(x)</pattern>
            </suite>
            
            <suite name="integration-critical" priority="4" weight="0.8" timeout="20">
                <description>Critical path integration tests</description>
                <pattern>**/__tests__/integration/critical/**/*.test.[jt]s?(x)</pattern>
            </suite>
            
            <suite name="system" priority="3" weight="0.4" timeout="15">
                <description>System-level tests</description>
                <pattern>**/__tests__/system/**/*.test.[jt]s?(x)</pattern>
            </suite>
            
            <suite name="imports" priority="1" weight="0.2" timeout="5">
                <description>Import verification tests - run sequentially</description>
                <pattern>**/__tests__/imports/**/*.test.[jt]s?(x)</pattern>
                <max_workers>1</max_workers>
            </suite>
            
            <suite name="core" priority="2" weight="0.4" timeout="10">
                <description>Core unified tests</description>
                <pattern>**/__tests__/unified-*.test.[jt]s?(x)</pattern>
            </suite>
        </test_suites>
        
        <execution_strategy>
            <principle>Priority-based execution with intelligent resource allocation</principle>
            <steps>
                <step order="1">Group tests by priority level (1-4)</step>
                <step order="2">Calculate parallel slots based on suite weights</step>
                <step order="3">Execute priority groups sequentially</step>
                <step order="4">Within each priority, run compatible suites in parallel</step>
                <step order="5">Monitor and enforce suite timeouts</step>
                <step order="6">Aggregate results with custom reporter</step>
            </steps>
            
            <resource_management>
                <cpu_usage>75% of available cores by default</cpu_usage>
                <weight_system>Weights determine parallel slot allocation</weight_system>
                <chunking>Compatible tests grouped for parallel execution</chunking>
            </resource_management>
        </execution_strategy>
        
        <npm_scripts>
            <script name="test:suites">Run all test suites in parallel</script>
            <script name="test:suites:components">Run component tests only</script>
            <script name="test:suites:chat">Run chat tests only</script>
            <script name="test:suites:hooks">Run hooks tests only</script>
            <script name="test:suites:auth">Run auth tests only</script>
            <script name="test:suites:integration">Run all integration tests</script>
            <script name="test:suites:fast">Run priority 1 tests only</script>
            <script name="test:suites:medium">Run priority 1-2 tests</script>
            <script name="test:suites:slow">Run all tests including heavy integration</script>
            <script name="test:suites:parallel">Run with max 8 parallel suites</script>
            <script name="test:suites:sequential">Run all suites sequentially</script>
            <script name="test:suites:watch">Watch mode for development</script>
            <script name="test:suites:coverage">Generate coverage report</script>
        </npm_scripts>
        
        <benefits>
            <benefit>Reduced test execution time through intelligent parallelization</benefit>
            <benefit>Granular control over test execution</benefit>
            <benefit>Priority-based execution for faster feedback</benefit>
            <benefit>Resource-aware parallel execution prevents system overload</benefit>
            <benefit>Detailed suite-level reporting and metrics</benefit>
            <benefit>Easy CI/CD integration with configurable parallelization</benefit>
        </benefits>
    </frontend_parallel_testing>
    
    <bad_test_detection>
        <description>
            Automatic detection and tracking of consistently failing tests to identify
            tests that need immediate attention, refactoring, or deletion.
        </description>
        
        <features>
            <feature>Automatic tracking of test failures across runs</feature>
            <feature>Persistent storage of failure history in test_reports/bad_tests.json</feature>
            <feature>Smart detection algorithms for identifying problematic tests</feature>
            <feature>Detailed reporting with recommendations</feature>
            <feature>Integration with pytest through custom plugin</feature>
            <feature>CLI tool for viewing and managing bad test data</feature>
        </features>
        
        <detection_criteria>
            <criterion name="consistently_failing">
                <threshold>5+ consecutive failures</threshold>
                <action>Recommend immediate fix</action>
                <priority>HIGH</priority>
            </criterion>
            <criterion name="high_failure_rate">
                <threshold>70% failure rate with 10+ total runs</threshold>
                <action>Consider refactoring</action>
                <priority>MEDIUM</priority>
            </criterion>
            <criterion name="very_high_failure_rate">
                <threshold>90% failure rate</threshold>
                <action>Recommend deletion or complete rewrite</action>
                <priority>CRITICAL</priority>
            </criterion>
        </detection_criteria>
        
        <components>
            <component name="BadTestDetector" path="test_framework/bad_test_detector.py">
                <description>Core detection engine that tracks and analyzes test failures</description>
                <responsibilities>
                    <responsibility>Track test results (pass/fail) with error details</responsibility>
                    <responsibility>Maintain persistent storage of failure history</responsibility>
                    <responsibility>Identify bad tests based on criteria</responsibility>
                    <responsibility>Generate reports and statistics</responsibility>
                </responsibilities>
            </component>
            
            <component name="PytestBadTestPlugin" path="test_framework/pytest_bad_test_plugin.py">
                <description>Pytest plugin for automatic integration</description>
                <responsibilities>
                    <responsibility>Hook into pytest test execution</responsibility>
                    <responsibility>Record test outcomes automatically</responsibility>
                    <responsibility>Mark bad tests with pytest markers</responsibility>
                    <responsibility>Display warnings for bad tests</responsibility>
                </responsibilities>
            </component>
            
            <component name="BadTestReporter" path="test_framework/bad_test_reporter.py">
                <description>CLI tool for viewing and managing bad test reports</description>
                <commands>
                    <command>python -m test_framework.bad_test_reporter</command>
                    <command>python -m test_framework.bad_test_reporter --summary</command>
                    <command>python -m test_framework.bad_test_reporter --details</command>
                    <command>python -m test_framework.bad_test_reporter --test "test_name"</command>
                    <command>python -m test_framework.bad_test_reporter --reset</command>
                    <command>python -m test_framework.bad_test_reporter --export report.json</command>
                </commands>
            </component>
        </components>
        
        <usage>
            <note>Bad test detection is enabled by default for all test runs</note>
            <disable_command>python scripts/test_backend.py --no-bad-test-detection</disable_command>
            <view_report>python -m test_framework.bad_test_reporter</view_report>
            <reset_data>python -m test_framework.bad_test_reporter --reset</reset_data>
        </usage>
        
        <data_structure>
            <file>test_reports/bad_tests.json</file>
            <format>
                {
                    "version": "1.0",
                    "tests": {
                        "test_name": {
                            "component": "backend|frontend|e2e",
                            "first_seen": "ISO timestamp",
                            "last_failure": "ISO timestamp",
                            "last_pass": "ISO timestamp",
                            "total_failures": "integer",
                            "total_passes": "integer",
                            "consecutive_failures": "integer",
                            "recent_failures": ["array of last 10 failure details"],
                            "marked_as_bad": "boolean",
                            "bad_reason": "string"
                        }
                    },
                    "runs": ["array of run statistics"],
                    "stats": {
                        "total_runs": "integer",
                        "last_updated": "ISO timestamp"
                    }
                }
            </format>
        </data_structure>
        
        <integration_points>
            <integration>test_framework/runner.py - Main test runner integration</integration>
            <integration>test_framework/test_runners.py - Backend test execution</integration>
            <integration>scripts/test_backend.py - Command-line argument support</integration>
        </integration_points>
    </bad_test_detection>
    
    <working_test_files>
        <test_file name="test_simple_health.py">
            <path>app/tests/test_simple_health.py</path>
            <description>Basic tests that always pass, good for validation</description>
            <test_count>5</test_count>
            <reliability>100%</reliability>
        </test_file>
        
        <test_file name="test_working_health.py">
            <path>app/tests/test_working_health.py</path>
            <description>Tests app imports and configuration without hanging</description>
            <test_count>5</test_count>
            <reliability>100%</reliability>
        </test_file>
    </working_test_files>
    
    <e2e_real_llm_tests>
        <description>
            Comprehensive E2E tests making real LLM calls for the 9 example prompts.
            Each prompt tested with 10 unique variations for total of 90 tests.
        </description>
        
        <test_file>app/tests/agents/test_example_prompts_e2e_real.py</test_file>
        
        <features>
            <feature>Real LLM calls through complete agent workflow</feature>
            <feature>Synthetic data generation for consistent context</feature>
            <feature>Quality gate validation for all responses</feature>
            <feature>10 unique variations per example prompt</feature>
            <feature>Full sub-agent orchestration testing</feature>
            <feature>Corpus generation when needed</feature>
            <feature>Performance metrics collection</feature>
        </features>
        
        <example_prompts_tested>
            <prompt id="1" category="cost_optimization">Cost reduction with quality preservation</prompt>
            <prompt id="2" category="latency_optimization">3x latency improvement without cost increase</prompt>
            <prompt id="3" category="capacity_planning">50% usage increase impact analysis</prompt>
            <prompt id="4" category="function_optimization">Advanced function optimization methods</prompt>
            <prompt id="5" category="model_selection">New model effectiveness evaluation</prompt>
            <prompt id="6" category="audit">KV cache audit for optimizations</prompt>
            <prompt id="7" category="multi_objective">20% cost reduction + 2x latency improvement</prompt>
            <prompt id="8" category="tool_migration">GPT-5 migration recommendations</prompt>
            <prompt id="9" category="rollback_analysis">Upgrade worth analysis and rollback</prompt>
        </example_prompts_tested>
        
        <variation_types>
            <variation num="0">Original prompt unchanged</variation>
            <variation num="1">Add budget context</variation>
            <variation num="2">Mark as urgent (24 hour deadline)</variation>
            <variation num="3">Include GPU infrastructure details</variation>
            <variation num="4">Change to team perspective (I -> our)</variation>
            <variation num="5">Add regional context</variation>
            <variation num="6">Include error rate constraints</variation>
            <variation num="7">Use CAPS for urgency</variation>
            <variation num="8">Frame as follow-up</variation>
            <variation num="9">Include GPU count information</variation>
        </variation_types>
        
        <synthetic_data_contexts>
            <context type="cost_optimization">
                <field>current_costs (daily/monthly/per_request)</field>
                <field>feature latencies and requirements</field>
                <field>models in use</field>
                <field>daily request volume</field>
            </context>
            <context type="latency_optimization">
                <field>current latency percentiles (p50/p95/p99)</field>
                <field>target improvement factor</field>
                <field>infrastructure specs (GPU type/count/memory)</field>
            </context>
            <context type="capacity_planning">
                <field>current usage metrics (RPS/daily requests)</field>
                <field>expected growth percentage</field>
                <field>rate limits per model</field>
                <field>cost per token metrics</field>
            </context>
            <context type="function_optimization">
                <field>function performance metrics</field>
                <field>identified bottlenecks</field>
                <field>available optimization methods</field>
            </context>
            <context type="model_selection">
                <field>current model configuration</field>
                <field>candidate models for evaluation</field>
                <field>evaluation criteria and thresholds</field>
                <field>workload characteristics</field>
            </context>
            <context type="audit">
                <field>KV cache instance count</field>
                <field>cache configurations (size/hit rate/TTL)</field>
                <field>optimization opportunities</field>
            </context>
            <context type="multi_objective">
                <field>multiple optimization objectives</field>
                <field>constraints (quality/error rate/budget)</field>
                <field>current system state</field>
            </context>
            <context type="tool_migration">
                <field>agent tool inventory</field>
                <field>migration criteria</field>
                <field>verbosity options</field>
            </context>
            <context type="rollback_analysis">
                <field>upgrade timestamp</field>
                <field>before/after metrics comparison</field>
                <field>affected endpoint count</field>
            </context>
        </synthetic_data_contexts>
        
        <quality_validation>
            <description>All responses validated through QualityGateService</description>
            <acceptance_levels>
                <level>EXCELLENT</level>
                <level>GOOD</level>
                <level>ACCEPTABLE</level>
            </acceptance_levels>
            <content_types>
                <type>OPTIMIZATION</type>
                <type>DATA_ANALYSIS</type>
                <type>ACTION_PLAN</type>
            </content_types>
            <validation_criteria>
                <criterion>Specificity score >= 0.6</criterion>
                <criterion>Actionability score >= 0.6</criterion>
                <criterion>No circular reasoning</criterion>
                <criterion>Minimal generic phrases</criterion>
                <criterion>Quantification where appropriate</criterion>
            </validation_criteria>
        </quality_validation>
    </e2e_real_llm_tests>
    
    <real_llm_testing>
        <description>
            Comprehensive testing with actual LLM API calls for critical validation of agent behaviors,
            response quality, and end-to-end workflows. Essential for validating real-world performance.
        </description>
        
        <configuration>
            <option name="--real-llm">
                <description>Enable real LLM calls instead of mocks for test execution</description>
                <command>python test_runner.py --level integration --real-llm</command>
                <applicable_levels>unit, integration, comprehensive, critical</applicable_levels>
                <note>Smoke tests always use mocks for speed</note>
            </option>
            
            <option name="--llm-model">
                <description>Specify which LLM model to use for testing</description>
                <command>python test_runner.py --level integration --real-llm --llm-model gemini-1.5-flash</command>
                <default>gemini-1.5-flash</default>
                <alternatives>
                    <model>gemini-2.5-pro</model>
                    <model>gpt-4</model>
                    <model>gpt-3.5-turbo</model>
                    <model>claude-3-sonnet</model>
                </alternatives>
                <considerations>
                    <consideration>Cost implications - Flash models are cheaper</consideration>
                    <consideration>Rate limits vary by model</consideration>
                    <consideration>Response quality differences</consideration>
                </considerations>
            </option>
            
            <option name="--llm-timeout">
                <description>Timeout for individual LLM calls during testing</description>
                <command>python test_runner.py --real-llm --llm-timeout 60</command>
                <default>30</default>
                <unit>seconds</unit>
                <recommended_range>30-120</recommended_range>
            </option>
        </configuration>
        
        <test_environments>
            <environment name="test_dedicated">
                <description>Dedicated testing environment with isolated resources</description>
                <database>
                    <connection>TEST_DATABASE_URL</connection>
                    <schema>netra_test</schema>
                    <isolation_level>READ_COMMITTED</isolation_level>
                    <setup_script>scripts/setup_test_db.sql</setup_script>
                    <teardown_script>scripts/teardown_test_db.sql</teardown_script>
                </database>
                <redis>
                    <connection>TEST_REDIS_URL</connection>
                    <database_index>1</database_index>
                    <namespace_prefix>test:</namespace_prefix>
                </redis>
                <clickhouse>
                    <connection>TEST_CLICKHOUSE_URL</connection>
                    <database>netra_test</database>
                    <tables_prefix>test_</tables_prefix>
                </clickhouse>
                <llm_keys>
                    <key_env>TEST_ANTHROPIC_API_KEY</key_env>
                    <key_env>TEST_GOOGLE_API_KEY</key_env>
                    <key_env>TEST_OPENAI_API_KEY</key_env>
                    <note>Separate keys with lower rate limits for testing</note>
                </llm_keys>
            </environment>
            
            <environment name="staging_shared">
                <description>Shared staging environment for pre-production validation</description>
                <database>
                    <connection>STAGING_DATABASE_URL</connection>
                    <schema>netra_staging</schema>
                    <note>Shared with staging deployments</note>
                </database>
                <caution>May have side effects on staging data</caution>
            </environment>
        </test_environments>
        
        <seed_data_management>
            <description>Consistent seed data for reproducible real LLM tests</description>
            
            <data_sets>
                <set name="basic_optimization">
                    <path>test_data/seed/basic_optimization.json</path>
                    <description>Basic cost and latency optimization scenarios</description>
                    <records>
                        <record type="user">5 test users with different permissions</record>
                        <record type="thread">10 conversation threads</record>
                        <record type="metrics">1000 sample metrics over 7 days</record>
                        <record type="models">3 different model configurations</record>
                    </records>
                </set>
                
                <set name="complex_workflows">
                    <path>test_data/seed/complex_workflows.json</path>
                    <description>Multi-agent coordination and complex optimizations</description>
                    <records>
                        <record type="corpus">Pre-generated corpus entries</record>
                        <record type="supply_chain">AI supply chain configurations</record>
                        <record type="kv_cache">50 KV cache instances</record>
                        <record type="benchmarks">Performance benchmark data</record>
                    </records>
                </set>
                
                <set name="edge_cases">
                    <path>test_data/seed/edge_cases.json</path>
                    <description>Boundary conditions and error scenarios</description>
                    <records>
                        <record type="malformed">Intentionally malformed requests</record>
                        <record type="rate_limited">Rate limit simulation data</record>
                        <record type="timeout">Long-running query scenarios</record>
                        <record type="memory">High memory usage patterns</record>
                    </records>
                </set>
            </data_sets>
            
            <loading_strategy>
                <step>1. Check if test database exists, create if not</step>
                <step>2. Clear existing test data with teardown script</step>
                <step>3. Load seed data based on test requirements</step>
                <step>4. Verify data integrity with checksums</step>
                <step>5. Create snapshot for potential rollback</step>
            </loading_strategy>
            
            <data_isolation>
                <method>Transaction-based isolation per test</method>
                <rollback>Automatic rollback after each test</rollback>
                <parallel_support>Database-level isolation for parallel tests</parallel_support>
            </data_isolation>
        </seed_data_management>
        
        <execution_patterns>
            <pattern name="sequential_critical">
                <description>Run critical real LLM tests sequentially to avoid rate limits</description>
                <command>python test_runner.py --level critical --real-llm --parallel 1</command>
                <use_case>When testing with production API keys</use_case>
            </pattern>
            
            <pattern name="parallel_development">
                <description>Run with test API keys in parallel for speed</description>
                <command>python test_runner.py --level unit --real-llm --parallel auto</command>
                <use_case>Development testing with dedicated test keys</use_case>
            </pattern>
            
            <pattern name="cost_controlled">
                <description>Run subset of tests to control API costs</description>
                <command>python test_runner.py --real-llm -k "test_critical" --llm-model gemini-1.5-flash</command>
                <use_case>Regular CI/CD runs with cost constraints</use_case>
            </pattern>
            
            <pattern name="comprehensive_validation">
                <description>Full validation before major releases</description>
                <command>python test_runner.py --level comprehensive --real-llm --llm-timeout 120</command>
                <use_case>Pre-release validation</use_case>
                <expected_duration>30-45 minutes</expected_duration>
                <estimated_cost>$5-10 depending on model</estimated_cost>
            </pattern>
        </execution_patterns>
        
        <performance_expectations>
            <metric name="response_time">
                <flash_models>1-3 seconds per call</flash_models>
                <pro_models>3-8 seconds per call</pro_models>
                <timeout_threshold>30 seconds default, 120 for complex</timeout_threshold>
            </metric>
            
            <metric name="test_duration">
                <level name="unit_real">3-5 minutes (vs 1-2 with mocks)</level>
                <level name="integration_real">10-15 minutes (vs 3-5 with mocks)</level>
                <level name="comprehensive_real">30-45 minutes (vs 10-15 with mocks)</level>
            </metric>
            
            <metric name="api_costs">
                <model name="gemini-1.5-flash">~$0.50 per 100 tests</model>
                <model name="gemini-2.5-pro">~$2.00 per 100 tests</model>
                <model name="gpt-3.5-turbo">~$1.00 per 100 tests</model>
                <model name="gpt-4">~$5.00 per 100 tests</model>
            </metric>
        </performance_expectations>
        
        <validation_criteria>
            <criterion name="response_structure">
                <check>Valid JSON/structured output</check>
                <check>Required fields present</check>
                <check>Appropriate response length</check>
            </criterion>
            
            <criterion name="content_quality">
                <check>Specificity score >= 0.6</check>
                <check>Actionability score >= 0.6</check>
                <check>No hallucinations in metrics</check>
                <check>Consistent with seed data context</check>
            </criterion>
            
            <criterion name="agent_coordination">
                <check>Correct agent routing</check>
                <check>Proper sub-agent invocation</check>
                <check>Response layer accumulation</check>
                <check>Error propagation handling</check>
            </criterion>
        </validation_criteria>
        
        <monitoring_and_reporting>
            <metrics_collection>
                <metric>Total LLM calls made</metric>
                <metric>Average response time per model</metric>
                <metric>Token usage statistics</metric>
                <metric>API error rates</metric>
                <metric>Cost estimation per run</metric>
            </metrics_collection>
            
            <report_enhancement>
                <section>Real LLM Test Summary</section>
                <includes>
                    <field>LLM model used</field>
                    <field>Total API calls</field>
                    <field>Average response time</field>
                    <field>Estimated cost</field>
                    <field>Quality gate pass rate</field>
                </includes>
            </report_enhancement>
        </monitoring_and_reporting>
        
        <best_practices>
            <practice>Use test API keys with lower rate limits to avoid production impact</practice>
            <practice>Run real LLM tests selectively - not every commit needs them</practice>
            <practice>Cache successful LLM responses for retry scenarios</practice>
            <practice>Monitor API costs and set budget alerts</practice>
            <practice>Use flash/cheaper models for most tests, pro models for critical validation</practice>
            <practice>Implement exponential backoff for rate limit handling</practice>
            <practice>Log all LLM interactions for debugging failed tests</practice>
            <practice>Set up dedicated test data that doesn't change between runs</practice>
            <practice>Use deterministic prompts to reduce response variability</practice>
            <practice>Validate both successful responses and error handling paths</practice>
        </best_practices>
        
        <troubleshooting>
            <issue>
                <problem>Rate limit errors during test execution</problem>
                <solution>Add delays between tests or reduce parallelism with --parallel 1</solution>
            </issue>
            <issue>
                <problem>Inconsistent test results between runs</problem>
                <solution>Use temperature=0 for deterministic responses, ensure seed data consistency</solution>
            </issue>
            <issue>
                <problem>High API costs from test runs</problem>
                <solution>Use gemini-1.5-flash for most tests, limit comprehensive real LLM runs</solution>
            </issue>
            <issue>
                <problem>Timeout errors with complex prompts</problem>
                <solution>Increase --llm-timeout to 120 seconds for comprehensive tests</solution>
            </issue>
            <issue>
                <problem>Database conflicts with parallel tests</problem>
                <solution>Use transaction isolation or separate test databases per worker</solution>
            </issue>
        </troubleshooting>
    </real_llm_testing>
    
    <feature_flag_testing>
        <description>
            Comprehensive feature flag system enabling TDD workflow while maintaining 100% CI/CD pass rate.
            Tests can be written before implementation and flagged appropriately based on feature development status.
        </description>
        
        <feature_status_types>
            <status name="enabled">
                <description>Feature is complete and tests should pass reliably</description>
                <test_behavior>Tests run normally and must pass</test_behavior>
                <usage>Production-ready features</usage>
            </status>
            
            <status name="in_development">
                <description>Feature in progress, tests may fail during development</description>
                <test_behavior>Tests are skipped or marked as expected to fail</test_behavior>
                <usage>TDD workflow - tests written before implementation</usage>
            </status>
            
            <status name="disabled">
                <description>Feature disabled, tests should be skipped</description>
                <test_behavior>Tests are skipped with clear reason</test_behavior>
                <usage>Features temporarily disabled or future development</usage>
            </status>
            
            <status name="experimental">
                <description>Feature is experimental, tests are optional</description>
                <test_behavior>Tests only run when explicitly enabled</test_behavior>
                <usage>Research features or proof of concepts</usage>
            </status>
        </feature_status_types>
        
        <configuration>
            <config_file>test_feature_flags.json</config_file>
            <structure>
                {
                    "features": {
                        "feature_name": {
                            "status": "enabled|in_development|disabled|experimental",
                            "description": "Feature description",
                            "owner": "team-name",
                            "target_release": "v1.x.x",
                            "dependencies": ["other_feature"],
                            "metadata": { "jira_ticket": "TICKET-123" }
                        }
                    }
                }
            </structure>
        </configuration>
        
        <environment_overrides>
            <description>Environment variables can override feature flag configuration</description>
            <format>TEST_FEATURE_&lt;FEATURE_NAME&gt;=&lt;status&gt;</format>
            <examples>
                <example>TEST_FEATURE_ROI_CALCULATOR=enabled</example>
                <example>TEST_FEATURE_GITHUB_INTEGRATION=disabled</example>
                <example>ENABLE_EXPERIMENTAL_TESTS=true</example>
            </examples>
            <use_cases>
                <use_case>Enable feature for specific test runs</use_case>
                <use_case>Disable problematic features temporarily</use_case>
                <use_case>Test different feature combinations</use_case>
            </use_cases>
        </environment_overrides>
        
        <backend_decorators>
            <decorator name="@feature_flag">
                <usage>@feature_flag("feature_name")</usage>
                <behavior>Skip test if feature not enabled</behavior>
                <example>Used for basic feature gating</example>
            </decorator>
            
            <decorator name="@tdd_test">
                <usage>@tdd_test("feature_name", expected_to_fail=True)</usage>
                <behavior>Mark test as expected to fail during development</behavior>
                <example>TDD workflow for features in development</example>
            </decorator>
            
            <decorator name="@requires_feature">
                <usage>@requires_feature("feat_a", "feat_b")</usage>
                <behavior>Require all specified features to be enabled</behavior>
                <example>Integration tests requiring multiple features</example>
            </decorator>
            
            <decorator name="@experimental_test">
                <usage>@experimental_test("Testing new ML algorithm")</usage>
                <behavior>Only run when ENABLE_EXPERIMENTAL_TESTS=true</behavior>
                <example>Research and experimental features</example>
            </decorator>
            
            <decorator name="@performance_test">
                <usage>@performance_test(threshold_ms=100)</usage>
                <behavior>Performance tests with automatic threshold checking</behavior>
                <example>API response time validation</example>
            </decorator>
            
            <decorator name="@integration_only">
                <usage>@integration_only()</usage>
                <behavior>Only run during integration test levels</behavior>
                <example>Database integration tests</example>
            </decorator>
            
            <decorator name="@requires_env">
                <usage>@requires_env("API_KEY", "DATABASE_URL")</usage>
                <behavior>Skip if required environment variables not set</behavior>
                <example>Tests requiring external services</example>
            </decorator>
            
            <decorator name="@flaky_test">
                <usage>@flaky_test(max_retries=3, reason="Network dependency")</usage>
                <behavior>Retry tests up to max_retries on failure</behavior>
                <example>Tests with external API dependencies</example>
            </decorator>
        </backend_decorators>
        
        <frontend_utilities>
            <utility name="describeFeature">
                <usage>describeFeature("feature_name", "Test Suite", testFn)</usage>
                <behavior>Feature-flagged test suite</behavior>
                <framework>Jest/TypeScript</framework>
            </utility>
            
            <utility name="testFeature">
                <usage>testFeature("feature_name", "Test Name", testFn)</usage>
                <behavior>Feature-flagged individual test</behavior>
                <framework>Jest/TypeScript</framework>
            </utility>
            
            <utility name="testTDD">
                <usage>testTDD("feature_name", "Test Name", testFn)</usage>
                <behavior>TDD test marked as todo during development</behavior>
                <framework>Jest/TypeScript</framework>
            </utility>
            
            <utility name="testRequiresFeatures">
                <usage>testRequiresFeatures(["feat_a", "feat_b"], "Test", testFn)</usage>
                <behavior>Test requiring multiple features</behavior>
                <framework>Jest/TypeScript</framework>
            </utility>
            
            <utility name="testExperimental">
                <usage>testExperimental("Test Name", testFn)</usage>
                <behavior>Experimental test only when opted in</behavior>
                <framework>Jest/TypeScript</framework>
            </utility>
        </frontend_utilities>
        
        <test_runner_integration>
            <feature_summary>
                <description>Test runner displays feature flag summary</description>
                <includes>
                    <item>Enabled features count</item>
                    <item>In development features count</item>
                    <item>Disabled features count</item>
                    <item>Experimental features count</item>
                    <item>Environment overrides active</item>
                </includes>
            </feature_summary>
            
            <visibility>
                <description>Clear visibility of feature status in test output</description>
                <benefits>
                    <benefit>Product managers can track feature readiness</benefit>
                    <benefit>Developers understand which features are testable</benefit>
                    <benefit>CI/CD reports show feature development progress</benefit>
                </benefits>
            </visibility>
        </test_runner_integration>
        
        <business_benefits>
            <benefit name="tdd_enablement">
                <description>Enables true TDD workflow without breaking CI/CD</description>
                <impact>50% faster feature development through early test writing</impact>
            </benefit>
            
            <benefit name="ci_cd_reliability">
                <description>Maintains 100% test pass rate while features in development</description>
                <impact>Eliminates build failures from incomplete features</impact>
            </benefit>
            
            <benefit name="feature_visibility">
                <description>Clear tracking of feature readiness and dependencies</description>
                <impact>Improved product planning and release coordination</impact>
            </benefit>
            
            <benefit name="environment_flexibility">
                <description>Environment-based testing for different deployment stages</description>
                <impact>Consistent testing across dev, staging, and production</impact>
            </benefit>
        </business_benefits>
        
        <implementation_files>
            <file>test_framework/feature_flags.py</file>
            <file>test_framework/decorators.py</file>
            <file>frontend/test-utils/feature-flags.ts</file>
            <file>test_feature_flags.json</file>
            <file>app/tests/unit/test_feature_flags_example.py</file>
        </implementation_files>
        
        <usage_examples>
            <example name="basic_feature_flag">
                @feature_flag("roi_calculator")
                def test_roi_calculation():
                    # Test skipped if roi_calculator not enabled
                    assert calculate_roi(1000, 0.2) == 200
            </example>
            
            <example name="tdd_workflow">
                @tdd_test("new_auth_system", expected_to_fail=True)
                def test_oauth_flow():
                    # Written before implementation, expected to fail
                    user = oauth_login("user@example.com")
                    assert user.is_authenticated
            </example>
            
            <example name="multiple_requirements">
                @requires_feature("auth_integration", "websocket_streaming")
                @integration_only()
                def test_authenticated_websocket():
                    # Requires multiple features and integration testing
                    pass
            </example>
            
            <example name="frontend_tdd">
                testTDD("roi_calculator", "should calculate ROI correctly", () => {
                    // Jest test marked as todo during development
                    const result = calculateROI(1000, 0.2);
                    expect(result).toBe(200);
                });
            </example>
        </usage_examples>
    </feature_flag_testing>
    
    <unified_commands>
        <command name="pre_commit" description="Quick pre-commit validation">
            python test_runner.py --level smoke
        </command>
        <command name="development" description="Unit tests during development">
            python test_runner.py --level unit
        </command>
        <command name="feature_validation" description="Integration testing">
            python test_runner.py --level integration
        </command>
        <command name="pre_release" description="Full comprehensive testing">
            python test_runner.py --level comprehensive
        </command>
        <command name="critical_only" description="Essential functionality only">
            python test_runner.py --level critical
        </command>
        <command name="backend_only" description="Backend tests only">
            python test_runner.py --level unit --backend-only
        </command>
        <command name="frontend_only" description="Frontend tests only">
            python test_runner.py --level unit --frontend-only
        </command>
        <command name="simple_fallback" description="When main runner has issues">
            python test_runner.py --simple
        </command>
        <command name="e2e_real" description="Real LLM E2E tests (direct pytest)">
            pytest app/tests/agents/test_example_prompts_e2e_real.py -v -s
        </command>
        <command name="real_llm_unit" description="Unit tests with real LLM calls">
            python test_runner.py --level unit --real-llm
        </command>
        <command name="real_llm_integration" description="Integration tests with real LLM calls">
            python test_runner.py --level integration --real-llm
        </command>
        <command name="real_llm_critical" description="Critical tests with real LLM (sequential)">
            python test_runner.py --level critical --real-llm --parallel 1
        </command>
        <command name="real_llm_comprehensive" description="Full suite with real LLM (30-45 min)">
            python test_runner.py --level comprehensive --real-llm --llm-timeout 120
        </command>
        <command name="real_llm_cheap" description="Cost-optimized real LLM testing">
            python test_runner.py --real-llm --llm-model gemini-1.5-flash -k "test_critical"
        </command>
    </unified_commands>
    
    <speed_optimization_commands>
        <description>Speed optimization options for faster test execution</description>
        
        <safe_optimizations>
            <command name="ci_mode" description="Safe speed optimizations for CI/CD">
                python test_runner.py --level unit --ci
            </command>
            <command name="no_warnings" description="Suppress warnings for faster execution">
                python test_runner.py --level unit --no-warnings
            </command>
            <command name="no_coverage" description="Skip coverage collection for speed">
                python test_runner.py --level unit --no-coverage
            </command>
            <command name="fast_fail" description="Stop on first failure">
                python test_runner.py --level unit --fast-fail
            </command>
            <command name="combined_safe" description="All safe optimizations">
                python test_runner.py --level unit --ci --no-warnings --no-coverage --fast-fail
            </command>
        </safe_optimizations>
        
        <aggressive_optimizations>
            <command name="speed_mode" description="WARNING: May skip slow tests">
                python test_runner.py --level unit --speed
            </command>
            <note>Use --speed with caution - it may skip slow tests to improve execution time</note>
            <note>Prefer --ci for safe speed optimizations that don't skip tests</note>
        </aggressive_optimizations>
        
        <parallelization>
            <command name="sequential" description="Run tests sequentially">
                python test_runner.py --level unit --parallel 1
            </command>
            <command name="auto_parallel" description="Auto-detect optimal parallelism">
                python test_runner.py --level unit --parallel auto
            </command>
            <command name="custom_parallel" description="Specify number of workers">
                python test_runner.py --level unit --parallel 4
            </command>
        </parallelization>
    </speed_optimization_commands>
    
    <troubleshooting>
        <issue>
            <problem>Import test failures</problem>
            <solution>Update app/tests/test_internal_imports.py and test_external_imports.py</solution>
        </issue>
        <issue>
            <problem>WebSocket test failures</problem>
            <solution>Ensure WebSocketProvider wraps test components</solution>
        </issue>
        <issue>
            <problem>ClickHouse test errors</problem>
            <solution>Check SPEC/clickhouse.xml for array handling patterns</solution>
        </issue>
        <issue>
            <problem>E2E tests timeout</problem>
            <solution>Increase pytest timeout: pytest --timeout=300</solution>
        </issue>
        <issue>
            <problem>LLM API rate limits</problem>
            <solution>Add delays between tests or use retry logic</solution>
        </issue>
        <issue>
            <problem>Test runner scanning site-packages and virtual environments</problem>
            <solution>Exclude venv, .venv, venv_test, and site-packages directories from test discovery. Test scanners must only search project test directories, not dependencies.</solution>
        </issue>
    </troubleshooting>
    
    <best_practices>
        <practice>Always run quick tests before commits</practice>
        <practice>Use mocks for unit tests, real services for E2E</practice>
        <practice>Maintain test isolation - no shared state</practice>
        <practice>Update tests when modifying functionality</practice>
        <practice>Document non-obvious test behaviors</practice>
        <practice>Use fixtures for common test setup</practice>
        <practice>Validate both success and failure paths</practice>
        <practice>Test edge cases and boundary conditions</practice>
        <practice>Always implement actual test logic that validates functionality - tests must contain meaningful assertions and verification</practice>
        <practice>When creating test files, ensure they contain real test functions with proper assertions rather than placeholder print statements</practice>
        <practice>When running comprehensive tests, stay committed to the comprehensive level - leverage modular comprehensive options for focused testing rather than falling back to simpler test levels</practice>
        <practice>Use comprehensive-backend, comprehensive-agents, or other modular options to maintain thorough testing while reducing execution time</practice>
    </best_practices>
    
    <fake_test_detection>
        <description>
            Criteria and patterns for identifying "fake tests" that provide no real value.
            Fake tests inflate coverage metrics without actually testing functionality.
        </description>
        
        <fake_test_types>
            <type name="auto_pass_flags">
                <description>Tests with flags or conditions that force them to always pass</description>
                <patterns>
                    <pattern>@pytest.mark.skip without justification</pattern>
                    <pattern>if False: pytest.fail()</pattern>
                    <pattern>assert True with no actual testing</pattern>
                    <pattern>try/except blocks that catch all failures silently</pattern>
                </patterns>
            </type>
            
            <type name="runner_bypass">
                <description>Tests that appear valid locally but are bypassed by test runners</description>
                <patterns>
                    <pattern>Tests excluded in config/pytest.ini or test configurations</pattern>
                    <pattern>Tests with custom markers that are never executed</pattern>
                    <pattern>Tests in files not included in test discovery patterns</pattern>
                </patterns>
            </type>
            
            <type name="trivial_assertions">
                <description>Tests that don't test anything significant</description>
                <patterns>
                    <pattern>Testing constants: assert MY_CONSTANT == "value"</pattern>
                    <pattern>Testing language features: assert 1 + 1 == 2</pattern>
                    <pattern>Testing third-party library behavior without app context</pattern>
                    <pattern>Simple type checks: assert isinstance(x, str)</pattern>
                    <pattern>Testing getters/setters with no logic</pattern>
                </patterns>
            </type>
            
            <type name="mock_only_tests">
                <description>Tests that only test mocks, not actual functionality</description>
                <patterns>
                    <pattern>Mocking the system under test itself</pattern>
                    <pattern>Testing mock.called without verifying behavior</pattern>
                    <pattern>Asserting mock configuration instead of outcomes</pattern>
                    <pattern>Tests where everything is mocked including the tested logic</pattern>
                </patterns>
            </type>
            
            <type name="tautological_tests">
                <description>Tests that test their own test setup</description>
                <patterns>
                    <pattern>Setting a value then immediately asserting it</pattern>
                    <pattern>Testing test fixtures instead of application code</pattern>
                    <pattern>Circular logic: x = func(); assert func() == x</pattern>
                </patterns>
            </type>
            
            <type name="empty_tests">
                <description>Tests with no assertions or meaningful verification</description>
                <patterns>
                    <pattern>Test functions with only pass statement</pattern>
                    <pattern>Tests that only print/log without assertions</pattern>
                    <pattern>Tests with commented-out assertions</pattern>
                    <pattern>Tests that only check for no exceptions without validating output</pattern>
                </patterns>
            </type>
            
            <type name="duplicate_tests">
                <description>Tests that duplicate other tests without adding value</description>
                <patterns>
                    <pattern>Copy-pasted tests with minor variations that don't affect coverage</pattern>
                    <pattern>Multiple tests for the same code path</pattern>
                    <pattern>Tests that are subsets of more comprehensive tests</pattern>
                </patterns>
            </type>
        </fake_test_types>
        
        <detection_strategy>
            <step>1. Scan for auto-pass patterns and skip decorators</step>
            <step>2. Check test runner configurations for exclusions</step>
            <step>3. Analyze assertion quality and depth</step>
            <step>4. Review mock usage for over-mocking</step>
            <step>5. Identify tautological and circular patterns</step>
            <step>6. Find empty or assertion-less tests</step>
            <step>7. Detect duplicate test logic</step>
        </detection_strategy>
        
        <remediation>
            <action>Remove tests that provide no value</action>
            <action>Replace trivial tests with meaningful ones</action>
            <action>Reduce mocking to test actual behavior</action>
            <action>Consolidate duplicate tests</action>
            <action>Add proper assertions to empty tests or remove them</action>
            <action>Document why tests are skipped if truly necessary</action>
        </remediation>
        
        <exceptions>
            <exception>Smoke tests deliberately testing basic imports</exception>
            <exception>Contract tests verifying interface compliance</exception>
            <exception>Tests temporarily skipped with clear fix timeline</exception>
            <exception>Performance benchmark tests without functional assertions</exception>
        </exceptions>
    </fake_test_detection>
    
    <coverage_requirements>
        <requirement>
            <target>97%</target>
            <scope>Overall codebase</scope>
            <excludes>__pycache__, migrations, test files</excludes>
        </requirement>
        <requirement>
            <target>100%</target>
            <scope>Critical business logic</scope>
            <includes>supervisor agent, auth, websocket handlers</includes>
        </requirement>
        <requirement>
            <target>90%</target>
            <scope>API endpoints</scope>
            <includes>all routes/*</includes>
        </requirement>
    </coverage_requirements>
    
    <test_runner_implementation_requirements>
        <description>Critical implementation requirements for test_runner.py</description>
        
        <coverage_generation>
            <requirement priority="HIGH">
                Coverage summary MUST be generated for unit level and above (not just comprehensive)
            </requirement>
            <implementation>
                Modify TEST_LEVELS configuration to set run_coverage=True for unit, integration, critical, and comprehensive levels
            </implementation>
        </coverage_generation>
        
        <report_structure>
            <requirement priority="HIGH">
                Latest report MUST have test counts at the top (total, passed, failed, skipped)
            </requirement>
            <requirement priority="HIGH">
                Coverage summary and environment info should be near the top of report
            </requirement>
            <implementation>
                Restructure markdown report generation to follow this order:
                1. Test Summary with counts
                2. Coverage Summary (if applicable)
                3. Environment/Configuration
                4. Test Output
                5. Error Details (if any)
            </implementation>
        </report_structure>
        
        <file_management>
            <requirement priority="HIGH">
                Delete per-run .md and .json files - only keep latest report
            </requirement>
            <requirement priority="HIGH">
                Create history/ folder for archiving previous reports
            </requirement>
            <implementation>
                1. Remove json_path and md_path generation in save_test_report()
                2. Check if latest report exists before overwriting
                3. If exists, move to history/ folder with timestamp
                4. Write new latest report
            </implementation>
        </file_management>
        
        <test_counting>
            <requirement priority="HIGH">
                Parse pytest output to extract exact test counts
            </requirement>
            <implementation>
                Use regex patterns to extract from pytest output:
                - "X passed" pattern for passed count
                - "X failed" pattern for failed count  
                - "X skipped" pattern for skipped count
                - "X errors" pattern for error count
                - Calculate total from sum of all counts
            </implementation>
        </test_counting>
    </test_runner_implementation_requirements>
</specification>