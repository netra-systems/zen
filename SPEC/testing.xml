<?xml version="1.0" encoding="UTF-8"?>
<specification>
    <metadata>
        <name>Testing Specification</name>
        <type>testing</type>
        <version>3.1</version>
        <last_updated>2025-08-11</last_updated>
        <changes>
            <change version="3.0" date="2025-08-11">
                - Coverage generation for unit level and above
                - Restructured report format with test counts at top
                - Removed per-run .md/.json files
                - Added history folder for archiving
                - Enhanced latest report structure
            </change>
            <change version="3.1" date="2025-08-11">
                - Added real LLM testing capabilities
                - Introduced --real-llm option for test runner
                - Added dedicated test environments configuration
                - Defined seed data management for real LLM tests
            </change>
        </changes>
    </metadata>
    
    <overview>
        <description>
            Comprehensive testing strategy for the Netra AI Optimization Platform.
            Targets 97% code coverage with focus on E2E tests, quality gates, and real LLM validation.
        </description>
        <goals>
            <goal>Maintain 97% overall test coverage</goal>
            <goal>Fixing tests must fix the actual spirit of the test, Fixing the SUT (System under test) and if needed the test iteself.</goal>
            <goal>Tests that attempt to test SUT that doesn't actually exist must be removed</goal>
            <goal>E2E tests with real LLM calls for validation</goal>
            <goal>Quality gate integration for output validation</goal>
            <goal>Synthetic data generation for consistent testing</goal>
            <goal>Fast feedback loops with quick test mode</goal>
        </goals>
    </overview>
    
    <test_categories>
        <category name="unit">
            <description>Fast, isolated tests for individual components</description>
            <location>app/tests/services/*, app/tests/core/*</location>
            <coverage_target>98%</coverage_target>
        </category>
        
        <category name="integration">
            <description>Tests for component interactions</description>
            <location>app/tests/integration/*</location>
            <coverage_target>95%</coverage_target>
        </category>
        
        <category name="e2e">
            <description>End-to-end tests with real services</description>
            <location>app/tests/agents/test_*_e2e_*.py</location>
            <coverage_target>90%</coverage_target>
        </category>
        
        <category name="critical">
            <description>Business-critical path tests</description>
            <location>app/tests/*_critical.py</location>
            <coverage_target>100%</coverage_target>
        </category>
    </test_categories>
    
    <unified_test_runner>
        <description>Single entry point for all Netra AI Platform testing with automatic report generation</description>
        <path>test_runner.py</path>
        
        <test_levels>
            <level name="smoke" recommended="pre_commit">
                <description>Quick smoke tests for basic functionality (< 30 seconds)</description>
                <purpose>Pre-commit validation, basic health checks</purpose>
                <command>python test_runner.py --level smoke</command>
                <timeout>30</timeout>
                <coverage>false</coverage>
                <components>backend+frontend</components>
            </level>
            
            <level name="unit">
                <description>Unit tests for isolated components (1-2 minutes)</description>
                <purpose>Development validation, component testing</purpose>
                <command>python test_runner.py --level unit</command>
                <timeout>120</timeout>
                <coverage>true</coverage>
                <components>backend+frontend</components>
                <note>Coverage summary generated for unit level and above</note>
            </level>
            
            <level name="integration">
                <description>Integration tests for component interaction (3-5 minutes)</description>
                <purpose>Feature validation, API testing</purpose>
                <command>python test_runner.py --level integration</command>
                <timeout>300</timeout>
                <coverage>true</coverage>
                <components>backend+frontend</components>
                <note>Coverage summary generated for unit level and above</note>
            </level>
            
            <level name="comprehensive">
                <description>Full test suite with coverage (10-15 minutes)</description>
                <purpose>Pre-release validation, full system testing</purpose>
                <command>python test_runner.py --level comprehensive</command>
                <timeout>900</timeout>
                <coverage>true</coverage>
                <components>backend+frontend</components>
            </level>
            
            <level name="critical">
                <description>Critical path tests only (1-2 minutes)</description>
                <purpose>Essential functionality verification</purpose>
                <command>python test_runner.py --level critical</command>
                <timeout>120</timeout>
                <coverage>true</coverage>
                <components>backend_only</components>
                <note>Coverage summary generated for unit level and above</note>
            </level>
        </test_levels>
        
        <fallback_runners>
            <runner name="simple_fallback">
                <description>Simple fallback for when main runner has issues</description>
                <command>python test_runner.py --simple</command>
                <recommended_for>Troubleshooting test infrastructure issues</recommended_for>
            </runner>
            
            <runner name="direct_pytest">
                <description>Direct pytest for specific test debugging</description>
                <command>python -m pytest app/tests/test_simple_health.py -v</command>
                <recommended_for>Individual test file debugging</recommended_for>
            </runner>
        </fallback_runners>
        
        <report_generation>
            <description>Unified test report generation with history tracking</description>
            
            <latest_report>
                <description>Always-current report that overwrites on each run</description>
                <location>test_reports/latest_{level}_report.md</location>
                <structure>
                    <section order="1">Test Summary (total/passed/failed/skipped counts)</section>
                    <section order="2">Coverage Summary (if level >= unit)</section>
                    <section order="3">Environment Variables and Configuration</section>
                    <section order="4">Test Output Details</section>
                    <section order="5">Error Summary (if any failures)</section>
                </structure>
                <required_metrics>
                    <metric>Total test count</metric>
                    <metric>Passed test count</metric>
                    <metric>Failed test count</metric>
                    <metric>Skipped test count</metric>
                    <metric>Test duration</metric>
                    <metric>Coverage percentage (if applicable)</metric>
                </required_metrics>
            </latest_report>
            
            <history_tracking>
                <description>Previous test reports archived for trend analysis</description>
                <location>test_reports/history/</location>
                <naming>test_report_{level}_{timestamp}.md</naming>
                <process>
                    <step>When running tests, check if latest report exists</step>
                    <step>If exists, move to history folder with timestamp</step>
                    <step>Generate new latest report in main folder</step>
                </process>
            </history_tracking>
            
            <deprecated_formats>
                <format status="removed">Per-run JSON reports (test_report_{level}_{timestamp}.json)</format>
                <format status="removed">Per-run MD reports (test_report_{level}_{timestamp}.md)</format>
                <reason>Eliminated redundancy - history folder serves archival needs</reason>
            </deprecated_formats>
            
            <coverage_requirements>
                <requirement>Coverage summary MUST be generated for unit level and above</requirement>
                <requirement>Coverage data included in report header for visibility</requirement>
                <requirement>Branch coverage and line coverage both reported</requirement>
            </coverage_requirements>
        </report_generation>
    </unified_test_runner>
    
    <working_test_files>
        <test_file name="test_simple_health.py">
            <path>app/tests/test_simple_health.py</path>
            <description>Basic tests that always pass, good for validation</description>
            <test_count>5</test_count>
            <reliability>100%</reliability>
        </test_file>
        
        <test_file name="test_working_health.py">
            <path>app/tests/test_working_health.py</path>
            <description>Tests app imports and configuration without hanging</description>
            <test_count>5</test_count>
            <reliability>100%</reliability>
        </test_file>
    </working_test_files>
    
    <e2e_real_llm_tests>
        <description>
            Comprehensive E2E tests making real LLM calls for the 9 example prompts.
            Each prompt tested with 10 unique variations for total of 90 tests.
        </description>
        
        <test_file>app/tests/agents/test_example_prompts_e2e_real.py</test_file>
        
        <features>
            <feature>Real LLM calls through complete agent workflow</feature>
            <feature>Synthetic data generation for consistent context</feature>
            <feature>Quality gate validation for all responses</feature>
            <feature>10 unique variations per example prompt</feature>
            <feature>Full sub-agent orchestration testing</feature>
            <feature>Corpus generation when needed</feature>
            <feature>Performance metrics collection</feature>
        </features>
        
        <example_prompts_tested>
            <prompt id="1" category="cost_optimization">Cost reduction with quality preservation</prompt>
            <prompt id="2" category="latency_optimization">3x latency improvement without cost increase</prompt>
            <prompt id="3" category="capacity_planning">50% usage increase impact analysis</prompt>
            <prompt id="4" category="function_optimization">Advanced function optimization methods</prompt>
            <prompt id="5" category="model_selection">New model effectiveness evaluation</prompt>
            <prompt id="6" category="audit">KV cache audit for optimizations</prompt>
            <prompt id="7" category="multi_objective">20% cost reduction + 2x latency improvement</prompt>
            <prompt id="8" category="tool_migration">GPT-5 migration recommendations</prompt>
            <prompt id="9" category="rollback_analysis">Upgrade worth analysis and rollback</prompt>
        </example_prompts_tested>
        
        <variation_types>
            <variation num="0">Original prompt unchanged</variation>
            <variation num="1">Add budget context</variation>
            <variation num="2">Mark as urgent (24 hour deadline)</variation>
            <variation num="3">Include GPU infrastructure details</variation>
            <variation num="4">Change to team perspective (I -> our)</variation>
            <variation num="5">Add regional context</variation>
            <variation num="6">Include error rate constraints</variation>
            <variation num="7">Use CAPS for urgency</variation>
            <variation num="8">Frame as follow-up</variation>
            <variation num="9">Include GPU count information</variation>
        </variation_types>
        
        <synthetic_data_contexts>
            <context type="cost_optimization">
                <field>current_costs (daily/monthly/per_request)</field>
                <field>feature latencies and requirements</field>
                <field>models in use</field>
                <field>daily request volume</field>
            </context>
            <context type="latency_optimization">
                <field>current latency percentiles (p50/p95/p99)</field>
                <field>target improvement factor</field>
                <field>infrastructure specs (GPU type/count/memory)</field>
            </context>
            <context type="capacity_planning">
                <field>current usage metrics (RPS/daily requests)</field>
                <field>expected growth percentage</field>
                <field>rate limits per model</field>
                <field>cost per token metrics</field>
            </context>
            <context type="function_optimization">
                <field>function performance metrics</field>
                <field>identified bottlenecks</field>
                <field>available optimization methods</field>
            </context>
            <context type="model_selection">
                <field>current model configuration</field>
                <field>candidate models for evaluation</field>
                <field>evaluation criteria and thresholds</field>
                <field>workload characteristics</field>
            </context>
            <context type="audit">
                <field>KV cache instance count</field>
                <field>cache configurations (size/hit rate/TTL)</field>
                <field>optimization opportunities</field>
            </context>
            <context type="multi_objective">
                <field>multiple optimization objectives</field>
                <field>constraints (quality/error rate/budget)</field>
                <field>current system state</field>
            </context>
            <context type="tool_migration">
                <field>agent tool inventory</field>
                <field>migration criteria</field>
                <field>verbosity options</field>
            </context>
            <context type="rollback_analysis">
                <field>upgrade timestamp</field>
                <field>before/after metrics comparison</field>
                <field>affected endpoint count</field>
            </context>
        </synthetic_data_contexts>
        
        <quality_validation>
            <description>All responses validated through QualityGateService</description>
            <acceptance_levels>
                <level>EXCELLENT</level>
                <level>GOOD</level>
                <level>ACCEPTABLE</level>
            </acceptance_levels>
            <content_types>
                <type>OPTIMIZATION</type>
                <type>DATA_ANALYSIS</type>
                <type>ACTION_PLAN</type>
            </content_types>
            <validation_criteria>
                <criterion>Specificity score >= 0.6</criterion>
                <criterion>Actionability score >= 0.6</criterion>
                <criterion>No circular reasoning</criterion>
                <criterion>Minimal generic phrases</criterion>
                <criterion>Quantification where appropriate</criterion>
            </validation_criteria>
        </quality_validation>
    </e2e_real_llm_tests>
    
    <real_llm_testing>
        <description>
            Comprehensive testing with actual LLM API calls for critical validation of agent behaviors,
            response quality, and end-to-end workflows. Essential for validating real-world performance.
        </description>
        
        <configuration>
            <option name="--real-llm">
                <description>Enable real LLM calls instead of mocks for test execution</description>
                <command>python test_runner.py --level integration --real-llm</command>
                <applicable_levels>unit, integration, comprehensive, critical</applicable_levels>
                <note>Smoke tests always use mocks for speed</note>
            </option>
            
            <option name="--llm-model">
                <description>Specify which LLM model to use for testing</description>
                <command>python test_runner.py --level integration --real-llm --llm-model gemini-1.5-flash</command>
                <default>gemini-1.5-flash</default>
                <alternatives>
                    <model>gemini-1.5-pro</model>
                    <model>gpt-4</model>
                    <model>gpt-3.5-turbo</model>
                    <model>claude-3-sonnet</model>
                </alternatives>
                <considerations>
                    <consideration>Cost implications - Flash models are cheaper</consideration>
                    <consideration>Rate limits vary by model</consideration>
                    <consideration>Response quality differences</consideration>
                </considerations>
            </option>
            
            <option name="--llm-timeout">
                <description>Timeout for individual LLM calls during testing</description>
                <command>python test_runner.py --real-llm --llm-timeout 60</command>
                <default>30</default>
                <unit>seconds</unit>
                <recommended_range>30-120</recommended_range>
            </option>
        </configuration>
        
        <test_environments>
            <environment name="test_dedicated">
                <description>Dedicated testing environment with isolated resources</description>
                <database>
                    <connection>TEST_DATABASE_URL</connection>
                    <schema>netra_test</schema>
                    <isolation_level>READ_COMMITTED</isolation_level>
                    <setup_script>scripts/setup_test_db.sql</setup_script>
                    <teardown_script>scripts/teardown_test_db.sql</teardown_script>
                </database>
                <redis>
                    <connection>TEST_REDIS_URL</connection>
                    <database_index>1</database_index>
                    <namespace_prefix>test:</namespace_prefix>
                </redis>
                <clickhouse>
                    <connection>TEST_CLICKHOUSE_URL</connection>
                    <database>netra_test</database>
                    <tables_prefix>test_</tables_prefix>
                </clickhouse>
                <llm_keys>
                    <key_env>TEST_ANTHROPIC_API_KEY</key_env>
                    <key_env>TEST_GOOGLE_API_KEY</key_env>
                    <key_env>TEST_OPENAI_API_KEY</key_env>
                    <note>Separate keys with lower rate limits for testing</note>
                </llm_keys>
            </environment>
            
            <environment name="staging_shared">
                <description>Shared staging environment for pre-production validation</description>
                <database>
                    <connection>STAGING_DATABASE_URL</connection>
                    <schema>netra_staging</schema>
                    <note>Shared with staging deployments</note>
                </database>
                <caution>May have side effects on staging data</caution>
            </environment>
        </test_environments>
        
        <seed_data_management>
            <description>Consistent seed data for reproducible real LLM tests</description>
            
            <data_sets>
                <set name="basic_optimization">
                    <path>test_data/seed/basic_optimization.json</path>
                    <description>Basic cost and latency optimization scenarios</description>
                    <records>
                        <record type="user">5 test users with different permissions</record>
                        <record type="thread">10 conversation threads</record>
                        <record type="metrics">1000 sample metrics over 7 days</record>
                        <record type="models">3 different model configurations</record>
                    </records>
                </set>
                
                <set name="complex_workflows">
                    <path>test_data/seed/complex_workflows.json</path>
                    <description>Multi-agent coordination and complex optimizations</description>
                    <records>
                        <record type="corpus">Pre-generated corpus entries</record>
                        <record type="supply_chain">AI supply chain configurations</record>
                        <record type="kv_cache">50 KV cache instances</record>
                        <record type="benchmarks">Performance benchmark data</record>
                    </records>
                </set>
                
                <set name="edge_cases">
                    <path>test_data/seed/edge_cases.json</path>
                    <description>Boundary conditions and error scenarios</description>
                    <records>
                        <record type="malformed">Intentionally malformed requests</record>
                        <record type="rate_limited">Rate limit simulation data</record>
                        <record type="timeout">Long-running query scenarios</record>
                        <record type="memory">High memory usage patterns</record>
                    </records>
                </set>
            </data_sets>
            
            <loading_strategy>
                <step>1. Check if test database exists, create if not</step>
                <step>2. Clear existing test data with teardown script</step>
                <step>3. Load seed data based on test requirements</step>
                <step>4. Verify data integrity with checksums</step>
                <step>5. Create snapshot for potential rollback</step>
            </loading_strategy>
            
            <data_isolation>
                <method>Transaction-based isolation per test</method>
                <rollback>Automatic rollback after each test</rollback>
                <parallel_support>Database-level isolation for parallel tests</parallel_support>
            </data_isolation>
        </seed_data_management>
        
        <execution_patterns>
            <pattern name="sequential_critical">
                <description>Run critical real LLM tests sequentially to avoid rate limits</description>
                <command>python test_runner.py --level critical --real-llm --parallel 1</command>
                <use_case>When testing with production API keys</use_case>
            </pattern>
            
            <pattern name="parallel_development">
                <description>Run with test API keys in parallel for speed</description>
                <command>python test_runner.py --level unit --real-llm --parallel auto</command>
                <use_case>Development testing with dedicated test keys</use_case>
            </pattern>
            
            <pattern name="cost_controlled">
                <description>Run subset of tests to control API costs</description>
                <command>python test_runner.py --real-llm -k "test_critical" --llm-model gemini-1.5-flash</command>
                <use_case>Regular CI/CD runs with cost constraints</use_case>
            </pattern>
            
            <pattern name="comprehensive_validation">
                <description>Full validation before major releases</description>
                <command>python test_runner.py --level comprehensive --real-llm --llm-timeout 120</command>
                <use_case>Pre-release validation</use_case>
                <expected_duration>30-45 minutes</expected_duration>
                <estimated_cost>$5-10 depending on model</estimated_cost>
            </pattern>
        </execution_patterns>
        
        <performance_expectations>
            <metric name="response_time">
                <flash_models>1-3 seconds per call</flash_models>
                <pro_models>3-8 seconds per call</pro_models>
                <timeout_threshold>30 seconds default, 120 for complex</timeout_threshold>
            </metric>
            
            <metric name="test_duration">
                <level name="unit_real">3-5 minutes (vs 1-2 with mocks)</level>
                <level name="integration_real">10-15 minutes (vs 3-5 with mocks)</level>
                <level name="comprehensive_real">30-45 minutes (vs 10-15 with mocks)</level>
            </metric>
            
            <metric name="api_costs">
                <model name="gemini-1.5-flash">~$0.50 per 100 tests</model>
                <model name="gemini-1.5-pro">~$2.00 per 100 tests</model>
                <model name="gpt-3.5-turbo">~$1.00 per 100 tests</model>
                <model name="gpt-4">~$5.00 per 100 tests</model>
            </metric>
        </performance_expectations>
        
        <validation_criteria>
            <criterion name="response_structure">
                <check>Valid JSON/structured output</check>
                <check>Required fields present</check>
                <check>Appropriate response length</check>
            </criterion>
            
            <criterion name="content_quality">
                <check>Specificity score >= 0.6</check>
                <check>Actionability score >= 0.6</check>
                <check>No hallucinations in metrics</check>
                <check>Consistent with seed data context</check>
            </criterion>
            
            <criterion name="agent_coordination">
                <check>Correct agent routing</check>
                <check>Proper sub-agent invocation</check>
                <check>Response layer accumulation</check>
                <check>Error propagation handling</check>
            </criterion>
        </validation_criteria>
        
        <monitoring_and_reporting>
            <metrics_collection>
                <metric>Total LLM calls made</metric>
                <metric>Average response time per model</metric>
                <metric>Token usage statistics</metric>
                <metric>API error rates</metric>
                <metric>Cost estimation per run</metric>
            </metrics_collection>
            
            <report_enhancement>
                <section>Real LLM Test Summary</section>
                <includes>
                    <field>LLM model used</field>
                    <field>Total API calls</field>
                    <field>Average response time</field>
                    <field>Estimated cost</field>
                    <field>Quality gate pass rate</field>
                </includes>
            </report_enhancement>
        </monitoring_and_reporting>
        
        <best_practices>
            <practice>Use test API keys with lower rate limits to avoid production impact</practice>
            <practice>Run real LLM tests selectively - not every commit needs them</practice>
            <practice>Cache successful LLM responses for retry scenarios</practice>
            <practice>Monitor API costs and set budget alerts</practice>
            <practice>Use flash/cheaper models for most tests, pro models for critical validation</practice>
            <practice>Implement exponential backoff for rate limit handling</practice>
            <practice>Log all LLM interactions for debugging failed tests</practice>
            <practice>Set up dedicated test data that doesn't change between runs</practice>
            <practice>Use deterministic prompts to reduce response variability</practice>
            <practice>Validate both successful responses and error handling paths</practice>
        </best_practices>
        
        <troubleshooting>
            <issue>
                <problem>Rate limit errors during test execution</problem>
                <solution>Add delays between tests or reduce parallelism with --parallel 1</solution>
            </issue>
            <issue>
                <problem>Inconsistent test results between runs</problem>
                <solution>Use temperature=0 for deterministic responses, ensure seed data consistency</solution>
            </issue>
            <issue>
                <problem>High API costs from test runs</problem>
                <solution>Use gemini-1.5-flash for most tests, limit comprehensive real LLM runs</solution>
            </issue>
            <issue>
                <problem>Timeout errors with complex prompts</problem>
                <solution>Increase --llm-timeout to 120 seconds for comprehensive tests</solution>
            </issue>
            <issue>
                <problem>Database conflicts with parallel tests</problem>
                <solution>Use transaction isolation or separate test databases per worker</solution>
            </issue>
        </troubleshooting>
    </real_llm_testing>
    
    <unified_commands>
        <command name="pre_commit" description="Quick pre-commit validation">
            python test_runner.py --level smoke
        </command>
        <command name="development" description="Unit tests during development">
            python test_runner.py --level unit
        </command>
        <command name="feature_validation" description="Integration testing">
            python test_runner.py --level integration
        </command>
        <command name="pre_release" description="Full comprehensive testing">
            python test_runner.py --level comprehensive
        </command>
        <command name="critical_only" description="Essential functionality only">
            python test_runner.py --level critical
        </command>
        <command name="backend_only" description="Backend tests only">
            python test_runner.py --level unit --backend-only
        </command>
        <command name="frontend_only" description="Frontend tests only">
            python test_runner.py --level unit --frontend-only
        </command>
        <command name="simple_fallback" description="When main runner has issues">
            python test_runner.py --simple
        </command>
        <command name="e2e_real" description="Real LLM E2E tests (direct pytest)">
            pytest app/tests/agents/test_example_prompts_e2e_real.py -v -s
        </command>
        <command name="real_llm_unit" description="Unit tests with real LLM calls">
            python test_runner.py --level unit --real-llm
        </command>
        <command name="real_llm_integration" description="Integration tests with real LLM calls">
            python test_runner.py --level integration --real-llm
        </command>
        <command name="real_llm_critical" description="Critical tests with real LLM (sequential)">
            python test_runner.py --level critical --real-llm --parallel 1
        </command>
        <command name="real_llm_comprehensive" description="Full suite with real LLM (30-45 min)">
            python test_runner.py --level comprehensive --real-llm --llm-timeout 120
        </command>
        <command name="real_llm_cheap" description="Cost-optimized real LLM testing">
            python test_runner.py --real-llm --llm-model gemini-1.5-flash -k "test_critical"
        </command>
    </unified_commands>
    
    <troubleshooting>
        <issue>
            <problem>Import test failures</problem>
            <solution>Update app/tests/test_internal_imports.py and test_external_imports.py</solution>
        </issue>
        <issue>
            <problem>WebSocket test failures</problem>
            <solution>Ensure WebSocketProvider wraps test components</solution>
        </issue>
        <issue>
            <problem>ClickHouse test errors</problem>
            <solution>Check SPEC/clickhouse.xml for array handling patterns</solution>
        </issue>
        <issue>
            <problem>E2E tests timeout</problem>
            <solution>Increase pytest timeout: pytest --timeout=300</solution>
        </issue>
        <issue>
            <problem>LLM API rate limits</problem>
            <solution>Add delays between tests or use retry logic</solution>
        </issue>
    </troubleshooting>
    
    <best_practices>
        <practice>Always run quick tests before commits</practice>
        <practice>Use mocks for unit tests, real services for E2E</practice>
        <practice>Maintain test isolation - no shared state</practice>
        <practice>Update tests when modifying functionality</practice>
        <practice>Document non-obvious test behaviors</practice>
        <practice>Use fixtures for common test setup</practice>
        <practice>Validate both success and failure paths</practice>
        <practice>Test edge cases and boundary conditions</practice>
        <practice>Always implement actual test logic that validates functionality - tests must contain meaningful assertions and verification</practice>
        <practice>When creating test files, ensure they contain real test functions with proper assertions rather than placeholder print statements</practice>
    </best_practices>
    
    <fake_test_detection>
        <description>
            Criteria and patterns for identifying "fake tests" that provide no real value.
            Fake tests inflate coverage metrics without actually testing functionality.
        </description>
        
        <fake_test_types>
            <type name="auto_pass_flags">
                <description>Tests with flags or conditions that force them to always pass</description>
                <patterns>
                    <pattern>@pytest.mark.skip without justification</pattern>
                    <pattern>if False: pytest.fail()</pattern>
                    <pattern>assert True with no actual testing</pattern>
                    <pattern>try/except blocks that catch all failures silently</pattern>
                </patterns>
            </type>
            
            <type name="runner_bypass">
                <description>Tests that appear valid locally but are bypassed by test runners</description>
                <patterns>
                    <pattern>Tests excluded in pytest.ini or test configurations</pattern>
                    <pattern>Tests with custom markers that are never executed</pattern>
                    <pattern>Tests in files not included in test discovery patterns</pattern>
                </patterns>
            </type>
            
            <type name="trivial_assertions">
                <description>Tests that don't test anything significant</description>
                <patterns>
                    <pattern>Testing constants: assert MY_CONSTANT == "value"</pattern>
                    <pattern>Testing language features: assert 1 + 1 == 2</pattern>
                    <pattern>Testing third-party library behavior without app context</pattern>
                    <pattern>Simple type checks: assert isinstance(x, str)</pattern>
                    <pattern>Testing getters/setters with no logic</pattern>
                </patterns>
            </type>
            
            <type name="mock_only_tests">
                <description>Tests that only test mocks, not actual functionality</description>
                <patterns>
                    <pattern>Mocking the system under test itself</pattern>
                    <pattern>Testing mock.called without verifying behavior</pattern>
                    <pattern>Asserting mock configuration instead of outcomes</pattern>
                    <pattern>Tests where everything is mocked including the tested logic</pattern>
                </patterns>
            </type>
            
            <type name="tautological_tests">
                <description>Tests that test their own test setup</description>
                <patterns>
                    <pattern>Setting a value then immediately asserting it</pattern>
                    <pattern>Testing test fixtures instead of application code</pattern>
                    <pattern>Circular logic: x = func(); assert func() == x</pattern>
                </patterns>
            </type>
            
            <type name="empty_tests">
                <description>Tests with no assertions or meaningful verification</description>
                <patterns>
                    <pattern>Test functions with only pass statement</pattern>
                    <pattern>Tests that only print/log without assertions</pattern>
                    <pattern>Tests with commented-out assertions</pattern>
                    <pattern>Tests that only check for no exceptions without validating output</pattern>
                </patterns>
            </type>
            
            <type name="duplicate_tests">
                <description>Tests that duplicate other tests without adding value</description>
                <patterns>
                    <pattern>Copy-pasted tests with minor variations that don't affect coverage</pattern>
                    <pattern>Multiple tests for the same code path</pattern>
                    <pattern>Tests that are subsets of more comprehensive tests</pattern>
                </patterns>
            </type>
        </fake_test_types>
        
        <detection_strategy>
            <step>1. Scan for auto-pass patterns and skip decorators</step>
            <step>2. Check test runner configurations for exclusions</step>
            <step>3. Analyze assertion quality and depth</step>
            <step>4. Review mock usage for over-mocking</step>
            <step>5. Identify tautological and circular patterns</step>
            <step>6. Find empty or assertion-less tests</step>
            <step>7. Detect duplicate test logic</step>
        </detection_strategy>
        
        <remediation>
            <action>Remove tests that provide no value</action>
            <action>Replace trivial tests with meaningful ones</action>
            <action>Reduce mocking to test actual behavior</action>
            <action>Consolidate duplicate tests</action>
            <action>Add proper assertions to empty tests or remove them</action>
            <action>Document why tests are skipped if truly necessary</action>
        </remediation>
        
        <exceptions>
            <exception>Smoke tests deliberately testing basic imports</exception>
            <exception>Contract tests verifying interface compliance</exception>
            <exception>Tests temporarily skipped with clear fix timeline</exception>
            <exception>Performance benchmark tests without functional assertions</exception>
        </exceptions>
    </fake_test_detection>
    
    <coverage_requirements>
        <requirement>
            <target>97%</target>
            <scope>Overall codebase</scope>
            <excludes>__pycache__, migrations, test files</excludes>
        </requirement>
        <requirement>
            <target>100%</target>
            <scope>Critical business logic</scope>
            <includes>supervisor agent, auth, websocket handlers</includes>
        </requirement>
        <requirement>
            <target>90%</target>
            <scope>API endpoints</scope>
            <includes>all routes/*</includes>
        </requirement>
    </coverage_requirements>
    
    <test_runner_implementation_requirements>
        <description>Critical implementation requirements for test_runner.py</description>
        
        <coverage_generation>
            <requirement priority="HIGH">
                Coverage summary MUST be generated for unit level and above (not just comprehensive)
            </requirement>
            <implementation>
                Modify TEST_LEVELS configuration to set run_coverage=True for unit, integration, critical, and comprehensive levels
            </implementation>
        </coverage_generation>
        
        <report_structure>
            <requirement priority="HIGH">
                Latest report MUST have test counts at the top (total, passed, failed, skipped)
            </requirement>
            <requirement priority="HIGH">
                Coverage summary and environment info should be near the top of report
            </requirement>
            <implementation>
                Restructure markdown report generation to follow this order:
                1. Test Summary with counts
                2. Coverage Summary (if applicable)
                3. Environment/Configuration
                4. Test Output
                5. Error Details (if any)
            </implementation>
        </report_structure>
        
        <file_management>
            <requirement priority="HIGH">
                Delete per-run .md and .json files - only keep latest report
            </requirement>
            <requirement priority="HIGH">
                Create history/ folder for archiving previous reports
            </requirement>
            <implementation>
                1. Remove json_path and md_path generation in save_test_report()
                2. Check if latest report exists before overwriting
                3. If exists, move to history/ folder with timestamp
                4. Write new latest report
            </implementation>
        </file_management>
        
        <test_counting>
            <requirement priority="HIGH">
                Parse pytest output to extract exact test counts
            </requirement>
            <implementation>
                Use regex patterns to extract from pytest output:
                - "X passed" pattern for passed count
                - "X failed" pattern for failed count  
                - "X skipped" pattern for skipped count
                - "X errors" pattern for error count
                - Calculate total from sum of all counts
            </implementation>
        </test_counting>
    </test_runner_implementation_requirements>
</specification>