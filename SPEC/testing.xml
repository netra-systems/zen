<?xml version="1.0" encoding="UTF-8"?>
<specification>
    <metadata>
        <name>Testing Specification</name>
        <type>testing</type>
        <version>2.1</version>
        <last_updated>2025-08-11</last_updated>
    </metadata>
    
    <overview>
        <description>
            Comprehensive testing strategy for the Netra AI Optimization Platform.
            Targets 97% code coverage with focus on E2E tests, quality gates, and real LLM validation.
        </description>
        <goals>
            <goal>Maintain 97% overall test coverage</goal>
            <goal>E2E tests with real LLM calls for validation</goal>
            <goal>Quality gate integration for output validation</goal>
            <goal>Synthetic data generation for consistent testing</goal>
            <goal>Fast feedback loops with quick test mode</goal>
        </goals>
    </overview>
    
    <test_categories>
        <category name="unit">
            <description>Fast, isolated tests for individual components</description>
            <location>app/tests/services/*, app/tests/core/*</location>
            <coverage_target>98%</coverage_target>
        </category>
        
        <category name="integration">
            <description>Tests for component interactions</description>
            <location>app/tests/integration/*</location>
            <coverage_target>95%</coverage_target>
        </category>
        
        <category name="e2e">
            <description>End-to-end tests with real services</description>
            <location>app/tests/agents/test_*_e2e_*.py</location>
            <coverage_target>90%</coverage_target>
        </category>
        
        <category name="critical">
            <description>Business-critical path tests</description>
            <location>app/tests/*_critical.py</location>
            <coverage_target>100%</coverage_target>
        </category>
    </test_categories>
    
    <unified_test_runner>
        <description>Single entry point for all Netra AI Platform testing with automatic report generation</description>
        <path>test_runner.py</path>
        
        <test_levels>
            <level name="smoke" recommended="pre_commit">
                <description>Quick smoke tests for basic functionality (< 30 seconds)</description>
                <purpose>Pre-commit validation, basic health checks</purpose>
                <command>python test_runner.py --level smoke</command>
                <timeout>30</timeout>
                <coverage>false</coverage>
                <components>backend+frontend</components>
            </level>
            
            <level name="unit">
                <description>Unit tests for isolated components (1-2 minutes)</description>
                <purpose>Development validation, component testing</purpose>
                <command>python test_runner.py --level unit</command>
                <timeout>120</timeout>
                <coverage>true</coverage>
                <components>backend+frontend</components>
                <note>Coverage summary generated for unit level and above</note>
            </level>
            
            <level name="integration">
                <description>Integration tests for component interaction (3-5 minutes)</description>
                <purpose>Feature validation, API testing</purpose>
                <command>python test_runner.py --level integration</command>
                <timeout>300</timeout>
                <coverage>true</coverage>
                <components>backend+frontend</components>
                <note>Coverage summary generated for unit level and above</note>
            </level>
            
            <level name="comprehensive">
                <description>Full test suite with coverage (10-15 minutes)</description>
                <purpose>Pre-release validation, full system testing</purpose>
                <command>python test_runner.py --level comprehensive</command>
                <timeout>900</timeout>
                <coverage>true</coverage>
                <components>backend+frontend</components>
            </level>
            
            <level name="critical">
                <description>Critical path tests only (1-2 minutes)</description>
                <purpose>Essential functionality verification</purpose>
                <command>python test_runner.py --level critical</command>
                <timeout>120</timeout>
                <coverage>true</coverage>
                <components>backend_only</components>
                <note>Coverage summary generated for unit level and above</note>
            </level>
        </test_levels>
        
        <fallback_runners>
            <runner name="simple_fallback">
                <description>Simple fallback for when main runner has issues</description>
                <command>python test_runner.py --simple</command>
                <recommended_for>Troubleshooting test infrastructure issues</recommended_for>
            </runner>
            
            <runner name="direct_pytest">
                <description>Direct pytest for specific test debugging</description>
                <command>python -m pytest app/tests/test_simple_health.py -v</command>
                <recommended_for>Individual test file debugging</recommended_for>
            </runner>
        </fallback_runners>
        
        <report_generation>
            <description>Unified test report generation with history tracking</description>
            
            <latest_report>
                <description>Always-current report that overwrites on each run</description>
                <location>test_reports/latest_{level}_report.md</location>
                <structure>
                    <section order="1">Test Summary (total/passed/failed/skipped counts)</section>
                    <section order="2">Coverage Summary (if level >= unit)</section>
                    <section order="3">Environment Variables and Configuration</section>
                    <section order="4">Test Output Details</section>
                    <section order="5">Error Summary (if any failures)</section>
                </structure>
                <required_metrics>
                    <metric>Total test count</metric>
                    <metric>Passed test count</metric>
                    <metric>Failed test count</metric>
                    <metric>Skipped test count</metric>
                    <metric>Test duration</metric>
                    <metric>Coverage percentage (if applicable)</metric>
                </required_metrics>
            </latest_report>
            
            <history_tracking>
                <description>Previous test reports archived for trend analysis</description>
                <location>test_reports/history/</location>
                <naming>test_report_{level}_{timestamp}.md</naming>
                <process>
                    <step>When running tests, check if latest report exists</step>
                    <step>If exists, move to history folder with timestamp</step>
                    <step>Generate new latest report in main folder</step>
                </process>
            </history_tracking>
            
            <deprecated_formats>
                <format status="removed">Per-run JSON reports (test_report_{level}_{timestamp}.json)</format>
                <format status="removed">Per-run MD reports (test_report_{level}_{timestamp}.md)</format>
                <reason>Eliminated redundancy - history folder serves archival needs</reason>
            </deprecated_formats>
            
            <coverage_requirements>
                <requirement>Coverage summary MUST be generated for unit level and above</requirement>
                <requirement>Coverage data included in report header for visibility</requirement>
                <requirement>Branch coverage and line coverage both reported</requirement>
            </coverage_requirements>
        </report_generation>
    </unified_test_runner>
    
    <working_test_files>
        <test_file name="test_simple_health.py">
            <path>app/tests/test_simple_health.py</path>
            <description>Basic tests that always pass, good for validation</description>
            <test_count>5</test_count>
            <reliability>100%</reliability>
        </test_file>
        
        <test_file name="test_working_health.py">
            <path>app/tests/test_working_health.py</path>
            <description>Tests app imports and configuration without hanging</description>
            <test_count>5</test_count>
            <reliability>100%</reliability>
        </test_file>
    </working_test_files>
    
    <e2e_real_llm_tests>
        <description>
            Comprehensive E2E tests making real LLM calls for the 9 example prompts.
            Each prompt tested with 10 unique variations for total of 90 tests.
        </description>
        
        <test_file>app/tests/agents/test_example_prompts_e2e_real.py</test_file>
        
        <features>
            <feature>Real LLM calls through complete agent workflow</feature>
            <feature>Synthetic data generation for consistent context</feature>
            <feature>Quality gate validation for all responses</feature>
            <feature>10 unique variations per example prompt</feature>
            <feature>Full sub-agent orchestration testing</feature>
            <feature>Corpus generation when needed</feature>
            <feature>Performance metrics collection</feature>
        </features>
        
        <example_prompts_tested>
            <prompt id="1" category="cost_optimization">Cost reduction with quality preservation</prompt>
            <prompt id="2" category="latency_optimization">3x latency improvement without cost increase</prompt>
            <prompt id="3" category="capacity_planning">50% usage increase impact analysis</prompt>
            <prompt id="4" category="function_optimization">Advanced function optimization methods</prompt>
            <prompt id="5" category="model_selection">New model effectiveness evaluation</prompt>
            <prompt id="6" category="audit">KV cache audit for optimizations</prompt>
            <prompt id="7" category="multi_objective">20% cost reduction + 2x latency improvement</prompt>
            <prompt id="8" category="tool_migration">GPT-5 migration recommendations</prompt>
            <prompt id="9" category="rollback_analysis">Upgrade worth analysis and rollback</prompt>
        </example_prompts_tested>
        
        <variation_types>
            <variation num="0">Original prompt unchanged</variation>
            <variation num="1">Add budget context</variation>
            <variation num="2">Mark as urgent (24 hour deadline)</variation>
            <variation num="3">Include GPU infrastructure details</variation>
            <variation num="4">Change to team perspective (I -> our)</variation>
            <variation num="5">Add regional context</variation>
            <variation num="6">Include error rate constraints</variation>
            <variation num="7">Use CAPS for urgency</variation>
            <variation num="8">Frame as follow-up</variation>
            <variation num="9">Include GPU count information</variation>
        </variation_types>
        
        <synthetic_data_contexts>
            <context type="cost_optimization">
                <field>current_costs (daily/monthly/per_request)</field>
                <field>feature latencies and requirements</field>
                <field>models in use</field>
                <field>daily request volume</field>
            </context>
            <context type="latency_optimization">
                <field>current latency percentiles (p50/p95/p99)</field>
                <field>target improvement factor</field>
                <field>infrastructure specs (GPU type/count/memory)</field>
            </context>
            <context type="capacity_planning">
                <field>current usage metrics (RPS/daily requests)</field>
                <field>expected growth percentage</field>
                <field>rate limits per model</field>
                <field>cost per token metrics</field>
            </context>
            <context type="function_optimization">
                <field>function performance metrics</field>
                <field>identified bottlenecks</field>
                <field>available optimization methods</field>
            </context>
            <context type="model_selection">
                <field>current model configuration</field>
                <field>candidate models for evaluation</field>
                <field>evaluation criteria and thresholds</field>
                <field>workload characteristics</field>
            </context>
            <context type="audit">
                <field>KV cache instance count</field>
                <field>cache configurations (size/hit rate/TTL)</field>
                <field>optimization opportunities</field>
            </context>
            <context type="multi_objective">
                <field>multiple optimization objectives</field>
                <field>constraints (quality/error rate/budget)</field>
                <field>current system state</field>
            </context>
            <context type="tool_migration">
                <field>agent tool inventory</field>
                <field>migration criteria</field>
                <field>verbosity options</field>
            </context>
            <context type="rollback_analysis">
                <field>upgrade timestamp</field>
                <field>before/after metrics comparison</field>
                <field>affected endpoint count</field>
            </context>
        </synthetic_data_contexts>
        
        <quality_validation>
            <description>All responses validated through QualityGateService</description>
            <acceptance_levels>
                <level>EXCELLENT</level>
                <level>GOOD</level>
                <level>ACCEPTABLE</level>
            </acceptance_levels>
            <content_types>
                <type>OPTIMIZATION</type>
                <type>DATA_ANALYSIS</type>
                <type>ACTION_PLAN</type>
            </content_types>
            <validation_criteria>
                <criterion>Specificity score >= 0.6</criterion>
                <criterion>Actionability score >= 0.6</criterion>
                <criterion>No circular reasoning</criterion>
                <criterion>Minimal generic phrases</criterion>
                <criterion>Quantification where appropriate</criterion>
            </validation_criteria>
        </quality_validation>
    </e2e_real_llm_tests>
    
    <unified_commands>
        <command name="pre_commit" description="Quick pre-commit validation">
            python test_runner.py --level smoke
        </command>
        <command name="development" description="Unit tests during development">
            python test_runner.py --level unit
        </command>
        <command name="feature_validation" description="Integration testing">
            python test_runner.py --level integration
        </command>
        <command name="pre_release" description="Full comprehensive testing">
            python test_runner.py --level comprehensive
        </command>
        <command name="critical_only" description="Essential functionality only">
            python test_runner.py --level critical
        </command>
        <command name="backend_only" description="Backend tests only">
            python test_runner.py --level unit --backend-only
        </command>
        <command name="frontend_only" description="Frontend tests only">
            python test_runner.py --level unit --frontend-only
        </command>
        <command name="simple_fallback" description="When main runner has issues">
            python test_runner.py --simple
        </command>
        <command name="e2e_real" description="Real LLM E2E tests (direct pytest)">
            pytest app/tests/agents/test_example_prompts_e2e_real.py -v -s
        </command>
    </unified_commands>
    
    <troubleshooting>
        <issue>
            <problem>Import test failures</problem>
            <solution>Update app/tests/test_internal_imports.py and test_external_imports.py</solution>
        </issue>
        <issue>
            <problem>WebSocket test failures</problem>
            <solution>Ensure WebSocketProvider wraps test components</solution>
        </issue>
        <issue>
            <problem>ClickHouse test errors</problem>
            <solution>Check SPEC/clickhouse.xml for array handling patterns</solution>
        </issue>
        <issue>
            <problem>E2E tests timeout</problem>
            <solution>Increase pytest timeout: pytest --timeout=300</solution>
        </issue>
        <issue>
            <problem>LLM API rate limits</problem>
            <solution>Add delays between tests or use retry logic</solution>
        </issue>
    </troubleshooting>
    
    <best_practices>
        <practice>Always run quick tests before commits</practice>
        <practice>Use mocks for unit tests, real services for E2E</practice>
        <practice>Maintain test isolation - no shared state</practice>
        <practice>Update tests when modifying functionality</practice>
        <practice>Document non-obvious test behaviors</practice>
        <practice>Use fixtures for common test setup</practice>
        <practice>Validate both success and failure paths</practice>
        <practice>Test edge cases and boundary conditions</practice>
        <practice>Always implement actual test logic that validates functionality - tests must contain meaningful assertions and verification</practice>
        <practice>When creating test files, ensure they contain real test functions with proper assertions rather than placeholder print statements</practice>
    </best_practices>
    
    <fake_test_detection>
        <description>
            Criteria and patterns for identifying "fake tests" that provide no real value.
            Fake tests inflate coverage metrics without actually testing functionality.
        </description>
        
        <fake_test_types>
            <type name="auto_pass_flags">
                <description>Tests with flags or conditions that force them to always pass</description>
                <patterns>
                    <pattern>@pytest.mark.skip without justification</pattern>
                    <pattern>if False: pytest.fail()</pattern>
                    <pattern>assert True with no actual testing</pattern>
                    <pattern>try/except blocks that catch all failures silently</pattern>
                </patterns>
            </type>
            
            <type name="runner_bypass">
                <description>Tests that appear valid locally but are bypassed by test runners</description>
                <patterns>
                    <pattern>Tests excluded in pytest.ini or test configurations</pattern>
                    <pattern>Tests with custom markers that are never executed</pattern>
                    <pattern>Tests in files not included in test discovery patterns</pattern>
                </patterns>
            </type>
            
            <type name="trivial_assertions">
                <description>Tests that don't test anything significant</description>
                <patterns>
                    <pattern>Testing constants: assert MY_CONSTANT == "value"</pattern>
                    <pattern>Testing language features: assert 1 + 1 == 2</pattern>
                    <pattern>Testing third-party library behavior without app context</pattern>
                    <pattern>Simple type checks: assert isinstance(x, str)</pattern>
                    <pattern>Testing getters/setters with no logic</pattern>
                </patterns>
            </type>
            
            <type name="mock_only_tests">
                <description>Tests that only test mocks, not actual functionality</description>
                <patterns>
                    <pattern>Mocking the system under test itself</pattern>
                    <pattern>Testing mock.called without verifying behavior</pattern>
                    <pattern>Asserting mock configuration instead of outcomes</pattern>
                    <pattern>Tests where everything is mocked including the tested logic</pattern>
                </patterns>
            </type>
            
            <type name="tautological_tests">
                <description>Tests that test their own test setup</description>
                <patterns>
                    <pattern>Setting a value then immediately asserting it</pattern>
                    <pattern>Testing test fixtures instead of application code</pattern>
                    <pattern>Circular logic: x = func(); assert func() == x</pattern>
                </patterns>
            </type>
            
            <type name="empty_tests">
                <description>Tests with no assertions or meaningful verification</description>
                <patterns>
                    <pattern>Test functions with only pass statement</pattern>
                    <pattern>Tests that only print/log without assertions</pattern>
                    <pattern>Tests with commented-out assertions</pattern>
                    <pattern>Tests that only check for no exceptions without validating output</pattern>
                </patterns>
            </type>
            
            <type name="duplicate_tests">
                <description>Tests that duplicate other tests without adding value</description>
                <patterns>
                    <pattern>Copy-pasted tests with minor variations that don't affect coverage</pattern>
                    <pattern>Multiple tests for the same code path</pattern>
                    <pattern>Tests that are subsets of more comprehensive tests</pattern>
                </patterns>
            </type>
        </fake_test_types>
        
        <detection_strategy>
            <step>1. Scan for auto-pass patterns and skip decorators</step>
            <step>2. Check test runner configurations for exclusions</step>
            <step>3. Analyze assertion quality and depth</step>
            <step>4. Review mock usage for over-mocking</step>
            <step>5. Identify tautological and circular patterns</step>
            <step>6. Find empty or assertion-less tests</step>
            <step>7. Detect duplicate test logic</step>
        </detection_strategy>
        
        <remediation>
            <action>Remove tests that provide no value</action>
            <action>Replace trivial tests with meaningful ones</action>
            <action>Reduce mocking to test actual behavior</action>
            <action>Consolidate duplicate tests</action>
            <action>Add proper assertions to empty tests or remove them</action>
            <action>Document why tests are skipped if truly necessary</action>
        </remediation>
        
        <exceptions>
            <exception>Smoke tests deliberately testing basic imports</exception>
            <exception>Contract tests verifying interface compliance</exception>
            <exception>Tests temporarily skipped with clear fix timeline</exception>
            <exception>Performance benchmark tests without functional assertions</exception>
        </exceptions>
    </fake_test_detection>
    
    <coverage_requirements>
        <requirement>
            <target>97%</target>
            <scope>Overall codebase</scope>
            <excludes>__pycache__, migrations, test files</excludes>
        </requirement>
        <requirement>
            <target>100%</target>
            <scope>Critical business logic</scope>
            <includes>supervisor agent, auth, websocket handlers</includes>
        </requirement>
        <requirement>
            <target>90%</target>
            <scope>API endpoints</scope>
            <includes>all routes/*</includes>
        </requirement>
    </coverage_requirements>
</specification>