<?xml version="1.0" encoding="UTF-8"?>
<learnings>
  <learning id="staging-critical-issues-testing-patterns" date="2025-08-28" category="testing">
    <title>Staging Critical Issues Testing Patterns</title>
    <problem>
      Staging environment exhibits critical issues that prevent proper operation:
      1. Frontend cannot connect to backend API (ECONNREFUSED 127.0.0.1:8000)
      2. SECRET_KEY too short (must be at least 32 characters)
      3. Missing Google OAuth credentials (GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET)
      4. Auth service missing /auth/google/login route (404)
      
      These issues need systematic testing to ensure they are properly reproduced
      and validated when fixed.
    </problem>
    <root_cause>
      Multiple configuration and deployment issues in staging environment:
      - Network connectivity problems between frontend and backend services
      - Security configuration not meeting minimum requirements
      - OAuth integration incomplete or improperly configured
      - Authentication service routes not properly registered or available
    </root_cause>
    <solution>
      Created comprehensive failing test suite in tests/e2e/staging/ to reproduce
      all critical staging issues:
      
      1. test_frontend_backend_connection.py - ECONNREFUSED issues
      2. test_secret_key_validation.py - SECRET_KEY length validation
      3. test_oauth_configuration.py - Missing OAuth credentials
      4. test_auth_routes.py - Missing /auth/google/login route (404)
      5. test_environment_configuration.py - Comprehensive env var validation
      6. test_network_connectivity_variations.py - Additional network test cases
      7. test_security_config_variations.py - Additional security/config test cases
    </solution>
    <implementation_details>
      <test_structure>
        - All tests located in tests/e2e/staging/ directory
        - Each test file focuses on one primary issue category
        - Multiple test methods per file cover different aspects and variations
        - All tests marked with @staging_only decorator
        - Tests use env_requires decorator to specify service dependencies
      </test_structure>
      
      <test_philosophy>
        - Tests are designed to FAIL initially, demonstrating exact issues
        - Each test includes detailed assertions explaining expected failures
        - Test names clearly indicate what should fail and why
        - Tests provide diagnostic information about the nature of failures
        - Additional test variations provide comprehensive coverage
      </test_philosophy>
      
      <execution_requirements>
        - Tests run only in staging environment: python unified_test_runner.py --env staging
        - Tests require staging_only environment marker to execute
        - Tests validate real staging infrastructure, no mocks
        - Tests provide clear failure messages explaining root causes
      </execution_requirements>
    </implementation_details>
    <test_patterns_to_avoid>
      <pattern type="false_positive_avoidance">
        Avoid creating tests that pass when they should fail due to:
        - Incorrect environment detection
        - Mocked services instead of real staging services
        - Overly permissive assertions that accept failure conditions
        - Testing wrong endpoints or configuration variables
      </pattern>
      
      <pattern type="comprehensive_coverage">
        Ensure test coverage includes:
        - Primary failure scenario (exact issue reproduction)
        - Edge cases and variations of the primary issue
        - Cross-service impact validation
        - Configuration consistency checking
        - Network connectivity from multiple angles
      </pattern>
      
      <pattern type="maintainable_assertions">
        Structure assertions to:
        - Clearly explain what should fail and why
        - Provide diagnostic information in failure messages
        - Reference specific audit findings or error patterns
        - Distinguish between expected failures and unexpected successes
        - Include remediation guidance in assertion messages
      </pattern>
    </test_patterns_to_avoid>
    <files_created>
      <file>tests/e2e/staging/__init__.py</file>
      <file>tests/e2e/staging/test_frontend_backend_connection.py</file>
      <file>tests/e2e/staging/test_secret_key_validation.py</file>
      <file>tests/e2e/staging/test_oauth_configuration.py</file>
      <file>tests/e2e/staging/test_auth_routes.py</file>
      <file>tests/e2e/staging/test_environment_configuration.py</file>
      <file>tests/e2e/staging/test_network_connectivity_variations.py</file>
      <file>tests/e2e/staging/test_security_config_variations.py</file>
    </files_created>
    <validation_approach>
      <primary_validation>
        Execute tests against broken staging environment to ensure they fail:
        python unified_test_runner.py --env staging
        
        Expected results:
        - All tests should FAIL with specific error messages
        - Failure messages should match audit findings
        - Tests should provide clear diagnostic information
        - No false positives (tests passing when they should fail)
      </primary_validation>
      
      <fix_validation>
        After staging issues are resolved:
        - Re-run tests to verify they now pass
        - Update test assertions if needed to reflect fixed state
        - Maintain tests as regression prevention
        - Document any changes in test behavior expectations
      </fix_validation>
      
      <regression_prevention>
        Keep tests as permanent part of staging validation suite:
        - Include in staging deployment validation pipeline
        - Run tests before any staging environment changes
        - Alert on test failures indicating regression
        - Maintain test accuracy as staging environment evolves
      </regression_prevention>
    </validation_approach>
    <key_insights>
      <insight type="test_design">
        Failing tests are more valuable than passing tests when reproducing issues:
        - They prove the issue exists and can be detected
        - They provide clear reproduction steps
        - They validate that fixes actually resolve the root cause
        - They prevent false confidence from inadequate testing
      </insight>
      
      <insight type="staging_validation">
        Staging environment testing requires different approach than unit testing:
        - Must use real services and infrastructure
        - Environment markers are crucial for proper execution
        - Configuration validation is as important as functional testing
        - Cross-service integration issues are common and need specific testing
      </insight>
      
      <insight type="issue_categorization">
        Staging issues typically fall into predictable categories:
        - Network connectivity (service-to-service communication)
        - Security configuration (keys, secrets, authentication)
        - Environment variables (missing, incorrect, inconsistent)
        - Service integration (routes, endpoints, protocols)
        - Deployment configuration (URLs, ports, service discovery)
      </insight>
    </key_insights>
    <business_impact>
      <immediate_impact>
        - Staging environment issues prevent proper testing and validation
        - Development team cannot validate features before production
        - Integration issues discovered late in development cycle
        - Deployment confidence reduced due to staging instability
      </immediate_impact>
      
      <risk_mitigation>
        - Systematic test suite provides early detection of staging issues
        - Clear reproduction steps accelerate issue resolution
        - Comprehensive test coverage prevents partial fixes
        - Automated validation reduces manual testing overhead
      </risk_mitigation>
      
      <process_improvement>
        - Tests serve as documentation of staging requirements
        - Test patterns can be applied to other environments
        - Issue reproduction becomes systematic rather than ad-hoc
        - Development team gains confidence in staging environment reliability
      </process_improvement>
    </business_impact>
  </learning>

  <learning id="environment-marker-usage-patterns" date="2025-08-28" category="testing">
    <title>Environment Marker Usage Patterns for Staging Tests</title>
    <problem>
      Tests need to run only in specific environments but environment detection
      and test execution control was not properly implemented for staging-specific tests.
    </problem>
    <solution>
      Use test_framework.environment_markers module with proper decorators:
      
      @staging_only - Runs only in staging environment
      @env_requires(services=["auth_service"]) - Requires specific services
      @env("staging") - Alternative syntax for environment specification
      
      Environment detection through TEST_ENV environment variable.
    </solution>
    <implementation_details>
      <decorator_usage>
        from test_framework.environment_markers import staging_only, env_requires

        @staging_only
        @env_requires(services=["backend_service", "auth_service"])
        def test_staging_specific_functionality():
            pass
      </decorator_usage>
      
      <execution_control>
        Tests automatically skip if:
        - Environment doesn't match (TEST_ENV != staging)
        - Required services are not available
        - Safety constraints are not met
      </execution_control>
      
      <test_runner_integration>
        python unified_test_runner.py --env staging
        
        This sets TEST_ENV=staging and runs only staging-compatible tests.
      </test_runner_integration>
    </implementation_details>
    <best_practices>
      - Always use @staging_only for staging-specific tests
      - Specify service requirements with @env_requires
      - Include clear failure messages explaining environment requirements
      - Test environment detection logic separately from business logic
      - Use appropriate timeout values for staging network conditions
    </best_practices>
  </learning>

  <learning id="failing-test-assertion-patterns" date="2025-08-28" category="testing">
    <title>Failing Test Assertion Patterns for Issue Reproduction</title>
    <problem>
      When creating tests to reproduce known issues, assertions need to be structured
      to expect and validate failure conditions while providing clear diagnostic information.
    </problem>
    <solution>
      Structure failing test assertions with:
      1. Clear expectation of what should fail
      2. Detailed diagnostic information in failure messages
      3. Reference to specific audit findings or error patterns
      4. Distinction between expected failures and unexpected successes
    </solution>
    <assertion_patterns>
      <expect_failure_pattern>
        # Test expects connection failures
        assert len(connection_failures) > 0, (
            f"Expected connection failures to backend at {backend_url}, "
            f"but all {len(api_endpoints)} endpoints connected successfully. "
            f"Either backend is running when it shouldn't be, or the proxy "
            f"configuration has been fixed."
        )
      </expect_failure_pattern>
      
      <specific_error_validation>
        # Test expects specific error types
        econnrefused_errors = [
            f for f in connection_failures 
            if "Connection refused" in f["error"] or f["error_type"] == "ECONNREFUSED"
        ]
        
        assert len(econnrefused_errors) >= 2, (
            f"Expected at least 2 ECONNREFUSED errors (like in audit), "
            f"but got {len(econnrefused_errors)}. "
            f"Connection failures: {connection_failures}"
        )
      </specific_error_validation>
      
      <configuration_validation>
        # Test expects configuration issues
        assert len(missing_env_vars) >= 3, (
            f"Expected at least 3 missing critical environment variables "
            f"(typical staging configuration gap), but only {len(missing_env_vars)} missing: "
            f"{[v['variable'] for v in missing_env_vars]}. "
            f"Invalid vars: {[v['variable'] for v in invalid_env_vars]}. "
            f"Staging typically has multiple missing environment variables."
        )
      </configuration_validation>
    </assertion_patterns>
    <anti_patterns_to_avoid>
      <overly_permissive>
        # BAD - Too permissive, might pass when it should fail
        assert response.status_code != 200
        
        # GOOD - Specific expectation with diagnostic info
        assert response.status_code == 404, (
            f"Expected 404 for /auth/google/login (matching audit), "
            f"but got {response.status_code}. "
            f"Response: {response.text[:100]}"
        )
      </overly_permissive>
      
      <unclear_expectations>
        # BAD - Unclear what constitutes failure
        assert something_failed
        
        # GOOD - Clear failure criteria with context
        assert len(oauth_failures) >= 2, (
            f"Expected OAuth configuration failures for both CLIENT_ID and CLIENT_SECRET "
            f"(matching staging audit), but only {len(oauth_failures)} failures found: "
            f"{oauth_failures}"
        )
      </unclear_expectations>
    </anti_patterns_to_avoid>
  </learning>
</learnings>