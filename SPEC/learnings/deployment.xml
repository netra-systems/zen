<?xml version='1.0' encoding='utf-8'?>
<specification>
  <metadata>
    <name>Deployment Learnings</name>
    <type>learnings</type>
    <version>1.0</version>
    <last_updated>2025-08-22</last_updated>
    <description>Comprehensive deployment learnings including staging environment setup</description>
    <critical>true</critical>
    <business_impact>Mission Critical - Deployment Success Required for Customer Access</business_impact>
  </metadata>
  
  <staging_deployment_learnings>
    <learning>
      <id>staging-gunicorn-uvicorn-workers</id>
      <category>deployment</category>
      <date>2025-08-22</date>
      <severity>critical</severity>
      <title>Use Gunicorn with Uvicorn Workers for Cloud Run</title>
      <problem>
        <description>Default uvicorn server doesn't handle Cloud Run requirements properly</description>
        <symptoms>
          <symptom>Service startup failures in Cloud Run</symptom>
          <symptom>Request handling issues under load</symptom>
          <symptom>Poor performance in production environment</symptom>
        </symptoms>
        <root_cause>Single-process uvicorn not optimal for Cloud Run container lifecycle</root_cause>
      </problem>
      <solution>
        <description>Use gunicorn with uvicorn workers for Cloud Run deployments</description>
        <implementation>
          <configuration>gunicorn --worker-class uvicorn.workers.UvicornWorker</configuration>
          <benefits>
            <benefit>Better process management</benefit>
            <benefit>Improved request handling</benefit>
            <benefit>Cloud Run compatibility</benefit>
          </benefits>
        </implementation>
        <verification>
          <step>Deploy to staging with gunicorn configuration</step>
          <step>Verify service starts correctly</step>
          <step>Test request handling under load</step>
        </verification>
      </solution>
      <prevention>
        <guideline>Always use gunicorn with uvicorn workers for production deployments</guideline>
        <guideline>Test deployment configuration in staging before production</guideline>
      </prevention>
    </learning>
    
    <learning>
      <id>staging-database-ssl-requirements</id>
      <category>deployment</category>
      <date>2025-08-22</date>
      <severity>critical</severity>
      <title>#removed-legacyMust Include sslmode=require for Cloud SQL</title>
      <problem>
        <description>Database connections fail without proper SSL configuration</description>
        <symptoms>
          <symptom>Database connection errors in staging/production</symptom>
          <symptom>SSL-related connection failures</symptom>
          <symptom>Health checks failing with database errors</symptom>
        </symptoms>
        <root_cause>Cloud SQL requires SSL connections but URL doesn't specify sslmode</root_cause>
      </problem>
      <solution>
        <description>Include sslmode=require parameter in #removed-legacyfor staging/production</description>
        <implementation>
          <example>postgresql://user:pass@host:5432/db?sslmode=require</example>
          <note>Use sslmode=require in secrets - DatabaseManager will convert for asyncpg</note>
          <conversion_matrix>
            <sync_driver>Keeps sslmode=require (psycopg2 understands it)</sync_driver>
            <async_driver>Converts sslmode=require to ssl=require (asyncpg requirement)</async_driver>
            <unix_socket>Removes ALL SSL parameters (Cloud SQL proxy handles SSL)</unix_socket>
          </conversion_matrix>
        </implementation>
        <verification>
          <step>Test database connection with SSL parameter</step>
          <step>Verify DatabaseManager converts sslmode to ssl for asyncpg</step>
          <step>Check health checks pass in staging</step>
        </verification>
      </solution>
      <prevention>
        <guideline>Always include sslmode=require in GCP secrets (standard format)</guideline>
        <guideline>Use DatabaseManager for all URL conversions</guideline>
        <guideline>Never bypass DatabaseManager conversion logic</guideline>
      </prevention>
    </learning>
    
    <learning>
      <id>database-url-secret-source-and-flow</id>
      <category>deployment</category>
      <date>2025-08-23</date>
      <severity>critical</severity>
      <title>Database URL Flow from GCP Secrets to Application</title>
      <problem>
        <description>Health checker fails with sslmode parameter errors in staging</description>
        <symptoms>
          <symptom>connect() got an unexpected keyword argument 'sslmode'</symptom>
          <symptom>Health checks fail in GCP staging but work locally</symptom>
        </symptoms>
        <root_cause>
          <source>GCP Secret Manager stores #removed-legacywith sslmode=require</source>
          <bug>DatabaseManager.get_application_url_async() had logic bug preventing conversion</bug>
        </root_cause>
      </problem>
      <solution>
        <description>Fixed DatabaseManager conversion logic for async connections</description>
        <url_flow>
          <step1>deploy_to_gcp.py creates secret with sslmode=require (standard format)</step1>
          <step2>Cloud Run mounts secret as #removed-legacyenvironment variable</step2>
          <step3>DatabaseManager.get_base_database_url() reads and normalizes</step3>
          <step4>DatabaseManager.get_application_url_async() converts for asyncpg</step4>
        </url_flow>
        <critical_fix>
          Changed "elif" to "else" in get_application_url_async() to ensure conversion happens
        </critical_fix>
      </solution>
      <prevention>
        <guideline>Store URLs with sslmode=require in secrets (standard psycopg2 format)</guideline>
        <guideline>Always use DatabaseManager for URL processing</guideline>
        <guideline>Test URL conversion with actual staging URLs</guideline>
      </prevention>
    </learning>
    
    <learning>
      <id>staging-frontend-api-url-configuration</id>
      <category>deployment</category>
      <date>2025-08-22</date>
      <severity>high</severity>
      <title>Frontend API URL Must Point to Backend for Proxy Rewrites</title>
      <problem>
        <description>Frontend cannot connect to backend API in staging</description>
        <symptoms>
          <symptom>API requests from frontend failing</symptom>
          <symptom>CORS errors in staging environment</symptom>
          <symptom>Proxy rewrites not working correctly</symptom>
        </symptoms>
        <root_cause>NEXT_PUBLIC_API_URL not pointing to correct backend URL</root_cause>
      </problem>
      <solution>
        <description>Configure NEXT_PUBLIC_API_URL to point to backend API URL</description>
        <implementation>
          <example>NEXT_PUBLIC_API_URL=https://api.staging.netrasystems.ai</example>
          <note>Required for proxy rewrites to work correctly</note>
        </implementation>
        <verification>
          <step>Test frontend API calls work in staging</step>
          <step>Verify proxy rewrites function correctly</step>
          <step>Check CORS configuration allows requests</step>
        </verification>
      </solution>
      <prevention>
        <guideline>Always configure frontend API URL for each environment</guideline>
        <guideline>Test frontend-backend connectivity in staging</guideline>
        <guideline>Document API URL requirements for each environment</guideline>
      </prevention>
    </learning>
    
    <learning>
      <id>staging-health-check-503-errors</id>
      <category>deployment</category>
      <date>2025-08-22</date>
      <severity>high</severity>
      <title>Health Check 503 Errors Due to Database Connectivity</title>
      <problem>
        <description>Health check endpoints returning 503 in staging</description>
        <symptoms>
          <symptom>/health/ready endpoint fails with 503</symptom>
          <symptom>Database connectivity issues in health checks</symptom>
          <symptom>Service marked as unhealthy in Cloud Run</symptom>
        </symptoms>
        <root_cause>Database connectivity not properly configured for health checks</root_cause>
      </problem>
      <solution>
        <description>Fix database connectivity and SSL settings for health checks</description>
        <implementation>
          <check>Verify #removed-legacyincludes SSL settings</check>
          <check>Ensure database credentials are correct</check>
          <check>Test database connection from health check code</check>
        </implementation>
        <verification>
          <step>Test /health/ready endpoint returns 200</step>
          <step>Verify database connection works in health check</step>
          <step>Check Cloud Run service shows as healthy</step>
        </verification>
      </solution>
      <prevention>
        <guideline>Always test health checks before deploying</guideline>
        <guideline>Validate database connectivity in staging</guideline>
        <guideline>Monitor health check endpoints post-deployment</guideline>
      </prevention>
    </learning>
    
    <learning>
      <id>staging-oauth-separate-domain</id>
      <category>deployment</category>
      <date>2025-08-22</date>
      <severity>high</severity>
      <title>OAuth Flow Requires Auth Service at Separate Domain</title>
      <problem>
        <description>OAuth authentication flow fails in staging</description>
        <symptoms>
          <symptom>OAuth redirects not working</symptom>
          <symptom>CORS issues with auth service</symptom>
          <symptom>Callback URL configuration problems</symptom>
        </symptoms>
        <root_cause>Auth service needs separate domain for proper OAuth flow</root_cause>
      </problem>
      <solution>
        <description>Deploy auth service at separate domain with proper CORS and callback configuration</description>
        <implementation>
          <example>auth.staging.netrasystems.ai</example>
          <requirements>
            <requirement>Separate domain for auth service</requirement>
            <requirement>Proper CORS configuration</requirement>
            <requirement>Correct callback URL setup</requirement>
          </requirements>
        </implementation>
        <verification>
          <step>Test OAuth flow works end-to-end</step>
          <step>Verify callback URLs are correctly configured</step>
          <step>Check CORS allows cross-domain requests</step>
        </verification>
      </solution>
      <prevention>
        <guideline>Always use separate domain for auth service</guideline>
        <guideline>Test OAuth flow in staging before production</guideline>
        <guideline>Document OAuth domain requirements</guideline>
      </prevention>
    </learning>
    
    <learning>
      <id>staging-auth-service-configuration</id>
      <category>deployment</category>
      <date>2025-08-22</date>
      <severity>critical</severity>
      <title>USE_AUTH_SERVICE Must Be "true" for Token Validation</title>
      <problem>
        <description>Backend cannot validate tokens through auth service</description>
        <symptoms>
          <symptom>Token validation failures between services</symptom>
          <symptom>Cross-service authentication not working</symptom>
          <symptom>Users cannot access protected endpoints</symptom>
        </symptoms>
        <root_cause>USE_AUTH_SERVICE not set to "true" even when AUTH_SERVICE_URL is configured</root_cause>
      </problem>
      <solution>
        <description>Set USE_AUTH_SERVICE="true" for backend to validate tokens through auth service</description>
        <implementation>
          <requirement>USE_AUTH_SERVICE=true</requirement>
          <requirement>AUTH_SERVICE_URL=https://auth.staging.netrasystems.ai</requirement>
          <note>Both settings required for proper token validation</note>
        </implementation>
        <verification>
          <step>Test token validation works between services</step>
          <step>Verify protected endpoints are accessible with valid tokens</step>
          <step>Check cross-service authentication flows</step>
        </verification>
      </solution>
      <prevention>
        <guideline>Always set USE_AUTH_SERVICE=true when using auth service</guideline>
        <guideline>Validate token validation configuration in staging</guideline>
        <guideline>Test cross-service authentication before deployment</guideline>
      </prevention>
    </learning>
    
    <learning>
      <id>staging-deployment-command</id>
      <category>deployment</category>
      <date>2025-08-22</date>
      <severity>info</severity>
      <title>Recommended Staging Deployment Command</title>
      <description>Established best practice command for staging deployments</description>
      <command>python scripts/deploy_to_gcp.py --project netra-staging --build-local --run-checks</command>
      <benefits>
        <benefit>Local builds are 5-10x faster than Cloud Build</benefit>
        <benefit>Pre-deployment checks catch issues early</benefit>
        <benefit>Reliable deployment process</benefit>
      </benefits>
      <verification>
        <step>All services deploy successfully</step>
        <step>Health checks pass post-deployment</step>
        <step>Services are accessible at staging URLs</step>
      </verification>
    </learning>
    
    <learning>
      <id>staging-missing-secrets</id>
      <category>deployment</category>
      <date>2025-08-22</date>
      <severity>medium</severity>
      <title>Missing GCP Secrets for Full Staging Functionality</title>
      <problem>
        <description>Some features not working due to missing secrets</description>
        <symptoms>
          <symptom>LLM agent functionality limited</symptom>
          <symptom>Encryption features not available</symptom>
          <symptom>Some integrations failing</symptom>
        </symptoms>
        <missing_secrets>
          <secret>OPENAI_API_KEY</secret>
          <secret>FERNET_KEY</secret>
          <secret>Additional LLM API keys</secret>
        </missing_secrets>
      </problem>
      <solution>
        <description>Add missing secrets to GCP Secret Manager for full functionality</description>
        <implementation>
          <step>Identify all required secrets for staging</step>
          <step>Add secrets to GCP Secret Manager</step>
          <step>Update deployment configuration to use secrets</step>
          <step>Test full functionality with all secrets available</step>
        </implementation>
        <verification>
          <step>All LLM agent features work in staging</step>
          <step>Encryption functionality operational</step>
          <step>All integrations working correctly</step>
        </verification>
      </solution>
      <prevention>
        <guideline>Audit required secrets before staging deployment</guideline>
        <guideline>Maintain secret inventory for each environment</guideline>
        <guideline>Test functionality requiring secrets in staging</guideline>
      </prevention>
    </learning>
    
    <learning>
      <id>staging-service-discovery-needed</id>
      <category>deployment</category>
      <date>2025-08-22</date>
      <severity>medium</severity>
      <title>Service URL Discovery Needed for Dynamic Configuration</title>
      <problem>
        <description>Services need to discover each other's URLs dynamically</description>
        <symptoms>
          <symptom>Hard-coded service URLs in configuration</symptom>
          <symptom>Manual URL updates required for deployments</symptom>
          <symptom>Configuration drift between environments</symptom>
        </symptoms>
        <root_cause>No automated service discovery mechanism</root_cause>
      </problem>
      <solution>
        <description>Implement service discovery for dynamic URL configuration</description>
        <implementation>
          <approach>Use GCP service discovery APIs</approach>
          <approach>Environment-based service URL configuration</approach>
          <approach>Automatic service registration and discovery</approach>
        </implementation>
        <verification>
          <step>Services automatically discover each other</step>
          <step>Configuration updates automatically</step>
          <step>No manual URL configuration required</step>
        </verification>
      </solution>
      <prevention>
        <guideline>Implement service discovery from the start</guideline>
        <guideline>Avoid hard-coded service URLs</guideline>
        <guideline>Use environment-based configuration</guideline>
      </prevention>
    </learning>
  </staging_deployment_learnings>
  
  <deployment_best_practices>
    <practice priority="critical">
      <title>Pre-Deployment Validation</title>
      <steps>
        <step>Run all tests locally before deployment</step>
        <step>Validate configuration for target environment</step>
        <step>Check all required secrets are available</step>
        <step>Verify database connectivity settings</step>
        <step>Test OAuth configuration for environment</step>
      </steps>
    </practice>
    
    <practice priority="high">
      <title>Deployment Process</title>
      <steps>
        <step>Use --build-local for faster builds</step>
        <step>Always use --run-checks for validation</step>
        <step>Monitor deployment logs for errors</step>
        <step>Verify all services start successfully</step>
        <step>Test critical user flows post-deployment</step>
      </steps>
    </practice>
    
    <practice priority="medium">
      <title>Post-Deployment Verification</title>
      <steps>
        <step>Test all health check endpoints</step>
        <step>Verify frontend-backend connectivity</step>
        <step>Test authentication flows</step>
        <step>Check WebSocket connections</step>
        <step>Validate LLM agent functionality</step>
      </steps>
    </practice>
  </deployment_best_practices>
  
  <troubleshooting>
    <issue symptom="503 health check errors">
      <cause>Database connectivity issues</cause>
      <solution>Check #removed-legacyincludes sslmode=require</solution>
    </issue>
    
    <issue symptom="OAuth flow failures">
      <cause>Callback URL or domain configuration</cause>
      <solution>Verify auth service domain and OAuth settings</solution>
    </issue>
    
    <issue symptom="Frontend API failures">
      <cause>NEXT_PUBLIC_API_URL misconfiguration</cause>
      <solution>Point frontend to correct backend API URL</solution>
    </issue>
    
    <issue symptom="Token validation failures">
      <cause>USE_OAUTH_PROXY not set correctly</cause>
      <solution>Set USE_OAUTH_PROXY=true for auth service proxy</solution>
    </issue>
    
    <issue symptom="Service startup failures">
      <cause>Wrong server configuration for Cloud Run</cause>
      <solution>Use gunicorn with uvicorn workers</solution>
    </issue>
  </troubleshooting>
  
  <critical_deployment_learnings>
    <learning>
      <id>no-minimal-backend-versions</id>
      <category>deployment</category>
      <date>2025-08-23</date>
      <severity>critical</severity>
      <title>NEVER Create Minimal Backend Versions for Staging/Production</title>
      <problem>
        <description>Creating "minimal" versions of the backend (like main_minimal.py) for deployment causes severe issues</description>
        <symptoms>
          <symptom>Missing critical functionality in staging/production</symptom>
          <symptom>Incomplete API endpoints deployed</symptom>
          <symptom>WebSocket functionality missing</symptom>
          <symptom>Agent systems not available</symptom>
          <symptom>Database migrations not executed</symptom>
          <symptom>Health checks incomplete or misleading</symptom>
        </symptoms>
        <root_cause>Attempting to reduce resource usage by deploying stripped-down versions</root_cause>
      </problem>
      <solution>
        <description>ALWAYS deploy the full main.py application to all environments</description>
        <implementation>
          <requirement>Use netra_backend.app.main:app in Dockerfile CMD</requirement>
          <requirement>Never create main_minimal.py or similar reduced versions</requirement>
          <requirement>Deploy complete application with all features</requirement>
          <note>Resource optimization should be done through proper configuration, not code removal</note>
        </implementation>
        <verification>
          <step>Verify Dockerfile uses main:app not main_minimal:app</step>
          <step>Ensure no minimal versions exist in codebase</step>
          <step>Test all features work in staging</step>
          <step>Confirm WebSocket and agent systems functional</step>
        </verification>
      </solution>
      <prevention>
        <guideline>NEVER create minimal versions of the main application</guideline>
        <guideline>This is a VIOLATION of deployment best practices</guideline>
        <guideline>Use environment variables and configuration for resource management</guideline>
        <guideline>Deploy the same codebase to all environments</guideline>
        <guideline>Use Cloud Run scaling settings to manage resources</guideline>
      </prevention>
      <violation_severity>CRITICAL</violation_severity>
      <violation_message>Creating minimal backend versions is a CRITICAL VIOLATION that breaks production functionality</violation_message>
    </learning>
    
    <learning>
      <id>docker-build-node-modules-exclusion</id>
      <category>deployment</category>
      <date>2025-08-23</date>
      <severity>critical</severity>
      <title>Frontend Docker Builds Must Exclude node_modules</title>
      <problem>
        <description>Frontend Docker builds include massive node_modules directory causing 1.4GB+ context transfers</description>
        <symptoms>
          <symptom>Docker build context exceeds 1GB for frontend</symptom>
          <symptom>Extremely slow build times (10+ minutes)</symptom>
          <symptom>Deployment failures due to timeouts</symptom>
          <symptom>Unnecessary bandwidth usage</symptom>
          <symptom>Docker daemon performance issues</symptom>
        </symptoms>
        <root_cause>Missing or improperly configured .dockerignore for frontend directory</root_cause>
      </problem>
      <solution>
        <description>Create frontend-specific .dockerignore to exclude node_modules and other unnecessary files</description>
        <implementation>
          <step>Create frontend/.dockerignore file</step>
          <step>Include node_modules as first line</step>
          <step>Add other build artifacts (.next, coverage, etc.)</step>
          <step>Optionally add cleanup step in Dockerfile: RUN rm -rf node_modules</step>
          <note>Root .dockerignore may not apply to subdirectory contexts</note>
        </implementation>
        <example_dockerignore>
node_modules
.next
coverage
cypress
__tests__
*.log
*.md
.env
.env.*
test-*
jest.*
*.test.*
*.spec.*
tsconfig.tsbuildinfo
        </example_dockerignore>
        <verification>
          <step>Monitor Docker build context size (should be under 100MB)</step>
          <step>Verify build completes in under 2 minutes</step>
          <step>Check that node_modules is excluded from context</step>
        </verification>
      </solution>
      <prevention>
        <guideline>ALWAYS create service-specific .dockerignore files</guideline>
        <guideline>Monitor Docker build context sizes during deployment</guideline>
        <guideline>Test Docker builds locally before deploying</guideline>
        <guideline>Never rely solely on root .dockerignore for subdirectories</guideline>
      </prevention>
      <impact>
        <performance>10x faster Docker builds</performance>
        <bandwidth>1.3GB+ bandwidth saved per build</bandwidth>
        <reliability>Prevents deployment timeouts</reliability>
      </impact>
    </learning>
    
    <learning>
      <id>dockerignore-multiple-files-conflict</id>
      <category>deployment</category>
      <date>2025-08-23</date>
      <severity>high</severity>
      <title>Multiple .dockerignore Files Can Cause Confusion</title>
      <problem>
        <description>Having .dockerignore files in both root and subdirectories creates confusion about which exclusions apply</description>
        <symptoms>
          <symptom>Unexpected files included in Docker context</symptom>
          <symptom>Subdirectory .dockerignore files ignored when building from root</symptom>
          <symptom>Different behavior between local and CI/CD builds</symptom>
          <symptom>Context size varies unexpectedly</symptom>
        </symptoms>
        <root_cause>Docker only uses the .dockerignore from the build context directory, not subdirectories</root_cause>
      </problem>
      <solution>
        <description>Use only root .dockerignore with proper path specifications for all services</description>
        <implementation>
          <step>Remove subdirectory .dockerignore files (e.g., frontend/.dockerignore)</step>
          <step>Consolidate all exclusions in root .dockerignore</step>
          <step>Use proper path prefixes for service-specific exclusions</step>
          <example>
# Root .dockerignore
# Frontend exclusions
frontend/node_modules
frontend/.next
frontend/coverage
frontend/__tests__
frontend/cypress
# Backend exclusions  
**/__pycache__
**/*.pyc
# General exclusions
**/node_modules
**/.env
          </example>
        </implementation>
        <verification>
          <step>Verify only one .dockerignore exists at root</step>
          <step>Test Docker builds for all services</step>
          <step>Monitor context sizes remain small</step>
        </verification>
      </solution>
      <prevention>
        <guideline>ONLY use root .dockerignore when building from root context</guideline>
        <guideline>Document Docker build context clearly in deployment scripts</guideline>
        <guideline>Test exclusions work for all services</guideline>
        <guideline>Remove any subdirectory .dockerignore files to avoid confusion</guideline>
      </prevention>
      <key_insight>When Docker builds from root with "docker build -f deployment/docker/frontend.gcp.Dockerfile .", it uses root .dockerignore, NOT frontend/.dockerignore</key_insight>
    </learning>

    <learning>
      <id>docker-context-size-optimization-success</id>
      <category>deployment</category>
      <date>2025-08-23</date>
      <severity>info</severity>
      <title>Successfully Reduced Frontend Docker Context from 1.5GB to 179MB</title>
      <problem_solved>
        <original_issue>Frontend Docker build context was 1.5GB+ causing slow deployments</original_issue>
        <root_cause>node_modules and test files were being included in context</root_cause>
      </problem_solved>
      <solution_applied>
        <description>Fixed root .dockerignore to properly exclude frontend/node_modules and other unnecessary files</description>
        <changes>
          <change>Added frontend/node_modules to root .dockerignore</change>
          <change>Added **/node_modules for comprehensive exclusion</change>
          <change>Added frontend-specific test and build artifact exclusions</change>
          <change>Kept Dockerfile simple with COPY frontend/ instead of selective copying</change>
        </changes>
        <results>
          <result>Backend context: 147MB (good)</result>
          <result>Auth context: 7.8MB (excellent)</result>
          <result>Frontend context: 179MB (down from 1.5GB - 88% reduction)</result>
          <result>Build time reduced significantly</result>
          <result>Deployment reliability improved</result>
        </results>
      </solution_applied>
      <best_practices>
        <practice>Always check Docker context size during builds</practice>
        <practice>Use root .dockerignore for all exclusions when building from root</practice>
        <practice>Test .dockerignore effectiveness before deployments</practice>
        <practice>Keep Dockerfile COPY commands simple, let .dockerignore handle exclusions</practice>
      </best_practices>
    </learning>
    <learning>
      <id>auth-service-enhanced-security-secrets</id>
      <category>deployment</category>
      <date>2025-08-23</date>
      <severity>critical</severity>
      <title>Auth Service Requires SERVICE_SECRET and SERVICE_ID in Staging/Production</title>
      <problem>
        <description>Auth service fails to start in staging/production without enhanced security environment variables</description>
        <symptoms>
          <symptom>Service startup failures with ValueError: SERVICE_SECRET must be set in production/staging</symptom>
          <symptom>Worker processes crash during gunicorn/uvicorn initialization</symptom>
          <symptom>Health checks return 503 Service Unavailable</symptom>
          <symptom>Continuous worker restart loops in Cloud Run logs</symptom>
          <symptom>Service marked as unhealthy despite container being ready</symptom>
        </symptoms>
        <root_cause>Enhanced JWT security requires SERVICE_SECRET and SERVICE_ID for production environments</root_cause>
      </problem>
      <solution>
        <description>Create and map SERVICE_SECRET and SERVICE_ID secrets in GCP Secret Manager</description>
        <implementation>
          <step>Generate secure SERVICE_SECRET: openssl rand -hex 32</step>
          <step>Create SERVICE_ID with timestamp: netra-auth-staging-$(date +%s)</step>
          <step>Add secrets to GCP Secret Manager:
            - gcloud secrets create service-secret-staging --data-file=-
            - gcloud secrets create service-id-staging --data-file=-
          </step>
          <step>Update Cloud Run service to map secrets:
            --update-secrets="SERVICE_SECRET=service-secret-staging:latest,SERVICE_ID=service-id-staging:latest"
          </step>
          <step>Update deploy_to_gcp.py to include these secrets in auth service deployment</step>
        </implementation>
        <verification>
          <step>Check /health endpoint returns 200 OK</step>
          <step>Verify service uptime is stable</step>
          <step>Confirm no worker restart loops in logs</step>
          <step>Test JWT token validation works correctly</step>
        </verification>
      </solution>
      <prevention>
        <guideline>Always include SERVICE_SECRET and SERVICE_ID in auth service deployments</guideline>
        <guideline>Document all required environment variables in deployment scripts</guideline>
        <guideline>Test auth service startup in staging before production</guideline>
        <guideline>Monitor for startup failures after deployments</guideline>
      </prevention>
      <impact>Without these secrets, auth service cannot start and authentication is completely unavailable</impact>
    </learning>

    <learning>
      <id>staging-url-domain-consistency</id>
      <category>deployment</category>
      <date>2025-08-23</date>
      <severity>high</severity>
      <title>Always Use Staging Domain Names, Not Raw Cloud Run URLs</title>
      <problem>
        <description>Regression where staging domain URLs were replaced with raw Cloud Run URLs</description>
        <symptoms>
          <symptom>Frontend configured with Cloud Run URLs instead of domain names</symptom>
          <symptom>API calls may fail if Cloud Run URLs change</symptom>
          <symptom>CORS configuration becomes brittle</symptom>
          <symptom>SSL certificate validation issues</symptom>
          <symptom>Difficult to migrate or update services</symptom>
        </symptoms>
        <root_cause>Direct use of Cloud Run URLs bypasses domain abstraction layer</root_cause>
      </problem>
      <solution>
        <description>Always use consistent staging domain names for service configuration</description>
        <implementation>
          <correct_configuration>
            staging_api_url = "https://api.staging.netrasystems.ai"
            staging_auth_url = "https://auth.staging.netrasystems.ai"
            staging_ws_url = "wss://api.staging.netrasystems.ai/ws"
          </correct_configuration>
          <incorrect_configuration>
            # NEVER use raw Cloud Run URLs like these:
            staging_api_url = "https://netra-backend-staging-701982941522.us-central1.run.app"
            staging_auth_url = "https://netra-auth-service-701982941522.us-central1.run.app"
          </incorrect_configuration>
        </implementation>
        <verification>
          <step>Check deploy_to_gcp.py uses domain names not Cloud Run URLs</step>
          <step>Verify frontend environment variables use proper domains</step>
          <step>Test that services are accessible via domain names</step>
        </verification>
      </solution>
      <prevention>
        <guideline>ALWAYS use domain names for service URLs in configuration</guideline>
        <guideline>Never hardcode Cloud Run URLs in deployment scripts</guideline>
        <guideline>Set up domain mapping in Cloud Run for all services</guideline>
        <guideline>Document the correct domain URLs for each environment</guideline>
      </prevention>
      <benefits>
        <benefit>Services can be migrated without breaking configurations</benefit>
        <benefit>Consistent URLs across deployments</benefit>
        <benefit>Better SSL certificate management</benefit>
        <benefit>Cleaner CORS configuration</benefit>
      </benefits>
    </learning>

    <learning>
      <id>nextjs-standalone-docker-configuration</id>
      <category>deployment</category>
      <date>2025-08-24</date>
      <severity>critical</severity>
      <title>Next.js Standalone Mode Requires Specific Docker Configuration</title>
      <problem>
        <description>Frontend staging deployment hangs due to incorrect Dockerfile configuration for Next.js standalone builds</description>
        <symptoms>
          <symptom>Container appears to start but application never becomes available</symptom>
          <symptom>Health checks timeout waiting for port 3000</symptom>
          <symptom>No error messages but service is unreachable</symptom>
          <symptom>Cloud Run deployment appears stuck in "deploying" state</symptom>
          <symptom>Service eventually fails with timeout errors</symptom>
        </symptoms>
        <root_cause>Dockerfile using "npm start" instead of running standalone server.js directly</root_cause>
      </problem>
      <solution>
        <description>Configure Dockerfile to properly handle Next.js standalone build output</description>
        <implementation>
          <incorrect_approach>
            # WRONG - This won't work with standalone builds
            COPY --from=builder /app/.next ./.next
            COPY --from=builder /app/package*.json ./
            RUN npm ci --production
            CMD ["npm", "start"]
          </incorrect_approach>
          <correct_approach>
            # CORRECT - Copy standalone build and run server.js directly
            COPY --from=builder /app/.next/standalone ./
            COPY --from=builder /app/.next/static ./.next/static
            COPY --from=builder /app/public ./public
            CMD ["node", "server.js"]
          </correct_approach>
          <key_differences>
            <difference>Copy .next/standalone directory contents to root</difference>
            <difference>Copy static files separately to .next/static</difference>
            <difference>No need for npm ci in production stage</difference>
            <difference>Run server.js directly with node, not npm start</difference>
          </key_differences>
        </implementation>
        <verification>
          <step>Build Docker image locally and test it runs</step>
          <step>Verify server.js exists in container root</step>
          <step>Check application responds on port 3000</step>
          <step>Deploy to staging and verify health checks pass</step>
        </verification>
      </solution>
      <prevention>
        <guideline>Always test Docker builds locally before deploying</guideline>
        <guideline>Understand Next.js build output modes (standalone vs standard)</guideline>
        <guideline>Match Dockerfile configuration to Next.js build mode</guideline>
        <guideline>Document build mode requirements in deployment docs</guideline>
      </prevention>
      <technical_details>
        <detail>Next.js standalone mode creates a self-contained server at .next/standalone/server.js</detail>
        <detail>Standalone includes all necessary dependencies, no npm install needed</detail>
        <detail>Static files must be copied separately as they're not in standalone directory</detail>
        <detail>Environment variables like PORT and HOSTNAME control server behavior</detail>
      </technical_details>
    </learning>

    <learning>
      <id>frontend-memory-allocation-optimization</id>
      <category>deployment</category>
      <date>2025-08-24</date>
      <severity>high</severity>
      <title>Frontend Memory Allocation Should Match NODE_OPTIONS Settings</title>
      <problem>
        <description>Mismatch between Cloud Run memory allocation and Node.js memory limits</description>
        <symptoms>
          <symptom>Frontend allocated 8Gi in Cloud Run but NODE_OPTIONS limited to 2048MB</symptom>
          <symptom>Excessive resource usage and cost</symptom>
          <symptom>Potential memory pressure issues</symptom>
          <symptom>Slower cold starts due to large container size</symptom>
        </symptoms>
        <root_cause>Over-provisioning memory without matching Node.js configuration</root_cause>
      </problem>
      <solution>
        <description>Align Cloud Run memory allocation with NODE_OPTIONS settings</description>
        <implementation>
          <cloud_run_config>
            memory="2Gi"  # Match NODE_OPTIONS limit
            cpu="1"
            min_instances=1  # Keep warm for better performance
          </cloud_run_config>
          <node_options>
            ENV NODE_OPTIONS="--max-old-space-size=2048"
          </node_options>
          <rationale>2Gi is sufficient for Next.js production workloads</rationale>
        </implementation>
        <verification>
          <step>Monitor memory usage in Cloud Run metrics</step>
          <step>Verify no out-of-memory errors occur</step>
          <step>Check application performance remains good</step>
          <step>Validate cost reduction from lower memory allocation</step>
        </verification>
      </solution>
      <prevention>
        <guideline>Match container memory allocation to application limits</guideline>
        <guideline>Monitor actual memory usage before increasing allocation</guideline>
        <guideline>Start with conservative memory settings and scale up if needed</guideline>
        <guideline>Use min_instances=1 for critical services to avoid cold starts</guideline>
      </prevention>
      <cost_impact>Reduces Cloud Run costs by 75% for frontend service</cost_impact>
    </learning>

    <learning>
      <id>frontend-api-url-configuration-requirement</id>
      <category>deployment</category>
      <date>2025-08-24</date>
      <severity>high</severity>
      <title>Frontend Requires NEXT_PUBLIC_API_URL in Deployment Configuration</title>
      <problem>
        <description>Frontend cannot connect to backend API without proper environment variable</description>
        <symptoms>
          <symptom>API calls fail with connection errors</symptom>
          <symptom>Next.js rewrites not working correctly</symptom>
          <symptom>Frontend appears to work but has no data</symptom>
          <symptom>WebSocket connections fail to establish</symptom>
        </symptoms>
        <root_cause>Missing NEXT_PUBLIC_API_URL environment variable in deployment</root_cause>
      </problem>
      <solution>
        <description>Add NEXT_PUBLIC_API_URL to frontend deployment configuration</description>
        <implementation>
          <deployment_config>
            environment_vars={
                "NODE_ENV": "production",
                "NEXT_PUBLIC_API_URL": "https://api.staging.netrasystems.ai",
            }
          </deployment_config>
          <next_config_rewrites>
            # next.config.ts uses this for API proxy rewrites
            const backendUrl = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8000';
          </next_config_rewrites>
        </implementation>
        <verification>
          <step>Check environment variable is set in Cloud Run</step>
          <step>Verify API calls from frontend succeed</step>
          <step>Test WebSocket connections work</step>
          <step>Confirm data loads in frontend UI</step>
        </verification>
      </solution>
      <prevention>
        <guideline>Document all required environment variables for each service</guideline>
        <guideline>Include environment variables in deployment scripts</guideline>
        <guideline>Test frontend-backend connectivity after deployment</guideline>
        <guideline>Use staging domain URLs, not Cloud Run URLs</guideline>
      </prevention>
    </learning>
    <learning>
      <id>frontend-dockerfile-canonical-location</id>
      <category>deployment</category>
      <date>2025-08-28</date>
      <severity>critical</severity>
      <title>Frontend Dockerfile Must Use deployment/docker/frontend.gcp.Dockerfile - No Random Creation</title>
      <problem>
        <description>Random creation of frontend Dockerfiles in different locations causes confusion and deployment failures</description>
        <symptoms>
          <symptom>Multiple Dockerfile versions in different locations</symptom>
          <symptom>Deployment scripts using wrong Dockerfile path</symptom>
          <symptom>Git showing deleted frontend/Dockerfile while deployment expects it</symptom>
          <symptom>Build failures due to missing or incorrect Dockerfile references</symptom>
          <symptom>Inconsistent build configurations across environments</symptom>
        </symptoms>
        <root_cause>Lack of awareness about canonical Dockerfile location and tendency to create new files</root_cause>
      </problem>
      <solution>
        <description>ALWAYS use the existing Dockerfile at deployment/docker/frontend.gcp.Dockerfile</description>
        <implementation>
          <canonical_location>deployment/docker/frontend.gcp.Dockerfile</canonical_location>
          <build_command>docker build -t frontend -f deployment/docker/frontend.gcp.Dockerfile .</build_command>
          <cloudbuild_reference>
            steps:
            - name: gcr.io/cloud-builders/docker
              args: ['build', '-t', 'gcr.io/$PROJECT_ID/netra-frontend-staging:latest', 
                     '-f', 'deployment/docker/frontend.gcp.Dockerfile', '.']
          </cloudbuild_reference>
          <deployment_script_reference>
            # In scripts/deploy_to_gcp.py
            dockerfile="deployment/docker/frontend.gcp.Dockerfile"
          </deployment_script_reference>
        </implementation>
        <verification>
          <step>Verify only one frontend Dockerfile exists at deployment/docker/frontend.gcp.Dockerfile</step>
          <step>Check no frontend/Dockerfile exists (should be deleted)</step>
          <step>Confirm all deployment scripts reference correct path</step>
          <step>Test Docker build works with canonical Dockerfile</step>
        </verification>
      </solution>
      <prevention>
        <guideline>NEVER create new Dockerfiles for frontend - use existing deployment/docker/frontend.gcp.Dockerfile</guideline>
        <guideline>If frontend/Dockerfile is deleted in git, DO NOT recreate it</guideline>
        <guideline>All services use deployment/docker/*.gcp.Dockerfile pattern</guideline>
        <guideline>Update existing Dockerfile if changes needed, don't create new ones</guideline>
        <guideline>Check deployment scripts for correct Dockerfile paths before creating files</guideline>
      </prevention>
      <dockerfile_structure>
        <service>Frontend: deployment/docker/frontend.gcp.Dockerfile</service>
        <service>Backend: deployment/docker/backend.gcp.Dockerfile</service>
        <service>Auth: deployment/docker/auth.gcp.Dockerfile</service>
        <note>All Dockerfiles follow the same naming pattern and location</note>
      </dockerfile_structure>
      <violation_severity>CRITICAL</violation_severity>
      <violation_message>Creating random Dockerfiles instead of using canonical ones breaks deployment consistency</violation_message>
    </learning>
  </critical_deployment_learnings>
</specification>