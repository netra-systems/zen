<?xml version="1.0" encoding="UTF-8"?>
<deployment_learnings>
  <title>Deployment Learnings</title>
  <description>Lessons learned from GCP deployments and infrastructure management</description>
  
  <staging_deployment>
    <lesson id="staging-setup">
      <problem>Staging deployment process was unclear and mixed with production configs</problem>
      <solution>
        Created dedicated staging configuration:
        - terraform.staging.tfvars with cost-optimized resources
        - deploy-staging.sh automated script
        - Separate workspace and state bucket
      </solution>
      <key_files>
        - terraform-gcp/terraform.staging.tfvars
        - terraform-gcp/deploy-staging.sh
        - docs/STAGING_DEPLOYMENT.md
      </key_files>
    </lesson>
    
    <lesson id="docker-builds">
      <problem>Frontend Dockerfile naming inconsistency</problem>
      <solution>
        Use Dockerfile.frontend.staging for staging builds
        Use Dockerfile.frontend.optimized for production
      </solution>
      <commands>
        docker build -f Dockerfile.frontend.staging .
      </commands>
    </lesson>
    
    <lesson id="terraform-backend">
      <problem>Terraform backend not configured in main.tf</problem>
      <solution>
        Added GCS backend configuration to main.tf:
        backend "gcs" {
          # Bucket and prefix set via backend-config
        }
      </solution>
    </lesson>
    
    <lesson id="oauth-https-redirect">
      <problem>OAuth callback URLs using HTTP instead of HTTPS in staging environment</problem>
      <root_cause>
        When Cloud Run receives requests behind a proxy/load balancer, request.base_url
        returns HTTP instead of HTTPS, causing OAuth redirect_uri mismatch
      </root_cause>
      <solution>
        Force HTTPS for staging and production environments in auth routes:
        - Updated app/routes/auth/auth.py login endpoint
        - Updated app/routes/auth/auth.py config endpoint
        - Check environment and replace http:// with https:// for non-dev environments
      </solution>
      <affected_files>
        - app/routes/auth/auth.py
      </affected_files>
      <key_changes>
        if auth_env_config.environment.value in ["staging", "production"]:
            base_url = base_url.replace("http://", "https://")
      </key_changes>
    </lesson>
    
    <lesson id="cloud-armor-issues">
      <problem>cloud-armor-ip-blocking.tf references non-existent service</problem>
      <solution>
        Temporarily disabled file by renaming to .disabled
        Fix: Change google_cloud_run_service.app to google_cloud_run_service.backend
      </solution>
    </lesson>
    
    <lesson id="iam-restrictions">
      <problem>Organization policy prevents setting allUsers IAM binding</problem>
      <solution>
        Deploy services first, then manually add IAM bindings if needed:
        gcloud beta run services add-iam-policy-binding
      </solution>
    </lesson>
    
    <lesson id="cloud-run-uvicorn-workers">
      <problem>Cloud Run backend startup failing with uvicorn workers parameter error</problem>
      <root_cause>
        The cloud_run_startup.py script was passing 'workers' parameter to uvicorn.run(),
        but uvicorn.run() doesn't accept this parameter when called programmatically.
        The workers parameter is only available via command line.
      </root_cause>
      <error_trace>
        Traceback shows failure at uvicorn.run(**config) with workers in config dict
      </error_trace>
      <solution>
        Two fixes implemented:
        1. Fixed cloud_run_startup.py - removed 'workers' from uvicorn config dict
        2. Updated Dockerfile.backend - switched from cloud_run_startup.py to gunicorn with uvicorn workers
      </solution>
      <affected_files>
        - app/cloud_run_startup.py
        - Dockerfile.backend
      </affected_files>
      <key_changes>
        # cloud_run_startup.py - removed workers parameter
        return {
            "app": "app.main:app", "host": "0.0.0.0", "port": get_port(),
            "loop": "asyncio", "log_level": "info",
            "access_log": True, "use_colors": False, "timeout_keep_alive": get_timeout()
        }
        
        # Dockerfile.backend - switched to gunicorn
        CMD exec gunicorn app.main:app \
            --bind 0.0.0.0:$PORT \
            --workers $WORKERS \
            --worker-class uvicorn.workers.UvicornWorker \
            --timeout $TIMEOUT \
            --access-logfile - \
            --error-logfile - \
            --log-level info
      </key_changes>
      <prevention>
        Always use gunicorn with uvicorn workers for production Cloud Run deployments.
        Gunicorn provides better worker management and Cloud Run compatibility.
      </prevention>
    </lesson>
    
    <lesson id="frontend-api-url-configuration">
      <problem>Frontend was calling localhost instead of staging API URL</problem>
      <solution>
        Build frontend with staging API URL using build args:
        docker build -f Dockerfile.frontend.staging \
          --build-arg NEXT_PUBLIC_API_URL=https://api.staging.netrasystems.ai \
          --build-arg NEXT_PUBLIC_WS_URL=wss://api.staging.netrasystems.ai/ws \
          -t us-central1-docker.pkg.dev/netra-staging/netra-containers/frontend:latest .
      </solution>
      <details>
        The frontend uses environment variables at build time for API configuration.
        These must be set during Docker build, not at runtime.
        Default staging API URL: https://api.staging.netrasystems.ai
      </details>
    </lesson>
    
    <lesson id="oauth-credential-env-var-mismatch">
      <problem>OAuth authentication failing with invalid_client and CSRF state mismatch errors in staging</problem>
      <root_cause>
        Multiple configuration mismatches:
        1. OAuth client initialized with wrong credentials - used settings.oauth_config instead of environment-specific config
        2. Terraform set GOOGLE_CLIENT_ID but code expected GOOGLE_OAUTH_CLIENT_ID_STAGING
        3. Session middleware https_only flag not set for staging (only production)
        4. Frontend URL hardcoded to localhost:3010 instead of staging URL
      </root_cause>
      <solution>
        Fixed all configuration mismatches:
        1. Updated app/auth/auth.py to use auth_env_config.get_oauth_config() instead of settings.oauth_config
        2. Updated terraform-gcp/main.tf to set correct env var names:
           - GOOGLE_OAUTH_CLIENT_ID_STAGING (was GOOGLE_CLIENT_ID)
           - GOOGLE_OAUTH_CLIENT_SECRET_STAGING (was GOOGLE_CLIENT_SECRET)
        3. Updated middleware_setup.py to set https_only for both staging and production
        4. Added _get_frontend_url_for_environment() helper to dynamically determine frontend URL
        5. Added ENVIRONMENT=staging env var in Terraform for proper environment detection
      </solution>
      <affected_files>
        - app/auth/auth.py
        - app/routes/auth/auth.py
        - app/core/middleware_setup.py
        - terraform-gcp/main.tf
      </affected_files>
      <key_changes>
        # OAuth client initialization
        oauth_config = auth_env_config.get_oauth_config()
        self.google = self.oauth.register(
            client_id=oauth_config.client_id,
            client_secret=oauth_config.client_secret
        )
        
        # Terraform env vars
        env {
          name  = "GOOGLE_OAUTH_CLIENT_ID_STAGING"
          value_from { secret_key_ref { name = "google-client-id-staging" } }
        }
        
        # Session middleware
        https_only=(settings.environment in ["production", "staging"])
      </key_changes>
      <prevention>
        Always verify environment variable names match between infrastructure and application code.
        Use single source of truth for OAuth configuration - prefer environment-specific config over static settings.
      </prevention>
    </lesson>
    <lesson id="database-connection-staging">
      <problem>Staging backend database connection regression - DATABASE_URL not passed to Cloud Run</problem>
      <root_cause>
        The deploy-staging-reliable.ps1 script was missing DATABASE_URL and other critical
        environment variables when deploying to Cloud Run. The terraform configuration
        expected these but the PowerShell deployment script didn't include them.
      </root_cause>
      <solution>
        1. Updated deploy-staging-reliable.ps1 to include secrets via --set-secrets flag
        2. Created setup-staging-database-secret.ps1 to manage DATABASE_URL in Secret Manager
        3. Fixed gcloud command syntax: use --set-secrets not --update-secrets
      </solution>
      <key_files>
        - deploy-staging-reliable.ps1
        - setup-staging-database-secret.ps1
        - docs/deployment/GCP_STAGING_DEPLOYMENT.md
      </key_files>
      <commands>
        # Setup database secret first
        .\setup-staging-database-secret.ps1
        
        # Deploy with secrets properly configured
        .\deploy-staging-reliable.ps1
      </commands>
    </lesson>
  </staging_deployment>
  
  <quick_commands>
    <deploy_staging>
      # Complete staging deployment
      cd terraform-gcp
      ./deploy-staging.sh
    </deploy_staging>
    
    <build_and_push>
      # Build and push images
      docker build -t us-central1-docker.pkg.dev/netra-staging/netra-containers/backend:latest -f Dockerfile.backend .
      docker push us-central1-docker.pkg.dev/netra-staging/netra-containers/backend:latest
      
      # Frontend with staging API URL
      docker build -f Dockerfile.frontend.staging \
        --build-arg NEXT_PUBLIC_API_URL=https://api.staging.netrasystems.ai \
        --build-arg NEXT_PUBLIC_WS_URL=wss://api.staging.netrasystems.ai/ws \
        -t us-central1-docker.pkg.dev/netra-staging/netra-containers/frontend:latest .
      docker push us-central1-docker.pkg.dev/netra-staging/netra-containers/frontend:latest
    </build_and_push>
    
    <deploy_services>
      # Deploy to Cloud Run
      gcloud run deploy netra-backend --image us-central1-docker.pkg.dev/netra-staging/netra-containers/backend:latest --region us-central1 --platform managed --allow-unauthenticated --quiet
      gcloud run deploy netra-frontend --image us-central1-docker.pkg.dev/netra-staging/netra-containers/frontend:latest --region us-central1 --platform managed --allow-unauthenticated --quiet
    </deploy_services>
  </quick_commands>
  
  <staging_urls>
    <frontend>https://netra-frontend-701982941522.us-central1.run.app/</frontend>
    <backend>https://netra-backend-701982941522.us-central1.run.app/</backend>
  </staging_urls>
  
  <cost_optimization>
    <note>Staging uses minimal resources and scales to zero when idle</note>
    <db_tier>db-f1-micro</db_tier>
    <backend_resources>0.5 vCPU, 1GB RAM</backend_resources>
    <frontend_resources>0.25 vCPU, 512MB RAM</frontend_resources>
  </cost_optimization>
</deployment_learnings>