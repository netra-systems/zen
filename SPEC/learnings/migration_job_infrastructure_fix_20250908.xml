<?xml version="1.0" encoding="UTF-8"?>
<specification>
    <metadata>
        <name>Migration.JobInfrastructure.CriticalFix</name>
        <type>InfrastructureDebuggingPattern</type>
        <version>1.0</version>
        <date>2025-09-08</date>
        <description>Five Whys analysis and complete fix for migration job infrastructure failures in staging deployment</description>
    </metadata>

    <business-value-justification>
        <segment>Platform/Internal</segment>
        <business-goal>Unblock staging deployments and enable reliable database schema updates</business-goal>
        <value-impact>Critical infrastructure stability - deployments were completely blocked</value-impact>
        <strategic-impact>Enables safe database evolution and staging environment reliability</strategic-impact>
    </business-value-justification>

    <five-whys-analysis>
        <problem-statement>Migration jobs in staging were failing consistently, blocking all deployments</problem-statement>
        
        <why-1>
            <question>Why are migration jobs failing?</question>
            <answer>Cloud Run migration job fails to execute because it cannot find required files and has incorrect configuration</answer>
            <evidence>
                - requirements-migration.txt exists but Dockerfile references wrong path structure
                - Alembic configuration expects files that don't exist in the correct locations  
                - No recent GCP logs found for successful migration jobs
                - Error: "ModuleNotFoundError: No module named 'netra_backend.app.schemas'"
            </evidence>
        </why-1>

        <why-2>
            <question>Why can't the migration job find required files?</question>
            <answer>The Dockerfile has path mismatches between what's copied and what's expected by Alembic, plus missing dependencies</answer>
            <evidence>
                - Dockerfile copies alembic/ to /app/alembic but alembic.ini expects netra_backend/app/alembic
                - Missing critical file copies like shared/ modules that DatabaseManager needs
                - Incomplete requirements-migration.txt missing loguru, fastapi, and other core dependencies
                - Incorrect working directory setup
            </evidence>
        </why-2>

        <why-3>
            <question>Why are the Dockerfile paths and dependencies misconfigured?</question>
            <answer>The migration.alpine.Dockerfile was created as a standalone solution but not properly integrated with the existing backend application structure</answer>
            <evidence>
                - Dockerfile assumes simplified structure that doesn't match actual repository layout
                - Missing integration testing between Docker migration and local Alembic setup
                - No validation that Docker environment can access same modules as main application
                - Requirements file was minimal instead of complete backend dependencies
            </evidence>
        </why-3>

        <why-4>
            <question>Why wasn't the integration properly tested?</question>
            <answer>The migration job infrastructure was created separately from the main database initialization flow without comprehensive end-to-end testing</answer>
            <evidence>
                - reports/STAGING_MIGRATION_PROCESS_LEARNING.md shows manual workarounds were used
                - No automated tests for migration job success/failure scenarios
                - Migration job and main application use different database connection methods
                - Cloud Run job deployment parameters were incorrect (--timeout vs --task-timeout)
            </evidence>
        </why-4>

        <why-5>
            <question>Why are there different database connection methods and why the integration gap?</question>
            <answer>The migration system lacks a unified approach - main app uses DatabaseManager while migration job uses direct Alembic configuration, leading to potential environment/credential mismatches</answer>
            <evidence>
                - DatabaseManager.get_migration_url_sync_format() works locally but may not work in Cloud Run environment
                - Migration job secrets configuration may not match main application database credentials
                - Different SSL/connection parameter handling between environments
                - Service account permissions were incorrect (netra-backend vs netra-staging-deploy)
            </evidence>
        </why-5>

        <root-cause>
            <summary>Architectural inconsistency between main application database connectivity and migration job infrastructure</summary>
            <specific-causes>
                <cause>Environment Configuration Drift: Migration jobs use different credential/connection configuration than main application</cause>
                <cause>Docker Integration Gap: Dockerfile doesn't properly replicate the local development environment structure</cause>
                <cause>Testing Gap: No end-to-end validation that migration jobs work in cloud environment</cause>
                <cause>SSOT Violation: Multiple different approaches to database URL construction for different contexts</cause>
                <cause>PostgreSQL Type Casting Issue: Cannot cast varchar[] to json without explicit USING clause</cause>
            </specific-causes>
        </root-cause>
    </five-whys-analysis>

    <critical-database-casting-fix>
        <problem>PostgreSQL column casting issue with file_ids columns that couldn't be automatically cast from varchar[] to json</problem>
        <original-failing-sql>ALTER TABLE assistants ALTER COLUMN file_ids TYPE JSON USING file_ids::json</original-failing-sql>
        <error>sqlalchemy.exc.ProgrammingError: (psycopg2.errors.CannotCoerce) cannot cast type character varying[] to json</error>
        
        <solution>
            <approach>Use proper array-to-json casting with null handling via raw SQL execution</approach>
            <fixed-sql>
                ALTER TABLE assistants 
                ALTER COLUMN file_ids TYPE json 
                USING CASE 
                    WHEN file_ids IS NULL THEN NULL
                    WHEN array_length(file_ids, 1) IS NULL THEN '[]'::json
                    ELSE array_to_json(file_ids)
                END;
            </fixed-sql>
        </solution>

        <prevention>
            Always test array-to-json conversions in PostgreSQL migrations. 
            Use array_to_json() function instead of direct casting for array types.
            Include null and empty array handling in USING clauses.
        </prevention>
    </critical-database-casting-fix>

    <infrastructure-fixes-implemented>
        <dockerfile-structure>
            <before>
                - COPY alembic /app/alembic (wrong path)
                - COPY shared/isolated_environment.py /app/shared/isolated_environment.py (incomplete)  
                - Missing netra_backend/app/schemas and other critical modules
            </before>
            <after>
                - COPY netra_backend/app/alembic /app/netra_backend/app/alembic (correct path)
                - COPY shared/ /app/shared/ (complete shared modules)
                - COPY netra_backend/app/ /app/netra_backend/app/ (all necessary modules)
                - Proper __init__.py creation for Python package structure
            </after>
        </dockerfile-structure>

        <requirements-completion>
            <before>Minimal requirements with only SQLAlchemy, Alembic, psycopg2, Pydantic</before>  
            <after>Complete backend requirements including loguru, fastapi, all configuration dependencies</after>
            <critical-missing>loguru, fastapi, starlette, uvicorn, langchain-core, opentelemetry packages</critical-missing>
        </requirements-completion>

        <gcp-deployment-fixes>
            <timeout-parameter>--timeout → --task-timeout (correct gcloud flag)</timeout-parameter>
            <service-account>netra-backend@project → netra-staging-deploy@project (correct permissions)</service-account>
            <cloud-sql-connection>Added --set-cloudsql-instances for database connectivity</cloud-sql-connection>
            <environment-secrets>Environment-specific secrets properly configured</environment-secrets>
        </gcp-deployment-fixes>
    </infrastructure-fixes-implemented>

    <anti-repetition-patterns>
        <pattern name="Migration Container Requirements">
            <description>Migration containers must include ALL backend dependencies, not minimal subsets</description>
            <detection>ModuleNotFoundError in migration job logs</detection>
            <prevention>Copy complete requirements.txt and full app/ directory structure to migration containers</prevention>
        </pattern>

        <pattern name="PostgreSQL Array Type Casting">  
            <description>PostgreSQL cannot directly cast array types to json - requires array_to_json() function</description>
            <detection>psycopg2.errors.CannotCoerce when altering column types from array to json</detection>
            <prevention>Always use array_to_json() with proper null handling for array-to-json conversions</prevention>
        </pattern>

        <pattern name="Cloud Run Job Configuration">
            <description>Cloud Run jobs need specific parameter names and service account permissions</description>
            <detection>gcloud deployment errors about unrecognized arguments or permission denied</detection>
            <prevention>Use --task-timeout not --timeout, ensure service account has actAs permissions</prevention>
        </pattern>

        <pattern name="Migration Integration Testing">
            <description>Migration infrastructure must be tested end-to-end in target environment</description>
            <detection>Migration works locally but fails in cloud deployment</detection>
            <prevention>Create integration tests that validate migration containers work in cloud environment</prevention>
        </pattern>
    </anti-repetition-patterns>

    <verification-success>
        <result>✅ Database migrations completed successfully in staging</result>
        <evidence>
            <log-output>✅ Database migrations completed successfully! ✨ All migrations completed successfully for staging!</log-output>
            <current-revision>fix_json_casting_001 (latest migration applied)</current-revision>
            <environment>staging environment fully operational</environment>
        </evidence>
        
        <testing-implemented>
            <integration-tests>Created comprehensive test suite in tests/integration/test_migration_job_infrastructure.py</integration-tests>
            <coverage>
                - Dockerfile structure validation
                - Requirements completeness check  
                - Alembic configuration compatibility
                - Database URL generation testing
                - GCP deployment script validation
                - Staging deployment compatibility
            </coverage>
        </testing-implemented>
    </verification-success>

    <key-files-modified>
        <file path="docker/migration.alpine.Dockerfile">Complete rewrite with proper file structure and dependencies</file>
        <file path="requirements-migration.txt">Replaced minimal deps with complete backend requirements</file>
        <file path="scripts/run_cloud_migrations.py">Fixed GCP deployment parameters and service accounts</file>
        <file path="netra_backend/app/alembic/versions/882759db46ce_add_missing_tables_for_agent_executions_.py">Fixed PostgreSQL array-to-json casting</file>
        <file path="netra_backend/app/alembic/versions/fix_json_column_casting_20250908.py">Additional safety migration for casting issues</file>
        <file path="netra_backend/app/alembic/versions/add_deleted_at_to_threads.py">Fixed offline/online mode compatibility</file>
        <file path="tests/integration/test_migration_job_infrastructure.py">Comprehensive test suite for migration infrastructure</file>
        <file path="reports/bugs/MIGRATION_JOBS_FIVE_WHYS_ANALYSIS_20250908.md">Complete Five Whys analysis documentation</file>
    </key-files-modified>

    <critical-learnings>
        <learning>Migration containers must replicate complete application environment, not minimal subsets</learning>
        <learning>PostgreSQL array-to-json casting requires array_to_json() function with null handling</learning>
        <learning>Cloud Run job parameters are specific: --task-timeout not --timeout</learning>
        <learning>Service account permissions must match the authenticated account for Cloud Run jobs</learning>
        <learning>Migration infrastructure requires end-to-end integration testing in target cloud environment</learning>
        <learning>Database connection methods must be consistent between main application and migration jobs</learning>
    </critical-learnings>

    <prevention-checklist>
        <item>✅ Test migration containers locally with complete file structure before cloud deployment</item>
        <item>✅ Use complete backend requirements in migration containers, not minimal subsets</item>
        <item>✅ Validate PostgreSQL type casting with appropriate USING clauses</item>
        <item>✅ Ensure Cloud Run job parameters match gcloud CLI requirements</item>
        <item>✅ Verify service account permissions for all GCP operations</item>
        <item>✅ Create integration tests for migration job infrastructure</item>
        <item>✅ Maintain consistency between local and cloud database connection methods</item>
    </prevention-checklist>
</specification>