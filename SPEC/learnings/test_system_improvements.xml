<?xml version="1.0" encoding="UTF-8"?>
<learnings>
    <metadata>
        <title>Test System Improvements and E2E Fix</title>
        <created>2025-01-28</created>
        <category>testing</category>
        <priority>critical</priority>
    </metadata>

    <problem_statement>
        <issue>E2E tests were completely broken and blocking CI/CD</issue>
        <symptoms>
            <symptom>3,091 tests attempting to be collected in e2e directory</symptom>
            <symptom>564 test files causing collection errors and timeouts</symptom>
            <symptom>Unified test runner misconfigured - running ALL tests from three directories</symptom>
            <symptom>Tests hanging indefinitely when running full e2e suite</symptom>
            <symptom>No test execution history or tracking</symptom>
            <symptom>No visibility into test health trends</symptom>
        </symptoms>
    </problem_statement>

    <root_causes>
        <cause id="1">
            <description>Test category definitions were too broad</description>
            <details>E2E category included entire test directories from multiple services instead of actual e2e tests</details>
        </cause>
        <cause id="2">
            <description>No test execution tracking system</description>
            <details>No way to track which tests fail frequently, are flaky, or slow</details>
        </cause>
        <cause id="3">
            <description>Helper classes incorrectly placed in test directories</description>
            <details>Test discovery was picking up non-test classes as tests</details>
        </cause>
    </root_causes>

    <solutions_implemented>
        <solution id="1">
            <name>Test Execution Tracker</name>
            <file>scripts/test_execution_tracker.py</file>
            <features>
                <feature>SQLite-based persistent test history</feature>
                <feature>Automatic test categorization</feature>
                <feature>Flaky test detection with alternation scoring</feature>
                <feature>Performance tracking per test</feature>
                <feature>Smart test prioritization strategies</feature>
                <feature>Session-based tracking with metadata</feature>
            </features>
        </solution>

        <solution id="2">
            <name>Fixed E2E Category Definitions</name>
            <file>tests/unified_test_runner.py</file>
            <changes>
                <before><![CDATA["e2e": ["tests/e2e", "netra_backend/tests", "auth_service/tests"]]]></before>
                <after><![CDATA[
"e2e_critical": ["tests/e2e/critical"],  # Curated critical e2e tests
"e2e": ["tests/e2e/integration"],        # Actual e2e tests only
"e2e_full": ["tests/e2e"]                # Full suite (use with caution)
                ]]></after>
            </changes>
        </solution>

        <solution id="3">
            <name>Test Dashboard</name>
            <file>scripts/test_dashboard.py</file>
            <features>
                <feature>Interactive CLI dashboard</feature>
                <feature>Category health monitoring</feature>
                <feature>Actionable recommendations</feature>
                <feature>HTML report generation</feature>
                <feature>Failure trend analysis</feature>
            </features>
        </solution>

        <solution id="4">
            <name>Smart Default Categories</name>
            <description>Test runner now uses tracker to determine healthy default categories</description>
            <implementation>Categories with good pass rates are marked as default and run automatically</implementation>
        </solution>
    </solutions_implemented>

    <key_learnings>
        <learning id="1">
            <title>Test categories must be precisely scoped</title>
            <description>Never include entire service test directories in e2e categories. E2E should only include actual end-to-end integration tests.</description>
            <pattern>Create separate categories for different test scopes: critical, integration, full</pattern>
        </learning>

        <learning id="2">
            <title>Test tracking is essential for large codebases</title>
            <description>Without historical data, it's impossible to identify patterns like flaky tests, performance degradation, or failure trends</description>
            <pattern>Implement persistent test result tracking from day one</pattern>
        </learning>

        <learning id="3">
            <title>Helper classes must be outside test directories</title>
            <description>Test discovery tools will attempt to run any class starting with Test* in test directories</description>
            <pattern>Place test utilities in test_framework/ or dedicated utility directories</pattern>
        </learning>

        <learning id="4">
            <title>Default test sets should be data-driven</title>
            <description>Don't hardcode which tests run by default - use historical pass rates to determine safe defaults</description>
            <pattern>Mark categories as default based on recent health metrics</pattern>
        </learning>

        <learning id="5">
            <title>Test execution must be observable</title>
            <description>Teams need dashboards and reports to understand test system health</description>
            <pattern>Provide multiple views: CLI, HTML reports, trend analysis</pattern>
        </learning>
    </key_learnings>

    <best_practices>
        <practice>Always separate e2e tests into critical (smoke) and comprehensive suites</practice>
        <practice>Track every test execution with metadata (duration, status, environment)</practice>
        <practice>Identify flaky tests through alternation pattern analysis</practice>
        <practice>Prioritize recently failed tests to get faster feedback</practice>
        <practice>Generate regular test health reports for the team</practice>
        <practice>Use SQLite for test tracking - it's simple, fast, and requires no setup</practice>
    </best_practices>

    <metrics>
        <metric name="e2e_test_reduction">From 3,091 to ~50 critical tests</metric>
        <metric name="test_categories_created">3 new e2e categories (critical, standard, full)</metric>
        <metric name="tracking_features_added">6 (history, flaky detection, prioritization, dashboard, HTML reports, session tracking)</metric>
        <metric name="database_tables_created">3 (test_metadata, test_runs, test_sessions)</metric>
    </metrics>

    <usage_examples>
        <example>
            <description>Run default healthy categories</description>
            <command>python unified_test_runner.py</command>
        </example>
        <example>
            <description>Run critical E2E tests only</description>
            <command>python unified_test_runner.py --category e2e_critical</command>
        </example>
        <example>
            <description>View test dashboard</description>
            <command>python scripts/test_dashboard.py overview</command>
        </example>
        <example>
            <description>Generate HTML report</description>
            <command>python scripts/test_dashboard.py html</command>
        </example>
        <example>
            <description>Find flaky tests</description>
            <command>python scripts/test_execution_tracker.py flaky</command>
        </example>
    </usage_examples>

    <related_files>
        <file>scripts/test_execution_tracker.py</file>
        <file>scripts/test_dashboard.py</file>
        <file>tests/unified_test_runner.py</file>
        <file>scripts/business_value_test_index.py</file>
        <file>E2E_TEST_BLOCKING_AUDIT.md</file>
    </related_files>
</learnings>