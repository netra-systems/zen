<?xml version='1.0' encoding='utf-8'?>
<specification>
  <metadata>
    <name>Auth Learnings</name>
    <type>learnings</type>
    <version>2.1</version>
    <last_updated>2025-08-28</last_updated>
    <description>Comprehensive authentication learnings including OAuth startup validation</description>
    <last_edited>2025-08-28T15:35:00</last_edited>
    <critical>true</critical>
    <business_impact>Mission Critical - Authentication Required for All User Access</business_impact>
  </metadata>
  <learning id="mandatory-auth-integration" priority="CRITICAL">
    <title>ðŸ”´ MANDATORY: All Auth Must Use auth_integration Module</title>
    <problem>
            Multiple duplicate auth implementations scattered across codebase:
            - netra_backend/app/auth/ directory (legacy, MUST be removed)
            - netra_backend/app/schemas/Auth.py (duplicate models)
            - Test files importing from netra_backend.app.auth.*
            - Services implementing their own auth logic
        </problem>
    <root_cause>
            Historical evolution led to multiple auth implementations.
            No clear architectural boundary enforcement.
            Developers creating local auth implementations instead of using central service.
        </root_cause>
    <solution>
      <step>ALL auth MUST use netra_backend/app/auth_integration/auth.py</step>
      <step>Legacy netra_backend/app/auth/ directory completely removed</step>
      <step>All imports changed from netra_backend.app.auth.* to netra_backend.app.auth_integration.*</step>
      <step>Backward compatibility stubs added for migration period</step>
      <step>Security service uses auth_integration only</step>
    </solution>
    <files_modified>
      <file>netra_backend/app/auth_integration/auth.py - Added compatibility stubs</file>
      <file>SPEC/shared_auth_integration.xml - Updated to mandate single source</file>
      <file>All test files - Updated imports to auth_integration</file>
    </files_modified>
    <prevention>
            NEVER create duplicate auth logic.
            ALWAYS import from netra_backend.app.auth_integration.auth.
            Run: grep -r "from netra_backend.app.auth" --exclude-dir=auth_integration
            Expected: No results (only auth_integration should be imported).
        </prevention>
    <business_impact>
            Security: Single point for security updates and patches.
            Compliance: Centralized audit trail for all auth operations.
            Revenue: Prevents auth-related downtime that impacts customers.
            Enterprise: Consistent auth experience across all tiers.
        </business_impact>
  </learning>
  <learning id="dev-user-creation-duplication" priority="CRITICAL">
    <title>ðŸ”´ CRITICAL: Dev User Creation Must Use Single Source</title>
    <problem>
            Multiple duplicate get_or_create_dev_user implementations causing:
            - UniqueViolationError: duplicate key value violates unique constraint "ix_userbase_email"
            - Multiple functions trying to create same dev@example.com user
            - Race conditions between different creation points
            - Inconsistent dev user attributes across creation points
        </problem>
    <root_cause>
            Historical implementations created separate dev user creation logic in:
            - netra_backend/app/routes/auth_routes/dev_login.py
            - netra_backend/app/services/apex_optimizer_agent/dev_utils.py
            - netra_backend/app/auth_integration/auth.py (inline logic)
            Each trying to create users independently without proper duplicate checking.
        </root_cause>
    <solution>
      <step>Created SINGLE get_or_create_dev_user in user_service</step>
      <step>All dev user creation now uses user_service.get_or_create_dev_user()</step>
      <step>Function properly handles existing users and race conditions</step>
      <step>Uses IntegrityError catching for concurrent request handling</step>
    </solution>
    <files_modified>
      <file>netra_backend/app/services/user_service.py - Added centralized get_or_create_dev_user</file>
      <file>netra_backend/app/routes/auth_routes/dev_login.py - Updated to use centralized function</file>
      <file>netra_backend/app/services/apex_optimizer_agent/dev_utils.py - Updated to use centralized function</file>
      <file>netra_backend/app/auth_integration/auth.py - Simplified to use centralized function</file>
      <file>netra_backend/app/routes/auth_routes/callback_processor.py - Uses get_or_create for OAuth</file>
    </files_modified>
    <prevention>
            NEVER create User models directly for dev/test users.
            ALWAYS use user_service.get_or_create_dev_user() for dev users.
            ALWAYS use user_service.get_or_create() for regular users.
            Run: grep -r "User(" --include="*.py" | grep -v "test_" | grep email
            Expected: No direct User model creation outside user_service.
        </prevention>
    <business_impact>
            Stability: Prevents startup failures in development/staging.
            Developer Experience: No duplicate key errors blocking development.
            Revenue: Prevents auth service downtime from impacting customers.
            Testing: Consistent dev user creation for reliable testing.
        </business_impact>
  </learning>
  <learning id="oauth-startup-validation" priority="CRITICAL">
    <title>ðŸ”´ CRITICAL: OAuth Must Be Validated at Service Startup</title>
    <problem>
      Auth service can start with missing or invalid OAuth configuration, causing:
      - Users unable to login via OAuth in staging/production
      - Silent failures where service appears healthy but OAuth is broken
      - Missing GOOGLE_OAUTH_CLIENT_ID_STAGING and GOOGLE_OAUTH_CLIENT_SECRET_STAGING
      - Invalid OAuth credentials (placeholders, wrong format)
    </problem>
    <root_cause>
      No startup validation of OAuth configuration.
      Service starts successfully even without OAuth credentials.
      Health checks don't verify OAuth provider availability.
      OAuth failures only discovered when users try to login.
    </root_cause>
    <solution>
      <step>Added comprehensive OAuth validation at startup in main.py lifespan</step>
      <step>Validate OAuth client ID format (must end with .apps.googleusercontent.com)</step>
      <step>Check for placeholder values (REPLACE_*)</step>
      <step>Verify OAuth provider can initialize and generate URLs</step>
      <step>FAIL FAST in staging/production if OAuth is misconfigured</step>
      <step>Added self_check() and validate_configuration() to OAuth providers</step>
      <step>Created /oauth/status endpoint for runtime monitoring</step>
    </solution>
    <files_modified>
      <file>auth_service/main.py - Added OAuth validation at startup (lines 121-235)</file>
      <file>auth_service/auth_core/oauth/google_oauth.py - Added validate_configuration() and self_check()</file>
      <file>auth_service/main.py - Added /oauth/status endpoint (lines 598-666)</file>
    </files_modified>
    <prevention>
      ALWAYS validate OAuth at startup in staging/production.
      NEVER allow auth service to start without valid OAuth in production.
      Monitor /oauth/status endpoint for OAuth health.
      Run OAuth self-check before every deployment.
      Validate OAuth client ID format, not just presence.
    </prevention>
    <business_impact>
      Revenue: Prevents authentication failures that block user access ($75K+ MRR impact).
      User Experience: Users can always login via OAuth when service is running.
      Operations: Fail fast prevents silent OAuth failures in production.
      Monitoring: /oauth/status provides real-time OAuth health visibility.
    </business_impact>
  </learning>
  <learning id="oauth-local-staging-mismatch">
    <title>OAuth Configuration Mismatch for Local Development</title>
    <problem>
            OAuth returns errors when running locally:
            - invalid_client: Unauthorized
            - mismatching_state: CSRF Warning! State not equal in request and response
        </problem>
    <root_cause>
            Multiple configuration issues:
            1. Environment detection incorrectly uses staging OAuth for local development
            2. OAuth credentials not properly loaded from environment variables
            3. Session middleware configuration incompatible with local development
            4. Redirect URI mismatch between configured and actual URLs
        </root_cause>
    <solution>
      <step>Update environment detection to properly handle local development</step>
      <step>Support multiple OAuth credential naming conventions</step>
      <step>Configure session middleware for local development</step>
      <step>Add localhost:3010 to allowed redirect URIs</step>
    </solution>
    <files_modified>
      <file>netra_backend/app/auth/environment_config.py</file>
      <file>netra_backend/app/routes/auth/auth.py</file>
      <file>netra_backend/app/auth/auth.py</file>
      <file>netra_backend/app/core/middleware_setup.py</file>
    </files_modified>
    <prevention>
            Always test OAuth flow locally before deploying to staging.
            Use environment-specific configuration files (.env.local.staging).
            Run OAuth diagnostic script: python scripts/fix_oauth_local.py
        </prevention>
  </learning>
  <learning id="oauth-environment-variables">
    <title>OAuth Environment Variable Configuration</title>
    <problem>
            OAuth credentials not loading from environment variables
        </problem>
    <root_cause>
            Multiple naming conventions for OAuth credentials:
            - GOOGLE_CLIENT_ID vs GOOGLE_OAUTH_CLIENT_ID_DEV
            - Environment variables not loaded before checking
        </root_cause>
    <solution>
      <step>Support both naming conventions in environment_config.py</step>
      <step>Load .env file in diagnostic scripts</step>
      <step>Create .env.local.staging for local staging-like setup</step>
    </solution>
    <files_modified>
      <file>netra_backend/app/auth/environment_config.py</file>
      <file>scripts/fix_oauth_local.py</file>
      <file>.env.local.staging</file>
    </files_modified>
    <prevention>
            Standardize environment variable naming across environments.
            Document required environment variables in .env.example.
        </prevention>
  </learning>
  <learning id="oauth-session-middleware">
    <title>Session Middleware Configuration for OAuth</title>
    <problem>
            OAuth state parameter validation fails due to session issues
        </problem>
    <root_cause>
            Session middleware configured with https_only=true for local development.
            Same-site cookie policy incompatible with OAuth flow.
        </root_cause>
    <solution>
      <step>Configure session middleware based on environment</step>
      <step>Use same_site="lax" for local development</step>
      <step>Add DISABLE_HTTPS_ONLY option for local testing</step>
      <step>Set appropriate session max_age</step>
    </solution>
    <files_modified>
      <file>netra_backend/app/core/middleware_setup.py</file>
    </files_modified>
    <prevention>
            Test OAuth flow with different cookie configurations.
            Document session requirements for OAuth in each environment.
        </prevention>
  </learning>
  <learning id="oauth-redirect-uri-mismatch">
    <title>OAuth Redirect URI Configuration</title>
    <problem>
            Google OAuth returns redirect_uri_mismatch error
        </problem>
    <root_cause>
            Redirect URIs in Google Cloud Console don't match application configuration.
            Different ports used for local development (3000, 3010, 8000).
        </root_cause>
    <solution>
      <step>Add all local development URLs to Google Cloud Console OAuth app</step>
      <step>Update environment_config.py to include all redirect URIs</step>
      <step>Log redirect URI being used for debugging</step>
    </solution>
    <files_modified>
      <file>netra_backend/app/auth/environment_config.py</file>
      <file>netra_backend/app/routes/auth/auth.py</file>
    </files_modified>
    <prevention>
            Maintain list of authorized redirect URIs in documentation.
            Log OAuth configuration during initialization.
            Check redirect URIs match between code and Google Cloud Console.
        </prevention>
  </learning>
  <learning id="oauth-diagnostic-tool">
    <title>OAuth Diagnostic Tool Creation</title>
    <problem>
            Difficult to diagnose OAuth configuration issues
        </problem>
    <solution>
            Created diagnostic script: scripts/fix_oauth_local.py
            - Checks environment variables
            - Validates OAuth configuration
            - Suggests specific fixes
            - Loads .env file automatically
        </solution>
    <files_created>
      <file>scripts/fix_oauth_local.py</file>
    </files_created>
    <usage>
            Run: python scripts/fix_oauth_local.py
            Checks all OAuth-related configuration and suggests fixes.
        </usage>
  </learning>
  <learning id="oauth-localhost-2025">
    <title>OAuth Staging Errors on Localhost (January 2025)</title>
    <problem>
            When running locally with staging configuration:
            - invalid_client: Unauthorized (wrong OAuth credentials)
            - mismatching_state: CSRF Warning (session not maintaining state)
        </problem>
    <root_cause>
            Critical configuration mismatches:
            1. Environment detection was simplified and removed localhost checks
            2. Staging OAuth credentials used with localhost redirect URIs
            3. Session middleware not properly configured for localhost
            4. OAuth state parameter lost due to cookie configuration
        </root_cause>
    <solution>
      <step>Enhanced environment detection to force development OAuth for localhost</step>
      <step>Added localhost detection based on multiple indicators</step>
      <step>Fixed session middleware to use lax same-site for localhost</step>
      <step>Added validation to ensure redirect URI matches allowed list</step>
      <step>Created setup_oauth_local.py for proper configuration</step>
    </solution>
    <code_changes>
      <change file="netra_backend/app/auth/environment_config.py">
                Added localhost detection logic to _detect_environment()
                Forces development OAuth unless FORCE_STAGING_OAUTH=true
            </change>
      <change file="netra_backend/app/core/middleware_setup.py">
                Fixed session middleware to detect localhost properly
                Sets same_site="lax" and https_only=False for localhost
            </change>
      <change file="netra_backend/app/routes/auth/auth.py">
                Added OAuth credential validation
                Enhanced redirect URI validation
                Better error handling with specific messages
            </change>
    </code_changes>
    <prevention>
            ALWAYS force development OAuth for localhost URLs.
            Test OAuth flow locally before deploying.
            Run: python scripts/setup_oauth_local.py to configure.
            Check Google Console redirect URIs match exactly.
        </prevention>
  </learning>
  <learning id="oauth-configuration-hierarchy">
    <title>OAuth Configuration Environment Hierarchy</title>
    <insight>
            OAuth must use environment-appropriate credentials:
            - Localhost MUST use development credentials
            - Staging deployments use staging credentials
            - Production uses production credentials
        </insight>
    <rules>
      <rule>Never use staging/production OAuth with localhost URLs</rule>
      <rule>Always validate redirect URI is in allowed list</rule>
      <rule>Session cookies need same_site="lax" for localhost</rule>
      <rule>Log OAuth configuration for debugging</rule>
    </rules>
    <environment_detection>
            Priority order for detecting localhost:
            1. FRONTEND_URL contains localhost
            2. API_URL contains localhost
            3. PORT is 8000, 3000, or 3010
            4. Not running in Cloud Run (no K_SERVICE)
            5. FORCE_LOCAL_OAUTH=true
        </environment_detection>
  </learning>
  <learning id="oauth-setup-script">
    <title>OAuth Setup Automation</title>
    <problem>
            Manual OAuth configuration is error-prone
        </problem>
    <solution>
            Created comprehensive setup scripts:
            - setup_oauth_local.py: Configure for localhost
            - fix_oauth_local.py: Diagnose issues
        </solution>
    <usage>
            # Setup OAuth for local development
            python scripts/setup_oauth_local.py
            
            # Diagnose OAuth issues
            python scripts/fix_oauth_local.py
        </usage>
    <features>
            - Detects existing configuration
            - Validates Google Console setup
            - Creates .env.local with proper settings
            - Runs diagnostic checks
            - Provides clear next steps
        </features>
  </learning>
  <learning id="shared-auth-integration-mandatory" date="2025-01-17" severity="CRITICAL">
    <problem>
            Authentication logic was potentially duplicated across multiple files in the system.
            This created maintenance burden and risk of inconsistent auth behavior.
        </problem>
    <root_cause>
            Previous directory name "dependencies_auth" did not clearly communicate 
            that this was the MANDATORY shared auth service for the entire system.
        </root_cause>
    <solution>
            1. Renamed netra_backend/app/dependencies_auth/ to netra_backend/app/auth_integration/ for clarity
            2. Updated all imports throughout the system
            3. Created SPEC/shared_auth_integration.xml documenting MANDATORY use
            4. Added clear documentation that ALL auth MUST use this service
        </solution>
    <prevention>
            - Directory name "auth_integration" clearly indicates integration point
            - Documentation emphasizes MANDATORY use throughout system
            - No duplicate auth logic allowed anywhere
            - Single source of truth for all authentication
        </prevention>
    <key_functions>
            - get_current_user(): For protected routes (returns user or 401)
            - get_current_user_optional(): For optional auth (returns user or None)
            - validate_token(): For WebSocket authentication
        </key_functions>
    <impact>
            - Consistent auth behavior across entire system
            - Single point for security updates
            - Easier testing with single mock point
            - Reduced code duplication
            - Simplified compliance and auditing
        </impact>
  </learning>
  <learning id="auth-service-singleton-initialization" date="2025-08-18" severity="CRITICAL">
    <title>Auth Service Singleton Not Initialized in Routes</title>
    <problem>
            auth_routes.py uses auth_service variable throughout (lines 119, 135, 143, etc.)
            but never creates an instance of AuthService class.
            Error: "name 'auth_service' is not defined" when accessing /auth/dev/login endpoint.
        </problem>
    <root_cause>
            The file imports AuthService class but doesn't create a singleton instance.
            All route handlers expect auth_service to be a module-level singleton.
            Missing initialization code that creates and configures the AuthService instance.
        </root_cause>
    <solution>
            Add singleton initialization after imports in auth_routes.py:
            auth_service = AuthService()
            This creates single shared instance for all route handlers.
        </solution>
    <files_modified>
      <file>auth_service/auth_core/routes/auth_routes.py - Added auth_service initialization</file>
    </files_modified>
    <prevention>
            ALWAYS initialize service instances when importing service classes.
            Use singleton pattern for stateful services like auth.
            Test all endpoints after adding new route files.
        </prevention>
    <business_impact>
            Critical: Auth service failures block all user access.
            Revenue: Users cannot login means zero platform usage.
            Enterprise: Auth failures are immediate escalation issues.
        </business_impact>
  </learning>
  
  <learning id="cold-start-auth-service-dev-login" date="2025-08-22" severity="CRITICAL">
    <title>Auth Service Dev Login Endpoint Critical for Cold Start</title>
    <problem>
            Cold start validation requires functional authentication but OAuth setup
            is complex for development. Need reliable dev login endpoint for testing.
        </problem>
    <root_cause>
            Development environment needs simple authentication method that doesn't
            require external OAuth provider configuration.
        </root_cause>
    <solution>
            Auth service provides /auth/dev/login endpoint that:
            1. Creates fixed dev@example.com user automatically
            2. Generates valid JWT tokens for testing
            3. Works without OAuth provider setup
            4. Maintains same token format as production
        </solution>
    <implementation>
            Endpoint: POST /auth/dev/login
            Response: {"access_token": "jwt_token", "token_type": "bearer"}
            User: dev@example.com (created automatically if doesn't exist)
            
            Critical for:
            - Cold start validation scripts
            - Development environment testing
            - Integration test authentication
            - Local development workflow
        </implementation>
    <verification>
            1. POST request to /auth/dev/login returns valid JWT
            2. Token validates correctly in backend endpoints
            3. User dev@example.com exists in database
            4. Token format matches production OAuth tokens
        </verification>
    <prevention>
            ALWAYS test dev login endpoint in cold start validation.
            NEVER remove dev login functionality from auth service.
            Ensure dev login creates users with proper permissions.
        </prevention>
    <business_impact>
            Critical: Enables development and testing workflows.
            Developer Experience: Removes OAuth setup complexity for development.
            Testing: Enables automated authentication in test suites.
        </business_impact>
  </learning>
  
  <learning id="cold-start-jwt-secret-synchronization" date="2025-08-22" severity="CRITICAL">
    <title>JWT Secret Synchronization Between Services</title>
    <problem>
            Backend and auth service use different environment variables for JWT secrets:
            - Backend expects JWT_SECRET_KEY
            - Auth service expects JWT_SECRET
            This causes token validation failures between services.
        </problem>
    <root_cause>
            Inconsistent environment variable naming across microservices.
            Historical evolution led to different naming conventions.
        </root_cause>
    <solution>
            1. Set both JWT_SECRET_KEY and JWT_SECRET in environment
            2. Ensure both have identical 64+ character values
            3. Update configuration loading to support both names
            4. Validate secret synchronization during startup
        </solution>
    <implementation>
            File: .env (line 39)
            JWT_SECRET_KEY=your-64-character-secret-here
            JWT_SECRET=your-64-character-secret-here
            
            Both variables must have identical values for proper operation.
        </implementation>
    <verification>
            1. Backend can validate tokens generated by auth service
            2. Auth service can validate tokens generated by backend
            3. Cross-service authentication works correctly
            4. WebSocket authentication succeeds with auth service tokens
        </verification>
    <prevention>
            Standardize environment variable names across all services.
            Add startup validation to ensure JWT secrets match.
            Document required environment variables clearly.
            Use configuration validation scripts before deployment.
        </prevention>
    <business_impact>
            Critical: Prevents authentication failures between services.
            Revenue: Users can access all platform features without auth errors.
            Security: Ensures consistent token validation across system.
        </business_impact>
  </learning>
  
  <learning id="cold-start-auth-service-port-conflicts" date="2025-08-22" severity="HIGH">
    <title>Auth Service Port Conflict Resolution</title>
    <problem>
            Auth service cannot start on fixed port 8081 due to port conflicts.
            Static port assignment causes development environment failures.
        </problem>
    <root_cause>
            Multiple services or system processes using port 8081.
            No automatic port conflict detection and resolution.
        </root_cause>
    <solution>
            Implemented dynamic port allocation in dev launcher:
            1. Detect port conflicts automatically
            2. Allocate next available port (8082, 8083, etc.)
            3. Record allocated port in service discovery
            4. Update health checks to use dynamic ports
        </solution>
    <implementation>
            Dev launcher automatically:
            - Tests port availability before service startup
            - Allocates first available port starting from 8081
            - Creates .service_discovery/auth_service.json with actual port
            - Updates health check URLs to use discovered port
        </implementation>
    <verification>
            1. Auth service starts successfully on available port
            2. Service discovery file contains correct port
            3. Health checks use dynamic port from discovery
            4. Frontend configuration updated with correct port
        </verification>
    <prevention>
            Always use dynamic port allocation for development services.
            Never hard-code service ports in configuration.
            Implement service discovery for all inter-service communication.
        </prevention>
    <business_impact>
            Developer Experience: Eliminates port conflict setup issues.
            Velocity: Reduces time spent debugging port conflicts.
            Reliability: Ensures services start consistently across environments.
        </business_impact>
  </learning>
  
  <learning id="cold-start-oauth-vs-dev-login-strategy" date="2025-08-22" severity="HIGH">
    <title>OAuth vs Dev Login Strategy for Development</title>
    <problem>
            OAuth setup is complex and often fails in development environments.
            Need reliable authentication method for cold start validation.
        </problem>
    <strategy>
            Development Authentication Strategy:
            1. Primary: Use dev login endpoint (/auth/dev/login)
            2. Secondary: OAuth flow (when fully configured)
            3. Testing: Always use dev login for automated tests
            4. Staging: OAuth required for production-like testing
        </strategy>
    <implementation>
            Cold start validation:
            - Uses dev login endpoint for reliable authentication
            - Tests token generation and validation
            - Validates cross-service authentication
            - Skips OAuth complexity for basic functionality testing
            
            OAuth testing:
            - Separate OAuth configuration validation
            - Optional for basic development workflow
            - Required for staging environment validation
        </implementation>
    <benefits>
            1. Reliable cold start validation without OAuth setup
            2. Faster development environment setup
            3. Automated testing without external dependencies
            4. Clear separation between dev and production auth
        </benefits>
    <verification>
            1. Dev login works consistently across environments
            2. Token validation succeeds between all services
            3. OAuth can be tested separately when configured
            4. Authentication strategy documented clearly
        </verification>
  </learning>
  
  <learning id="cold-start-auth-database-sync" date="2025-08-22" severity="MEDIUM">
    <title>Auth Service Database Synchronization</title>
    <problem>
            Auth service maintains separate auth_users table but needs to sync
            user IDs with main database for proper authorization.
        </problem>
    <solution>
            1. Auth service creates users in auth_users table
            2. Main backend syncs user information from auth service
            3. User IDs maintained consistently across both databases
            4. Token validation works with synchronized user data
        </solution>
    <implementation>
            Auth service:
            - Maintains auth_users table for authentication
            - Provides user information via token validation
            
            Backend:
            - Syncs user data from auth service responses
            - Creates/updates users based on token validation
            - Maintains referential integrity across services
        </implementation>
    <verification>
            1. Users exist in both auth_users and main database
            2. User IDs are consistent across services
            3. Token validation returns correct user information
            4. Authorization works with synchronized data
        </verification>
    <prevention>
            Test user synchronization in cold start validation.
            Monitor user data consistency between services.
            Implement user sync validation in health checks.
        </prevention>
  </learning>
  
  <learning id="jwt-token-validation-performance-fix" date="2025-08-23" severity="CRITICAL">
    <title>JWT Token Validation Performance and Concurrency Fix</title>
    <problem>
            JWT token validation tests failing due to replay attack protection preventing
            legitimate concurrent validations of the same token:
            - test_validate_token_concurrent_validation: Only 1/100 validations succeeding
            - test_validate_token_performance: Average validation time > 10ms requirement
            - Error: "Token ID already used (replay attack)" for legitimate read operations
        </problem>
    <root_cause>
            JWT handler was incorrectly applying replay attack protection (JWT ID tracking) 
            to token VALIDATION operations. Validation is a read operation that should be 
            idempotent and concurrent-safe. JWT ID tracking should only apply to token 
            CONSUMPTION operations (like token exchange or refresh).
        </root_cause>
    <solution>
            1. Separated validation into two distinct operations:
               - validate_token(): For read operations (no replay protection)
               - validate_token_for_consumption(): For consumption operations (with replay protection)
            2. Removed JWT ID tracking from _validate_cross_service_token() method
            3. Added _validate_cross_service_token_with_replay_protection() for consumption
            4. Updated refresh_access_token() to use consumption validation
        </solution>
    <implementation>
            Fixed methods in auth_service/auth_core/core/jwt_handler.py:
            
            validate_token(): 
            - Used for authentication verification (read operations)
            - No JWT ID tracking to allow concurrent validations
            - Maintains all other security validations (signature, expiry, claims)
            
            validate_token_for_consumption():
            - Used for token exchange operations (write operations)  
            - Includes JWT ID tracking for replay attack prevention
            - Prevents token reuse in consumption scenarios
            
            _validate_cross_service_token():
            - Base security validation without replay protection
            - Validates issuer, audience, timing constraints
            - Safe for concurrent read operations
            
            _validate_cross_service_token_with_replay_protection():
            - Extends base validation with JWT ID tracking
            - Used only for token consumption operations
        </implementation>
    <performance_impact>
            Before fix:
            - Concurrent validation: 1/100 success (99% failure due to false replay detection)
            - Performance: > 10ms average (failed performance test)
            
            After fix:
            - Concurrent validation: 100/100 success (0% failure)
            - Performance: < 10ms average (passes performance requirements)
            - 10 threads Ã— 10 validations each = 100 successful concurrent validations
        </performance_impact>
    <security_analysis>
            Security maintains appropriate boundaries:
            
            Read Operations (validate_token):
            - Authentication verification for protected routes
            - WebSocket authentication checks  
            - User authorization lookups
            - Token introspection operations
            â†’ No replay protection needed (idempotent reads)
            
            Write Operations (validate_token_for_consumption):
            - Token refresh operations
            - Token exchange flows
            - One-time token consumption
            - Authorization code redemption
            â†’ Replay protection required (state-changing operations)
        </security_analysis>
    <verification>
            All tests now pass:
            - test_validate_token_concurrent_validation: 100/100 validations succeed
            - test_validate_token_performance: Average < 10ms, total < 1s
            - All 14 token validation tests pass without regression
            - 55 JWT-related tests pass across auth service
        </verification>
    <prevention>
            Design Principle: Distinguish between read and write operations
            - Read operations (validation): Should be idempotent and concurrent-safe
            - Write operations (consumption): Should have appropriate state protection
            
            Code Guidelines:
            - Use validate_token() for authentication checks
            - Use validate_token_for_consumption() for token exchange
            - Test both concurrent access and performance requirements
            - Consider replay protection only for state-changing operations
        </prevention>
    <business_impact>
            Performance: Token validation now meets <10ms SLA requirements
            Concurrency: System supports concurrent authentication without failures
            Security: Maintains appropriate replay protection for consumption operations
            Reliability: 100% test pass rate ensures stable authentication
            Scalability: Enables high-concurrency authentication scenarios
        </business_impact>
  </learning>
  
  <learning id="oauth-nonce-replay-attack-redis-mock-fix" date="2025-08-23" severity="HIGH">
    <title>OAuth Nonce Replay Attack Test - Redis Mock Configuration</title>
    <problem>
        OAuth nonce replay attack test failing due to incorrect Redis mock configuration.
        Test expected nonce to be blocked on replay but Redis mock wasn't properly storing
        used nonces, allowing replay attacks to succeed when they should fail.
    </problem>
    <root_cause>
        Redis mock in test was using wrong key pattern for nonce storage.
        Test was storing nonce with pattern "oauth_nonce_{nonce}" but checking 
        with pattern "nonce_{nonce}", causing lookup misses.
        AsyncMock configuration wasn't properly simulating Redis SET/GET operations.
    </root_cause>
    <solution>
        Fixed Redis mock configuration to properly track nonce usage:
        1. Corrected nonce key pattern consistency across store/check operations
        2. Enhanced AsyncMock to simulate real Redis behavior for nonce storage
        3. Added proper expiration handling for nonce entries (300 seconds)
        4. Verified nonce replay protection works correctly in tests
    </solution>
    <implementation>
        Corrected Redis Mock Pattern:
        
        # Store nonce with consistent key pattern
        redis_mock.set.return_value = True
        redis_mock.get.side_effect = lambda key: "used" if key == f"oauth_nonce_{nonce}" else None
        
        # Verify nonce checking uses same pattern
        nonce_key = f"oauth_nonce_{nonce}"
        is_used = await redis_client.get(nonce_key)
        
        Test verification:
        - First OAuth request with nonce succeeds
        - Replay attempt with same nonce fails with appropriate error
        - Different nonce values work independently
        - Nonce expiration respected (300 second TTL)
    </implementation>
    <verification>
        OAuth nonce replay attack test passes consistently
        Nonce reuse is properly blocked as security requirement
        Redis mock accurately simulates production behavior
        No false positives or negatives in nonce validation
    </verification>
    <files_affected>
        <file>auth_service/tests/integration/test_auth_oauth_errors.py</file>
        <file>auth_service/auth_core/oauth/nonce_manager.py</file>
    </files_affected>
    <prevention>
        Ensure Redis mock key patterns match production implementation
        Test both positive and negative cases for nonce validation
        Verify mock expiration behavior matches real Redis TTL
        Include nonce replay protection in all OAuth test scenarios
    </prevention>
    <security_impact>
        Prevents OAuth authorization code replay attacks
        Ensures nonce values can only be used once within time window
        Maintains security boundary between legitimate and malicious requests
        Critical for OAuth 2.0 security compliance
    </security_impact>
  </learning>
</specification>