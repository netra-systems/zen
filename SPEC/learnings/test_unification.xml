<?xml version='1.0' encoding='utf-8'?>
<learnings>
  <metadata>
    <title>Test Infrastructure Unification</title>
    <date>2025-08-21</date>
    <category>Testing Infrastructure</category>
    <impact>High</impact>
  </metadata>

  <context>
    <problem>Test infrastructure was scattered across multiple directories with duplicated runners and inconsistent interfaces</problem>
    <solution>Unified all test plumbing into root directory with single entry point while keeping test files in their service directories</solution>
  </context>

  <insights>
    <insight priority="high">
      <title>Separation of Plumbing vs Tests</title>
      <learning>
        Test infrastructure (runners, configs, utilities) should be centralized in one location,
        but actual test files should remain close to the code they test. This provides:
        - Single source of truth for test execution
        - Consistent interface across all services
        - Tests remain contextually located with their code
      </learning>
      <implementation>
        Created unified_test_runner.py in root with test_framework/ for utilities,
        while keeping netra_backend/tests, auth_service/tests, frontend/__tests__ in place
      </implementation>
    </insight>

    <insight priority="high">
      <title>Import Path Management</title>
      <learning>
        When consolidating modules, import paths must be updated systematically:
        1. Relative imports break when files are moved
        2. Absolute imports need updating across entire codebase
        3. Automated migration scripts are essential for large refactors
      </learning>
      <implementation>
        Created migration script to update 69+ files automatically,
        fixing imports from scripts.test_* to test_framework.*
      </implementation>
    </insight>

    <insight priority="medium">
      <title>Backwards Compatibility</title>
      <learning>
        During infrastructure migrations, maintain backwards compatibility temporarily:
        - Wrapper scripts can redirect old commands to new ones
        - Deprecation warnings guide users to new patterns
        - Gradual migration reduces breaking changes
      </learning>
      <implementation>
        Created test_runner.py wrapper that redirects to unified_test_runner.py
        with deprecation warning
      </implementation>
    </insight>

    <insight priority="medium">
      <title>Service-Agnostic Design</title>
      <learning>
        A unified runner must abstract service differences:
        - Backend/Auth use pytest
        - Frontend uses Jest/npm
        - Each has different config files and commands
        Solution: Service configurations dictionary with command builders
      </learning>
      <implementation>
        SERVICE_CONFIGS dictionary in unified_test_config.py abstracts
        service-specific details while providing consistent interface
      </implementation>
    </insight>

    <insight priority="high">
      <title>Configuration Centralization</title>
      <learning>
        Test configuration should be centralized but extensible:
        - Test levels, markers, and categories in one place
        - Environment configs (local, dev, staging) unified
        - Coverage requirements per service/level
        - Execution strategies (fast, thorough, debug, ci)
      </learning>
      <implementation>
        unified_test_config.py provides all configuration with getter functions
        for different aspects
      </implementation>
    </insight>
  </insights>

  <patterns>
    <pattern type="successful">
      <name>Unified Entry Point</name>
      <description>
        Single command interface for all test operations:
        python unified_test_runner.py --service [backend|auth|frontend|all] --level [smoke|unit|integration]
      </description>
    </pattern>

    <pattern type="successful">
      <name>Progressive Migration</name>
      <description>
        1. Create new structure alongside old
        2. Update imports programmatically
        3. Maintain compatibility wrappers
        4. Deprecate old patterns gradually
      </description>
    </pattern>

    <pattern type="avoid">
      <name>Scattered Test Runners</name>
      <description>
        Multiple test runners in different directories with different interfaces
        leads to confusion and maintenance overhead
      </description>
    </pattern>
  </patterns>

  <forward_items>
    <item priority="high" status="pending">
      <title>Test Discovery Enhancement</title>
      <description>
        Implement intelligent test discovery that can:
        - Auto-detect test files across all services
        - Map code changes to relevant tests
        - Suggest minimal test set for changes
      </description>
      <effort>Medium</effort>
    </item>

    <item priority="high" status="pending">
      <title>Parallel Execution Optimization</title>
      <description>
        Optimize parallel test execution:
        - Dynamic worker allocation based on test size
        - Test sharding across multiple machines
        - Intelligent test ordering (fast tests first)
      </description>
      <effort>High</effort>
    </item>

    <item priority="medium" status="pending">
      <title>Test Result Caching</title>
      <description>
        Implement test result caching:
        - Skip unchanged tests on re-runs
        - Cache based on code hash + test hash
        - Invalidate cache on dependency changes
      </description>
      <effort>Medium</effort>
    </item>

    <item priority="medium" status="pending">
      <title>Real-time Test Dashboard</title>
      <description>
        Create web-based test dashboard:
        - Real-time test execution progress
        - Historical trends and flakiness tracking
        - Coverage visualization
        - Performance metrics over time
      </description>
      <effort>High</effort>
    </item>

    <item priority="low" status="pending">
      <title>Test Generation from Specs</title>
      <description>
        Auto-generate test cases from specifications:
        - Parse XML specs for requirements
        - Generate test skeletons
        - Maintain traceability matrix
      </description>
      <effort>High</effort>
    </item>

    <item priority="high" status="pending">
      <title>Container-based Test Isolation</title>
      <description>
        Run each test suite in isolated containers:
        - Prevent test pollution
        - Enable true parallel execution
        - Consistent environment across machines
      </description>
      <effort>Medium</effort>
    </item>

    <item priority="medium" status="pending">
      <title>Test Impact Analysis</title>
      <description>
        Implement code coverage-based test selection:
        - Map which tests cover which code
        - Run only affected tests on changes
        - Reduce CI/CD time significantly
      </description>
      <effort>High</effort>
    </item>

    <item priority="low" status="pending">
      <title>AI-Powered Test Maintenance</title>
      <description>
        Use AI to maintain tests:
        - Auto-fix simple test failures
        - Update assertions for API changes
        - Generate missing test cases
      </description>
      <effort>Very High</effort>
    </item>
  </forward_items>

  <metrics>
    <metric name="test_execution_time">
      <before>Varies per runner, no unified measurement</before>
      <after>Centralized timing, can optimize globally</after>
    </metric>
    <metric name="test_discovery_time">
      <before>Manual navigation to different test directories</before>
      <after>Single command lists all available tests</after>
    </metric>
    <metric name="configuration_consistency">
      <before>Different configs per service</before>
      <after>Unified configuration with service-specific overrides</after>
    </metric>
  </metrics>

  <recommendations>
    <recommendation priority="immediate">
      Remove old test runner scripts from scripts/ directory after verification period
    </recommendation>
    <recommendation priority="immediate">
      Update CI/CD pipelines to use unified_test_runner.py
    </recommendation>
    <recommendation priority="short_term">
      Implement test result reporting to centralized database for tracking
    </recommendation>
    <recommendation priority="long_term">
      Consider migrating frontend tests to Python-based runner for complete unification
    </recommendation>
  </recommendations>
</learnings>