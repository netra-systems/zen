<?xml version="1.0" encoding="UTF-8"?>
<specification>
    <metadata>
        <name>TestInfrastructure.SSOT</name>
        <type>TestInfrastructure.SingleSourceOfTruth</type>
        <version>1.1</version>
        <description>Single Source of Truth specification for test infrastructure, patterns, utilities, and anti-patterns across the entire codebase</description>
        <created>2025-09-02</created>
        <updated>2025-09-14</updated>
        <priority>CRITICAL</priority>
        <status>94.5% SSOT COMPLIANCE ACHIEVED</status>
    </metadata>

    <core_principles>
        <principle priority="CRITICAL">
            <name>Unified Test Runner SSOT</name>
            <description>
                tests/unified_test_runner.py is the ONLY approved test execution entry point.
                All test execution MUST flow through this centralized runner to ensure
                consistent environment management, Docker orchestration, and reporting.
            </description>
            <enforcement>
                <rule>NO direct pytest calls in scripts or CI/CD</rule>
                <rule>NO custom test runners outside unified_test_runner.py</rule>
                <rule>ALL test execution via unified_test_runner.py</rule>
            </enforcement>
        </principle>

        <principle priority="CRITICAL">
            <name>Real Services Over Mocks</name>
            <description>
                MOCKS ARE FORBIDDEN in development, staging, and production testing.
                Tests MUST use real services, real databases, and real LLMs whenever possible.
                Mocks create false confidence and hide integration issues.
            </description>
            <rationale>
                Mocks are the #1 cause of production failures that pass in tests.
                Business > Real System > Tests. Tests serve the real system.
            </rationale>
        </principle>

        <principle priority="CRITICAL">
            <name>Service Boundary Respect</name>
            <description>
                Test files MUST respect microservice boundaries. Each service maintains
                its own test directory with NO cross-service test imports or shared fixtures
                that violate independence principles.
            </description>
        </principle>

        <principle priority="CRITICAL">
            <name>Environment Isolation SSOT</name>
            <description>
                ALL environment variable access in tests MUST go through IsolatedEnvironment.
                Direct os.environ access is FORBIDDEN. Each service MUST maintain environmental
                independence through its canonical environment configuration.
            </description>
        </principle>
    </core_principles>

    <test_runner_ssot>
        <canonical_implementation>
            <location>tests/unified_test_runner.py</location>
            <description>Single approved test execution entry point with advanced orchestration</description>
        </canonical_implementation>

        <execution_modes>
            <mode name="fast_feedback">
                <purpose>Quick validation cycle (2 minutes)</purpose>
                <categories>smoke, unit</categories>
                <command>python tests/unified_test_runner.py --execution-mode fast_feedback</command>
            </mode>
            
            <mode name="nightly">
                <purpose>Full layered execution (default)</purpose>
                <categories>all categories in dependency order</categories>
                <command>python tests/unified_test_runner.py --execution-mode nightly</command>
            </mode>
            
            <mode name="real_services">
                <purpose>Integration testing with Docker services</purpose>
                <docker_integration>automatic</docker_integration>
                <command>python tests/unified_test_runner.py --real-services</command>
            </mode>
        </execution_modes>

        <orchestration_layers>
            <layer name="fast_feedback" timeout="2min">
                <categories>smoke, unit</categories>
                <purpose>Immediate developer feedback</purpose>
            </layer>
            
            <layer name="core_integration" timeout="10min">
                <categories>database, api, websocket</categories>
                <purpose>Core system integration validation</purpose>
                <dependencies>Docker services required</dependencies>
            </layer>
            
            <layer name="service_integration" timeout="20min">
                <categories>agent, e2e_critical, frontend</categories>
                <purpose>Inter-service workflow validation</purpose>
                <dependencies>Real LLM, Real services</dependencies>
            </layer>
            
            <layer name="e2e_background" timeout="60min">
                <categories>cypress, e2e, performance</categories>
                <purpose>Full end-to-end and performance testing</purpose>
                <execution>background_mode</execution>
            </layer>
        </orchestration_layers>

        <forbidden_alternatives>
            <anti_pattern>Direct pytest execution</anti_pattern>
            <anti_pattern>Custom test runners in individual services</anti_pattern>
            <anti_pattern>Manual Docker management for tests</anti_pattern>
            <anti_pattern>Environment variable access without IsolatedEnvironment</anti_pattern>
        </forbidden_alternatives>
    </test_runner_ssot>

    <test_organization_ssot>
        <directory_structure>
            <service_tests>
                <pattern>{service_name}/tests/</pattern>
                <examples>
                    <example>netra_backend/tests/ - Backend service tests</example>
                    <example>auth_service/tests/ - Auth service tests</example>
                </examples>
                <rule>Each service maintains separate test directory</rule>
                <rule>NO cross-service test imports</rule>
            </service_tests>
            
            <e2e_tests>
                <location>tests/e2e/</location>
                <purpose>End-to-end integration tests across services</purpose>
                <rule>Root-level for cross-service scenarios</rule>
            </e2e_tests>
            
            <mission_critical_tests>
                <location>tests/mission_critical/</location>
                <purpose>Business-critical functionality validation</purpose>
                <examples>
                    <example>test_websocket_agent_events_suite.py</example>
                    <example>test_supervisor_golden_compliance.py</example>
                </examples>
            </mission_critical_tests>
            
            <test_framework>
                <location>test_framework/</location>
                <purpose>Shared test utilities and infrastructure</purpose>
                <rule>Infrastructure only, NO business logic</rule>
            </test_framework>
        </directory_structure>

        <naming_conventions>
            <test_files>
                <pattern>test_*.py</pattern>
                <forbidden>*_test.py, test*.py (without underscore)</forbidden>
                <examples>
                    <correct>test_user_service.py</correct>
                    <incorrect>user_service_test.py</incorrect>
                </examples>
            </test_files>
            
            <conftest_placement>
                <allowed_locations>
                    <location>auth_service/tests/conftest.py</location>
                    <location>netra_backend/tests/conftest.py</location>
                    <location>tests/conftest.py</location>
                </allowed_locations>
                <forbidden>ANY subdirectory conftest.py files</forbidden>
                <rationale>Prevents fixture complexity and maintains service boundaries</rationale>
            </conftest_placement>
        </naming_conventions>
    </test_organization_ssot>

    <docker_integration_ssot>
        <unified_docker_manager>
            <location>test_framework/unified_docker_manager.py</location>
            <description>SINGLE SOURCE for all Docker operations in testing</description>
            <features>
                <feature>Automatic conflict resolution</feature>
                <feature>Health monitoring</feature>
                <feature>Dynamic port allocation</feature>
                <feature>Cross-platform compatibility</feature>
            </features>
        </unified_docker_manager>

        <docker_orchestration>
            <automatic_management>
                <description>Tests automatically start Docker when --real-services flag used</description>
                <benefits>
                    <benefit>No manual Docker management</benefit>
                    <benefit>Conflict resolution</benefit>
                    <benefit>Health checks</benefit>
                </benefits>
            </automatic_management>
            
            <manual_operations>
                <script>scripts/docker_manual.py</script>
                <commands>
                    <command>start - Start test environment</command>
                    <command>stop - Stop all containers</command>
                    <command>restart - Restart services</command>
                    <command>clean - Clean up everything</command>
                </commands>
            </manual_operations>
        </docker_orchestration>

        <environment_configuration>
            <test_environment>
                <postgresql_port>5434</postgresql_port>
                <redis_port>6381</redis_port>
                <backend_port>8000</backend_port>
                <auth_port>8081</auth_port>
            </test_environment>
            
            <development_environment>
                <postgresql_port>5432</postgresql_port>
                <redis_port>6379</redis_port>
                <backend_port>8000</backend_port>
                <auth_port>8081</auth_port>
            </development_environment>
        </environment_configuration>
    </docker_integration_ssot>

    <database_testing_ssot>
        <database_utilities>
            <canonical_location>test_framework/helpers/database_helpers.py</canonical_location>
            <description>SINGLE SOURCE for all database test utilities</description>
            
            <core_utilities>
                <utility name="DatabaseSyncHelper">
                    <purpose>Cross-service database synchronization</purpose>
                    <location>tests/e2e/helpers/database_sync_helpers.py</location>
                </utility>
                
                <utility name="database_cleanup_fixtures">
                    <purpose>Transaction-based test isolation</purpose>
                    <pattern>Use transaction rollback over database recreation</pattern>
                </utility>
                
                <utility name="seed_data_manager">
                    <location>test_framework/seed_data_manager.py</location>
                    <purpose>Consistent test data across environments</purpose>
                </utility>
            </core_utilities>
        </database_utilities>

        <connection_management>
            <principle>ALL database connections MUST use CoreDatabaseManager</principle>
            <ssl_resolution>Automatic SSL parameter conflict resolution</ssl_resolution>
            <driver_support>
                <async_driver>asyncpg</async_driver>
                <sync_driver>psycopg2</sync_driver>
            </driver_support>
        </connection_management>

        <anti_patterns>
            <pattern>Direct database connection creation in tests</pattern>
            <pattern>Manual SSL parameter handling</pattern>
            <pattern>Database recreation instead of transaction rollback</pattern>
            <pattern>Cross-service database sharing</pattern>
        </anti_patterns>
    </database_testing_ssot>

    <websocket_testing_ssot>
        <websocket_utilities>
            <canonical_location>test_framework/utils/websocket.py</canonical_location>
            <description>SINGLE SOURCE for WebSocket testing utilities</description>
            
            <core_utilities>
                <utility name="WebSocketTestClient">
                    <purpose>Standardized WebSocket test client</purpose>
                    <features>
                        <feature>Authentication handling</feature>
                        <feature>Message validation</feature>
                        <feature>Connection lifecycle management</feature>
                    </features>
                </utility>
                
                <utility name="websocket_test_helpers">
                    <location>tests/e2e/helpers/websocket/websocket_test_helpers.py</location>
                    <purpose>Complex WebSocket test scenarios</purpose>
                </utility>
            </core_utilities>
        </websocket_utilities>

        <mission_critical_tests>
            <test_suite>tests/mission_critical/test_websocket_agent_events_suite.py</test_suite>
            <purpose>Validate WebSocket events critical for chat value delivery</purpose>
            <required_events>
                <event>agent_started</event>
                <event>agent_thinking</event>
                <event>tool_executing</event>
                <event>tool_completed</event>
                <event>agent_completed</event>
            </required_events>
            <integration_requirement>MUST run with real WebSocket connections</integration_requirement>
        </mission_critical_tests>

        <silent_failure_prevention>
            <principle>WebSocket silent failures are the most dangerous failure class</principle>
            <prevention_strategies>
                <strategy>Explicit error propagation with context logging</strategy>
                <strategy>Dependency validation at injection time</strategy>
                <strategy>State transition logging</strategy>
                <strategy>Silent failure detection tests</strategy>
                <strategy>Production monitoring</strategy>
            </prevention_strategies>
            <forbidden>Bare except clauses that swallow errors</forbidden>
        </silent_failure_prevention>
    </websocket_testing_ssot>

    <environment_management_ssot>
        <isolated_environment>
            <canonical_location>dev_launcher/isolated_environment.py</canonical_location>
            <description>SINGLE SOURCE for environment variable management in tests</description>
            <mandatory_usage>ALL tests MUST use IsolatedEnvironment for env access</mandatory_usage>
        </isolated_environment>

        <service_independence>
            <principle>Each service MUST maintain environmental independence</principle>
            <implementation>
                <rule>Import environment configs ONLY from service's canonical location</rule>
                <rule>NO cross-service environment access</rule>
                <rule>Service-specific env validation</rule>
            </implementation>
        </service_independence>

        <forbidden_patterns>
            <pattern>Direct os.environ access in tests</pattern>
            <pattern>Cross-service environment imports</pattern>
            <pattern>Hardcoded environment values</pattern>
            <pattern>Environment pollution between tests</pattern>
        </forbidden_patterns>
    </environment_management_ssot>

    <mock_policy_ssot>
        <core_policy>
            <principle priority="CRITICAL">MOCKS ARE FORBIDDEN</principle>
            <rationale>
                Mocks create false confidence and hide real integration issues.
                They are the #1 cause of production failures that pass in tests.
            </rationale>
        </core_policy>

        <approved_patterns>
            <pattern name="Real Services First">
                <description>Always prefer real services over mocks</description>
                <implementation>Use Docker orchestration for real service testing</implementation>
            </pattern>
            
            <pattern name="Real LLM Integration">
                <description>Use real LLM services in integration and E2E tests</description>
                <cost_mitigation>Feature flags for expensive operations</cost_mitigation>
            </pattern>
            
            <pattern name="Real Database Operations">
                <description>Use real databases with transaction rollback</description>
                <isolation>Transaction-based test isolation</isolation>
            </pattern>
        </approved_patterns>

        <limited_exceptions>
            <exception context="Unit Tests Only">
                <description>Unit tests may mock external dependencies when testing isolated logic</description>
                <requirement>Must be clearly marked as unit tests</requirement>
                <forbidden_scope>Integration, E2E, Mission Critical tests</forbidden_scope>
            </exception>
            
            <exception context="External API Failures">
                <description>Mock external APIs only when testing failure scenarios</description>
                <requirement>Primary tests must use real APIs</requirement>
            </exception>
        </limited_exceptions>

        <enforcement>
            <detection_script>tests/mission_critical/test_mock_policy_violations.py</detection_script>
            <pre_commit_hook>Scan for mock usage in forbidden contexts</pre_commit_hook>
            <ci_validation>Automated mock policy violation detection</ci_validation>
        </enforcement>
    </mock_policy_ssot>

    <test_categories_ssot>
        <category_system>
            <canonical_location>test_framework/test_categories.py</canonical_location>
            <description>SINGLE SOURCE for test categorization and execution priorities</description>
        </category_system>

        <priority_levels>
            <level name="CRITICAL" priority="1">
                <categories>smoke, startup</categories>
                <execution_time>Under 30 seconds</execution_time>
                <failure_impact>System unusable</failure_impact>
            </level>
            
            <level name="HIGH" priority="2">
                <categories>unit, security, database</categories>
                <execution_time>Under 5 minutes</execution_time>
                <failure_impact>Core functionality broken</failure_impact>
            </level>
            
            <level name="MEDIUM" priority="3">
                <categories>integration, api, websocket, agent</categories>
                <execution_time>Under 20 minutes</execution_time>
                <failure_impact>Feature functionality impacted</failure_impact>
            </level>
            
            <level name="LOW" priority="4">
                <categories>frontend, performance, e2e</categories>
                <execution_time>Under 60 minutes</execution_time>
                <failure_impact>User experience degraded</failure_impact>
            </level>
        </priority_levels>

        <categorization_rules>
            <rule>Tests MUST be categorized based on failure impact</rule>
            <rule>Mission critical functionality gets CRITICAL priority</rule>
            <rule>Business value determines execution order</rule>
            <rule>Performance tests run in LOW priority</rule>
        </categorization_rules>
    </test_categories_ssot>

    <fixture_management_ssot>
        <fixture_locations>
            <service_fixtures>
                <pattern>{service_name}/tests/conftest.py</pattern>
                <scope>Service-specific fixtures only</scope>
                <forbidden>Cross-service fixture imports</forbidden>
            </service_fixtures>
            
            <shared_fixtures>
                <location>test_framework/fixtures/</location>
                <purpose>Infrastructure fixtures (auth, database, etc.)</purpose>
                <rule>Infrastructure only, NO business logic</rule>
            </shared_fixtures>
            
            <real_services_fixtures>
                <location>test_framework/fixtures/real_services.py</location>
                <purpose>Docker service management fixtures</purpose>
                <integration>Unified Docker Manager</integration>
            </real_services_fixtures>
        </fixture_locations>

        <fixture_principles>
            <principle>Fixtures MUST respect service boundaries</principle>
            <principle>Shared fixtures contain NO business logic</principle>
            <principle>Real services over mocked fixtures</principle>
            <principle>Transaction-based database fixtures</principle>
        </fixture_principles>
    </fixture_management_ssot>

    <import_management_ssot>
        <absolute_imports_only>
            <principle priority="CRITICAL">ALL Python files MUST use absolute imports</principle>
            <rationale>Relative imports are the #1 cause of test infrastructure failures</rationale>
            <enforcement>
                <rule>NO relative imports (. or ..)</rule>
                <rule>Start from package root</rule>
                <rule>Pre-commit hooks validate imports</rule>
            </enforcement>
        </absolute_imports_only>

        <service_namespace_patterns>
            <pattern name="Service Tests">
                <format>{service_name}.tests.{module}</format>
                <examples>
                    <example>from netra_backend.tests.conftest import TestClient</example>
                    <example>from auth_service.tests.helpers import AuthHelper</example>
                </examples>
            </pattern>
            
            <pattern name="E2E Test Helpers">
                <format>tests.{module}</format>
                <examples>
                    <example>from tests.e2e.helpers.database_sync_helpers import DatabaseSyncHelper</example>
                    <example>from tests.harness_complete import UnifiedTestHarnessComplete</example>
                </examples>
            </pattern>
            
            <pattern name="Test Framework">
                <format>test_framework.{module}</format>
                <examples>
                    <example>from test_framework.unified_docker_manager import UnifiedDockerManager</example>
                    <example>from test_framework.helpers.database_helpers import DatabaseHelper</example>
                </examples>
            </pattern>
        </service_namespace_patterns>
    </import_management_ssot>

    <compliance_validation>
        <mission_critical_tests>
            <test name="WebSocket Agent Events">
                <location>tests/mission_critical/test_websocket_agent_events_suite.py</location>
                <purpose>Validate business-critical WebSocket events</purpose>
                <requirement>MUST run with real WebSocket connections</requirement>
            </test>
            
            <test name="SSOT Compliance">
                <location>tests/mission_critical/test_ssot_compliance_suite.py</location>
                <purpose>Validate Single Source of Truth violations</purpose>
                <scope>Test infrastructure patterns</scope>
            </test>
            
            <test name="Mock Policy Violations">
                <location>tests/mission_critical/test_mock_policy_violations.py</location>
                <purpose>Detect forbidden mock usage</purpose>
                <enforcement>Pre-commit and CI validation</enforcement>
            </test>
        </mission_critical_tests>

        <compliance_scripts>
            <script>scripts/check_architecture_compliance.py</script>
            <script>scripts/validate_test_infrastructure.py</script>
            <script>tests/unified_test_runner.py --validate-infrastructure</script>
        </compliance_scripts>
    </compliance_validation>

    <migration_guidelines>
        <from_legacy_patterns>
            <migration name="Custom Test Runners to Unified Runner">
                <steps>
                    <step>Identify custom test execution scripts</step>
                    <step>Migrate to unified_test_runner.py categories</step>
                    <step>Update CI/CD to use unified runner</step>
                    <step>Remove legacy test execution scripts</step>
                </steps>
            </migration>
            
            <migration name="Mock-Based Tests to Real Services">
                <steps>
                    <step>Audit existing mock usage</step>
                    <step>Set up Docker orchestration for real services</step>
                    <step>Replace mocks with real service calls</step>
                    <step>Update test assertions for real data</step>
                    <step>Remove mock infrastructure</step>
                </steps>
            </migration>
            
            <migration name="Relative Imports to Absolute">
                <steps>
                    <step>Run scripts/fix_all_import_issues.py --absolute-only</step>
                    <step>Verify all imports use absolute paths</step>
                    <step>Update pre-commit hooks</step>
                    <step>Validate with CI checks</step>
                </steps>
            </migration>
        </from_legacy_patterns>

        <breaking_changes>
            <change>Direct pytest execution no longer supported</change>
            <change>Mock-based integration tests will fail compliance</change>
            <change>Relative imports cause test failures</change>
            <change>Cross-service fixture imports forbidden</change>
        </breaking_changes>
    </migration_guidelines>

    <performance_optimization>
        <execution_strategies>
            <strategy name="Layer-Based Execution">
                <description>Execute tests in dependency-aware layers</description>
                <benefits>
                    <benefit>Parallel execution within layers</benefit>
                    <benefit>Early failure detection</benefit>
                    <benefit>Resource optimization</benefit>
                </benefits>
            </strategy>
            
            <strategy name="Background E2E">
                <description>Long-running E2E tests in background</description>
                <benefits>
                    <benefit>Non-blocking fast feedback</benefit>
                    <benefit>Continuous E2E validation</benefit>
                    <benefit>Resource efficiency</benefit>
                </benefits>
            </strategy>
        </execution_strategies>

        <optimization_targets>
            <target>Fast feedback under 2 minutes</target>
            <target>Core integration under 10 minutes</target>
            <target>Full test suite under 60 minutes</target>
            <target>Resource utilization optimization</target>
        </optimization_targets>
    </performance_optimization>

    <anti_patterns>
        <anti_pattern name="Direct Pytest Execution">
            <description>Running pytest directly instead of unified_test_runner.py</description>
            <problems>
                <problem>No Docker orchestration</problem>
                <problem>No environment isolation</problem>
                <problem>No categorization</problem>
                <problem>No progress tracking</problem>
            </problems>
            <solution>Always use unified_test_runner.py</solution>
        </anti_pattern>
        
        <anti_pattern name="Mock-Heavy Integration Tests">
            <description>Using mocks in integration or E2E tests</description>
            <problems>
                <problem>False confidence</problem>
                <problem>Hidden integration issues</problem>
                <problem>Production failures</problem>
            </problems>
            <solution>Use real services with Docker orchestration</solution>
        </anti_pattern>
        
        <anti_pattern name="Cross-Service Test Dependencies">
            <description>Tests in one service importing from another service's tests</description>
            <problems>
                <problem>Service boundary violations</problem>
                <problem>Deployment coupling</problem>
                <problem>Maintenance complexity</problem>
            </problems>
            <solution>Use shared test_framework utilities</solution>
        </anti_pattern>
        
        <anti_pattern name="Environment Variable Pollution">
            <description>Direct os.environ access without isolation</description>
            <problems>
                <problem>Test interdependence</problem>
                <problem>Non-reproducible failures</problem>
                <problem>Environment leakage</problem>
            </problems>
            <solution>Use IsolatedEnvironment exclusively</solution>
        </anti_pattern>
        
        <anti_pattern name="Manual Docker Management">
            <description>Custom Docker scripts instead of UnifiedDockerManager</description>
            <problems>
                <problem>Port conflicts</problem>
                <problem>Resource leakage</problem>
                <problem>Inconsistent setup</problem>
            </problems>
            <solution>Use UnifiedDockerManager through unified_test_runner.py</solution>
        </anti_pattern>
    </anti_patterns>

    <quality_gates>
        <gate name="Infrastructure Compliance">
            <validation>All tests use unified_test_runner.py</validation>
            <validation>No direct pytest execution in CI</validation>
            <validation>No mock usage in forbidden contexts</validation>
            <validation>Absolute imports only</validation>
        </gate>
        
        <gate name="Service Boundaries">
            <validation>Service-specific test directories</validation>
            <validation>No cross-service test imports</validation>
            <validation>Independent fixture management</validation>
        </gate>
        
        <gate name="Real Services Integration">
            <validation>Docker orchestration functional</validation>
            <validation>Database connectivity validated</validation>
            <validation>WebSocket events tested</validation>
            <validation>Mission critical tests pass</validation>
        </gate>
    </quality_gates>

    <business_alignment>
        <value_proposition>
            Test infrastructure that serves business goals by ensuring system reliability,
            reducing production failures, and enabling rapid deployment with confidence.
        </value_proposition>
        
        <customer_impact>
            <impact>Reduced production incidents through better testing</impact>
            <impact>Faster feature delivery through reliable CI/CD</impact>
            <impact>Higher system availability and performance</impact>
        </customer_impact>
        
        <cost_optimization>
            <optimization>Automated Docker management reduces manual effort</optimization>
            <optimization>Layer-based execution optimizes resource usage</optimization>
            <optimization>Early failure detection reduces debug time</optimization>
        </cost_optimization>
    </business_alignment>

    <reference_specifications>
        <spec>core.xml - Core system architecture</spec>
        <spec>type_safety.xml - Type safety and SSOT principles</spec>
        <spec>conventions.xml - Code quality standards</spec>
        <spec>docker_testing_ssot.xml - Docker testing architecture</spec>
        <spec>unified_environment_management.xml - Environment isolation</spec>
        <spec>import_management_architecture.xml - Import rules</spec>
        <spec>learnings/websocket_silent_failure_prevention_masterclass.xml - WebSocket testing</spec>
        <spec>learnings/ssot_consolidation_20250825.xml - SSOT consolidation patterns</spec>
    </reference_specifications>
</specification>