<?xml version='1.0' encoding='utf-8'?>
<specification>
  <metadata>
    <name>LLM</name>
    <type>llm</type>
    <version>2.0</version>
    <last_updated>2025-08-11</last_updated>
    <last_edited>2025-08-21T08:47:28.520150</last_edited>
  </metadata>
  <primary-spec>
    <description>LLM Specification with Structured Generation Support</description>
    <goals>
      <goal priority="critical">LLM working as a complete end to end system with structured outputs</goal>
      <goal priority="critical">The Coherence of the system working in harmony with the application</goal>
      <goal priority="high">Type-safe structured generation for reliable agent interactions</goal>
    </goals>
  </primary-spec>
  <sections>
    <section id="models" order="1">
      <title>Models</title>
      <requirements>
        <requirement>The system uses multiple LLM models</requirement>
        <requirement>Models are accessed through a unified interface</requirement>
        <requirement>Model selection is configurable</requirement>
        <requirement>Support for Google (Gemini) and OpenAI providers</requirement>
      </requirements>
    </section>
    <section id="prompts" order="2">
      <title>Prompts</title>
      <requirements>
        <requirement>Prompts are templated and reusable</requirement>
        <requirement>Prompts are versioned and managed</requirement>
        <requirement>Prompt engineering is a key focus</requirement>
        <requirement>Support for system, user, and assistant message roles</requirement>
      </requirements>
    </section>
    <section id="responses" order="3">
      <title>Responses</title>
      <requirements>
        <requirement>LLM responses are parsed and validated</requirement>
        <requirement>Responses are cached to reduce latency and cost</requirement>
        <requirement>Fallbacks are in place for failed LLM requests</requirement>
        <requirement>Support for streaming responses via astream</requirement>
      </requirements>
    </section>
    <section id="configuration" order="4">
      <title>Configuration and Usage</title>
      <locations>
        <location>Central LLM Manager: app/llm/llm_manager.py</location>
        <location>Configuration: app/schemas/Config.py:167-197</location>
        <location>LLM configs in AppConfig.llm_configs dictionary</location>
      </locations>
      <agent-contexts>
        <context>triage: Message classification and routing</context>
        <context>data: Data analysis and processing</context>
        <context>optimizations_core: AI workload optimization</context>
        <context>actions_to_meet_goals: Goal-driven task execution</context>
        <context>reporting: Report generation</context>
        <context>analysis: General analysis tasks</context>
      </agent-contexts>
      <adding-provider>
        <step>Add to Config in app/schemas/Config.py with provider, model_name, generation_config</step>
        <step>Add SecretReference for API key management</step>
        <step>Set environment variable for API key</step>
        <step>Use via: response = await llm_manager.ask_llm(prompt, "config_name")</step>
      </adding-provider>
      <features>
        <feature>Supports Google (Gemini) and OpenAI providers out of the box</feature>
        <feature>Handles caching, mocking for dev mode, and streaming</feature>
        <feature>Secrets managed via SecretReference system</feature>
        <feature>Dev mode can disable LLMs with DEV_MODE_DISABLE_LLM=true</feature>
      </features>
    </section>
    <section id="structured-generation" order="5">
      <title>Structured Generation</title>
      <description>Support for type-safe structured outputs from LLMs using Pydantic models</description>
      <requirements>
        <requirement priority="critical">LLM manager must support structured output via with_structured_output method</requirement>
        <requirement priority="critical">All agent responses should use Pydantic models for type safety</requirement>
        <requirement priority="high">Support JSON schema validation for outputs</requirement>
        <requirement priority="high">Graceful fallback to text parsing when structured output fails</requirement>
      </requirements>
      <implementation>
        <step>Add with_structured_output support to LLM manager</step>
        <step>Define Pydantic response models for each agent type</step>
        <step>Update agents to use structured generation</step>
        <step>Add validation and error handling</step>
      </implementation>
      <usage-examples>
        <example name="triage-agent">
          <description>Triage agent using structured output for classification</description>
          <code>
from pydantic import BaseModel, Field
from typing import Literal, Optional, List

class TriageResponse(BaseModel):
    category: Literal["data", "optimization", "reporting", "general"]
    confidence: float = Field(ge=0.0, le=1.0)
    reasoning: str
    suggested_actions: List[str]
    requires_human: bool = False

# In triage_sub_agent.py
llm = self.llm_manager.get_llm("triage")
structured_llm = llm.with_structured_output(TriageResponse)
response = await structured_llm.ainvoke(prompt)
# response is now a TriageResponse instance with validated fields
                    </code>
        </example>
        <example name="data-agent">
          <description>Data agent using SSOT structured output for analysis results</description>
          <code>
# Import SSOT schema
from netra_backend.app.schemas.shared_types import DataAnalysisResponse, PerformanceMetrics

# SSOT DataAnalysisResponse schema:
class DataAnalysisResponse(BaseModel):
    analysis_id: str
    status: str
    results: Dict[str, Any]
    metrics: PerformanceMetrics
    created_at: float
    completed_at: Optional[float] = None
    
# In data_sub_agent.py
structured_llm = llm.with_structured_output(DataAnalysisResponse)
response = await structured_llm.ainvoke(analysis_prompt)
                    </code>
        </example>
      </usage-examples>
    </section>
    <section id="configuration" order="5">
      <title>Configuration</title>
      <requirements>
        <requirement>LLM configurations stored in AppConfig.llm_configs</requirement>
        <requirement>Support for environment-specific settings</requirement>
        <requirement>Mock LLM support for development mode</requirement>
        <requirement>Configurable generation parameters (temperature, max_tokens, etc.)</requirement>
      </requirements>
    </section>
    <section id="caching" order="6">
      <title>Caching</title>
      <requirements>
        <requirement>Response caching via llm_cache_service</requirement>
        <requirement>Cache key generation based on prompt and model</requirement>
        <requirement>Configurable cache TTL and size limits</requirement>
        <requirement>Cache invalidation on model updates</requirement>
      </requirements>
    </section>
    <section id="observability" order="7">
      <title>Observability and Logging</title>
      <description>Comprehensive logging for LLM calls and agent interactions to enable debugging and monitoring</description>
      <requirements>
        <requirement priority="critical">INFO level: Heartbeat logging every 2-3 seconds for long-running LLM calls</requirement>
        <requirement priority="critical">DEBUG level: Log all LLM input data (prompts, parameters)</requirement>
        <requirement priority="critical">DEBUG level: Log all LLM output data (responses, tokens used)</requirement>
        <requirement priority="critical">INFO level: Log all input/output data between subagents</requirement>
        <requirement priority="high">Include correlation IDs for tracing requests across agents</requirement>
        <requirement priority="high">Track execution time for all LLM calls</requirement>
        <requirement priority="high">Log retry attempts and fallback operations</requirement>
      </requirements>
      <implementation>
        <heartbeat-logging>
          <description>Heartbeat mechanism for long-running operations</description>
          <features>
            <feature>Async task that logs heartbeat every 2-3 seconds while LLM call is running</feature>
            <feature>Include correlation ID, agent name, and elapsed time in heartbeat</feature>
            <feature>Stop heartbeat when LLM call completes or fails</feature>
            <feature>Thread-safe implementation using asyncio tasks</feature>
          </features>
          <log-format>
            <info-level>[INFO] LLM heartbeat: {agent_name} - {correlation_id} - elapsed: {elapsed_time}s - status: processing</info-level>
          </log-format>
        </heartbeat-logging>
        <data-logging>
          <description>Comprehensive data logging for debugging</description>
          <input-logging>
            <debug-level>[DEBUG] LLM input: {agent_name} - {correlation_id} - prompt_size: {size} - params: {params}</debug-level>
            <debug-level>[DEBUG] LLM input prompt: {truncated_prompt}</debug-level>
          </input-logging>
          <output-logging>
            <debug-level>[DEBUG] LLM output: {agent_name} - {correlation_id} - response_size: {size} - tokens: {tokens}</debug-level>
            <debug-level>[DEBUG] LLM response: {truncated_response}</debug-level>
          </output-logging>
        </data-logging>
        <subagent-communication>
          <description>Log all communication between agents</description>
          <info-level>[INFO] Agent communication: {from_agent} -&gt; {to_agent} - {correlation_id} - type: {message_type}</info-level>
          <info-level>[INFO] Agent input: {from_agent} -&gt; {to_agent} - data_size: {size}</info-level>
          <info-level>[INFO] Agent output: {to_agent} -&gt; {from_agent} - data_size: {size} - status: {status}</info-level>
        </subagent-communication>
      </implementation>
      <configuration>
        <setting name="llm_heartbeat_enabled" default="true">Enable/disable heartbeat logging</setting>
        <setting name="llm_heartbeat_interval_ms" default="2000">Heartbeat interval in milliseconds</setting>
        <setting name="llm_data_logging_enabled" default="true">Enable/disable data logging</setting>
        <setting name="llm_data_truncate_length" default="1000">Max characters to log for prompts/responses</setting>
        <setting name="subagent_logging_enabled" default="true">Enable/disable subagent communication logging</setting>
      </configuration>
    </section>
  </sections>
  <best-practices>
    <practice>Always use structured generation for agent responses to ensure type safety</practice>
    <practice>Define clear Pydantic models for each response type</practice>
    <practice>Include validation constraints in field definitions</practice>
    <practice>Provide fallback behavior when structured output fails</practice>
    <practice>Use appropriate generation config for each use case</practice>
    <practice>Cache structured responses to reduce costs</practice>
  </best-practices>
</specification>