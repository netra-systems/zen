<?xml version='1.0' encoding='utf-8'?>
<spec>
  <metadata>
    <name>Master WIP and Status Index</name>
    <description>Living index tracking alignment between codebase implementation and specifications</description>
    <version>1.1.0</version>
    <created>2025-01-20</created>
    <last_updated>2025-08-23</last_updated>
  </metadata>
  <purpose>
    Track the alignment between Netra Apex codebase and its specifications,
    providing a quantitative scoring system and actionable insights for
    maintaining architectural compliance and system coherence.
  </purpose>
  <scoring_methodology>
    <scale>0-100 percentage scale</scale>
    <thresholds>
      <excellent>85-100%</excellent>
      <good>70-84%</good>
      <needs_attention>55-69%</needs_attention>
      <critical>0-54%</critical>
    </thresholds>
    <dimensions>
      <dimension name="implementation_completeness" weight="25">
        Are all specified components implemented?
      </dimension>
      <dimension name="architectural_compliance" weight="25">
        Does the code follow specified patterns and boundaries?
      </dimension>
      <dimension name="test_health" weight="20">
        <subdimensions>
          <subdimension name="test_pyramid" weight="25">
            Balance between E2E (40%), Integration (40%), Unit (20%)
          </subdimension>
          <subdimension name="estimated_coverage" weight="35">
            Overall test coverage percentage
          </subdimension>
          <subdimension name="test_volume" weight="20">
            Absolute number of tests relative to codebase size
          </subdimension>
          <subdimension name="test_quality" weight="20">
            Tests use real services, no stubs, proper assertions
          </subdimension>
        </subdimensions>
      </dimension>
      <dimension name="documentation_alignment" weight="15">
        Is documentation current with implementation?
      </dimension>
      <dimension name="performance_standards" weight="15">
        Are specified performance metrics achieved?
      </dimension>
    </dimensions>
    <rollup_strategy>
      Balanced weighted average with critical violations having moderate impact.
      System score = (0.7 * weighted_average(all_scores)) + (0.3 * critical_compliance)
    </rollup_strategy>
    <gut_checks>
      <check name="sudden_spike_detection">
        If any dimension changes by more than 20% between updates, flag for manual review.
        Likely indicates measurement error rather than real change.
      </check>
      <check name="test_count_validation">
        If total test count changes by more than 15% without corresponding code changes,
        verify test discovery patterns are working correctly.
      </check>
      <check name="coverage_bounds">
        Coverage should typically be between 20-80%. Values outside this range
        should trigger verification of measurement methodology.
      </check>
      <check name="score_consistency">
        Individual category scores should not deviate more than 30% from overall score
        without clear documented reasons.
      </check>
    </gut_checks>
  </scoring_methodology>
  <index_structure>
    <hierarchy>
      <level1>System Overall Score</level1>
      <level2>Service Scores (Backend, Auth, Frontend)</level2>
      <level3>Category Scores (Architecture, Testing, etc.)</level3>
      <level4>Individual Spec Scores</level4>
    </hierarchy>
    <categories>
      <category name="core_architecture">
        <specs>
          <spec>architecture.xml</spec>
          <spec>system_boundaries.xml</spec>
          <spec>independent_services.xml</spec>
          <spec>growth_control.xml</spec>
        </specs>
      </category>
      <category name="testing">
        <specs>
          <spec>testing.xml</spec>
          <spec>coverage_requirements.xml</spec>
          <spec>test_runner_guide.xml</spec>
          <spec>e2e-testing-spec.xml</spec>
        </specs>
        <test_discovery>
          <pattern type="e2e">
            <path_pattern>tests/unified/e2e/</path_pattern>
            <name_pattern>test_.*_e2e|e2e_test_|TestE2E|test_end_to_end</name_pattern>
            <decorator_pattern>@e2e|@real_services</decorator_pattern>
          </pattern>
          <pattern type="integration">
            <path_pattern>tests/unified/(?!e2e)</path_pattern>
            <name_pattern>test_.*_integration|integration_test_</name_pattern>
          </pattern>
          <pattern type="unit">
            <path_pattern>app/tests/unit/</path_pattern>
            <name_pattern>test_.*|Test.*</name_pattern>
          </pattern>
        </test_discovery>
        <test_pyramid_targets>
          <e2e_target>40%</e2e_target>
          <integration_target>40%</integration_target>
          <unit_target>20%</unit_target>
          <tolerance>10%</tolerance>
          <rationale>
            Modern testing best practices emphasize higher-level tests for better
            confidence and maintainability. E2E and integration tests provide more
            business value and catch real issues, while unit tests focus on critical
            algorithmic correctness.
          </rationale>
        </test_pyramid_targets>
      </category>
      <category name="agents">
        <specs>
          <spec>agent_architecture.xml</spec>
          <spec>subagents.xml</spec>
          <spec>agent_tracking.xml</spec>
          <spec>supervisor_observability.xml</spec>
        </specs>
      </category>
      <category name="infrastructure">
        <specs>
          <spec>staging_environment.xml</spec>
          <spec>terraform_gcp.xml</spec>
          <spec>github_actions.xml</spec>
          <spec>deployment_naming_standards.xml</spec>
        </specs>
      </category>
      <category name="data">
        <specs>
          <spec>clickhouse.xml</spec>
          <spec>postgres.xml</spec>
          <spec>database.xml</spec>
        </specs>
      </category>
      <category name="frontend">
        <specs>
          <spec>ui_ux_master.xml</spec>
          <spec>frontend_layout.xml</spec>
          <spec>websockets.xml</spec>
          <spec>frontend_testing_guide.xml</spec>
        </specs>
      </category>
      <category name="security">
        <specs>
          <spec>PRODUCTION_SECRETS_ISOLATION.xml</spec>
          <spec>auth_environment_isolation.xml</spec>
          <spec>tool_auth_system.xml</spec>
        </specs>
      </category>
      <category name="quality">
        <specs>
          <spec>type_safety.xml</spec>
          <spec>conventions.xml</spec>
          <spec>ai_slop_prevention_spec.xml</spec>
          <spec>compliance_reporting.xml</spec>
        </specs>
      </category>
    </categories>
  </index_structure>
  <update_process>
    <steps>
      <step order="1">
        Check this document (master_wip_index.xml) for current methodology
      </step>
      <step order="2">
        Run compliance checking tools:
        - python scripts/check_architecture_compliance.py
        - python unified_test_runner.py --report-only
      </step>
      <step order="3">
        Analyze spec alignment using sub-agents for each category
      </step>
      <step order="4">
        Calculate scores using defined methodology
      </step>
      <step order="5">
        Generate/update MASTER_WIP_STATUS.md with results
      </step>
      <step order="6">
        Update this spec with any methodology improvements
      </step>
    </steps>
    <triggers>
      <trigger>Daily automated check</trigger>
      <trigger>Before major releases</trigger>
      <trigger>After significant refactors</trigger>
      <trigger>On-demand via command</trigger>
    </triggers>
  </update_process>
  <report_format>
    <sections>
      <section name="executive_summary">
        Overall system health score with trend
      </section>
      <section name="service_breakdown">
        Per-service scores with critical issues
      </section>
      <section name="category_analysis">
        Detailed category scores with spec links
      </section>
      <section name="critical_violations">
        High-priority issues requiring immediate attention
      </section>
      <section name="improvement_areas">
        Actionable recommendations with priorities
      </section>
      <section name="progress_tracking">
        Historical trends and improvement velocity
      </section>
    </sections>
    <linking>
      Always link to source specs rather than duplicating content.
      Include last_updated timestamps for all referenced materials.
    </linking>
  </report_format>
  <automation>
    <scripts>
      <script>scripts/generate_wip_report.py</script>
      <script>scripts/check_spec_alignment.py</script>
      <script>scripts/business_value_test_index.py</script>
    </scripts>
    <sub_agent_tasks>
      <task>Analyze architecture compliance per service</task>
      <task>Evaluate test coverage against requirements</task>
      <task>Check API contract adherence</task>
      <task>Verify security implementation</task>
    </sub_agent_tasks>
  </automation>
  <learnings>
    <learning>
      Use existing reporting infrastructure from compliance_reporting.xml
    </learning>
    <learning>
      Leverage test_reporting.xml patterns for metric collection
    </learning>
    <learning>
      Reference ai_factory_status_report.xml for report structure
    </learning>
  </learnings>
</spec>