#!/usr/bin/env python3
"""
Issue #566 Security Vulnerability Test Execution Script

This script executes the comprehensive security test suite for Issue #566
LLM cache isolation vulnerability. It follows CLAUDE.md requirements for
test execution and provides detailed reporting.

CRITICAL SECURITY VULNERABILITY:
- LLM Managers created without user context in startup_module.py:649 and smd.py:979
- User conversation data mixing (User A gets User B's cached responses)
- GDPR/CCPA violations and $500K+ ARR business risk

TEST EXECUTION STRATEGY:
1. Run tests that MUST FAIL initially to prove vulnerability exists
2. After security fix implementation, same tests MUST PASS
3. Automated regression prevention for future deployments

Usage:
    python scripts/test_issue_566_security_vulnerability.py [--mode initial|fixed]
    
    --mode initial: Expect tests to FAIL (proves vulnerability)
    --mode fixed: Expect tests to PASS (validates security fix)
"""

import sys
import os
import subprocess
import argparse
import time
from typing import List, Dict, Any
from datetime import datetime

# Add project root to path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

from shared.isolated_environment import IsolatedEnvironment


class Issue566SecurityTestRunner:
    """Test runner for Issue #566 security vulnerability validation."""
    
    def __init__(self, mode: str = "initial"):
        """
        Initialize test runner.
        
        Args:
            mode: "initial" (expect failures) or "fixed" (expect passes)
        """
        self.mode = mode
        self.env = IsolatedEnvironment()
        self.start_time = datetime.now()
        
        # Test files to execute
        self.security_tests = [
            "tests/security/test_issue_566_llm_cache_isolation_vulnerability.py",
            "tests/integration/test_issue_566_multiuser_llm_security_integration.py"
        ]
        
        # Expected results based on mode
        self.expected_results = {
            "initial": "FAIL",  # Tests should fail to prove vulnerability
            "fixed": "PASS"     # Tests should pass after fix
        }
    
    def run_security_test_suite(self) -> Dict[str, Any]:
        """
        Execute the complete Issue #566 security test suite.
        
        Returns:
            Dict containing test results and analysis
        """
        print(f"\n{'='*80}")
        print(f"ISSUE #566 SECURITY VULNERABILITY TEST EXECUTION")
        print(f"{'='*80}")
        print(f"Mode: {self.mode}")
        print(f"Expected Result: {self.expected_results[self.mode]}")
        print(f"Start Time: {self.start_time}")
        print(f"{'='*80}\n")
        
        results = {
            "mode": self.mode,
            "expected": self.expected_results[self.mode],
            "test_files": self.security_tests,
            "individual_results": [],
            "overall_status": "UNKNOWN",
            "security_analysis": "",
            "recommendations": []
        }
        
        # Execute each test file
        for test_file in self.security_tests:
            print(f"Executing: {test_file}")
            result = self._run_single_test_file(test_file)
            results["individual_results"].append(result)
            print(f"Result: {result['status']}")
            print("-" * 40)
        
        # Analyze overall results
        results["overall_status"] = self._analyze_overall_results(results["individual_results"])
        results["security_analysis"] = self._generate_security_analysis(results)
        results["recommendations"] = self._generate_recommendations(results)
        
        # Print final report
        self._print_final_report(results)
        
        return results
    
    def _run_single_test_file(self, test_file: str) -> Dict[str, Any]:
        """
        Run a single test file and capture results.
        
        Args:
            test_file: Path to test file
            
        Returns:
            Dict with test execution results
        """
        start_time = time.time()
        
        try:
            # Use unified test runner following CLAUDE.md requirements
            cmd = [
                sys.executable, 
                "tests/unified_test_runner.py",
                "--category", "security",
                "--file", test_file,
                "--no-coverage",  # Focus on security validation
                "--verbose"
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=project_root,
                timeout=300  # 5 minute timeout
            )
            
            execution_time = time.time() - start_time
            
            return {
                "test_file": test_file,
                "exit_code": result.returncode,
                "status": "PASS" if result.returncode == 0 else "FAIL",
                "execution_time": execution_time,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "command": " ".join(cmd)
            }
            
        except subprocess.TimeoutExpired:
            return {
                "test_file": test_file,
                "exit_code": -1,
                "status": "TIMEOUT",
                "execution_time": 300,
                "stdout": "",
                "stderr": "Test execution timed out after 5 minutes",
                "command": " ".join(cmd)
            }
        except Exception as e:
            return {
                "test_file": test_file,
                "exit_code": -1,
                "status": "ERROR",
                "execution_time": time.time() - start_time,
                "stdout": "",
                "stderr": str(e),
                "command": "FAILED_TO_EXECUTE"
            }
    
    def _analyze_overall_results(self, individual_results: List[Dict[str, Any]]) -> str:
        """
        Analyze overall test results for security implications.
        
        Args:
            individual_results: List of individual test results
            
        Returns:
            Overall status string
        """
        failed_tests = [r for r in individual_results if r["status"] == "FAIL"]
        passed_tests = [r for r in individual_results if r["status"] == "PASS"]
        error_tests = [r for r in individual_results if r["status"] in ["ERROR", "TIMEOUT"]]
        
        if self.mode == "initial":
            # In initial mode, we EXPECT tests to fail to prove vulnerability
            if len(failed_tests) == len(individual_results):
                return "VULNERABILITY_CONFIRMED"
            elif len(passed_tests) > 0:
                return "UNEXPECTED_PASSES"
            else:
                return "EXECUTION_ERRORS"
        else:  # fixed mode
            # In fixed mode, we expect tests to pass after security fix
            if len(passed_tests) == len(individual_results):
                return "SECURITY_FIX_VALIDATED"
            elif len(failed_tests) > 0:
                return "SECURITY_FIX_INCOMPLETE"
            else:
                return "EXECUTION_ERRORS"
    
    def _generate_security_analysis(self, results: Dict[str, Any]) -> str:
        """
        Generate security analysis based on test results.
        
        Args:
            results: Complete test results
            
        Returns:
            Security analysis string
        """
        overall_status = results["overall_status"]
        
        if overall_status == "VULNERABILITY_CONFIRMED":
            return (
                "CRITICAL SECURITY VULNERABILITY CONFIRMED:\n"
                "- LLM cache isolation is compromised\n"
                "- User conversation data mixing detected\n"
                "- startup_module.py:649 and smd.py:979 create global managers without user context\n"
                "- Cache keys lack user prefixes, causing collisions\n"
                "- IMMEDIATE ACTION REQUIRED: Implement user context factory pattern"
            )
        elif overall_status == "SECURITY_FIX_VALIDATED":
            return (
                "SECURITY FIX SUCCESSFULLY VALIDATED:\n"
                "- LLM cache isolation working correctly\n"
                "- User conversation data properly isolated\n"
                "- All cache keys have proper user prefixes\n"
                "- No cross-user data leakage detected\n"
                "- Security regression tests passing"
            )
        elif overall_status == "UNEXPECTED_PASSES":
            return (
                "UNEXPECTED TEST PASSES DETECTED:\n"
                "- Tests expected to fail are passing\n"
                "- This may indicate incomplete vulnerability reproduction\n"
                "- OR the vulnerability has been partially fixed\n"
                "- Manual investigation required"
            )
        elif overall_status == "SECURITY_FIX_INCOMPLETE":
            return (
                "SECURITY FIX INCOMPLETE:\n"
                "- Some security tests still failing\n"
                "- Vulnerability may be partially resolved\n"
                "- Additional security hardening required\n"
                "- Review failing tests for remaining issues"
            )
        else:
            return (
                "TEST EXECUTION ERRORS:\n"
                "- Unable to complete security validation\n"
                "- Infrastructure or configuration issues\n"
                "- Resolve execution errors before security assessment"
            )
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """
        Generate actionable recommendations based on results.
        
        Args:
            results: Complete test results
            
        Returns:
            List of recommendation strings
        """
        overall_status = results["overall_status"]
        
        if overall_status == "VULNERABILITY_CONFIRMED":
            return [
                "IMMEDIATE: Remove global LLM manager creation from startup_module.py:649",
                "IMMEDIATE: Remove global LLM manager creation from smd.py:1007",
                "IMPLEMENT: User context factory pattern for all LLM operations",
                "VALIDATE: All LLM cache keys include user prefixes",
                "AUDIT: Review all components that create LLM managers",
                "TESTING: Re-run tests after fix implementation",
                "COMPLIANCE: Document fix for GDPR/CCPA audit trail"
            ]
        elif overall_status == "SECURITY_FIX_VALIDATED":
            return [
                "DEPLOY: Security fix validated, ready for deployment",
                "MONITOR: Implement monitoring for cache isolation violations",
                "REGRESSION: Add these tests to CI/CD pipeline",
                "AUDIT: Update security documentation",
                "COMPLIANCE: Mark Issue #566 as resolved in audit trail"
            ]
        elif overall_status == "UNEXPECTED_PASSES":
            return [
                "INVESTIGATE: Why vulnerability tests are passing unexpectedly",
                "VERIFY: Check if vulnerability reproduction is correct",
                "REVIEW: Manual code inspection of startup files",
                "TEST: Try alternative vulnerability reproduction methods"
            ]
        elif overall_status == "SECURITY_FIX_INCOMPLETE":
            return [
                "ANALYZE: Review failing tests to identify remaining issues",
                "FIX: Address any remaining cache isolation problems",
                "VALIDATE: Ensure all components use proper user context",
                "RETEST: Re-run full security test suite after additional fixes"
            ]
        else:
            return [
                "FIX: Resolve test execution infrastructure issues",
                "CHECK: Verify test environment configuration",
                "RETRY: Attempt test execution after infrastructure fixes"
            ]
    
    def _print_final_report(self, results: Dict[str, Any]) -> None:
        """
        Print comprehensive final report.
        
        Args:
            results: Complete test results
        """
        print(f"\n{'='*80}")
        print(f"ISSUE #566 SECURITY TEST FINAL REPORT")
        print(f"{'='*80}")
        print(f"Mode: {results['mode']}")
        print(f"Expected: {results['expected']}")
        print(f"Overall Status: {results['overall_status']}")
        print(f"Execution Time: {datetime.now() - self.start_time}")
        print()
        
        print("INDIVIDUAL TEST RESULTS:")
        print("-" * 40)
        for result in results["individual_results"]:
            print(f"File: {result['test_file']}")
            print(f"Status: {result['status']}")
            print(f"Exit Code: {result['exit_code']}")
            print(f"Time: {result['execution_time']:.2f}s")
            if result['stderr']:
                print(f"Errors: {result['stderr'][:200]}...")
            print()
        
        print("SECURITY ANALYSIS:")
        print("-" * 40)
        print(results["security_analysis"])
        print()
        
        print("RECOMMENDATIONS:")
        print("-" * 40)
        for i, rec in enumerate(results["recommendations"], 1):
            print(f"{i}. {rec}")
        print()
        
        print(f"{'='*80}")


def main():
    """Main execution function."""
    parser = argparse.ArgumentParser(
        description="Execute Issue #566 security vulnerability tests"
    )
    parser.add_argument(
        "--mode",
        choices=["initial", "fixed"],
        default="initial",
        help="Test execution mode: 'initial' expects failures, 'fixed' expects passes"
    )
    
    args = parser.parse_args()
    
    # Create and run test suite
    runner = Issue566SecurityTestRunner(mode=args.mode)
    results = runner.run_security_test_suite()
    
    # Exit with appropriate code
    if results["overall_status"] in ["VULNERABILITY_CONFIRMED", "SECURITY_FIX_VALIDATED"]:
        sys.exit(0)  # Expected result achieved
    else:
        sys.exit(1)  # Unexpected result or errors


if __name__ == "__main__":
    main()