#!/usr/bin/env python3
"""
Issue #1176 Evidence-Based Test Execution Script

This script runs the comprehensive test strategy for Issue #1176 to expose
the truth about system state versus documentation claims.

These tests are designed to FAIL initially to prove they're testing real
functionality rather than providing false confidence.

Usage:
    python3 scripts/run_issue_1176_evidence_based_tests.py [options]

Business Value: Protects $500K+ ARR by exposing infrastructure truth
"""

import sys
import subprocess
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from enum import Enum


class TestPhase(Enum):
    """Test execution phases."""
    INFRASTRUCTURE_INTEGRITY = "infrastructure_integrity"
    WEBSOCKET_IMPORT_STANDARDIZATION = "websocket_import_standardization"
    AUTH_FLOW_VALIDATION = "auth_flow_validation"
    GOLDEN_PATH_STAGING = "golden_path_staging"
    INFRASTRUCTURE_HEALTH = "infrastructure_health"


@dataclass
class TestPhaseResult:
    """Result of a test phase execution."""
    phase: TestPhase
    success: bool
    duration_seconds: float
    tests_run: int
    tests_passed: int
    tests_failed: int
    error_message: Optional[str] = None
    evidence_summary: Optional[str] = None


@dataclass
class EvidenceReport:
    """Comprehensive evidence report for Issue #1176."""
    execution_timestamp: str
    total_duration_seconds: float
    overall_success: bool
    phase_results: List[TestPhaseResult]
    infrastructure_evidence: Dict[str, Any]
    business_impact_analysis: Dict[str, Any]
    recommendations: List[str]


class Issue1176EvidenceBasedTestRunner:
    """Execute evidence-based tests to expose system truth."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.setup_logging()
        self.project_root = Path(__file__).parent.parent
        self.results: List[TestPhaseResult] = []
        
    def setup_logging(self):
        """Setup logging for test execution."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.FileHandler('issue_1176_evidence_tests.log')
            ]
        )
    
    def run_all_phases(self) -> EvidenceReport:
        """Run all test phases and generate evidence report."""
        self.logger.info("=" * 80)
        self.logger.info("ISSUE #1176 EVIDENCE-BASED TEST EXECUTION")
        self.logger.info("OBJECTIVE: Expose truth about system state vs documentation claims")
        self.logger.info("EXPECTED: Tests will FAIL initially to prove real validation")
        self.logger.info("=" * 80)
        
        start_time = time.time()
        
        # Define test phases in execution order
        test_phases = [
            (TestPhase.INFRASTRUCTURE_INTEGRITY, self._run_infrastructure_integrity_tests),
            (TestPhase.AUTH_FLOW_VALIDATION, self._run_auth_flow_validation_tests),
            (TestPhase.GOLDEN_PATH_STAGING, self._run_golden_path_staging_tests),
            (TestPhase.INFRASTRUCTURE_HEALTH, self._run_infrastructure_health_tests)
        ]
        
        for phase, test_func in test_phases:
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"EXECUTING PHASE: {phase.value.upper()}")
            self.logger.info(f"{'='*60}")
            
            phase_result = test_func()
            self.results.append(phase_result)
            
            if phase_result.success:
                self.logger.info(f"âœ… PHASE {phase.value} PASSED - System claims validated")
            else:
                self.logger.error(f"âŒ PHASE {phase.value} FAILED - System issues exposed")
                self.logger.error(f"   Evidence: {phase_result.evidence_summary}")
                
                # For evidence-based testing, failures are expected and valuable
                self.logger.info(f"ðŸ’¡ FAILURE IS EVIDENCE - This proves the test validates real functionality")
        
        total_duration = time.time() - start_time
        
        # Generate comprehensive evidence report
        evidence_report = self._generate_evidence_report(total_duration)
        
        # Save evidence report
        self._save_evidence_report(evidence_report)
        
        # Display summary
        self._display_summary(evidence_report)
        
        return evidence_report
    
    def _run_infrastructure_integrity_tests(self) -> TestPhaseResult:
        """Run infrastructure integrity tests."""
        start_time = time.time()
        
        self.logger.info("Testing basic system imports and infrastructure integrity...")
        self.logger.info("Expected: FAILURES that expose infrastructure workarounds")
        
        try:
            # Run infrastructure integrity tests
            cmd = [
                sys.executable, "-m", "pytest",
                "tests/infrastructure_integrity/test_basic_system_imports.py",
                "-v", "--tb=short", "--no-docker",
                "--json-report", "--json-report-file=infrastructure_test_results.json"
            ]
            
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300  # 5 minute timeout
            )
            
            duration = time.time() - start_time
            
            # Parse test results
            tests_run, tests_passed, tests_failed = self._parse_pytest_output(result.stdout)
            
            # For evidence-based testing, failures are expected and valuable
            evidence_summary = self._extract_infrastructure_evidence(result.stdout, result.stderr)
            
            return TestPhaseResult(
                phase=TestPhase.INFRASTRUCTURE_INTEGRITY,
                success=(tests_failed == 0),  # Success if no failures
                duration_seconds=duration,
                tests_run=tests_run,
                tests_passed=tests_passed,
                tests_failed=tests_failed,
                evidence_summary=evidence_summary
            )
            
        except subprocess.TimeoutExpired:
            return TestPhaseResult(
                phase=TestPhase.INFRASTRUCTURE_INTEGRITY,
                success=False,
                duration_seconds=time.time() - start_time,
                tests_run=0,
                tests_passed=0,
                tests_failed=1,
                error_message="Test execution timeout - infrastructure severely broken",
                evidence_summary="Test execution timeout indicates severe infrastructure issues"
            )
        except Exception as e:
            return TestPhaseResult(
                phase=TestPhase.INFRASTRUCTURE_INTEGRITY,
                success=False,
                duration_seconds=time.time() - start_time,
                tests_run=0,
                tests_passed=0,
                tests_failed=1,
                error_message=str(e),
                evidence_summary=f"Test execution failed: {str(e)}"
            )
    
    def _run_auth_flow_validation_tests(self) -> TestPhaseResult:
        """Run authentication flow validation tests."""
        start_time = time.time()
        
        self.logger.info("Testing authentication flow without Docker dependencies...")
        self.logger.info("Expected: FAILURES that expose Docker dependencies")
        
        try:
            cmd = [
                sys.executable, "-m", "pytest",
                "tests/integration/auth_flow_validation/test_auth_flow_without_docker.py",
                "-v", "--tb=short", "--no-docker",
                "--json-report", "--json-report-file=auth_test_results.json"
            ]
            
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=600  # 10 minute timeout
            )
            
            duration = time.time() - start_time
            tests_run, tests_passed, tests_failed = self._parse_pytest_output(result.stdout)
            evidence_summary = self._extract_auth_evidence(result.stdout, result.stderr)
            
            return TestPhaseResult(
                phase=TestPhase.AUTH_FLOW_VALIDATION,
                success=(tests_failed == 0),
                duration_seconds=duration,
                tests_run=tests_run,
                tests_passed=tests_passed,
                tests_failed=tests_failed,
                evidence_summary=evidence_summary
            )
            
        except subprocess.TimeoutExpired:
            return TestPhaseResult(
                phase=TestPhase.AUTH_FLOW_VALIDATION,
                success=False,
                duration_seconds=time.time() - start_time,
                tests_run=0,
                tests_passed=0,
                tests_failed=1,
                error_message="Auth test timeout - authentication system broken",
                evidence_summary="Auth test timeout indicates authentication system issues"
            )
        except Exception as e:
            return TestPhaseResult(
                phase=TestPhase.AUTH_FLOW_VALIDATION,
                success=False,
                duration_seconds=time.time() - start_time,
                tests_run=0,
                tests_passed=0,
                tests_failed=1,
                error_message=str(e),
                evidence_summary=f"Auth test execution failed: {str(e)}"
            )
    
    def _run_golden_path_staging_tests(self) -> TestPhaseResult:
        """Run golden path staging tests."""
        start_time = time.time()
        
        self.logger.info("Testing complete golden path on staging environment...")
        self.logger.info("Expected: FAILURES that expose golden path operational claims as false")
        
        try:
            cmd = [
                sys.executable, "-m", "pytest",
                "tests/e2e/golden_path_staging/test_golden_path_complete_user_journey.py",
                "-v", "--tb=short", "--staging-e2e", "--no-docker",
                "--json-report", "--json-report-file=golden_path_test_results.json"
            ]
            
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=1200  # 20 minute timeout for E2E
            )
            
            duration = time.time() - start_time
            tests_run, tests_passed, tests_failed = self._parse_pytest_output(result.stdout)
            evidence_summary = self._extract_golden_path_evidence(result.stdout, result.stderr)
            
            return TestPhaseResult(
                phase=TestPhase.GOLDEN_PATH_STAGING,
                success=(tests_failed == 0),
                duration_seconds=duration,
                tests_run=tests_run,
                tests_passed=tests_passed,
                tests_failed=tests_failed,
                evidence_summary=evidence_summary
            )
            
        except subprocess.TimeoutExpired:
            return TestPhaseResult(
                phase=TestPhase.GOLDEN_PATH_STAGING,
                success=False,
                duration_seconds=time.time() - start_time,
                tests_run=0,
                tests_passed=0,
                tests_failed=1,
                error_message="Golden path test timeout - staging system non-functional",
                evidence_summary="Golden path timeout proves staging system is non-functional"
            )
        except Exception as e:
            return TestPhaseResult(
                phase=TestPhase.GOLDEN_PATH_STAGING,
                success=False,
                duration_seconds=time.time() - start_time,
                tests_run=0,
                tests_passed=0,
                tests_failed=1,
                error_message=str(e),
                evidence_summary=f"Golden path test execution failed: {str(e)}"
            )
    
    def _run_infrastructure_health_tests(self) -> TestPhaseResult:
        """Run infrastructure health validation tests."""
        start_time = time.time()
        
        self.logger.info("Testing infrastructure health claims accuracy...")
        self.logger.info("Expected: FAILURES that expose false health claims")
        
        try:
            # For now, run the existing infrastructure tests as health validation
            cmd = [
                sys.executable, "-m", "pytest",
                "tests/infrastructure_integrity/test_basic_system_imports.py::TestDocumentationVsRealityGaps",
                "-v", "--tb=short", "--no-docker",
                "--json-report", "--json-report-file=health_test_results.json"
            ]
            
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300  # 5 minute timeout
            )
            
            duration = time.time() - start_time
            tests_run, tests_passed, tests_failed = self._parse_pytest_output(result.stdout)
            evidence_summary = self._extract_health_evidence(result.stdout, result.stderr)
            
            return TestPhaseResult(
                phase=TestPhase.INFRASTRUCTURE_HEALTH,
                success=(tests_failed == 0),
                duration_seconds=duration,
                tests_run=tests_run,
                tests_passed=tests_passed,
                tests_failed=tests_failed,
                evidence_summary=evidence_summary
            )
            
        except subprocess.TimeoutExpired:
            return TestPhaseResult(
                phase=TestPhase.INFRASTRUCTURE_HEALTH,
                success=False,
                duration_seconds=time.time() - start_time,
                tests_run=0,
                tests_passed=0,
                tests_failed=1,
                error_message="Health test timeout - health monitoring broken",
                evidence_summary="Health test timeout indicates health monitoring system issues"
            )
        except Exception as e:
            return TestPhaseResult(
                phase=TestPhase.INFRASTRUCTURE_HEALTH,
                success=False,
                duration_seconds=time.time() - start_time,
                tests_run=0,
                tests_passed=0,
                tests_failed=1,
                error_message=str(e),
                evidence_summary=f"Health test execution failed: {str(e)}"
            )
    
    def _parse_pytest_output(self, output: str) -> tuple[int, int, int]:
        """Parse pytest output to extract test counts."""
        tests_run = 0
        tests_passed = 0
        tests_failed = 0
        
        lines = output.split('\n')
        for line in lines:
            if "failed" in line.lower() and "passed" in line.lower():
                # Parse line like "2 failed, 3 passed in 1.23s"
                parts = line.split()
                for i, part in enumerate(parts):
                    if part == "failed" and i > 0:
                        tests_failed = int(parts[i-1])
                    elif part == "passed" and i > 0:
                        tests_passed = int(parts[i-1])
                tests_run = tests_failed + tests_passed
            elif "passed" in line.lower() and "failed" not in line.lower():
                # Parse line like "5 passed in 2.34s"
                parts = line.split()
                for i, part in enumerate(parts):
                    if part == "passed" and i > 0:
                        tests_passed = int(parts[i-1])
                        tests_run = tests_passed
        
        return tests_run, tests_passed, tests_failed
    
    def _extract_infrastructure_evidence(self, stdout: str, stderr: str) -> str:
        """Extract evidence from infrastructure test output."""
        evidence_points = []
        
        output = stdout + stderr
        
        if "INFRASTRUCTURE BROKEN" in output:
            evidence_points.append("Infrastructure import system broken")
        if "PYTHONPATH" in output:
            evidence_points.append("System requires PYTHONPATH workarounds")
        if "ModuleNotFoundError" in output:
            evidence_points.append("Module import structure incomplete")
        if "SSOT WARNING" in output:
            evidence_points.append("SSOT compliance violations detected")
        if "TEST INFRASTRUCTURE BROKEN" in output:
            evidence_points.append("Test infrastructure compromised")
        
        if not evidence_points:
            return "Infrastructure tests passed - claims validated"
        
        return f"Infrastructure issues found: {'; '.join(evidence_points)}"
    
    def _extract_auth_evidence(self, stdout: str, stderr: str) -> str:
        """Extract evidence from auth test output."""
        evidence_points = []
        
        output = stdout + stderr
        
        if "AUTH INFRASTRUCTURE BROKEN" in output:
            evidence_points.append("Auth infrastructure import broken")
        if "REQUIRES DOCKER" in output:
            evidence_points.append("Auth system requires Docker dependencies")
        if "USER CONTEXT" in output and "BROKEN" in output:
            evidence_points.append("User context system broken")
        if "INTEGRATION BROKEN" in output:
            evidence_points.append("Auth service integration broken")
        
        if not evidence_points:
            return "Auth flow tests passed - Docker independence validated"
        
        return f"Auth system issues found: {'; '.join(evidence_points)}"
    
    def _extract_golden_path_evidence(self, stdout: str, stderr: str) -> str:
        """Extract evidence from golden path test output."""
        evidence_points = []
        
        output = stdout + stderr
        
        if "GOLDEN PATH NOT OPERATIONAL" in output:
            evidence_points.append("Golden path not operational")
        if "staging" in output.lower() and ("not accessible" in output.lower() or "failed" in output.lower()):
            evidence_points.append("Staging environment issues")
        if "WebSocket" in output and ("failed" in output.lower() or "error" in output.lower()):
            evidence_points.append("WebSocket infrastructure broken")
        if "timeout" in output.lower():
            evidence_points.append("Service response timeouts")
        
        if not evidence_points:
            return "Golden path operational - staging validation successful"
        
        return f"Golden path issues found: {'; '.join(evidence_points)}"
    
    def _extract_health_evidence(self, stdout: str, stderr: str) -> str:
        """Extract evidence from health test output."""
        evidence_points = []
        
        output = stdout + stderr
        
        if "HEALTH CLAIMS FALSE" in output:
            evidence_points.append("Health claims inaccurate")
        if "disabled" in output.lower() and "decorator" in output.lower():
            evidence_points.append("Test decorators systematically disabled")
        if "TEST INFRASTRUCTURE COMPROMISED" in output:
            evidence_points.append("Test infrastructure compromised")
        
        if not evidence_points:
            return "Health claims validated - system health accurate"
        
        return f"Health claim issues found: {'; '.join(evidence_points)}"
    
    def _generate_evidence_report(self, total_duration: float) -> EvidenceReport:
        """Generate comprehensive evidence report."""
        successful_phases = sum(1 for result in self.results if result.success)
        total_phases = len(self.results)
        overall_success = successful_phases == total_phases
        
        # Analyze infrastructure evidence
        infrastructure_evidence = {
            "import_system_functional": any("import" not in r.evidence_summary for r in self.results),
            "docker_independence": any("DOCKER" not in r.evidence_summary for r in self.results),
            "ssot_compliance": any("SSOT" not in r.evidence_summary for r in self.results),
            "test_infrastructure_integrity": any("compromised" not in r.evidence_summary.lower() for r in self.results)
        }
        
        # Business impact analysis
        failed_phases = [r for r in self.results if not r.success]
        business_impact = {
            "revenue_at_risk": "$500K+ ARR" if failed_phases else "Protected",
            "golden_path_operational": not any(r.phase == TestPhase.GOLDEN_PATH_STAGING and not r.success for r in self.results),
            "customer_access_functional": not any(r.phase == TestPhase.AUTH_FLOW_VALIDATION and not r.success for r in self.results),
            "infrastructure_stable": not any(r.phase == TestPhase.INFRASTRUCTURE_INTEGRITY and not r.success for r in self.results)
        }
        
        # Generate recommendations
        recommendations = []
        if not overall_success:
            recommendations.extend([
                "STOP claiming 'Golden Path FULLY OPERATIONAL' until empirically validated",
                "Fix underlying infrastructure issues instead of commenting out test decorators",
                "Implement authentic end-to-end testing without workarounds",
                "Restore test infrastructure integrity and remove commented @require_docker_services decorators",
                "Validate system functionality empirically before updating documentation"
            ])
        else:
            recommendations.extend([
                "System validation successful - continue monitoring",
                "Maintain test infrastructure integrity",
                "Keep empirical validation practices"
            ])
        
        return EvidenceReport(
            execution_timestamp=time.strftime('%Y-%m-%d %H:%M:%S'),
            total_duration_seconds=total_duration,
            overall_success=overall_success,
            phase_results=self.results,
            infrastructure_evidence=infrastructure_evidence,
            business_impact_analysis=business_impact,
            recommendations=recommendations
        )
    
    def _save_evidence_report(self, report: EvidenceReport):
        """Save evidence report to file."""
        report_file = self.project_root / "ISSUE_1176_EVIDENCE_BASED_TEST_REPORT.json"
        
        with open(report_file, 'w') as f:
            json.dump(asdict(report), f, indent=2, default=str)
        
        self.logger.info(f"Evidence report saved to: {report_file}")
        
        # Also create markdown summary
        self._create_markdown_summary(report)
    
    def _create_markdown_summary(self, report: EvidenceReport):
        """Create markdown summary of evidence report."""
        markdown_file = self.project_root / "ISSUE_1176_EVIDENCE_BASED_TEST_SUMMARY.md"
        
        with open(markdown_file, 'w') as f:
            f.write(f"# Issue #1176 Evidence-Based Test Results\n\n")
            f.write(f"**Execution Time:** {report.execution_timestamp}\n")
            f.write(f"**Total Duration:** {report.total_duration_seconds:.2f} seconds\n")
            f.write(f"**Overall Success:** {'âœ… PASSED' if report.overall_success else 'âŒ FAILED'}\n\n")
            
            f.write("## Executive Summary\n\n")
            if report.overall_success:
                f.write("ðŸŽ‰ **SYSTEM CLAIMS VALIDATED** - All tests passed, indicating system is genuinely operational.\n\n")
            else:
                f.write("ðŸš¨ **SYSTEM CLAIMS FALSE** - Test failures expose truth about infrastructure state.\n\n")
            
            f.write("## Phase Results\n\n")
            for result in report.phase_results:
                status = "âœ… PASSED" if result.success else "âŒ FAILED"
                f.write(f"### {result.phase.value.replace('_', ' ').title()}\n")
                f.write(f"- **Status:** {status}\n")
                f.write(f"- **Duration:** {result.duration_seconds:.2f}s\n")
                f.write(f"- **Tests:** {result.tests_run} total, {result.tests_passed} passed, {result.tests_failed} failed\n")
                f.write(f"- **Evidence:** {result.evidence_summary}\n\n")
            
            f.write("## Business Impact Analysis\n\n")
            for key, value in report.business_impact_analysis.items():
                f.write(f"- **{key.replace('_', ' ').title()}:** {value}\n")
            
            f.write("\n## Recommendations\n\n")
            for i, rec in enumerate(report.recommendations, 1):
                f.write(f"{i}. {rec}\n")
            
            f.write(f"\n---\n*Report generated by Issue #1176 Evidence-Based Test Runner*\n")
        
        self.logger.info(f"Markdown summary saved to: {markdown_file}")
    
    def _display_summary(self, report: EvidenceReport):
        """Display test execution summary."""
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ISSUE #1176 EVIDENCE-BASED TEST EXECUTION SUMMARY")
        self.logger.info("=" * 80)
        
        if report.overall_success:
            self.logger.info("ðŸŽ‰ OUTCOME: System claims VALIDATED - Infrastructure genuinely operational")
        else:
            self.logger.info("ðŸš¨ OUTCOME: System claims EXPOSED as FALSE - Infrastructure issues revealed")
        
        self.logger.info(f"\nExecution Duration: {report.total_duration_seconds:.2f} seconds")
        self.logger.info(f"Phases Executed: {len(report.phase_results)}")
        
        passed_phases = sum(1 for r in report.phase_results if r.success)
        failed_phases = len(report.phase_results) - passed_phases
        
        self.logger.info(f"Phases Passed: {passed_phases}")
        self.logger.info(f"Phases Failed: {failed_phases}")
        
        if failed_phases > 0:
            self.logger.info("\nðŸ’¡ IMPORTANT: Test failures are EVIDENCE of real system issues")
            self.logger.info("   These failures prove the tests validate actual functionality")
            self.logger.info("   Use this evidence to fix infrastructure before claiming operational status")
        
        self.logger.info(f"\nBusiness Impact: {report.business_impact_analysis['revenue_at_risk']}")
        self.logger.info("\nNext Steps:")
        for i, rec in enumerate(report.recommendations[:3], 1):
            self.logger.info(f"  {i}. {rec}")
        
        self.logger.info("=" * 80)


def main():
    """Main execution function."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Run Issue #1176 evidence-based tests to expose system truth"
    )
    parser.add_argument(
        "--phase",
        choices=[phase.value for phase in TestPhase],
        help="Run specific test phase only"
    )
    parser.add_argument(
        "--save-only",
        action="store_true",
        help="Only save reports, don't run tests"
    )
    
    args = parser.parse_args()
    
    if args.save_only:
        print("Report-only mode not implemented yet")
        return
    
    # Initialize and run test runner
    runner = Issue1176EvidenceBasedTestRunner()
    
    try:
        if args.phase:
            print(f"Running single phase: {args.phase}")
            # TODO: Implement single phase execution
            print("Single phase execution not implemented yet")
        else:
            evidence_report = runner.run_all_phases()
            
            # Exit with appropriate code
            sys.exit(0 if evidence_report.overall_success else 1)
            
    except KeyboardInterrupt:
        print("\nTest execution interrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"Test execution failed with error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()