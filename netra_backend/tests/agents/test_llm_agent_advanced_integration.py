"""
Advanced LLM Agent Integration Tests
Complex integration tests with â‰¤8 line functions for architectural compliance
"""

import sys
from pathlib import Path

# Test framework import - using pytest fixtures instead

import asyncio
import time
import uuid
from unittest.mock import AsyncMock, MagicMock, Mock, patch

import pytest

from netra_backend.app.agents.state import DeepAgentState
from netra_backend.app.llm.llm_manager import LLMManager
from netra_backend.tests.agents.test_fixtures import (
    mock_db_session,
    mock_llm_manager,
    mock_tool_dispatcher,
    mock_websocket_manager,
)
from netra_backend.tests.agents.test_helpers import (
    create_concurrent_tasks,
    create_llm_response_with_tools,
    create_mock_infrastructure,
    create_mock_persistence,
    create_multiple_supervisors,
    create_supervisor_with_mocks,
    create_tool_execution_mocks,
    execute_optimization_flow,
    execute_tool_calls,
    setup_llm_responses,
    setup_mock_llm_with_retry,
    setup_websocket_manager,
    verify_concurrent_results,
    verify_optimization_flow,
    verify_tool_execution,
)

def _setup_concurrent_infrastructure():
    """Setup infrastructure for concurrent testing"""
    db_session, llm_manager, ws_manager = create_mock_infrastructure()
    mock_persistence = create_mock_persistence()
    return db_session, llm_manager, ws_manager, mock_persistence

async def _run_concurrent_requests(supervisors):
    """Run concurrent requests and return results"""
    tasks = create_concurrent_tasks(supervisors)
    return await asyncio.gather(*tasks, return_exceptions=True)

@pytest.mark.asyncio
async def test_concurrent_request_handling(mock_db_session, mock_llm_manager,
                                          mock_websocket_manager, mock_tool_dispatcher):
    """Test handling multiple concurrent requests"""
    db_session, llm_manager, ws_manager, mock_persistence = _setup_concurrent_infrastructure()
    
    # Mock: Agent supervisor isolation for testing without spawning real agents
    with patch('app.agents.supervisor_consolidated.state_persistence_service', mock_persistence):
        supervisors = create_multiple_supervisors(db_session, llm_manager, ws_manager, mock_persistence, 5)
        results = await _run_concurrent_requests(supervisors)
        verify_concurrent_results(results, 5)

def _setup_performance_test(supervisor_agent):
    """Setup performance test timing"""
    start_time = time.time()
    run_id = str(uuid.uuid4())
    return start_time, run_id

async def _run_performance_test(supervisor_agent, run_id):
    """Run performance test"""
    await supervisor_agent.run(
        "Test performance",
        supervisor_agent.thread_id,
        supervisor_agent.user_id,
        run_id
    )

def _verify_performance_timing(start_time, max_expected_time=2.0):
    """Verify performance timing meets expectations"""
    execution_time = time.time() - start_time
    assert execution_time < max_expected_time, f"Execution took {execution_time}s, expected < {max_expected_time}s"

@pytest.mark.asyncio
async def test_performance_metrics(supervisor_agent):
    """Test performance metric collection"""
    start_time, run_id = _setup_performance_test(supervisor_agent)
    await _run_performance_test(supervisor_agent, run_id)
    _verify_performance_timing(start_time)

async def _test_llm_retry_mechanism(llm_manager):
    """Test LLM retry mechanism"""
    try:
        result = await llm_manager.call_llm("Test prompt")
    except Exception:
        result = await llm_manager.call_llm("Test prompt")
    return result

def _verify_retry_result(result, call_counter):
    """Verify retry mechanism worked correctly"""
    assert result["content"] == "Successful response after retry"
    assert call_counter["count"] == 2

@pytest.mark.asyncio
async def test_real_llm_interaction():
    """Test real LLM interaction with proper error handling"""
    llm_manager, call_counter = setup_mock_llm_with_retry()
    result = await _test_llm_retry_mechanism(llm_manager)
    _verify_retry_result(result, call_counter)

async def _execute_llm_tool_flow(dispatcher, llm_response):
    """Execute LLM tool flow"""
    for tool_call in llm_response["tool_calls"]:
        await dispatcher.dispatch_tool(tool_call["name"], tool_call["parameters"])

@pytest.mark.asyncio
async def test_tool_execution_with_llm():
    """Test tool execution triggered by LLM response"""
    dispatcher, tool_results = create_tool_execution_mocks()
    llm_response = create_llm_response_with_tools()
    await _execute_llm_tool_flow(dispatcher, llm_response)
    verify_tool_execution(tool_results, ["analyze_workload", "optimize_batch_size"])

def _setup_end_to_end_infrastructure():
    """Setup infrastructure for end-to-end test"""
    db_session, llm_manager, ws_manager = create_mock_infrastructure()
    setup_llm_responses(llm_manager)
    setup_websocket_manager(ws_manager)
    return db_session, llm_manager, ws_manager

def _setup_end_to_end_persistence():
    """Setup persistence for end-to-end test"""
    mock_persistence = create_mock_persistence()
    return mock_persistence

async def _run_end_to_end_flow(supervisor):
    """Run complete end-to-end optimization flow"""
    state = await execute_optimization_flow(supervisor)
    verify_optimization_flow(state, supervisor)
    return state

@pytest.mark.asyncio
async def test_end_to_end_optimization_flow():
    """Test complete end-to-end optimization flow"""
    db_session, llm_manager, ws_manager = _setup_end_to_end_infrastructure()
    mock_persistence = _setup_end_to_end_persistence()
    
    # Mock: Agent supervisor isolation for testing without spawning real agents
    with patch('app.agents.supervisor_consolidated.state_persistence_service', mock_persistence):
        supervisor = create_supervisor_with_mocks(db_session, llm_manager, ws_manager, mock_persistence)
        state = await _run_end_to_end_flow(supervisor)