"""Real WebSocket Batch Processing Tests

Business Value Justification (BVJ):
- Segment: Mid & Enterprise (High Volume Usage)
- Business Goal: Performance & Scalability
- Value Impact: Enables efficient processing of multiple messages/requests
- Strategic Impact: Supports high-volume enterprise usage and improves throughput

Tests real WebSocket batch message processing with Docker services.
Validates batch processing efficiency and message ordering.
"""

import asyncio
import json
import time
from typing import Any, Dict, List

import pytest
import websockets
from websockets import WebSocketException

from netra_backend.tests.real_services_test_fixtures import skip_if_no_real_services
from shared.isolated_environment import get_env

env = get_env()


@pytest.mark.real_services
@pytest.mark.websocket
@pytest.mark.batch_processing
@skip_if_no_real_services
class RealWebSocketBatchProcessingTests:
    """Test real WebSocket batch processing capabilities."""
    
    @pytest.fixture
    def websocket_url(self):
        backend_host = env.get("BACKEND_HOST", "localhost")
        backend_port = env.get("BACKEND_PORT", "8000")
        return f"ws://{backend_host}:{backend_port}/ws"
    
    @pytest.fixture
    def auth_headers(self):
        jwt_token = env.get("TEST_JWT_TOKEN", "test_token_123")
        return {
            "Authorization": f"Bearer {jwt_token}",
            "User-Agent": "Netra-BatchProcessing-Test/1.0"
        }
    
    @pytest.mark.asyncio
    async def test_batch_message_processing(self, websocket_url, auth_headers):
        """Test processing multiple messages in batches."""
        user_id = f"batch_test_{int(time.time())}"
        batch_results = []
        
        try:
            async with websockets.connect(
                websocket_url,
                extra_headers=auth_headers,
                timeout=25
            ) as websocket:
                # Connect
                await websocket.send(json.dumps({"type": "connect", "user_id": user_id}))
                await websocket.recv()
                
                # Send batch of messages rapidly
                messages_to_batch = []
                for i in range(10):
                    message = {
                        "type": "batch_message",
                        "user_id": user_id,
                        "content": f"Batch message {i+1}",
                        "batch_id": f"batch_{user_id}",
                        "sequence": i,
                        "timestamp": time.time()
                    }
                    messages_to_batch.append(message)
                    await websocket.send(json.dumps(message))
                    await asyncio.sleep(0.05)  # Very rapid sending
                
                # Collect batch processing responses
                timeout_time = time.time() + 15
                while time.time() < timeout_time and len(batch_results) < 15:
                    try:
                        response = json.loads(await asyncio.wait_for(websocket.recv(), timeout=3.0))
                        batch_results.append(response)
                        
                    except asyncio.TimeoutError:
                        break
                    except WebSocketException:
                        break
                
        except Exception as e:
            pytest.fail(f"Batch processing test failed: {e}")
        
        # Analyze batch processing
        print(f"Batch processing - Messages sent: {len(messages_to_batch)}, Responses: {len(batch_results)}")
        
        # Should receive responses for batch processing
        assert len(batch_results) > 0, "Should receive batch processing responses"
        
        # Check for batch processing indicators
        batch_indicators = [
            r for r in batch_results 
            if "batch" in str(r.get("type", "")).lower() or "batch_id" in r
        ]
        
        if batch_indicators:
            print(f"SUCCESS: Batch processing detected - {len(batch_indicators)} batch responses")
        else:
            print("INFO: Batch processing indicators not clearly detected")
            # Still validate basic processing
            processing_rate = len(batch_results) / len(messages_to_batch) if messages_to_batch else 0
            print(f"Processing rate: {processing_rate:.2%}")
    
    @pytest.mark.asyncio
    async def test_batch_ordering_preservation(self, websocket_url, auth_headers):
        """Test that batch processing preserves message ordering."""
        user_id = f"batch_order_test_{int(time.time())}"
        ordered_results = []
        
        try:
            async with websockets.connect(
                websocket_url,
                extra_headers=auth_headers,
                timeout=20
            ) as websocket:
                # Connect
                await websocket.send(json.dumps({"type": "connect", "user_id": user_id}))
                await websocket.recv()
                
                # Send ordered batch
                for i in range(5):
                    ordered_message = {
                        "type": "ordered_batch_message",
                        "user_id": user_id,
                        "content": f"Ordered message {i+1}",
                        "sequence_id": i,
                        "preserve_order": True
                    }
                    await websocket.send(json.dumps(ordered_message))
                    await asyncio.sleep(0.1)
                
                # Collect responses
                timeout_time = time.time() + 12
                while time.time() < timeout_time and len(ordered_results) < 8:
                    try:
                        response = json.loads(await asyncio.wait_for(websocket.recv(), timeout=2.0))
                        ordered_results.append(response)
                    except asyncio.TimeoutError:
                        break
                
        except Exception as e:
            pytest.fail(f"Batch ordering test failed: {e}")
        
        # Analyze ordering preservation
        print(f"Batch ordering - Responses received: {len(ordered_results)}")
        
        # Check for sequence preservation in responses
        sequence_ids = []
        for response in ordered_results:
            if "sequence_id" in response:
                sequence_ids.append(response["sequence_id"])
        
        if sequence_ids:
            is_ordered = all(sequence_ids[i] <= sequence_ids[i+1] for i in range(len(sequence_ids)-1))
            print(f"Sequence IDs: {sequence_ids}, Ordered: {is_ordered}")
            
            if is_ordered:
                print("SUCCESS: Batch ordering preserved")
            else:
                print("INFO: Batch ordering may not be strictly preserved (acceptable for some implementations)")
        else:
            print("INFO: Sequence information not found in responses")