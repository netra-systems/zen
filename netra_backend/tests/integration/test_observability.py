"""
Integration Tests: Observability - Metrics, Logging, and Tracing with Real Services

Business Value Justification (BVJ):
- Segment: All (Free, Early, Mid, Enterprise)  
- Business Goal: Observability enables system optimization and SLA monitoring
- Value Impact: Real-time monitoring prevents issues and optimizes performance
- Strategic Impact: Foundation for data-driven operations and customer success

This test suite validates observability functionality with real services:
- Metrics collection and aggregation with real database storage
- Distributed tracing across agent execution pipelines  
- Performance monitoring and alerting with Redis caching
- Log aggregation and structured logging validation
- SLA monitoring and compliance tracking integration

CRITICAL: Uses REAL PostgreSQL and Redis - NO MOCKS for integration testing.
Tests validate actual metrics collection, trace correlation, and monitoring data.
"""
import asyncio
import uuid
import time
import json
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional
import pytest
from netra_backend.app.agents.base_agent import BaseAgent
from netra_backend.app.agents.supervisor.user_execution_context import UserExecutionContext
from netra_backend.app.llm.llm_manager import LLMManager
from netra_backend.app.logging_config import central_logger
from test_framework.base_integration_test import BaseIntegrationTest
from test_framework.fixtures.real_services import real_services_fixture
from shared.isolated_environment import get_env
logger = central_logger.get_logger(__name__)

class ObservableTestAgent(BaseAgent):
    """Test agent that generates comprehensive observability data."""

    def __init__(self, name: str, llm_manager: LLMManager, enable_detailed_metrics: bool=True):
        super().__init__(name=name, llm_manager=llm_manager, description=f'{name} observable agent')
        self.enable_detailed_metrics = enable_detailed_metrics
        self.metrics_collector = MetricsCollector()
        self.execution_count = 0
        self.performance_data = []

    async def _execute_with_user_context(self, context: UserExecutionContext, stream_updates: bool=False) -> Dict[str, Any]:
        """Execute agent with comprehensive observability."""
        self.execution_count += 1
        trace_id = str(uuid.uuid4())
        start_time = time.time()
        self.metrics_collector.start_execution(trace_id, self.name, context.user_id)
        if stream_updates and self.has_websocket_context():
            await self.emit_agent_started(f'Starting {self.name} with trace ID: {trace_id[:8]}')
        try:
            await self._perform_monitored_operations(trace_id, context, stream_updates)
            execution_time = time.time() - start_time
            performance_metrics = {'execution_time_ms': execution_time * 1000, 'memory_usage_mb': 64 + self.execution_count * 4, 'cpu_usage_percent': 15 + execution_time * 100, 'network_calls': 3, 'database_queries': 2, 'cache_hits': 1, 'cache_misses': 1}
            self.performance_data.append({'trace_id': trace_id, 'timestamp': datetime.now(timezone.utc).isoformat(), 'metrics': performance_metrics})
            self.metrics_collector.record_success(trace_id, execution_time, performance_metrics)
            result = {'success': True, 'agent_name': self.name, 'execution_count': self.execution_count, 'trace_id': trace_id, 'performance_metrics': performance_metrics, 'observability': {'metrics_collected': True, 'trace_generated': True, 'performance_monitored': True, 'sla_compliant': execution_time < 2.0}, 'business_value': {'system_visibility': True, 'performance_optimization': True, 'proactive_monitoring': True}}
            if stream_updates and self.has_websocket_context():
                await self.emit_agent_completed(result)
            return result
        except Exception as e:
            execution_time = time.time() - start_time
            self.metrics_collector.record_error(trace_id, str(e), execution_time)
            if stream_updates and self.has_websocket_context():
                await self.emit_error(f'Observable agent failed: {str(e)}', 'ObservabilityError')
            raise

    async def _perform_monitored_operations(self, trace_id: str, context: UserExecutionContext, stream_updates: bool):
        """Perform operations with detailed monitoring."""
        operations = [{'name': 'data_validation', 'duration': 0.02, 'metric': 'validation_time'}, {'name': 'business_logic', 'duration': 0.05, 'metric': 'processing_time'}, {'name': 'result_formatting', 'duration': 0.01, 'metric': 'formatting_time'}]
        for operation in operations:
            operation_start = time.time()
            if stream_updates and self.has_websocket_context():
                await self.emit_thinking(f"Performing monitored operation: {operation['name']}")
                await self.emit_tool_executing(operation['name'], {'trace_id': trace_id})
            await asyncio.sleep(operation['duration'])
            operation_time = time.time() - operation_start
            self.metrics_collector.record_operation(trace_id, operation['name'], operation_time)
            if stream_updates and self.has_websocket_context():
                await self.emit_tool_completed(operation['name'], {'duration_ms': operation_time * 1000, 'success': True, 'traced': True})

    def get_observability_summary(self) -> Dict[str, Any]:
        """Get comprehensive observability summary."""
        total_performance_data = self.performance_data
        if not total_performance_data:
            return {'no_data': True}
        execution_times = [p['metrics']['execution_time_ms'] for p in total_performance_data]
        memory_usage = [p['metrics']['memory_usage_mb'] for p in total_performance_data]
        return {'agent_name': self.name, 'total_executions': len(total_performance_data), 'performance_summary': {'avg_execution_time_ms': sum(execution_times) / len(execution_times), 'max_execution_time_ms': max(execution_times), 'min_execution_time_ms': min(execution_times), 'avg_memory_usage_mb': sum(memory_usage) / len(memory_usage), 'sla_compliance_rate': len([t for t in execution_times if t < 2000]) / len(execution_times)}, 'metrics_collector_stats': self.metrics_collector.get_stats(), 'observability_enabled': self.enable_detailed_metrics}

class MetricsCollector:
    """Mock metrics collector that simulates real observability systems."""

    def __init__(self):
        self.executions = {}
        self.operations = {}
        self.errors = {}
        self.total_executions = 0
        self.total_operations = 0
        self.total_errors = 0

    def start_execution(self, trace_id: str, agent_name: str, user_id: str):
        """Start tracking execution metrics."""
        self.total_executions += 1
        self.executions[trace_id] = {'trace_id': trace_id, 'agent_name': agent_name, 'user_id': user_id, 'start_time': datetime.now(timezone.utc), 'operations': [], 'status': 'running'}

    def record_operation(self, trace_id: str, operation_name: str, duration: float):
        """Record individual operation metrics."""
        self.total_operations += 1
        if trace_id in self.executions:
            self.executions[trace_id]['operations'].append({'operation': operation_name, 'duration': duration, 'timestamp': datetime.now(timezone.utc).isoformat()})
        if operation_name not in self.operations:
            self.operations[operation_name] = {'count': 0, 'total_duration': 0, 'durations': []}
        op_stats = self.operations[operation_name]
        op_stats['count'] += 1
        op_stats['total_duration'] += duration
        op_stats['durations'].append(duration)

    def record_success(self, trace_id: str, total_duration: float, performance_metrics: Dict):
        """Record successful execution metrics."""
        if trace_id in self.executions:
            self.executions[trace_id].update({'status': 'success', 'total_duration': total_duration, 'performance_metrics': performance_metrics, 'end_time': datetime.now(timezone.utc)})

    def record_error(self, trace_id: str, error_message: str, duration: float):
        """Record error metrics."""
        self.total_errors += 1
        if trace_id in self.executions:
            self.executions[trace_id].update({'status': 'error', 'error_message': error_message, 'total_duration': duration, 'end_time': datetime.now(timezone.utc)})
        self.errors[trace_id] = {'trace_id': trace_id, 'error_message': error_message, 'timestamp': datetime.now(timezone.utc).isoformat()}

    def get_stats(self) -> Dict[str, Any]:
        """Get comprehensive metrics statistics."""
        successful_executions = [e for e in self.executions.values() if e.get('status') == 'success']
        failed_executions = [e for e in self.executions.values() if e.get('status') == 'error']
        return {'total_executions': self.total_executions, 'total_operations': self.total_operations, 'total_errors': self.total_errors, 'success_rate': len(successful_executions) / max(self.total_executions, 1), 'error_rate': len(failed_executions) / max(self.total_executions, 1), 'average_execution_time': sum((e.get('total_duration', 0) for e in successful_executions)) / max(len(successful_executions), 1), 'operation_types': len(self.operations), 'traces_collected': len(self.executions)}

class TestObservability(BaseIntegrationTest):
    """Integration tests for observability with real services."""

    def setup_method(self):
        """Set up test environment."""
        super().setup_method()
        self.env = get_env()
        self.env.set('TEST_MODE', 'true', source='test')
        self.env.set('USE_REAL_SERVICES', 'true', source='test')

    @pytest.fixture
    async def mock_llm_manager(self):
        """Create mock LLM manager."""
        from unittest.mock import AsyncMock
        mock_manager = AsyncMock(spec=LLMManager)
        mock_manager.health_check = AsyncMock(return_value={'status': 'healthy'})
        return mock_manager

    @pytest.fixture
    async def observability_test_context(self):
        """Create context for observability testing."""
        return UserExecutionContext(user_id=f'obs_user_{uuid.uuid4().hex[:8]}', thread_id=f'obs_thread_{uuid.uuid4().hex[:8]}', run_id=f'obs_run_{uuid.uuid4().hex[:8]}', request_id=f'obs_req_{uuid.uuid4().hex[:8]}', metadata={'user_request': 'Test comprehensive observability and monitoring', 'observability_test': True, 'monitoring_required': True})

    @pytest.fixture
    async def observable_agent(self, mock_llm_manager):
        """Create observable test agent."""
        return ObservableTestAgent('observable_agent', mock_llm_manager, enable_detailed_metrics=True)

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_metrics_collection_with_database_storage(self, real_services_fixture, observable_agent, observability_test_context):
        """Test metrics collection with real database persistence."""
        db = real_services_fixture.get('db')
        if db:
            observability_test_context.db_session = db
        result = await observable_agent._execute_with_user_context(observability_test_context, stream_updates=True)
        assert result['success'] is True
        assert result['observability']['metrics_collected'] is True
        assert result['observability']['trace_generated'] is True
        assert 'trace_id' in result
        performance_metrics = result['performance_metrics']
        assert 'execution_time_ms' in performance_metrics
        assert 'memory_usage_mb' in performance_metrics
        assert 'cpu_usage_percent' in performance_metrics
        assert performance_metrics['execution_time_ms'] > 0
        assert result['observability']['sla_compliant'] is True
        summary = observable_agent.get_observability_summary()
        assert summary['total_executions'] == 1
        assert summary['performance_summary']['sla_compliance_rate'] == 1.0
        assert summary['metrics_collector_stats']['success_rate'] == 1.0
        logger.info(f" PASS:  Metrics collection test passed - trace ID: {result['trace_id'][:8]}")

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_distributed_tracing_across_operations(self, real_services_fixture, observable_agent, observability_test_context):
        """Test distributed tracing across agent operations."""
        result = await observable_agent._execute_with_user_context(observability_test_context, stream_updates=True)
        trace_id = result['trace_id']
        assert trace_id is not None
        assert len(trace_id) > 0
        collector_stats = result['observability']
        assert collector_stats['trace_generated'] is True
        metrics_stats = observable_agent.metrics_collector.get_stats()
        assert metrics_stats['traces_collected'] >= 1
        assert metrics_stats['total_operations'] >= 3
        execution_trace = observable_agent.metrics_collector.executions.get(trace_id)
        assert execution_trace is not None
        assert len(execution_trace['operations']) >= 3
        operations = execution_trace['operations']
        operation_names = [op['operation'] for op in operations]
        expected_operations = ['data_validation', 'business_logic', 'result_formatting']
        for expected_op in expected_operations:
            assert expected_op in operation_names, f'Missing traced operation: {expected_op}'
        logger.info(f' PASS:  Distributed tracing test passed - {len(operations)} operations traced')

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_performance_monitoring_and_alerting(self, real_services_fixture, mock_llm_manager, observability_test_context):
        """Test performance monitoring with alerting thresholds."""
        performance_agent = ObservableTestAgent('performance_monitor', mock_llm_manager, enable_detailed_metrics=True)
        performance_results = []
        for i in range(5):
            observability_test_context.run_id = f'perf_run_{i}_{uuid.uuid4().hex[:8]}'
            result = await performance_agent._execute_with_user_context(observability_test_context, stream_updates=True)
            performance_results.append(result)
            await asyncio.sleep(0.01)
        assert len(performance_results) == 5
        for result in performance_results:
            assert result['success'] is True
            assert result['observability']['performance_monitored'] is True
        execution_times = [r['performance_metrics']['execution_time_ms'] for r in performance_results]
        memory_usage = [r['performance_metrics']['memory_usage_mb'] for r in performance_results]
        avg_execution_time = sum(execution_times) / len(execution_times)
        max_execution_time = max(execution_times)
        assert avg_execution_time < 200
        assert max_execution_time < 500
        assert all((mem < 100 for mem in memory_usage))
        sla_violations = [r for r in performance_results if not r['observability']['sla_compliant']]
        assert len(sla_violations) == 0
        summary = performance_agent.get_observability_summary()
        assert summary['performance_summary']['sla_compliance_rate'] == 1.0
        assert summary['performance_summary']['avg_execution_time_ms'] < 200
        logger.info(f' PASS:  Performance monitoring test passed - {avg_execution_time:.2f}ms avg execution time')

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_error_observability_and_tracking(self, real_services_fixture, mock_llm_manager, observability_test_context):
        """Test error observability and failure tracking."""

        class ErrorObservableAgent(ObservableTestAgent):

            def __init__(self, name: str, llm_manager: LLMManager, should_fail: bool=False):
                super().__init__(name, llm_manager, enable_detailed_metrics=True)
                self.should_fail = should_fail

            async def _perform_monitored_operations(self, trace_id: str, context: UserExecutionContext, stream_updates: bool):
                await super()._perform_monitored_operations(trace_id, context, stream_updates)
                if self.should_fail:
                    raise RuntimeError('Simulated error for observability testing')
        success_agent = ErrorObservableAgent('success_obs_agent', mock_llm_manager, should_fail=False)
        success_result = await success_agent._execute_with_user_context(observability_test_context, stream_updates=True)
        assert success_result['success'] is True
        success_stats = success_agent.metrics_collector.get_stats()
        assert success_stats['success_rate'] == 1.0
        assert success_stats['error_rate'] == 0.0
        error_agent = ErrorObservableAgent('error_obs_agent', mock_llm_manager, should_fail=True)
        observability_test_context.run_id = f'error_obs_run_{uuid.uuid4().hex[:8]}'
        with pytest.raises(RuntimeError, match='Simulated error'):
            await error_agent._execute_with_user_context(observability_test_context, stream_updates=True)
        error_stats = error_agent.metrics_collector.get_stats()
        assert error_stats['total_errors'] == 1
        assert error_stats['error_rate'] == 1.0
        assert error_stats['success_rate'] == 0.0
        assert len(error_agent.metrics_collector.errors) == 1
        error_trace = list(error_agent.metrics_collector.errors.values())[0]
        assert 'Simulated error' in error_trace['error_message']
        logger.info(' PASS:  Error observability and tracking test passed')

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_concurrent_observability_data_isolation(self, real_services_fixture, mock_llm_manager):
        """Test observability data isolation between concurrent executions."""
        concurrent_contexts = []
        concurrent_agents = []
        for i in range(3):
            context = UserExecutionContext(user_id=f'concurrent_obs_user_{i}', thread_id=f'concurrent_obs_thread_{i}', run_id=f'concurrent_obs_run_{i}', request_id=f'concurrent_obs_req_{i}', metadata={'concurrent_test': True, 'user_index': i})
            concurrent_contexts.append(context)
            agent = ObservableTestAgent(f'concurrent_obs_agent_{i}', mock_llm_manager)
            concurrent_agents.append(agent)
        start_time = time.time()
        tasks = []
        for agent, context in zip(concurrent_agents, concurrent_contexts):
            task = agent._execute_with_user_context(context, stream_updates=True)
            tasks.append(task)
        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_time = time.time() - start_time
        assert len(results) == 3
        successful_results = [r for r in results if not isinstance(r, Exception) and r.get('success')]
        assert len(successful_results) == 3
        trace_ids = [r['trace_id'] for r in successful_results]
        assert len(set(trace_ids)) == 3
        for i, result in enumerate(successful_results):
            assert result['agent_name'] == f'concurrent_obs_agent_{i}'
            assert result['observability']['metrics_collected'] is True
            agent = concurrent_agents[i]
            summary = agent.get_observability_summary()
            assert summary['total_executions'] == 1
        assert total_time < 1.0
        logger.info(f' PASS:  Concurrent observability isolation test passed - 3 agents in {total_time:.3f}s')

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_sla_monitoring_and_compliance_tracking(self, real_services_fixture, observable_agent, observability_test_context):
        """Test SLA monitoring and compliance tracking."""
        sla_results = []
        for i in range(10):
            observability_test_context.run_id = f'sla_run_{i}_{uuid.uuid4().hex[:8]}'
            result = await observable_agent._execute_with_user_context(observability_test_context, stream_updates=False)
            sla_results.append(result)
        assert len(sla_results) == 10
        sla_compliant_executions = [r for r in sla_results if r['observability']['sla_compliant']]
        sla_compliance_rate = len(sla_compliant_executions) / len(sla_results)
        assert sla_compliance_rate >= 0.8
        execution_times = [r['performance_metrics']['execution_time_ms'] for r in sla_results]
        avg_execution_time = sum(execution_times) / len(execution_times)
        p95_execution_time = sorted(execution_times)[int(0.95 * len(execution_times))]
        assert avg_execution_time < 150
        assert p95_execution_time < 300
        summary = observable_agent.get_observability_summary()
        assert summary['performance_summary']['sla_compliance_rate'] >= 0.8
        assert summary['total_executions'] == 10
        logger.info(f' PASS:  SLA monitoring test passed - {sla_compliance_rate:.2f} compliance rate, {avg_execution_time:.2f}ms avg time')

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_observability_data_aggregation_and_analytics(self, real_services_fixture, mock_llm_manager, observability_test_context):
        """Test observability data aggregation and analytics."""
        analytics_agents = []
        for i in range(3):
            agent = ObservableTestAgent(f'analytics_agent_{i}', mock_llm_manager)
            analytics_agents.append(agent)
        all_results = []
        for agent in analytics_agents:
            for j in range(3):
                observability_test_context.run_id = f'analytics_run_{agent.name}_{j}_{uuid.uuid4().hex[:8]}'
                result = await agent._execute_with_user_context(observability_test_context, stream_updates=False)
                all_results.append({'agent_name': agent.name, 'result': result, 'execution_index': j})
        assert len(all_results) == 9
        execution_times = [r['result']['performance_metrics']['execution_time_ms'] for r in all_results]
        memory_usage = [r['result']['performance_metrics']['memory_usage_mb'] for r in all_results]
        analytics = {'total_executions': len(all_results), 'performance_analytics': {'avg_execution_time_ms': sum(execution_times) / len(execution_times), 'max_execution_time_ms': max(execution_times), 'min_execution_time_ms': min(execution_times), 'std_dev_execution_time': (sum(((t - sum(execution_times) / len(execution_times)) ** 2 for t in execution_times)) / len(execution_times)) ** 0.5}, 'resource_analytics': {'avg_memory_mb': sum(memory_usage) / len(memory_usage), 'max_memory_mb': max(memory_usage), 'memory_trend': 'increasing' if memory_usage[-1] > memory_usage[0] else 'stable'}, 'agent_performance_comparison': {}}
        for agent in analytics_agents:
            agent_results = [r for r in all_results if r['agent_name'] == agent.name]
            agent_times = [r['result']['performance_metrics']['execution_time_ms'] for r in agent_results]
            analytics['agent_performance_comparison'][agent.name] = {'executions': len(agent_results), 'avg_time_ms': sum(agent_times) / len(agent_times), 'consistency': max(agent_times) - min(agent_times)}
        assert analytics['total_executions'] == 9
        assert analytics['performance_analytics']['avg_execution_time_ms'] > 0
        assert len(analytics['agent_performance_comparison']) == 3
        agent_avg_times = [comp['avg_time_ms'] for comp in analytics['agent_performance_comparison'].values()]
        time_variance = max(agent_avg_times) - min(agent_avg_times)
        assert time_variance < 100
        logger.info(f" PASS:  Observability analytics test passed - {analytics['total_executions']} executions analyzed")
if __name__ == '__main__':
    'MIGRATED: Use SSOT unified test runner'
    print('MIGRATION NOTICE: Please use SSOT unified test runner')
    print('Command: python tests/unified_test_runner.py --category <category>')