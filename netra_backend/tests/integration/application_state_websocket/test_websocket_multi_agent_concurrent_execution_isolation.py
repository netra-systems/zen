"""Test #35: Multi-Agent Concurrent Execution Through WebSocket Channels with Isolation

Business Value Justification (BVJ):
- Segment: Mid, Enterprise (multi-user environments requiring concurrent agent execution)
- Business Goal: Enable multiple users to run AI optimization agents simultaneously without interference
- Value Impact: Concurrent execution scales platform to handle enterprise workloads ($500k+ annual contracts)
- Strategic Impact: Multi-tenant isolation is critical for enterprise SaaS offering and competitive differentiation

This test validates that multiple agents can execute concurrently through isolated WebSocket
channels while maintaining perfect user isolation and application state consistency.

CRITICAL: Multi-agent concurrent execution enables:
- Enterprise scalability: Multiple users running optimization simultaneously
- Resource efficiency: Parallel processing maximizes infrastructure utilization
- User experience: No blocking - users get immediate responses regardless of concurrent load
- Data security: Perfect isolation prevents cross-user data contamination
- Business continuity: System remains responsive under concurrent enterprise workloads

WITHOUT proper concurrent execution with isolation, enterprise customers cannot
use the platform for business-critical operations requiring multi-user access.
"""
import asyncio
import json
import time
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple
from unittest.mock import AsyncMock, MagicMock
from concurrent.futures import ThreadPoolExecutor
import threading
import pytest
from netra_backend.app.agents.supervisor.user_execution_engine import UserExecutionEngine as ExecutionEngine
from netra_backend.app.agents.supervisor.execution_context import AgentExecutionContext, AgentExecutionResult, PipelineStep
from netra_backend.app.agents.supervisor.user_execution_context import UserExecutionContext, validate_user_context
from netra_backend.app.agents.supervisor.agent_registry import AgentRegistry
from netra_backend.app.schemas.agent_models import DeepAgentState
from netra_backend.app.services.agent_websocket_bridge import AgentWebSocketBridge
from netra_backend.app.agents.base_agent import BaseAgent
from netra_backend.app.llm.llm_manager import LLMManager
from test_framework.base_integration_test import BaseIntegrationTest
from test_framework.real_services_test_fixtures import real_services_fixture
from shared.isolated_environment import get_env

class ConcurrentExecutionAgent(BaseAgent):
    """Agent designed for concurrent execution testing with isolation validation."""

    def __init__(self, name: str, llm_manager: LLMManager, execution_id: str):
        super().__init__(llm_manager=llm_manager, name=name, description=f'Concurrent execution {name}')
        self.websocket_bridge = None
        self.execution_id = execution_id
        self.isolation_markers = []
        self.concurrent_safety_checks = []
        self.thread_id = None

    def set_websocket_bridge(self, bridge: AgentWebSocketBridge, run_id: str):
        """Set WebSocket bridge for concurrent execution."""
        self.websocket_bridge = bridge
        self._run_id = run_id
        self.thread_id = threading.current_thread().ident

    async def execute(self, state: DeepAgentState, run_id: str, stream_updates: bool=True) -> Dict[str, Any]:
        """Execute agent with concurrent safety validation."""
        if not stream_updates or not self.websocket_bridge:
            raise ValueError('WebSocket bridge required for concurrent execution')
        execution_start_marker = {'agent_name': self.name, 'execution_id': self.execution_id, 'user_id': getattr(state, 'user_id', None), 'run_id': run_id, 'thread_id': self.thread_id, 'start_time': datetime.now(timezone.utc), 'isolation_context': self._generate_isolation_context(state)}
        self.isolation_markers.append(execution_start_marker)
        await self.websocket_bridge.notify_agent_started(run_id, self.name, {'concurrent_execution': True, 'isolation_context': execution_start_marker['isolation_context'], 'execution_id': self.execution_id, 'thread_safe': True})
        self.concurrent_safety_checks.append(await self._perform_isolation_check('startup', state))
        thinking_phases = [f'Initializing concurrent execution for {self.name} (ID: {self.execution_id})', f'Processing user-specific data with isolation guarantees', f'Executing concurrent-safe algorithms for {self.name}', f'Validating data isolation during concurrent processing']
        for i, thinking in enumerate(thinking_phases):
            await self.websocket_bridge.notify_agent_thinking(run_id, self.name, thinking, step_number=i + 1, progress_percentage=int((i + 1) / len(thinking_phases) * 25), concurrent_execution_id=self.execution_id, thread_id=self.thread_id)
            await asyncio.sleep(0.05)
            if i == 1:
                self.concurrent_safety_checks.append(await self._perform_isolation_check('mid_thinking', state))
        concurrent_tools = [(f'concurrent_analyzer_{self.execution_id}', {'isolation': True, 'thread_safe': True}), (f'data_processor_{self.execution_id}', {'concurrent_safe': True, 'user_isolated': True})]
        tool_results = []
        for tool_name, tool_params in concurrent_tools:
            self.concurrent_safety_checks.append(await self._perform_isolation_check(f'pre_{tool_name}', state))
            await self.websocket_bridge.notify_tool_executing(run_id, self.name, tool_name, {**tool_params, 'execution_id': self.execution_id, 'concurrent_context': True})
            tool_result = await self._execute_concurrent_safe_tool(tool_name, tool_params, state)
            tool_results.append(tool_result)
            await self.websocket_bridge.notify_tool_completed(run_id, self.name, tool_name, {'result': tool_result, 'concurrent_execution_safe': True, 'isolation_verified': tool_result.get('isolation_verified', False), 'execution_id': self.execution_id})
            self.concurrent_safety_checks.append(await self._perform_isolation_check(f'post_{tool_name}', state))
        await self.websocket_bridge.notify_agent_thinking(run_id, self.name, f'Finalizing concurrent execution results with isolation validation for {self.name}', step_number=len(thinking_phases) + 1, progress_percentage=95, concurrent_execution_complete=True)
        final_result = {'success': True, 'agent_name': self.name, 'concurrent_execution': {'execution_id': self.execution_id, 'thread_id': self.thread_id, 'isolation_markers': len(self.isolation_markers), 'safety_checks_passed': all((check['isolation_safe'] for check in self.concurrent_safety_checks)), 'concurrent_tools_executed': len(tool_results), 'data_isolation_verified': self._validate_final_isolation(state)}, 'business_analysis': self._generate_concurrent_business_results(state, tool_results), 'isolation_validation': {'user_data_protected': True, 'cross_execution_contamination': 'none_detected', 'concurrent_safety_score': self._calculate_concurrent_safety_score(), 'isolation_checks': len(self.concurrent_safety_checks)}, 'execution_metadata': {'total_isolation_markers': len(self.isolation_markers), 'total_safety_checks': len(self.concurrent_safety_checks), 'thread_safe_execution': True}, 'timestamp': datetime.now(timezone.utc).isoformat()}
        self.concurrent_safety_checks.append(await self._perform_isolation_check('completion', state))
        await self.websocket_bridge.notify_agent_completed(run_id, self.name, final_result, execution_time_ms=200, concurrent_execution_safe=True, isolation_score=self._calculate_concurrent_safety_score())
        return final_result

    def _generate_isolation_context(self, state: DeepAgentState) -> Dict[str, Any]:
        """Generate isolation context for concurrent execution."""
        return {'user_id': getattr(state, 'user_id', None), 'execution_boundary': self.execution_id, 'thread_context': self.thread_id, 'isolation_level': 'strict', 'data_scope': 'user_isolated', 'concurrent_safe': True}

    async def _perform_isolation_check(self, checkpoint: str, state: DeepAgentState) -> Dict[str, Any]:
        """Perform isolation safety check during concurrent execution."""
        user_id = getattr(state, 'user_id', None)
        check_result = {'checkpoint': checkpoint, 'timestamp': datetime.now(timezone.utc), 'user_id': user_id, 'execution_id': self.execution_id, 'thread_id': self.thread_id, 'isolation_safe': True, 'data_contamination': 'none_detected', 'concurrent_conflicts': 'none', 'memory_isolation': 'verified'}
        return check_result

    async def _execute_concurrent_safe_tool(self, tool_name: str, tool_params: Dict, state: DeepAgentState) -> Dict[str, Any]:
        """Execute tool with concurrent safety guarantees."""
        user_id = getattr(state, 'user_id', None)
        await asyncio.sleep(0.08)
        if 'analyzer' in tool_name:
            return {'analysis_complete': True, 'isolation_verified': True, 'concurrent_execution_id': self.execution_id, 'user_scope': user_id, 'thread_safe': True, 'data_isolation': 'maintained', 'insights': [f'Concurrent analysis for user {user_id[:8]}', f'Isolated processing in execution {self.execution_id[:8]}', 'Thread-safe data access validated'], 'concurrent_metrics': {'processing_isolated': True, 'memory_protected': True, 'state_separated': True}}
        else:
            return {'processing_complete': True, 'isolation_verified': True, 'concurrent_execution_id': self.execution_id, 'user_data_protected': True, 'thread_isolation': 'verified', 'results': [f'User-specific processing for {user_id[:8]}', f'Concurrent-safe execution {self.execution_id[:8]}', 'Data isolation boundaries maintained'], 'safety_validation': {'cross_user_contamination': 'prevented', 'concurrent_access_safe': True, 'isolation_boundaries': 'intact'}}

    def _validate_final_isolation(self, state: DeepAgentState) -> bool:
        """Validate final isolation state."""
        all_checks_passed = all((check['isolation_safe'] for check in self.concurrent_safety_checks))
        user_id = getattr(state, 'user_id', None)
        consistent_user = all((marker['isolation_context']['user_id'] == user_id for marker in self.isolation_markers))
        return all_checks_passed and consistent_user

    def _calculate_concurrent_safety_score(self) -> float:
        """Calculate concurrent safety score."""
        if not self.concurrent_safety_checks:
            return 0.0
        safe_checks = sum((1 for check in self.concurrent_safety_checks if check['isolation_safe']))
        return safe_checks / len(self.concurrent_safety_checks) * 100

    def _generate_concurrent_business_results(self, state: DeepAgentState, tool_results: List[Dict]) -> Dict[str, Any]:
        """Generate business results for concurrent execution."""
        user_id = getattr(state, 'user_id', None)
        return {'user_specific_analysis': {'user_id': user_id, 'isolation_maintained': True, 'concurrent_processing': 'successful', 'business_value': f'Isolated optimization analysis for user {user_id[:8]}'}, 'concurrent_execution_benefits': {'parallel_processing': True, 'resource_efficiency': 'optimized', 'user_experience': 'non_blocking', 'scalability_demonstrated': True}, 'tool_results_summary': {'total_tools': len(tool_results), 'isolation_verified': all((tr.get('isolation_verified', False) for tr in tool_results)), 'concurrent_safe': all((tr.get('thread_safe', False) for tr in tool_results))}}

class ConcurrentExecutionOrchestrator:
    """Orchestrates and monitors concurrent agent execution."""

    def __init__(self):
        self.execution_sessions = {}
        self.websocket_bridges = {}
        self.isolation_violations = []
        self.execution_timeline = []

    async def create_isolated_execution_session(self, user_context: UserExecutionContext, agent_name: str) -> Tuple[str, Any, Any]:
        """Create isolated execution session for concurrent testing."""
        session_id = f'{user_context.user_id}_{agent_name}_{uuid.uuid4().hex[:8]}'
        bridge = AsyncMock(spec=AgentWebSocketBridge)
        bridge.user_context = user_context
        bridge.session_id = session_id
        bridge.events = []

        async def track_concurrent_event(event_type: str, run_id: str, agent_name: str, data: Any=None, **kwargs):
            """Track WebSocket event with concurrent execution context."""
            event = {'event_type': event_type, 'run_id': run_id, 'agent_name': agent_name, 'user_id': user_context.user_id, 'session_id': session_id, 'data': data, 'timestamp': datetime.now(timezone.utc), 'thread_id': threading.current_thread().ident, 'kwargs': kwargs}
            bridge.events.append(event)
            self.execution_timeline.append(event)
            await self._check_isolation_violation(event)
            return True
        bridge.notify_agent_started = AsyncMock(side_effect=lambda run_id, agent_name, context=None: track_concurrent_event('agent_started', run_id, agent_name, context))
        bridge.notify_agent_thinking = AsyncMock(side_effect=lambda run_id, agent_name, thinking, **kwargs: track_concurrent_event('agent_thinking', run_id, agent_name, {'reasoning': thinking}, **kwargs))
        bridge.notify_tool_executing = AsyncMock(side_effect=lambda run_id, agent_name, tool_name, params: track_concurrent_event('tool_executing', run_id, agent_name, {'tool_name': tool_name, 'parameters': params}))
        bridge.notify_tool_completed = AsyncMock(side_effect=lambda run_id, agent_name, tool_name, result: track_concurrent_event('tool_completed', run_id, agent_name, {'tool_name': tool_name, **result}))
        bridge.notify_agent_completed = AsyncMock(side_effect=lambda run_id, agent_name, result, **kwargs: track_concurrent_event('agent_completed', run_id, agent_name, result, **kwargs))
        self.websocket_bridges[session_id] = bridge
        self.execution_sessions[session_id] = {'user_context': user_context, 'agent_name': agent_name, 'bridge': bridge, 'start_time': datetime.now(timezone.utc), 'status': 'initialized'}
        return (session_id, bridge, user_context)

    async def _check_isolation_violation(self, event: Dict[str, Any]):
        """Check for isolation violations in concurrent execution."""
        event_user = event['user_id']
        event_data_str = str(event['data'])
        for other_session_id, session_info in self.execution_sessions.items():
            if session_info['user_context'].user_id != event_user:
                other_user_id = session_info['user_context'].user_id
                if other_user_id in event_data_str:
                    self.isolation_violations.append({'violation_type': 'cross_user_data_contamination', 'violating_event': event['event_type'], 'violating_user': event_user, 'contaminated_with': other_user_id, 'session_id': event['session_id'], 'timestamp': event['timestamp']})

    def analyze_concurrent_execution(self) -> Dict[str, Any]:
        """Analyze concurrent execution results."""
        if not self.execution_timeline:
            return {'error': 'No execution data to analyze'}
        events_by_user = {}
        for event in self.execution_timeline:
            user_id = event['user_id']
            if user_id not in events_by_user:
                events_by_user[user_id] = []
            events_by_user[user_id].append(event)
        total_sessions = len(self.execution_sessions)
        concurrent_users = len(events_by_user)
        total_events = len(self.execution_timeline)
        isolation_violations = len(self.isolation_violations)
        if len(self.execution_timeline) > 1:
            start_time = min((event['timestamp'] for event in self.execution_timeline))
            end_time = max((event['timestamp'] for event in self.execution_timeline))
            total_duration = (end_time - start_time).total_seconds()
        else:
            total_duration = 0
        return {'concurrent_execution_metrics': {'total_sessions': total_sessions, 'concurrent_users': concurrent_users, 'total_events': total_events, 'events_per_user': {user: len(events) for user, events in events_by_user.items()}, 'total_duration_seconds': total_duration, 'events_per_second': total_events / total_duration if total_duration > 0 else 0}, 'isolation_analysis': {'isolation_violations': isolation_violations, 'isolation_success_rate': (total_events - isolation_violations) / total_events * 100 if total_events > 0 else 100, 'violations_details': self.isolation_violations}, 'performance_analysis': {'concurrent_throughput': total_sessions / total_duration if total_duration > 0 else 0, 'average_events_per_session': total_events / total_sessions if total_sessions > 0 else 0, 'concurrent_efficiency': 'excellent' if isolation_violations == 0 else 'needs_improvement'}}

    def validate_perfect_isolation(self) -> Dict[str, Any]:
        """Validate perfect isolation between concurrent executions."""
        validation_result = {'perfect_isolation': len(self.isolation_violations) == 0, 'isolation_score': 100.0 if len(self.isolation_violations) == 0 else 0.0, 'violations_found': len(self.isolation_violations), 'violation_details': self.isolation_violations, 'concurrent_safety': 'verified' if len(self.isolation_violations) == 0 else 'compromised'}
        user_events = {}
        for event in self.execution_timeline:
            user_id = event['user_id']
            if user_id not in user_events:
                user_events[user_id] = []
            user_events[user_id].append(event)
        session_mixing_violations = 0
        for user_id, events in user_events.items():
            user_sessions = set((event['session_id'] for event in events))
            if len(user_sessions) > 1:
                pass
        validation_result['session_integrity'] = {'users_with_events': len(user_events), 'session_mixing_violations': session_mixing_violations, 'clean_separation': session_mixing_violations == 0}
        return validation_result

class WebSocketMultiAgentConcurrentExecutionIsolationTests(BaseIntegrationTest):
    """Integration test for multi-agent concurrent execution with WebSocket isolation."""

    def setup_method(self):
        """Set up test environment for concurrent execution testing."""
        super().setup_method()
        self.env = get_env()
        self.env.set('TESTING', '1', source='integration_test')
        self.orchestrator = ConcurrentExecutionOrchestrator()

    @pytest.fixture
    async def mock_llm_manager(self):
        """Create mock LLM manager."""
        llm_manager = AsyncMock(spec=LLMManager)
        llm_manager.health_check = AsyncMock(return_value={'status': 'healthy'})
        llm_manager.initialize = AsyncMock()
        return llm_manager

    @pytest.fixture
    async def concurrent_user_contexts(self):
        """Create multiple user contexts for concurrent execution testing."""
        return [UserExecutionContext(user_id=f'concurrent_user_alpha_{uuid.uuid4().hex[:8]}', thread_id=f'alpha_thread_{uuid.uuid4().hex[:8]}', run_id=f'alpha_run_{uuid.uuid4().hex[:8]}', request_id=f'alpha_req_{uuid.uuid4().hex[:8]}', metadata={'user_type': 'enterprise_alpha', 'optimization_focus': 'cost_reduction', 'concurrent_execution': True, 'isolation_required': True}), UserExecutionContext(user_id=f'concurrent_user_beta_{uuid.uuid4().hex[:8]}', thread_id=f'beta_thread_{uuid.uuid4().hex[:8]}', run_id=f'beta_run_{uuid.uuid4().hex[:8]}', request_id=f'beta_req_{uuid.uuid4().hex[:8]}', metadata={'user_type': 'enterprise_beta', 'optimization_focus': 'performance_improvement', 'concurrent_execution': True, 'isolation_required': True}), UserExecutionContext(user_id=f'concurrent_user_gamma_{uuid.uuid4().hex[:8]}', thread_id=f'gamma_thread_{uuid.uuid4().hex[:8]}', run_id=f'gamma_run_{uuid.uuid4().hex[:8]}', request_id=f'gamma_req_{uuid.uuid4().hex[:8]}', metadata={'user_type': 'enterprise_gamma', 'optimization_focus': 'resource_optimization', 'concurrent_execution': True, 'isolation_required': True})]

    @pytest.fixture
    async def concurrent_agent_registry(self, mock_llm_manager):
        """Create registry with concurrent execution agents."""
        agents = {}
        for i, agent_type in enumerate(['optimizer_alpha', 'optimizer_beta', 'optimizer_gamma']):
            execution_id = f'exec_{agent_type}_{uuid.uuid4().hex[:8]}'
            agents[agent_type] = ConcurrentExecutionAgent(agent_type, mock_llm_manager, execution_id)
        registry = MagicMock(spec=AgentRegistry)
        registry.get = lambda name: agents.get(name)
        registry.get_async = AsyncMock(side_effect=lambda name, context=None: agents.get(name))
        registry.list_keys = lambda: list(agents.keys())
        return (registry, agents)

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_multi_agent_concurrent_execution_with_perfect_isolation(self, real_services_fixture, concurrent_user_contexts, concurrent_agent_registry, mock_llm_manager):
        """CRITICAL: Test multiple agents executing concurrently with perfect user isolation."""
        registry, agents = concurrent_agent_registry
        user_contexts = concurrent_user_contexts
        execution_sessions = []
        execution_engines = []
        for i, user_context in enumerate(user_contexts):
            agent_name = f"optimizer_{['alpha', 'beta', 'gamma'][i]}"
            session_id, websocket_bridge, context = await self.orchestrator.create_isolated_execution_session(user_context, agent_name)
            execution_engine = ExecutionEngine._init_from_factory(registry=registry, websocket_bridge=websocket_bridge, user_context=context)
            execution_sessions.append((session_id, agent_name, context))
            execution_engines.append(execution_engine)

        async def execute_concurrent_agent(engine, user_context, agent_name, user_index):
            """Execute agent concurrently with isolation monitoring."""
            exec_context = AgentExecutionContext(user_id=user_context.user_id, thread_id=user_context.thread_id, run_id=user_context.run_id, request_id=user_context.request_id, agent_name=agent_name, step=PipelineStep.PROCESSING, execution_timestamp=datetime.now(timezone.utc), pipeline_step_num=1)
            agent_state = DeepAgentState(user_request={'concurrent_optimization': True, 'user_specific': user_context.metadata['optimization_focus'], 'isolation_test': f'user_{user_index}'}, user_id=user_context.user_id, chat_thread_id=user_context.thread_id, run_id=user_context.run_id, agent_input={'concurrent_execution': True, 'user_index': user_index, 'isolation_validation': True})
            return await engine.execute_agent(exec_context, user_context)
        start_time = time.time()
        concurrent_tasks = []
        for i, ((session_id, agent_name, context), engine) in enumerate(zip(execution_sessions, execution_engines)):
            task = execute_concurrent_agent(engine, context, agent_name, i)
            concurrent_tasks.append(task)
        results = await asyncio.gather(*concurrent_tasks, return_exceptions=True)
        execution_time = time.time() - start_time
        assert len(results) == len(user_contexts), f'Expected {len(user_contexts)} results, got {len(results)}'
        for i, result in enumerate(results):
            assert not isinstance(result, Exception), f'Execution {i} failed: {result}'
            assert result is not None, f'Result {i} is None'
            assert result.success is True, f"Execution {i} unsuccessful: {getattr(result, 'error', 'Unknown')}"
        isolation_validation = self.orchestrator.validate_perfect_isolation()
        assert isolation_validation['perfect_isolation'] is True, f"Isolation violations detected: {isolation_validation['violation_details']}"
        assert isolation_validation['isolation_score'] == 100.0, f"Isolation score must be perfect: {isolation_validation['isolation_score']}"
        assert isolation_validation['violations_found'] == 0, f"Found {isolation_validation['violations_found']} isolation violations"
        assert isolation_validation['concurrent_safety'] == 'verified', f"Concurrent safety compromised: {isolation_validation['concurrent_safety']}"
        execution_analysis = self.orchestrator.analyze_concurrent_execution()
        concurrent_metrics = execution_analysis['concurrent_execution_metrics']
        assert concurrent_metrics['total_sessions'] == len(user_contexts), f"Expected {len(user_contexts)} sessions, got {concurrent_metrics['total_sessions']}"
        assert concurrent_metrics['concurrent_users'] == len(user_contexts), f"Expected {len(user_contexts)} concurrent users, got {concurrent_metrics['concurrent_users']}"
        assert concurrent_metrics['total_events'] >= len(user_contexts) * 5, f"Expected minimum {len(user_contexts) * 5} events, got {concurrent_metrics['total_events']}"
        isolation_analysis = execution_analysis['isolation_analysis']
        assert isolation_analysis['isolation_violations'] == 0, f"Isolation violations: {isolation_analysis['isolation_violations']}"
        assert isolation_analysis['isolation_success_rate'] == 100.0, f"Isolation success rate: {isolation_analysis['isolation_success_rate']}%"
        for i, result in enumerate(results):
            concurrent_execution = result.data.get('concurrent_execution', {})
            assert concurrent_execution.get('safety_checks_passed') is True, f'Agent {i} safety checks failed'
            assert concurrent_execution.get('concurrent_tools_executed', 0) >= 2, f'Agent {i} insufficient tool execution'
            assert concurrent_execution.get('data_isolation_verified') is True, f'Agent {i} data isolation failed'
            isolation_validation = result.data.get('isolation_validation', {})
            assert isolation_validation.get('user_data_protected') is True, f'Agent {i} user data not protected'
            assert isolation_validation.get('cross_execution_contamination') == 'none_detected', f'Agent {i} cross-execution contamination detected'
            assert isolation_validation.get('concurrent_safety_score', 0) >= 90, f"Agent {i} concurrent safety score too low: {isolation_validation.get('concurrent_safety_score', 0)}"
        performance_analysis = execution_analysis['performance_analysis']
        assert execution_time < 10.0, f'Concurrent execution too slow: {execution_time:.2f}s'
        assert performance_analysis['concurrent_efficiency'] == 'excellent', f"Concurrent efficiency: {performance_analysis['concurrent_efficiency']}"
        assert performance_analysis['concurrent_throughput'] > 0, f'No concurrent throughput measured'
        if real_services_fixture['database_available']:
            db_session = real_services_fixture['db']
            if db_session:
                for i, ((session_id, agent_name, context), result) in enumerate(zip(execution_sessions, results)):
                    concurrent_record = {'session_id': session_id, 'user_id': context.user_id, 'agent_name': agent_name, 'success': result.success, 'isolation_verified': True, 'concurrent_safe': True, 'execution_index': i}
                    assert concurrent_record['success'] is True
                    assert concurrent_record['isolation_verified'] is True
                    assert concurrent_record['concurrent_safe'] is True
        self.logger.info(f" PASS:  Multi-agent concurrent execution test PASSED - {len(user_contexts)} users, {concurrent_metrics['total_events']} events, {isolation_analysis['isolation_success_rate']}% isolation, {execution_time:.3f}s execution time")

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_concurrent_agent_execution_scalability_stress_testing(self, concurrent_agent_registry, mock_llm_manager):
        """Test concurrent agent execution under higher load stress testing."""
        registry, agents = concurrent_agent_registry
        concurrent_users = 8
        user_contexts = []
        for i in range(concurrent_users):
            context = UserExecutionContext(user_id=f'stress_user_{i}_{uuid.uuid4().hex[:8]}', thread_id=f'stress_thread_{i}_{uuid.uuid4().hex[:8]}', run_id=f'stress_run_{i}_{uuid.uuid4().hex[:8]}', request_id=f'stress_req_{i}_{uuid.uuid4().hex[:8]}', metadata={'stress_test': True, 'user_index': i, 'concurrent_load': concurrent_users, 'isolation_critical': True})
            user_contexts.append(context)
        execution_tasks = []
        for i, user_context in enumerate(user_contexts):
            agent_name = f"optimizer_{['alpha', 'beta', 'gamma'][i % 3]}"
            session_id, websocket_bridge, context = await self.orchestrator.create_isolated_execution_session(user_context, agent_name)
            execution_engine = ExecutionEngine._init_from_factory(registry=registry, websocket_bridge=websocket_bridge, user_context=context)
            exec_context = AgentExecutionContext(user_id=context.user_id, thread_id=context.thread_id, run_id=context.run_id, request_id=context.request_id, agent_name=agent_name, step=PipelineStep.PROCESSING, execution_timestamp=datetime.now(timezone.utc), pipeline_step_num=1)
            agent_state = DeepAgentState(user_request={'stress_test': True, 'user_load_index': i}, user_id=context.user_id, chat_thread_id=context.thread_id, run_id=context.run_id, agent_input={'concurrent_stress': True})
            task = execution_engine.execute_agent(exec_context, context)
            execution_tasks.append(task)
        stress_start_time = time.time()
        stress_results = await asyncio.gather(*execution_tasks, return_exceptions=True)
        stress_execution_time = time.time() - stress_start_time
        successful_executions = sum((1 for result in stress_results if not isinstance(result, Exception) and getattr(result, 'success', False)))
        assert successful_executions >= concurrent_users * 0.8, f'Stress test success rate too low: {successful_executions}/{concurrent_users}'
        stress_isolation = self.orchestrator.validate_perfect_isolation()
        assert stress_isolation['isolation_score'] >= 95.0, f"Isolation degraded under stress: {stress_isolation['isolation_score']}%"
        stress_analysis = self.orchestrator.analyze_concurrent_execution()
        stress_performance = stress_analysis['performance_analysis']
        assert stress_execution_time < 15.0, f'Stress test execution too slow: {stress_execution_time:.2f}s'
        assert stress_performance['concurrent_throughput'] > 0, 'No throughput measured under stress'
        assert stress_performance['concurrent_efficiency'] in ['excellent', 'good'], f"Performance degraded under stress: {stress_performance['concurrent_efficiency']}"
        self.logger.info(f" PASS:  Concurrent execution stress test PASSED - {concurrent_users} users, {successful_executions} successful, {stress_isolation['isolation_score']:.1f}% isolation, {stress_execution_time:.3f}s total time")

    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_concurrent_websocket_event_ordering_and_consistency(self, concurrent_user_contexts, concurrent_agent_registry):
        """Test WebSocket event ordering and consistency during concurrent execution."""
        registry, agents = concurrent_agent_registry
        user_contexts = concurrent_user_contexts[:2]
        user_event_sequences = {}
        for user_context in user_contexts:
            agent_name = 'optimizer_alpha'
            session_id, websocket_bridge, context = await self.orchestrator.create_isolated_execution_session(user_context, agent_name)
            user_event_sequences[user_context.user_id] = []
            execution_engine = ExecutionEngine._init_from_factory(registry=registry, websocket_bridge=websocket_bridge, user_context=context)
            exec_context = AgentExecutionContext(user_id=context.user_id, thread_id=context.thread_id, run_id=context.run_id, request_id=context.request_id, agent_name=agent_name, step=PipelineStep.PROCESSING, execution_timestamp=datetime.now(timezone.utc), pipeline_step_num=1)
            agent_state = DeepAgentState(user_request={'event_ordering_test': True}, user_id=context.user_id, chat_thread_id=context.thread_id, run_id=context.run_id, agent_input={'sequence_validation': True})
            await execution_engine.execute_agent(exec_context, context)
        for event in self.orchestrator.execution_timeline:
            user_id = event['user_id']
            if user_id in user_event_sequences:
                user_event_sequences[user_id].append(event['event_type'])
        for user_id, event_sequence in user_event_sequences.items():
            assert len(event_sequence) >= 5, f'User {user_id} missing events: {len(event_sequence)}'
            assert event_sequence[0] == 'agent_started', f'User {user_id} wrong first event: {event_sequence[0]}'
            assert 'agent_completed' in event_sequence, f'User {user_id} missing completion event'
            thinking_events = [e for e in event_sequence if e == 'agent_thinking']
            assert len(thinking_events) >= 2, f'User {user_id} insufficient thinking events: {len(thinking_events)}'
            tool_executing = [e for e in event_sequence if e == 'tool_executing']
            tool_completed = [e for e in event_sequence if e == 'tool_completed']
            assert len(tool_executing) == len(tool_completed), f'User {user_id} unmatched tool events: {len(tool_executing)} executing, {len(tool_completed)} completed'
        timeline_analysis = self.orchestrator.analyze_concurrent_execution()
        isolation_analysis = timeline_analysis['isolation_analysis']
        assert isolation_analysis['isolation_violations'] == 0, f"Event isolation violations: {isolation_analysis['isolation_violations']}"
        self.logger.info(f" PASS:  WebSocket event ordering and consistency test PASSED - {len(user_event_sequences)} users, Total events: {sum((len(seq) for seq in user_event_sequences.values()))}, Isolation: {isolation_analysis['isolation_success_rate']}%")
if __name__ == '__main__':
    'MIGRATED: Use SSOT unified test runner'
    print('MIGRATION NOTICE: Please use SSOT unified test runner')
    print('Command: python tests/unified_test_runner.py --category <category>')