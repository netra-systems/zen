"""
Tool Dispatcher Integration Tests with Real Business Scenarios

These tests validate tool dispatcher execution patterns, business value delivery,
and integration with agent workflows without external API dependencies.

Business Value Focus:
- Tool execution with real business scenarios and validation
- Tool result processing and business impact measurement
- Multiple tool coordination within single agent execution
- Tool error handling and fallback strategies
- Tool execution timing and performance metrics

CRITICAL: Tests real tool dispatcher logic but mocks external API calls only.
"""

import asyncio
import time
import pytest
from typing import Dict, Any, List, Optional
from unittest.mock import AsyncMock, patch, MagicMock

from netra_backend.app.agents.supervisor.user_execution_context import UserExecutionContext
from netra_backend.app.core.tools.unified_tool_dispatcher import UnifiedToolDispatcherFactory
from netra_backend.app.core.registry.universal_registry import ToolRegistry
from netra_backend.app.schemas.tool import BaseTool
from netra_backend.tests.integration.agent_execution.base_agent_execution_test import BaseAgentExecutionTest


class MockBusinessTool(BaseTool):
    """Mock business tool that simulates real business logic execution."""
    
    def __init__(self, tool_name: str, execution_time: float = 0.5, success_rate: float = 1.0):
        super().__init__()
        self.name = tool_name
        self.description = f"Mock {tool_name} for business scenario testing"
        self.execution_time = execution_time
        self.success_rate = success_rate
        self.call_count = 0
        
    async def execute(self, context: UserExecutionContext, **kwargs) -> Dict[str, Any]:
        """Execute mock business logic with realistic timing and results."""
        self.call_count += 1
        
        # Simulate realistic execution time
        await asyncio.sleep(self.execution_time)
        
        # Simulate occasional failures based on success rate
        if self.call_count * self.success_rate < 1.0:
            raise RuntimeError(f"{self.name} simulated failure (call {self.call_count})")
            
        # Return business-relevant result
        return {
            "tool_name": self.name,
            "status": "completed",
            "execution_time": self.execution_time,
            "call_count": self.call_count,
            "business_value": f"{self.name} delivered expected business outcome",
            "metrics": {
                "processing_items": 100 + self.call_count * 50,
                "cost_optimization": f"{5 + self.call_count * 2}%",
                "performance_improvement": f"{10 + self.call_count * 3}%"
            },
            "recommendations": [
                f"Recommendation 1 from {self.name}",
                f"Recommendation 2 from {self.name}",
                f"Action item generated by {self.name}"
            ]
        }
        
    def get_business_impact(self) -> Dict[str, Any]:
        """Return business impact metrics for validation."""
        return {
            "executions": self.call_count,
            "avg_execution_time": self.execution_time,
            "success_rate": self.success_rate,
            "business_value_delivered": self.call_count > 0
        }


class TestToolDispatcherIntegration(BaseAgentExecutionTest):
    """Test tool dispatcher integration with real business scenarios."""
    
    async def create_tool_dispatcher_with_business_tools(self, 
                                                        context: UserExecutionContext,
                                                        tool_configs: List[Dict[str, Any]]) -> tuple:
        """Create tool dispatcher with mock business tools.
        
        Args:
            context: User execution context for isolation
            tool_configs: List of tool configuration dictionaries
            
        Returns:
            Tuple of (dispatcher, tools_dict, registry)
        """
        # Create business tools based on configs
        tools = []
        tools_dict = {}
        
        for config in tool_configs:
            tool = MockBusinessTool(
                tool_name=config['name'],
                execution_time=config.get('execution_time', 0.5),
                success_rate=config.get('success_rate', 1.0)
            )
            tools.append(tool)
            tools_dict[config['name']] = tool
            
        # Create dispatcher with real isolation patterns
        dispatcher = await UnifiedToolDispatcherFactory.create_request_scoped(
            user_context=context,
            tools=tools,
            websocket_manager=self.mock_websocket_manager,
            permission_service=None
        )
        
        # Get registry from dispatcher
        registry = dispatcher.registry
        
        return dispatcher, tools_dict, registry

    @pytest.mark.asyncio
    async def test_tool_execution_with_real_business_scenarios(self):
        """Test tool execution with realistic business scenarios and validation.
        
        Validates:
        - Tools execute with proper UserExecutionContext isolation
        - Business results are captured and validated
        - Performance metrics are tracked
        - WebSocket notifications are sent during execution
        """
        # Create context for cost optimization scenario
        context = self.create_user_execution_context(
            user_request="Optimize AI infrastructure costs and analyze usage patterns",
            additional_metadata={
                "scenario": "cost_optimization",
                "data_sources": ["usage_metrics", "cost_data"],
                "optimization_goals": ["reduce_costs", "improve_performance"]
            }
        )
        
        # Create business tools for the scenario
        tool_configs = [
            {
                "name": "cost_analyzer",
                "execution_time": 0.8,
                "success_rate": 1.0
            },
            {
                "name": "usage_pattern_detector", 
                "execution_time": 1.2,
                "success_rate": 1.0
            },
            {
                "name": "optimization_recommender",
                "execution_time": 0.6,
                "success_rate": 1.0
            }
        ]
        
        dispatcher, tools_dict, registry = await self.create_tool_dispatcher_with_business_tools(
            context, tool_configs
        )
        
        # Execute tools in realistic business sequence
        execution_results = {}
        start_time = time.time()
        
        # Step 1: Analyze costs
        cost_result = await dispatcher.execute_tool(
            tool_name="cost_analyzer",
            context=context,
            parameters={"data_sources": context.metadata["data_sources"]}
        )
        execution_results["cost_analysis"] = cost_result
        
        # Step 2: Detect usage patterns  
        usage_result = await dispatcher.execute_tool(
            tool_name="usage_pattern_detector",
            context=context,
            parameters={"optimization_goals": context.metadata["optimization_goals"]}
        )
        execution_results["usage_patterns"] = usage_result
        
        # Step 3: Generate recommendations
        recommendations_result = await dispatcher.execute_tool(
            tool_name="optimization_recommender", 
            context=context,
            parameters={
                "cost_analysis": cost_result,
                "usage_patterns": usage_result
            }
        )
        execution_results["recommendations"] = recommendations_result
        
        total_execution_time = time.time() - start_time
        
        # Validate business execution results
        assert len(execution_results) == 3, "All three business tools should execute"
        
        for tool_name, result in execution_results.items():
            assert result["status"] == "completed", f"{tool_name} should complete successfully"
            assert "business_value" in result, f"{tool_name} should deliver business value"
            assert "metrics" in result, f"{tool_name} should provide business metrics"
            assert "recommendations" in result, f"{tool_name} should generate recommendations"
            
            # Validate business impact metrics
            metrics = result["metrics"]
            assert "cost_optimization" in metrics
            assert "performance_improvement" in metrics
            
        # Validate WebSocket tool execution events  
        tool_events = self.mock_websocket_manager.get_events_by_type('tool_executing')
        assert len(tool_events) >= 3, "Should emit tool execution events"
        
        # Validate performance
        assert total_execution_time < 5.0, "Business scenario should complete within reasonable time"
        
        # Validate business value indicators
        total_cost_savings = 0
        total_performance_improvement = 0
        
        for result in execution_results.values():
            metrics = result["metrics"]
            cost_savings = float(metrics["cost_optimization"].replace('%', ''))
            performance_improvement = float(metrics["performance_improvement"].replace('%', ''))
            
            total_cost_savings += cost_savings
            total_performance_improvement += performance_improvement
            
        assert total_cost_savings > 0, "Should demonstrate cost optimization value"
        assert total_performance_improvement > 0, "Should demonstrate performance improvement value"

    @pytest.mark.asyncio
    async def test_tool_result_processing_and_validation(self):
        """Test tool result processing and business impact validation.
        
        Validates:
        - Tool results are properly structured and contain business data
        - Result validation catches business-critical issues
        - Results can be aggregated for business reporting
        - Error states are handled gracefully with business context
        """
        context = self.create_user_execution_context(
            user_request="Process data analysis results and validate business outcomes",
            additional_metadata={"analysis_type": "comprehensive", "validation_required": True}
        )
        
        # Create tools with different result types
        tool_configs = [
            {"name": "data_processor", "execution_time": 0.4, "success_rate": 1.0},
            {"name": "result_validator", "execution_time": 0.3, "success_rate": 1.0},
            {"name": "business_reporter", "execution_time": 0.5, "success_rate": 1.0}
        ]
        
        dispatcher, tools_dict, registry = await self.create_tool_dispatcher_with_business_tools(
            context, tool_configs
        )
        
        # Execute tool sequence with result processing
        results_chain = []
        
        # Step 1: Process initial data
        data_result = await dispatcher.execute_tool(
            tool_name="data_processor",
            context=context,
            parameters={"input_data": "sample_business_data"}
        )
        results_chain.append(("data_processing", data_result))
        
        # Step 2: Validate processing results
        validation_result = await dispatcher.execute_tool(
            tool_name="result_validator",
            context=context,
            parameters={"data_to_validate": data_result["metrics"]}
        )
        results_chain.append(("validation", validation_result))
        
        # Step 3: Generate business report
        report_result = await dispatcher.execute_tool(
            tool_name="business_reporter",
            context=context,
            parameters={
                "processed_data": data_result,
                "validation_results": validation_result
            }
        )
        results_chain.append(("reporting", report_result))
        
        # Validate result processing chain
        assert len(results_chain) == 3, "Complete result processing chain should execute"
        
        # Validate each stage has proper business structure
        for stage_name, result in results_chain:
            # Standard business result structure
            assert "status" in result and result["status"] == "completed"
            assert "business_value" in result
            assert "metrics" in result
            assert "recommendations" in result
            
            # Business impact validation
            metrics = result["metrics"]
            assert "processing_items" in metrics
            assert isinstance(metrics["processing_items"], int)
            assert metrics["processing_items"] > 0
            
        # Validate result aggregation capabilities
        aggregated_metrics = {}
        total_recommendations = []
        
        for stage_name, result in results_chain:
            # Aggregate numerical metrics
            for metric_name, metric_value in result["metrics"].items():
                if isinstance(metric_value, (int, float)):
                    aggregated_metrics[metric_name] = aggregated_metrics.get(metric_name, 0) + metric_value
                elif isinstance(metric_value, str) and '%' in metric_value:
                    # Handle percentage metrics
                    percentage_value = float(metric_value.replace('%', ''))
                    aggregated_metrics[f"{metric_name}_total"] = aggregated_metrics.get(f"{metric_name}_total", 0) + percentage_value
                    
            # Aggregate recommendations
            total_recommendations.extend(result["recommendations"])
            
        # Validate aggregated business value
        assert len(aggregated_metrics) > 0, "Should aggregate numerical business metrics"
        assert len(total_recommendations) >= 9, "Should aggregate recommendations from all tools (3 each)"
        assert aggregated_metrics.get("processing_items", 0) > 300, "Should process significant amount of items"

    @pytest.mark.asyncio
    async def test_multiple_tool_coordination_within_agent(self):
        """Test coordination of multiple tools within a single agent execution.
        
        Validates:
        - Multiple tools can execute in sequence with shared context
        - Tool results feed into subsequent tool executions  
        - Context isolation maintained across tool executions
        - Performance of coordinated tool execution
        """
        context = self.create_user_execution_context(
            user_request="Execute coordinated multi-tool analysis workflow",
            additional_metadata={
                "workflow_type": "coordinated_analysis",
                "requires_sequencing": True,
                "performance_tracking": True
            }
        )
        
        # Create coordinated tool chain
        tool_configs = [
            {"name": "data_collector", "execution_time": 0.3, "success_rate": 1.0},
            {"name": "data_transformer", "execution_time": 0.4, "success_rate": 1.0},
            {"name": "pattern_analyzer", "execution_time": 0.6, "success_rate": 1.0},
            {"name": "insight_generator", "execution_time": 0.5, "success_rate": 1.0},
            {"name": "action_planner", "execution_time": 0.4, "success_rate": 1.0}
        ]
        
        dispatcher, tools_dict, registry = await self.create_tool_dispatcher_with_business_tools(
            context, tool_configs
        )
        
        # Execute coordinated workflow
        workflow_state = {"data": None, "transformations": [], "insights": [], "actions": []}
        execution_timeline = []
        
        start_time = time.time()
        
        # Tool 1: Collect data
        collect_result = await dispatcher.execute_tool(
            tool_name="data_collector",
            context=context,
            parameters={"collection_scope": "comprehensive"}
        )
        workflow_state["data"] = collect_result
        execution_timeline.append(("collect", time.time() - start_time))
        
        # Tool 2: Transform data (depends on collection)
        transform_result = await dispatcher.execute_tool(
            tool_name="data_transformer",
            context=context,
            parameters={"input_data": collect_result["metrics"]}
        )
        workflow_state["transformations"].append(transform_result)
        execution_timeline.append(("transform", time.time() - start_time))
        
        # Tool 3: Analyze patterns (depends on transformation)
        analyze_result = await dispatcher.execute_tool(
            tool_name="pattern_analyzer",
            context=context,
            parameters={"transformed_data": transform_result["metrics"]}
        )
        workflow_state["insights"].append(analyze_result)
        execution_timeline.append(("analyze", time.time() - start_time))
        
        # Tool 4: Generate insights (depends on analysis)
        insight_result = await dispatcher.execute_tool(
            tool_name="insight_generator",
            context=context,
            parameters={"analysis_results": analyze_result["metrics"]}
        )
        workflow_state["insights"].append(insight_result)
        execution_timeline.append(("insight", time.time() - start_time))
        
        # Tool 5: Plan actions (depends on insights)
        action_result = await dispatcher.execute_tool(
            tool_name="action_planner",
            context=context,
            parameters={"insights": [analyze_result, insight_result]}
        )
        workflow_state["actions"].append(action_result)
        execution_timeline.append(("action", time.time() - start_time))
        
        total_execution_time = time.time() - start_time
        
        # Validate coordinated execution
        assert len(execution_timeline) == 5, "All coordinated tools should execute"
        assert workflow_state["data"] is not None, "Data collection should succeed"
        assert len(workflow_state["transformations"]) == 1, "Data transformation should occur"
        assert len(workflow_state["insights"]) == 2, "Pattern analysis and insight generation should occur"
        assert len(workflow_state["actions"]) == 1, "Action planning should occur"
        
        # Validate execution sequencing (later tools should start after earlier ones)
        for i in range(1, len(execution_timeline)):
            prev_time = execution_timeline[i-1][1]
            curr_time = execution_timeline[i][1]
            assert curr_time > prev_time, f"Tool {execution_timeline[i][0]} should start after {execution_timeline[i-1][0]}"
            
        # Validate context isolation across tools
        unique_contexts = set()
        for tool_name, tool in tools_dict.items():
            if hasattr(tool, 'call_count') and tool.call_count > 0:
                unique_contexts.add(id(context))  # All should use same context instance
                
        assert len(unique_contexts) == 1, "All tools should share same context instance for coordination"
        
        # Validate performance
        assert total_execution_time < 4.0, "Coordinated execution should complete efficiently"

    @pytest.mark.asyncio
    async def test_tool_error_handling_and_fallback_strategies(self):
        """Test tool error handling and fallback strategies in business scenarios.
        
        Validates:
        - Graceful handling of tool execution failures
        - Fallback strategies maintain business value delivery
        - Error context preserves debugging information
        - Partial success scenarios handled correctly
        """
        context = self.create_user_execution_context(
            user_request="Execute workflow with potential tool failures and recovery",
            additional_metadata={
                "scenario": "error_recovery",
                "fallback_enabled": True,
                "partial_success_acceptable": True
            }
        )
        
        # Create tools with mixed success rates
        tool_configs = [
            {"name": "reliable_tool", "execution_time": 0.3, "success_rate": 1.0},
            {"name": "unreliable_tool", "execution_time": 0.5, "success_rate": 0.3},  # Will fail
            {"name": "fallback_tool", "execution_time": 0.4, "success_rate": 1.0},
            {"name": "critical_tool", "execution_time": 0.6, "success_rate": 1.0}
        ]
        
        dispatcher, tools_dict, registry = await self.create_tool_dispatcher_with_business_tools(
            context, tool_configs
        )
        
        # Execute workflow with error handling
        execution_results = {}
        error_log = []
        
        # Tool 1: Reliable execution (should succeed)
        try:
            reliable_result = await dispatcher.execute_tool(
                tool_name="reliable_tool",
                context=context,
                parameters={"operation": "baseline"}
            )
            execution_results["reliable"] = reliable_result
        except Exception as e:
            error_log.append(("reliable_tool", str(e)))
            
        # Tool 2: Unreliable execution (expected to fail)
        try:
            unreliable_result = await dispatcher.execute_tool(
                tool_name="unreliable_tool",
                context=context,
                parameters={"operation": "risky"}
            )
            execution_results["unreliable"] = unreliable_result
        except Exception as e:
            error_log.append(("unreliable_tool", str(e)))
            
            # Fallback strategy: Use fallback tool instead
            try:
                fallback_result = await dispatcher.execute_tool(
                    tool_name="fallback_tool",
                    context=context,
                    parameters={
                        "operation": "fallback_for_unreliable",
                        "original_failure": str(e)
                    }
                )
                execution_results["fallback"] = fallback_result
            except Exception as fallback_error:
                error_log.append(("fallback_tool", str(fallback_error)))
                
        # Tool 3: Critical tool (must succeed for business value)
        try:
            critical_result = await dispatcher.execute_tool(
                tool_name="critical_tool",
                context=context,
                parameters={
                    "operation": "critical_business_process",
                    "previous_results": execution_results
                }
            )
            execution_results["critical"] = critical_result
        except Exception as e:
            error_log.append(("critical_tool", str(e)))
            
        # Validate error handling results
        assert "reliable" in execution_results, "Reliable tool should succeed"
        assert execution_results["reliable"]["status"] == "completed"
        
        # Should have either unreliable success OR fallback success (not both)
        fallback_used = "fallback" in execution_results
        unreliable_succeeded = "unreliable" in execution_results
        
        # At least one of unreliable or fallback should have worked
        assert fallback_used or unreliable_succeeded, "Either unreliable tool should succeed or fallback should be used"
        
        if fallback_used:
            assert execution_results["fallback"]["status"] == "completed"
            # Validate fallback contains error context
            fallback_params = {"original_failure": "unreliable_tool simulated failure"}
            # Fallback should acknowledge it's replacing failed tool
            
        assert "critical" in execution_results, "Critical tool should succeed for business value"
        assert execution_results["critical"]["status"] == "completed"
        
        # Validate error logging
        expected_errors = ["unreliable_tool"] if not unreliable_succeeded else []
        actual_errors = [tool_name for tool_name, error in error_log]
        
        for expected_error in expected_errors:
            assert expected_error in actual_errors, f"Expected error from {expected_error} should be logged"
            
        # Validate partial success delivers business value
        total_successful_tools = len([r for r in execution_results.values() if r.get("status") == "completed"])
        assert total_successful_tools >= 2, "Should achieve partial success with business value"

    @pytest.mark.asyncio 
    async def test_tool_execution_timing_and_performance_metrics(self):
        """Test tool execution timing and performance metrics collection.
        
        Validates:
        - Tool execution timing is tracked accurately
        - Performance metrics are collected and validated
        - Resource usage patterns are reasonable
        - Concurrent tool execution performance
        """
        context = self.create_user_execution_context(
            user_request="Execute performance-focused tool testing scenario",
            additional_metadata={
                "performance_testing": True,
                "timing_critical": True,
                "resource_monitoring": True
            }
        )
        
        # Create tools with different performance characteristics
        tool_configs = [
            {"name": "fast_tool", "execution_time": 0.1, "success_rate": 1.0},
            {"name": "medium_tool", "execution_time": 0.5, "success_rate": 1.0},
            {"name": "slow_tool", "execution_time": 1.0, "success_rate": 1.0},
            {"name": "variable_tool", "execution_time": 0.3, "success_rate": 1.0}
        ]
        
        dispatcher, tools_dict, registry = await self.create_tool_dispatcher_with_business_tools(
            context, tool_configs
        )
        
        # Test 1: Sequential execution timing
        sequential_times = {}
        sequential_start = time.time()
        
        for tool_config in tool_configs:
            tool_name = tool_config["name"]
            
            tool_start = time.time()
            result = await dispatcher.execute_tool(
                tool_name=tool_name,
                context=context,
                parameters={"performance_test": "sequential"}
            )
            tool_end = time.time()
            
            execution_time = tool_end - tool_start
            expected_time = tool_config["execution_time"]
            
            sequential_times[tool_name] = {
                "actual_time": execution_time,
                "expected_time": expected_time,
                "variance": abs(execution_time - expected_time),
                "business_metrics": result["metrics"]
            }
            
        sequential_total = time.time() - sequential_start
        
        # Test 2: Concurrent execution timing
        concurrent_start = time.time()
        concurrent_tasks = []
        
        for tool_config in tool_configs:
            tool_name = tool_config["name"]
            task = dispatcher.execute_tool(
                tool_name=tool_name,
                context=context,
                parameters={"performance_test": "concurrent"}
            )
            concurrent_tasks.append((tool_name, task))
            
        concurrent_results = await asyncio.gather(
            *[task for _, task in concurrent_tasks],
            return_exceptions=True
        )
        concurrent_total = time.time() - concurrent_start
        
        # Validate sequential timing accuracy
        for tool_name, timing_data in sequential_times.items():
            # Allow for reasonable variance in execution timing (up to 50% for test environment)
            max_variance = timing_data["expected_time"] * 0.5
            assert timing_data["variance"] <= max_variance, \
                f"{tool_name} timing variance {timing_data['variance']:.3f}s exceeds maximum {max_variance:.3f}s"
                
        # Validate concurrent execution performance
        expected_concurrent_time = max(config["execution_time"] for config in tool_configs)
        concurrent_variance = abs(concurrent_total - expected_concurrent_time)
        
        # Concurrent execution should be significantly faster than sequential
        assert concurrent_total < sequential_total * 0.8, \
            "Concurrent execution should be faster than sequential"
            
        # Validate business metrics from performance tests
        total_items_processed = 0
        total_performance_improvements = 0
        
        for tool_name, timing_data in sequential_times.items():
            metrics = timing_data["business_metrics"]
            total_items_processed += metrics["processing_items"]
            
            perf_improvement = metrics["performance_improvement"]
            perf_value = float(perf_improvement.replace('%', ''))
            total_performance_improvements += perf_value
            
        # Validate performance delivered business value
        assert total_items_processed > 400, "Should process significant items across all tools"
        assert total_performance_improvements > 50, "Should demonstrate measurable performance improvements"
        
        # Validate concurrent results
        successful_concurrent = sum(1 for result in concurrent_results if not isinstance(result, Exception))
        assert successful_concurrent == len(tool_configs), "All concurrent tools should succeed"
        
        # Performance metrics summary
        performance_summary = {
            "sequential_total_time": sequential_total,
            "concurrent_total_time": concurrent_total,
            "speedup_factor": sequential_total / concurrent_total,
            "total_business_items_processed": total_items_processed,
            "total_performance_improvements": total_performance_improvements,
            "tool_count": len(tool_configs)
        }
        
        # Validate overall performance metrics
        assert performance_summary["speedup_factor"] > 1.2, "Concurrent execution should provide meaningful speedup"
        assert performance_summary["total_business_items_processed"] > 0, "Should deliver measurable business processing"
        
        # Log performance summary for analysis
        self.logger.info(f"Tool dispatcher performance summary: {performance_summary}")