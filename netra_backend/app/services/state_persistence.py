"""Enhanced Agent State Persistence Service - 3-Tier Architecture

This service implements the optimal 3-tier state persistence architecture:
1. Redis: PRIMARY storage for active states (high-performance, frequent updates)
2. ClickHouse: Historical analytics and time-series data (completed runs)
3. PostgreSQL: Metadata and critical recovery checkpoints only

Follows the 25-line function limit and maintains backward compatibility.
"""

import json
import time
import uuid
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple

from sqlalchemy import desc, func, select, update
from sqlalchemy.ext.asyncio import AsyncSession

from netra_backend.app.agents.state import DeepAgentState
from netra_backend.app.core.exceptions import NetraException
from netra_backend.app.db.models_agent_state import (
    AgentStateSnapshot,
    AgentStateTransaction,
)
from netra_backend.app.logging_config import central_logger
from netra_backend.app.redis_manager import redis_manager
from netra_backend.app.schemas.agent_state import (
    CheckpointType,
    RecoveryType,
    SerializationFormat,
    StatePersistenceRequest,
    StateRecoveryRequest,
)
from netra_backend.app.services.state_cache_manager import state_cache_manager
from netra_backend.app.services.state_recovery_manager import state_recovery_manager
from netra_backend.app.services.state_serialization import (
    DateTimeEncoder,
    StateSerializer,
    StateValidator,
)

logger = central_logger.get_logger(__name__)


class StatePersistenceService:
    """Enhanced service using optimal 3-tier state persistence architecture.
    
    Architecture:
    - Redis: PRIMARY active state storage (authoritative for running agents)
    - ClickHouse: Historical analytics (completed runs, time-series data)
    - PostgreSQL: Metadata and recovery checkpoints (minimal critical data)
    
    Includes optimizations:
    - Intelligent deduplication to avoid redundant writes
    - In-memory caching for recent states
    - Configurable optimization strategies
    """
    
    def __init__(self):
        self.redis_manager = redis_manager
        self.compression_threshold = 1024  # Compress if > 1KB for ClickHouse
        self.max_checkpoints_per_run = 10  # Reduced from 50 snapshots
        self.checkpoint_frequency = 10  # Create checkpoint every 10 steps
        self.serializer = StateSerializer()
        self.validator = StateValidator()
        self._use_legacy_mode = False  # Flag for backward compatibility
        
        # Optimization features from OptimizedStatePersistence
        self._state_cache = {}  # In-memory cache for recent states
        self._cache_max_size = 1000  # Maximum number of cached states
        self._enable_deduplication = False  # Disabled by default for backward compatibility
        self._enable_compression = False  # Additional compression beyond threshold
        self._optimization_enabled = False  # Master flag for all optimizations
        
        # Auto-enable optimizations based on environment
        self._configure_optimizations()
    
    async def save_agent_state(self, *args, **kwargs) -> Tuple[bool, Optional[str]]:
        """Save agent state using optimal 3-tier architecture with optional optimizations.
        
        Flow:
        1. Check for deduplication opportunities (if enabled)
        2. Save to Redis (PRIMARY) - immediate, high-performance
        3. Optionally create PostgreSQL checkpoint (critical recovery points only)
        4. Schedule ClickHouse migration for completed runs
        """
        request, db_session = self._parse_save_arguments(*args, **kwargs)
        
        # Apply optimizations if enabled
        if self._optimization_enabled:
            # Check for deduplication opportunity
            if await self._should_skip_persistence(request):
                logger.debug(f"Skipping redundant state persistence for run {request.run_id}")
                return True, self._get_cached_snapshot_id(request.run_id)
            
            # Apply state optimizations
            if self._is_optimizable_save(request):
                request = self._optimize_state_data(request)
        
        return await self._execute_new_save_workflow(request, db_session)

    def _parse_save_arguments(self, *args, **kwargs) -> Tuple[StatePersistenceRequest, AsyncSession]:
        """Parse and validate save arguments for backward compatibility."""
        if len(args) == 2 and isinstance(args[0], StatePersistenceRequest):
            return args[0], args[1]
        return self._parse_kwargs_arguments(kwargs)

    def _parse_kwargs_arguments(self, kwargs: Dict[str, Any]) -> Tuple[StatePersistenceRequest, AsyncSession]:
        """Parse kwargs-style arguments for save operation."""
        if 'run_id' in kwargs and 'db_session' in kwargs:
            request = self._build_persistence_request_from_kwargs(kwargs)
            return request, kwargs['db_session']
        raise ValueError("Invalid arguments for save_agent_state")

    async def _execute_new_save_workflow(self, request: StatePersistenceRequest, 
                                        db_session: AsyncSession) -> Tuple[bool, Optional[str]]:
        """Execute new 3-tier save workflow with comprehensive error handling."""
        try:
            # Step 1: Save to Redis (PRIMARY) - must succeed
            redis_success = await state_cache_manager.save_primary_state(request)
            if not redis_success:
                logger.error(f"PRIMARY Redis save failed for run {request.run_id}")
                return await self._fallback_to_legacy_save(request, db_session)
            
            # Step 2: Optionally create PostgreSQL checkpoint
            checkpoint_id = await self._create_recovery_checkpoint_if_needed(request, db_session)
            
            # Step 3: Schedule ClickHouse migration if completed
            await self._schedule_clickhouse_migration_if_completed(request)
            
            state_id = checkpoint_id or f"redis:{request.run_id}"
            self._log_save_success(state_id, request.run_id)
            return True, state_id
        except Exception as e:
            return self._handle_save_error(request, e)

    async def _execute_state_save_transaction(self, request: StatePersistenceRequest, db_session: AsyncSession) -> str:
        """Execute the complete state save transaction."""
        snapshot_id = None
        transaction_id = None
        async with db_session.begin():
            snapshot_id, transaction_id = await self._create_snapshot_and_transaction(request, db_session)
            await self._finalize_state_save(request, transaction_id, db_session)
        return snapshot_id

    async def _create_snapshot_and_transaction(self, request: StatePersistenceRequest, db_session: AsyncSession) -> Tuple[str, str]:
        """Create snapshot and transaction records."""
        snapshot_id = await self._create_state_snapshot(request, db_session)
        transaction_id = await self._log_state_transaction(
            snapshot_id, request, "create", db_session
        )
        return snapshot_id, transaction_id

    async def _finalize_state_save(self, request: StatePersistenceRequest, transaction_id: str, db_session: AsyncSession) -> None:
        """Complete state save with caching and cleanup."""
        await state_cache_manager.cache_state_in_redis(request)
        await self._cleanup_old_snapshots(request.run_id, db_session)
        await self._complete_transaction(transaction_id, "committed", db_session)
    
    async def load_agent_state(self, run_id: str, snapshot_id: Optional[str] = None,
                              db_session: Optional[AsyncSession] = None) -> Optional[DeepAgentState]:
        """Load agent state using optimal 3-tier architecture.
        
        Load order:
        1. Redis (PRIMARY) - fastest, most recent state
        2. PostgreSQL checkpoints - recovery points  
        3. ClickHouse - historical data (if needed)
        4. Legacy PostgreSQL snapshots - backward compatibility
        """
        return await self._execute_new_load_workflow(run_id, snapshot_id, db_session)

    async def _execute_new_load_workflow(self, run_id: str, snapshot_id: Optional[str],
                                        db_session: Optional[AsyncSession]) -> Optional[DeepAgentState]:
        """Execute new 3-tier load workflow with fallback chain."""
        try:
            # Step 1: Try Redis (PRIMARY) - fastest for active states
            if not snapshot_id:  # Only for current state, not specific snapshots
                state = await state_cache_manager.load_primary_state(run_id)
                if state:
                    logger.debug(f"Loaded state for {run_id} from Redis PRIMARY")
                    return state
            
            # Step 2: Try PostgreSQL checkpoints - recovery points
            state = await self._attempt_checkpoint_load(run_id, snapshot_id, db_session)
            if state:
                return state
            
            # Step 3: Try ClickHouse - historical data (if implemented)
            # state = await self._attempt_clickhouse_load(run_id, snapshot_id)
            # if state: return state
            
            # Step 4: Fallback to legacy PostgreSQL snapshots
            return await self._attempt_legacy_database_load(run_id, snapshot_id, db_session)
        except Exception as e:
            return self._handle_load_error(run_id, e)

    async def _attempt_cache_load(self, run_id: str, snapshot_id: Optional[str]) -> Optional[DeepAgentState]:
        """Try loading state from Redis cache first."""
        if not snapshot_id:
            return await state_cache_manager.load_from_redis_cache(run_id)
        return None

    async def _attempt_database_load(self, run_id: str, snapshot_id: Optional[str], 
                                    db_session: Optional[AsyncSession]) -> Optional[DeepAgentState]:
        """Load state from database snapshots."""
        if not db_session:
            return self._handle_no_session_warning(run_id)
        snapshot = await self._get_latest_snapshot(run_id, snapshot_id, db_session)
        return await self._process_snapshot_result(run_id, snapshot)

    def _handle_no_session_warning(self, run_id: str) -> None:
        """Handle case when no database session is provided."""
        logger.warning(f"No state found for run {run_id}")
        return None

    async def _process_snapshot_result(self, run_id: str, snapshot) -> Optional[DeepAgentState]:
        """Process snapshot result or handle missing snapshot."""
        if not snapshot:
            logger.warning(f"No state found for run {run_id}")
            return None
        return await self._process_database_snapshot(run_id, snapshot)

    async def _process_database_snapshot(self, run_id: str, snapshot) -> DeepAgentState:
        """Process database snapshot and return state."""
        state_data = await self._deserialize_state_data(
            snapshot.state_data, snapshot.serialization_format
        )
        await state_cache_manager.cache_deserialized_state(run_id, state_data)
        logger.info(f"Loaded state from snapshot {snapshot.id}")
        return DeepAgentState(**state_data)
    
    async def recover_agent_state(self, request: StateRecoveryRequest,
                                 db_session: AsyncSession) -> Tuple[bool, Optional[str]]:
        """Recover agent state from a specific checkpoint."""
        recovery_id = str(uuid.uuid4())
        return await self._execute_recovery_with_error_handling(request, recovery_id, db_session)

    async def _execute_recovery_with_error_handling(self, request: StateRecoveryRequest, 
                                                   recovery_id: str, db_session: AsyncSession) -> Tuple[bool, Optional[str]]:
        """Execute recovery operation with comprehensive error handling."""
        try:
            success = await self._execute_recovery_operation(request, recovery_id, db_session)
            self._log_recovery_result(recovery_id, success)
            return self._format_recovery_result(success, recovery_id)
        except Exception as e:
            return await self._handle_recovery_error(request, recovery_id, db_session, e)

    async def _handle_recovery_error(self, request: StateRecoveryRequest, recovery_id: str,
                                   db_session: AsyncSession, error: Exception) -> Tuple[bool, None]:
        """Handle recovery operation errors."""
        logger.error(f"Recovery failed for run {request.run_id}: {error}")
        await state_recovery_manager.complete_recovery_log(recovery_id, False, db_session, str(error))
        return False, None

    async def _execute_recovery_operation(self, request: StateRecoveryRequest, recovery_id: str, db_session: AsyncSession) -> bool:
        """Execute the recovery operation workflow."""
        return await state_recovery_manager.execute_recovery_operation(request, recovery_id, db_session)
    
    async def _create_state_snapshot(self, request: StatePersistenceRequest,
                                    db_session: AsyncSession) -> str:
        """Create a new state snapshot in database."""
        # Skip state persistence for run_ prefixed IDs (these are test/temporary IDs)
        if request.user_id and request.user_id.startswith("run_"):
            logger.debug(f"Skipping state persistence for temporary run ID: {request.user_id}")
            return str(uuid.uuid4())  # Return a dummy snapshot ID
        
        # Handle missing user_id by setting to None to avoid FK constraint
        if not request.user_id:
            logger.warning("No user_id provided for state snapshot, setting to None")
            # Modify the request object directly
            request.user_id = None
        else:
            # Ensure user exists before creating snapshot
            try:
                await self._ensure_user_exists_for_snapshot(request.user_id, db_session)
            except Exception as e:
                logger.warning(f"Failed to ensure user {request.user_id} exists, setting user_id to None: {e}")
                # Modify the request object directly to avoid FK violation
                request.user_id = None
        
        snapshot_id = str(uuid.uuid4())
        snapshot = await self._prepare_snapshot_for_database(snapshot_id, request)
        return await self._insert_snapshot_to_database(snapshot, db_session, snapshot_id)

    async def _prepare_snapshot_for_database(self, snapshot_id: str, request: StatePersistenceRequest) -> AgentStateSnapshot:
        """Prepare and validate snapshot data for database insertion."""
        self._validate_request_state(request)
        serialization_format = self._choose_serialization_format(request.state_data)
        json_safe_data = self._prepare_json_safe_data(request.state_data)
        return self._build_snapshot_record(snapshot_id, request, serialization_format, json_safe_data)

    async def _insert_snapshot_to_database(self, snapshot: AgentStateSnapshot, 
                                         db_session: AsyncSession, snapshot_id: str) -> str:
        """Insert prepared snapshot into database."""
        db_session.add(snapshot)
        await db_session.flush()
        return snapshot_id
    
    async def _log_state_transaction(self, snapshot_id: str, request: StatePersistenceRequest,
                                    operation_type: str, db_session: AsyncSession) -> str:
        """Log state transaction for audit trail."""
        transaction_id = str(uuid.uuid4())
        transaction = self._build_transaction_record(transaction_id, snapshot_id, request, operation_type)
        return await self._insert_transaction_to_database(transaction, db_session, transaction_id)

    def _build_transaction_record(self, transaction_id: str, snapshot_id: str, 
                                 request: StatePersistenceRequest, operation_type: str) -> AgentStateTransaction:
        """Build transaction record for database insertion."""
        execution_phase = self._extract_value(request.agent_phase) if request.agent_phase else None
        return AgentStateTransaction(
            id=transaction_id, snapshot_id=snapshot_id, run_id=request.run_id,
            operation_type=operation_type, triggered_by="system",
            execution_phase=execution_phase, status="pending")

    async def _insert_transaction_to_database(self, transaction: AgentStateTransaction, 
                                            db_session: AsyncSession, transaction_id: str) -> str:
        """Insert transaction record into database."""
        db_session.add(transaction)
        await db_session.flush()
        return transaction_id
    
    async def _cleanup_old_snapshots(self, run_id: str, db_session: AsyncSession) -> None:
        """Clean up old snapshots to maintain performance."""
        old_snapshot_ids = await self._get_old_snapshot_ids(run_id, db_session)
        if old_snapshot_ids:
            await self._delete_snapshots_batch(old_snapshot_ids, db_session)

    async def _get_old_snapshot_ids(self, run_id: str, db_session: AsyncSession) -> List[str]:
        """Get IDs of old snapshots that should be cleaned up."""
        result = await db_session.execute(
            select(AgentStateSnapshot.id)
            .where(AgentStateSnapshot.run_id == run_id)
            .order_by(desc(AgentStateSnapshot.created_at))
            .offset(self.max_snapshots_per_run))
        return result.scalars().all()
    
    async def _delete_snapshots_batch(self, snapshot_ids: List[str], 
                                     db_session: AsyncSession) -> None:
        """Delete snapshots and related data in batch."""
        await self._delete_related_transactions(snapshot_ids, db_session)
        await self._delete_snapshots_records(snapshot_ids, db_session)

    async def _delete_related_transactions(self, snapshot_ids: List[str], db_session: AsyncSession) -> None:
        """Delete transactions related to snapshots."""
        await db_session.execute(
            AgentStateTransaction.__table__.delete()
            .where(AgentStateTransaction.snapshot_id.in_(snapshot_ids)))

    async def _delete_snapshots_records(self, snapshot_ids: List[str], db_session: AsyncSession) -> None:
        """Delete snapshot records from database."""
        await db_session.execute(
            AgentStateSnapshot.__table__.delete()
            .where(AgentStateSnapshot.id.in_(snapshot_ids)))
    
    async def _complete_transaction(self, transaction_id: str, status: str,
                                   db_session: AsyncSession, error_message: Optional[str] = None) -> None:
        """Complete a state transaction with final status."""
        await db_session.execute(
            update(AgentStateTransaction)
            .where(AgentStateTransaction.id == transaction_id)
            .values(status=status, completed_at=datetime.now(timezone.utc),
                   error_message=error_message))
    
    def _choose_serialization_format(self, state_data: Dict[str, Any]) -> SerializationFormat:
        """Choose optimal serialization format based on data size."""
        estimated_size = len(json.dumps(state_data, cls=DateTimeEncoder))
        return SerializationFormat.COMPRESSED_JSON if estimated_size > self.compression_threshold else SerializationFormat.JSON
    
    def _calculate_expiry_date(self) -> datetime:
        """Calculate default expiry date for snapshots."""
        return datetime.now(timezone.utc) + timedelta(days=self.default_retention_days)
    
    def _validate_request_state(self, request: StatePersistenceRequest) -> None:
        """Validate state data in request."""
        validation_result = self.validator.validate_state(request.state_data)
        if not validation_result.is_valid:
            raise NetraException(f"Invalid state data: {validation_result.errors}")
    
    def _prepare_json_safe_data(self, state_data: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare JSON-safe data for database storage."""
        import copy
        # Use deep copy instead of double JSON serialization to avoid performance overhead
        # This preserves the data structure without the expensive serialize/deserialize cycle
        json_safe_data = copy.deepcopy(state_data)
        
        # Convert any datetime objects to ISO strings for JSON compatibility
        return self._convert_datetime_objects(json_safe_data)
    
    def _convert_datetime_objects(self, data: Any) -> Any:
        """Convert datetime objects to ISO strings recursively."""
        if isinstance(data, datetime):
            return data.isoformat()
        elif isinstance(data, dict):
            return {key: self._convert_datetime_objects(value) for key, value in data.items()}
        elif isinstance(data, list):
            return [self._convert_datetime_objects(item) for item in data]
        else:
            return data
    
    def _build_snapshot_record(self, snapshot_id: str, request: StatePersistenceRequest,
                              serialization_format: SerializationFormat, 
                              json_safe_data: Dict[str, Any]) -> AgentStateSnapshot:
        """Build snapshot record for database insertion."""
        core_fields = self._build_snapshot_core_fields(snapshot_id, request, json_safe_data)
        metadata_fields = self._build_snapshot_metadata_fields(request, serialization_format)
        return AgentStateSnapshot(**core_fields, **metadata_fields)

    def _build_snapshot_core_fields(self, snapshot_id: str, request: StatePersistenceRequest,
                                   json_safe_data: Dict[str, Any]) -> Dict[str, Any]:
        """Build core fields for snapshot record."""
        return {
            'id': snapshot_id, 'run_id': request.run_id, 'thread_id': request.thread_id,
            'user_id': request.user_id, 'state_data': json_safe_data,
            'execution_context': request.execution_context, 'is_recovery_point': request.is_recovery_point
        }

    def _build_snapshot_metadata_fields(self, request: StatePersistenceRequest,
                                       serialization_format: SerializationFormat) -> Dict[str, Any]:
        """Build metadata fields for snapshot record."""
        return {
            'serialization_format': self._extract_value(serialization_format),
            'checkpoint_type': self._extract_value(request.checkpoint_type),
            'agent_phase': self._extract_value(request.agent_phase) if request.agent_phase else None,
            'expires_at': request.expires_at or self._calculate_expiry_date()
        }
    
    def _extract_value(self, obj) -> str:
        """Extract value from enum or return as-is."""
        return obj.value if hasattr(obj, 'value') else obj
    
    async def _get_latest_snapshot(self, run_id: str, snapshot_id: Optional[str],
                                  db_session: AsyncSession) -> Optional[AgentStateSnapshot]:
        """Get the latest or specific snapshot for a run."""
        if snapshot_id:
            return await self._get_specific_snapshot(snapshot_id, db_session)
        return await self._get_latest_run_snapshot(run_id, db_session)

    async def _get_specific_snapshot(self, snapshot_id: str, db_session: AsyncSession) -> Optional[AgentStateSnapshot]:
        """Get a specific snapshot by ID."""
        result = await db_session.execute(
            select(AgentStateSnapshot).where(AgentStateSnapshot.id == snapshot_id))
        return result.scalar_one_or_none()

    async def _get_latest_run_snapshot(self, run_id: str, db_session: AsyncSession) -> Optional[AgentStateSnapshot]:
        """Get the latest snapshot for a run."""
        result = await db_session.execute(
            select(AgentStateSnapshot).where(AgentStateSnapshot.run_id == run_id)
            .order_by(desc(AgentStateSnapshot.created_at)).limit(1))
        return result.scalar_one_or_none()
    
    async def _deserialize_state_data(self, state_data: Dict[str, Any], 
                                     serialization_format: str) -> Dict[str, Any]:
        """Deserialize state data from database format."""
        # For now, state_data is already stored as JSON in database
        # In future, could support other formats stored as BLOB
        return state_data

    def _build_persistence_request_from_kwargs(self, kwargs: Dict[str, Any]) -> StatePersistenceRequest:
        """Build StatePersistenceRequest from legacy kwargs."""
        state_data = self._extract_state_data_from_kwargs(kwargs)
        core_fields = self._build_request_core_fields_from_kwargs(kwargs, state_data)
        optional_fields = self._build_request_optional_fields_from_kwargs(kwargs)
        return StatePersistenceRequest(**core_fields, **optional_fields)

    def _extract_state_data_from_kwargs(self, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """Extract and process state data from kwargs."""
        return kwargs['state'].model_dump() if hasattr(kwargs['state'], 'model_dump') else kwargs['state']

    def _build_request_core_fields_from_kwargs(self, kwargs: Dict[str, Any], state_data: Dict[str, Any]) -> Dict[str, Any]:
        """Build core required fields for StatePersistenceRequest."""
        return {
            'run_id': kwargs['run_id'], 'thread_id': kwargs.get('thread_id'),
            'user_id': kwargs.get('user_id'), 'state_data': state_data
        }

    def _build_request_optional_fields_from_kwargs(self, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """Build optional fields for StatePersistenceRequest."""
        return {
            'checkpoint_type': kwargs.get('checkpoint_type', CheckpointType.MANUAL),
            'agent_phase': kwargs.get('agent_phase'), 'execution_context': kwargs.get('execution_context', {}),
            'is_recovery_point': kwargs.get('is_recovery_point', False), 'expires_at': kwargs.get('expires_at')
        }
    
    def _log_save_success(self, snapshot_id: str, run_id: str) -> None:
        """Log successful save operation."""
        logger.info(f"Saved state snapshot {snapshot_id} for run {run_id}")
    
    def _handle_save_error(self, request: StatePersistenceRequest, error: Exception) -> Tuple[bool, None]:
        """Handle save operation error with specific handling for FK violations."""
        error_message = str(error)
        
        # Check if this is a foreign key violation related to user_id
        if "agent_state_snapshots_user_id_fkey" in error_message:
            logger.error(f"Foreign key violation for user_id '{request.user_id}' in run {request.run_id}. "
                        f"User does not exist in userbase table. State persistence failed.")
            logger.info(f"Consider creating user '{request.user_id}' in the userbase table or "
                       f"setting user_id to None for development scenarios.")
        else:
            logger.error(f"Failed to save state for run {request.run_id}: {error}")
        
        return False, None
    
    def _format_recovery_result(self, success: bool, recovery_id: str) -> Tuple[bool, Optional[str]]:
        """Format recovery operation result."""
        return success, recovery_id if success else None
    
    def _handle_load_error(self, run_id: str, error: Exception) -> None:
        """Handle load operation error."""
        logger.error(f"Failed to load state for run {run_id}: {error}")
        return None
    
    async def _ensure_user_exists_for_snapshot(self, user_id: str, db_session: AsyncSession) -> None:
        """Ensure user exists before creating snapshot to prevent foreign key violations.
        
        This method checks if a user exists and creates a development user if needed.
        This is critical for preventing foreign key constraint violations when saving state.
        """
        if not user_id:
            return  # Skip if no user_id provided
        
        from netra_backend.app.services.user_service import user_service
        
        # Check if user exists
        existing_user = await user_service.get(db_session, id=user_id)
        if existing_user:
            return  # User exists, nothing to do
        
        # Handle dev/test users automatically
        if 'dev-temp' in user_id or user_id.startswith('test-') or user_id.startswith('run_'):
            logger.info(f"Auto-creating dev/test user for state persistence: {user_id}")
            try:
                # Use the specialized dev user creation method that bypasses email validation
                # Generate a valid email for dev users
                dev_email = f"{user_id.replace('-', '_')}@example.com"
                await user_service.get_or_create_dev_user(db_session, email=dev_email, user_id=user_id)
                logger.info(f"Created dev user {user_id} for state persistence")
            except Exception as e:
                # Log but don't fail - the foreign key error will be more descriptive
                logger.warning(f"Could not auto-create dev user {user_id}: {e}")
    
    def _log_recovery_result(self, recovery_id: str, success: bool) -> None:
        """Log recovery operation result."""
        status = 'completed' if success else 'failed'
        logger.info(f"Recovery {recovery_id} {status}")
    
    async def get_thread_context(self, thread_id: str = None) -> Dict[str, Any]:
        """Get thread context for agent orchestration."""
        # Return empty context for now - can be enhanced to store thread-specific data
        return {}
    
    # New methods for 3-tier architecture
    async def _create_recovery_checkpoint_if_needed(self, request: StatePersistenceRequest, 
                                                  db_session: AsyncSession) -> Optional[str]:
        """Create PostgreSQL recovery checkpoint if conditions are met."""
        # Only create checkpoints for recovery points or every N steps
        should_checkpoint = (
            getattr(request, 'is_recovery_point', False) or
            self._should_create_periodic_checkpoint(request)
        )
        
        if not should_checkpoint:
            return None
            
        try:
            from netra_backend.app.db.models_agent_state import AgentStateCheckpoint, AgentStateMetadata
            
            # Ensure metadata record exists
            await self._ensure_metadata_record(request, db_session)
            
            # Create checkpoint with minimal essential data
            essential_state = self._extract_essential_state(request.state_data)
            
            checkpoint = AgentStateCheckpoint(
                run_id=request.run_id,
                checkpoint_sequence=self._get_next_checkpoint_sequence(request.run_id),
                checkpoint_type="recovery" if getattr(request, 'is_recovery_point', False) else "periodic",
                agent_phase=self._extract_value(request.agent_phase) if request.agent_phase else "unknown",
                step_count=getattr(request.state_data, 'steps', 0) if hasattr(request.state_data, 'steps') else 0,
                essential_state=essential_state,
                redis_key=f"agent_state:{request.run_id}",
                recovery_priority="high" if getattr(request, 'is_recovery_point', False) else "normal"
            )
            
            db_session.add(checkpoint)
            await db_session.flush()
            
            logger.info(f"Created recovery checkpoint {checkpoint.id} for run {request.run_id}")
            return checkpoint.id
            
        except Exception as e:
            logger.error(f"Failed to create recovery checkpoint for {request.run_id}: {e}")
            return None
    
    async def _schedule_clickhouse_migration_if_completed(self, request: StatePersistenceRequest) -> None:
        """Schedule ClickHouse migration for completed runs."""
        # Check if this is a final/completion state
        is_final = (
            self._extract_value(request.checkpoint_type) == "final" or
            getattr(request.state_data, 'status', None) in ['completed', 'failed', 'terminated']
        )
        
        if is_final:
            try:
                # Mark state as completed in Redis (reduces TTL)
                await state_cache_manager.mark_state_completed(request.run_id)
                
                # TODO: Schedule async job to migrate to ClickHouse
                # For now, just log the intent
                logger.info(f"Scheduled ClickHouse migration for completed run {request.run_id}")
                
                # Immediate migration for demonstration (in production, use async job)
                await self._migrate_to_clickhouse_immediate(request)
                
            except Exception as e:
                logger.error(f"Failed to schedule ClickHouse migration for {request.run_id}: {e}")
    
    async def _migrate_to_clickhouse_immediate(self, request: StatePersistenceRequest) -> None:
        """Immediately migrate completed state to ClickHouse."""
        try:
            from netra_backend.app.db.clickhouse import insert_agent_state_history
            
            # Prepare metadata for ClickHouse
            metadata = {
                'thread_id': request.thread_id,
                'user_id': request.user_id,
                'agent_phase': self._extract_value(request.agent_phase) if request.agent_phase else 'completed',
                'checkpoint_type': self._extract_value(request.checkpoint_type),
                'step_count': getattr(request.state_data, 'steps', 0) if hasattr(request.state_data, 'steps') else 0,
                'is_recovery_point': getattr(request, 'is_recovery_point', False),
                'execution_status': 'completed'
            }
            
            # Insert into ClickHouse for analytics
            success = await insert_agent_state_history(request.run_id, request.state_data, metadata)
            
            if success:
                logger.info(f"Successfully migrated run {request.run_id} to ClickHouse")
            else:
                logger.warning(f"ClickHouse migration failed for run {request.run_id} (data still in Redis)")
                
        except Exception as e:
            logger.error(f"Immediate ClickHouse migration failed for {request.run_id}: {e}")
    
    async def _fallback_to_legacy_save(self, request: StatePersistenceRequest, 
                                     db_session: AsyncSession) -> Tuple[bool, Optional[str]]:
        """Fallback to legacy PostgreSQL save if Redis fails."""
        logger.warning(f"Falling back to legacy save for run {request.run_id}")
        self._use_legacy_mode = True
        
        try:
            snapshot_id = await self._execute_state_save_transaction(request, db_session)
            # Also try to cache in Redis for future loads
            await state_cache_manager.cache_legacy_state(request.run_id, request.state_data)
            return True, snapshot_id
        except Exception as e:
            logger.error(f"Legacy save also failed for run {request.run_id}: {e}")
            return False, None
    
    async def _attempt_checkpoint_load(self, run_id: str, snapshot_id: Optional[str], 
                                     db_session: Optional[AsyncSession]) -> Optional[DeepAgentState]:
        """Attempt to load from PostgreSQL recovery checkpoints."""
        if not db_session:
            return None
            
        try:
            from netra_backend.app.db.models_agent_state import AgentStateCheckpoint
            from sqlalchemy import desc, select
            
            query = select(AgentStateCheckpoint).where(AgentStateCheckpoint.run_id == run_id)
            
            if snapshot_id:
                query = query.where(AgentStateCheckpoint.id == snapshot_id)
            else:
                # Get latest checkpoint
                query = query.order_by(desc(AgentStateCheckpoint.checkpoint_sequence)).limit(1)
            
            result = await db_session.execute(query)
            checkpoint = result.scalar_one_or_none()
            
            if checkpoint and checkpoint.essential_state:
                logger.info(f"Loaded state for {run_id} from PostgreSQL checkpoint {checkpoint.id}")
                return DeepAgentState(**checkpoint.essential_state)
                
        except Exception as e:
            logger.error(f"Failed to load from checkpoints for {run_id}: {e}")
        
        return None
    
    async def _attempt_legacy_database_load(self, run_id: str, snapshot_id: Optional[str],
                                          db_session: Optional[AsyncSession]) -> Optional[DeepAgentState]:
        """Attempt to load from legacy PostgreSQL snapshots (backward compatibility)."""
        if not db_session:
            return None
        
        try:
            snapshot = await self._get_latest_snapshot(run_id, snapshot_id, db_session)
            if snapshot:
                state_data = await self._deserialize_state_data(
                    snapshot.state_data, getattr(snapshot, 'serialization_format', 'json')
                )
                
                # Cache in Redis for future loads
                await state_cache_manager.cache_legacy_state(run_id, state_data)
                
                logger.info(f"Loaded state for {run_id} from legacy PostgreSQL snapshot {snapshot.id}")
                return DeepAgentState(**state_data)
                
        except Exception as e:
            logger.error(f"Failed to load from legacy snapshots for {run_id}: {e}")
        
        return None
    
    def _should_create_periodic_checkpoint(self, request: StatePersistenceRequest) -> bool:
        """Determine if a periodic checkpoint should be created."""
        step_count = getattr(request.state_data, 'steps', 0) if hasattr(request.state_data, 'steps') else 0
        return step_count > 0 and step_count % self.checkpoint_frequency == 0
    
    async def _ensure_metadata_record(self, request: StatePersistenceRequest, db_session: AsyncSession) -> None:
        """Ensure agent state metadata record exists."""
        try:
            from netra_backend.app.db.models_agent_state import AgentStateMetadata
            from sqlalchemy import select
            
            # Check if metadata exists
            result = await db_session.execute(
                select(AgentStateMetadata).where(AgentStateMetadata.run_id == request.run_id)
            )
            existing = result.scalar_one_or_none()
            
            if not existing:
                # Create new metadata record
                metadata = AgentStateMetadata(
                    run_id=request.run_id,
                    thread_id=request.thread_id,
                    user_id=request.user_id,
                    run_status="active",
                    redis_key=f"agent_state:{request.run_id}",
                    initial_phase=self._extract_value(request.agent_phase) if request.agent_phase else "unknown",
                    current_phase=self._extract_value(request.agent_phase) if request.agent_phase else "unknown"
                )
                db_session.add(metadata)
                await db_session.flush()
            else:
                # Update existing metadata
                existing.current_phase = self._extract_value(request.agent_phase) if request.agent_phase else existing.current_phase
                existing.last_updated = func.now()
                
        except Exception as e:
            logger.error(f"Failed to ensure metadata record for {request.run_id}: {e}")
    
    def _get_next_checkpoint_sequence(self, run_id: str) -> int:
        """Get next checkpoint sequence number for a run."""
        # Simple implementation - could be enhanced with DB lookup
        import time
        return int(time.time() % 1000000)  # Use timestamp-based sequence for uniqueness
    
    def _extract_essential_state(self, state_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extract only essential state data for recovery checkpoints."""
        # Only keep critical fields needed for recovery
        essential_keys = [
            'current_phase', 'steps', 'status', 'context', 'memory',
            'agent_type', 'thread_state', 'critical_data'
        ]
        
        essential = {}
        for key in essential_keys:
            if key in state_data:
                essential[key] = state_data[key]
        
        # Always include some basic recovery info
        essential['checkpoint_created_at'] = time.time()
        essential['recovery_version'] = '3-tier-v1'
        
        return essential
    
    # Optimization methods merged from OptimizedStatePersistence
    def _configure_optimizations(self) -> None:
        """Configure optimizations based on environment settings."""
        from shared.isolated_environment import get_env
        
        # Check if optimized persistence is enabled
        # Note: The get_env() function already reads from environment or defaults
        env_value = get_env().get("ENABLE_OPTIMIZED_PERSISTENCE", "false")
        self._optimization_enabled = str(env_value).lower() == "true"
        
        if self._optimization_enabled:
            self._enable_deduplication = True
            self._enable_compression = True
            logger.info("State persistence optimizations enabled (deduplication, compression)")
        else:
            logger.debug("State persistence optimizations disabled")
    
    def _is_optimizable_save(self, request: StatePersistenceRequest) -> bool:
        """Determine if this save can be optimized."""
        # Optimize non-critical checkpoints
        non_critical_types = {
            CheckpointType.AUTO,
            CheckpointType.INTERMEDIATE,
            CheckpointType.PIPELINE_COMPLETE
        }
        
        # Don't optimize critical save points
        if hasattr(request, 'checkpoint_type') and request.checkpoint_type:
            checkpoint_type = request.checkpoint_type
            if hasattr(checkpoint_type, 'value'):
                checkpoint_type = checkpoint_type.value
            
            return checkpoint_type in {ct.value for ct in non_critical_types}
        
        # Default to optimizable for backwards compatibility
        return True
    
    async def _should_skip_persistence(self, request: StatePersistenceRequest) -> bool:
        """Check if we can skip this persistence operation due to deduplication."""
        if not self._enable_deduplication:
            return False
        
        # Calculate state hash for deduplication
        state_hash = self._calculate_state_hash(request.state_data)
        cache_key = f"{request.run_id}:{request.user_id}"
        
        # Check if we've already persisted this exact state
        cached_info = self._state_cache.get(cache_key)
        if cached_info and cached_info.get('state_hash') == state_hash:
            # State hasn't changed, skip persistence
            return True
        
        # Update cache with new state hash
        self._update_state_cache(cache_key, state_hash)
        return False
    
    def _calculate_state_hash(self, state_data: Dict[str, Any]) -> str:
        """Calculate hash of state data for deduplication."""
        import hashlib
        try:
            # Create a deterministic string representation of the state
            state_str = json.dumps(state_data, sort_keys=True, default=str)
            return hashlib.md5(state_str.encode()).hexdigest()
        except Exception as e:
            logger.warning(f"Failed to calculate state hash: {e}")
            # Return random hash to disable deduplication for this state
            return str(uuid.uuid4())
    
    def _update_state_cache(self, cache_key: str, state_hash: str) -> None:
        """Update the state cache with new state hash."""
        # Implement LRU-style cache eviction if needed
        if len(self._state_cache) >= self._cache_max_size:
            # Remove oldest entry (simple FIFO for now)
            oldest_key = next(iter(self._state_cache))
            del self._state_cache[oldest_key]
        
        snapshot_id = str(uuid.uuid4())
        self._state_cache[cache_key] = {
            'state_hash': state_hash,
            'snapshot_id': snapshot_id,
            'timestamp': datetime.now(timezone.utc)
        }
    
    def _get_cached_snapshot_id(self, run_id: str) -> Optional[str]:
        """Get cached snapshot ID for deduplication scenarios."""
        # Find cache entry by run_id
        for cache_key, cache_info in self._state_cache.items():
            if cache_key.startswith(f"{run_id}:"):
                return cache_info.get('snapshot_id')
        return str(uuid.uuid4())  # Return dummy ID if not found
    
    def _optimize_state_data(self, request: StatePersistenceRequest) -> StatePersistenceRequest:
        """Apply optimizations to state data before persistence."""
        import copy
        # Deep copy the request to avoid modifying the original
        optimized_data = copy.deepcopy(request.state_data)
        
        # Apply compression optimizations if enabled
        if self._enable_compression:
            optimized_data = self._compress_state_data(optimized_data)
        
        # Create new request with optimized data
        return StatePersistenceRequest(
            run_id=request.run_id,
            thread_id=request.thread_id,
            user_id=request.user_id,
            state_data=optimized_data,
            checkpoint_type=request.checkpoint_type,
            agent_phase=request.agent_phase,
            execution_context=request.execution_context,
            is_recovery_point=request.is_recovery_point,
            expires_at=request.expires_at
        )
    
    def _compress_state_data(self, state_data: Dict[str, Any]) -> Dict[str, Any]:
        """Apply compression optimizations to state data."""
        # For now, this is a placeholder for compression logic
        # Could implement:
        # - Remove redundant fields
        # - Compress large text fields
        # - Optimize data structures
        return state_data
    
    def configure(self, **config_options) -> None:
        """Configure optimization settings."""
        if 'enable_optimizations' in config_options:
            self._optimization_enabled = config_options['enable_optimizations']
            if self._optimization_enabled:
                self._enable_deduplication = True
                self._enable_compression = True
            logger.info(f"Optimizations {'enabled' if self._optimization_enabled else 'disabled'}")
        
        if 'enable_deduplication' in config_options:
            self._enable_deduplication = config_options['enable_deduplication']
            logger.info(f"Deduplication {'enabled' if self._enable_deduplication else 'disabled'}")
        
        if 'enable_compression' in config_options:
            self._enable_compression = config_options['enable_compression']
            logger.info(f"Compression {'enabled' if self._enable_compression else 'disabled'}")
        
        if 'cache_max_size' in config_options:
            self._cache_max_size = config_options['cache_max_size']
            logger.info(f"Cache max size set to {self._cache_max_size}")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics for monitoring."""
        return {
            'cache_size': len(self._state_cache),
            'cache_max_size': self._cache_max_size,
            'optimization_enabled': self._optimization_enabled,
            'deduplication_enabled': self._enable_deduplication,
            'compression_enabled': self._enable_compression,
            'cache_entries': list(self._state_cache.keys())
        }
    
    def clear_cache(self) -> None:
        """Clear the state cache (useful for testing)."""
        self._state_cache.clear()
        logger.debug("State cache cleared")

# Global instance
state_persistence_service = StatePersistenceService()