[pytest]
asyncio_mode = auto
asyncio_default_fixture_loop_scope = session
addopts = --tb=short -s --strict-markers --timeout=30
filterwarnings =
    ignore::RuntimeWarning:unittest.case
console_output_style = count
log_cli = true
log_cli_level = WARNING
log_cli_format = %(message)s
# Windows compatibility: capture=sys works better than capture=no with our plugins
# Disabled test_framework.pytest_bad_test_plugin temporarily to avoid capture conflicts
# Note: plugins configuration option is deprecated in pytest.ini, use pytest_plugins in conftest.py instead
markers =
    smoke: marks tests as smoke tests (quick validation tests)
    stress: marks tests as stress tests (high load/scale tests)
    slow: marks tests as slow tests (long-running tests)
    performance: marks tests as performance tests (performance benchmarks)
    critical: marks tests as critical tests (essential system reliability tests)
    real_llm: marks tests that require real LLM API calls (uses actual API keys)
    real_database: marks tests that require real database connections (PostgreSQL)
    real_redis: marks tests that require real Redis connection
    real_clickhouse: marks tests that require real ClickHouse connection
    real_services: marks tests that require any real external services
    real_data: marks tests that require real data generation and processing
    real_quality: marks tests that require real quality validation services
    mock_only: marks tests that use only mocks and no external dependencies
    unit: marks tests as unit tests (isolated component testing)
    integration: marks tests as integration tests (component interaction)
    e2e: marks tests as end-to-end tests (full system flow)
    real_e2e: marks tests as real end-to-end tests requiring actual services
    flaky: marks tests as flaky (tests that may fail intermittently)
    first_time_user: marks tests for first time user flow
    first_time_user_validation: marks tests for first time user validation
    resilience: marks tests as resilience and recovery validation tests
    redis: marks tests that depend on Redis service
    staging: marks tests specific to staging environment
    dev: marks tests specific to development environment
    env: marks tests with environment compatibility (test, dev, staging, prod)
    env_requires: marks tests with environment-specific requirements
    env_safe: marks tests as safe for specific environments
    test: marks tests for test environment only
    prod: marks tests compatible with production environment
    env_test: marks tests for test environment compatibility
    agents: marks tests in the agents category
    database: marks tests in the database category
    L0: marks tests as L0 (Fully Mocked/Simulated) on Mock-Real Spectrum
    L1: marks tests as L1 (Real SUT with Mocked Dependencies) on Mock-Real Spectrum  
    L2: marks tests as L2 (Real SUT with Real Internal Dependencies) on Mock-Real Spectrum
    l2_integration: marks tests as L2 integration tests
    L3: marks tests as L3 (Real SUT with Real Local Services) on Mock-Real Spectrum
    l3: marks tests as L3 (Real SUT with Real Local Services) on Mock-Real Spectrum
    l3_realism: marks tests as L3 realism (Real local services with minimal mocking)
    L4: marks tests as L4 (Real SUT with Real Shared Environment) on Mock-Real Spectrum
    l4: marks tests as L4 (Real SUT with Real Shared Environment) on Mock-Real Spectrum
    l4_staging: marks tests as L4 staging tests (staging environment required)
    L5: marks tests as L5 (Real Production) on Mock-Real Spectrum
    fast: marks tests as fast tests (quick execution tests)
    fast_test: marks tests optimized for performance (iteration 61-70)
    monitoring: marks tests for monitoring and observability systems
    agent: marks tests for agent functionality and performance
    websocket: marks tests for WebSocket functionality
    api: marks tests for API endpoints and routes
    cache: marks tests for caching functionality
    security: marks tests for security validation
    recovery: marks tests for error recovery and resilience
    agent_state: marks tests for agent state management
    workflow: marks tests for workflow functionality
    cycle_51: marks tests for cycle 51 validation
    cycle_52: marks tests for cycle 52 validation
    cycle_53: marks tests for cycle 53 validation
    cycle_54: marks tests for cycle 54 validation
    cycle_55: marks tests for cycle 55 validation
    config: marks tests for configuration validation
    critical_path: marks tests as critical path tests (core business functionality)
    health: marks tests for health monitoring and endpoint validation
    environment: marks tests for environment configuration and validation
    models: marks tests for data models and schemas
    realtime: marks tests for real-time features like WebSocket communication
    mission_critical: marks tests as mission critical for system operations
    business_value: marks tests for business value validation and delivery
    orchestration: marks tests for agent orchestration and coordination
    multi_user: marks tests for multi-user scenarios and isolation
    enterprise: marks tests for enterprise tier functionality
    shared_components: marks tests for shared configuration components
    cross_service: marks tests for cross-service functionality
    comprehensive: marks tests as comprehensive test coverage
    user_context: marks tests for UserExecutionContext functionality
    auth_validation: marks tests for authentication and authorization validation
    auth_required: marks tests that require authentication service to be running
    startup_init: marks tests for system startup INIT phase validation
    startup_dependencies: marks tests for system startup DEPENDENCIES phase validation
    startup_database: marks tests for system startup DATABASE phase validation
    startup_cache: marks tests for system startup CACHE phase validation

[coverage:run]
source = netra_backend.app

[coverage:report]
# Regexes for lines to exclude from consideration
exclude_lines =
    # Have to re-enable the standard pragma
    pragma: no cover

    # Don't complain about missing debug-only code:
    def __repr__(self):
    def __str__(self):

    # Don't complain if tests don't hit defensive assertion code:
    raise AssertionError
    raise NotImplementedError

    # Don't complain if non-runnable code isn't run:
    if __name__ == .__main__.:

    # Don't complain about abstract methods, they aren't run:
    @abstractmethod

ignore_errors = True
