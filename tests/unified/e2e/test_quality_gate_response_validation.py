"""Quality Gate Response Validation Integration Test

Business Value Justification (BVJ):
- Segment: Enterprise ($25K MRR protection)
- Business Goal: Quality Assurance for AI Response Standards
- Value Impact: Protects enterprise customers from AI response quality degradation
- Revenue Impact: Prevents $25K MRR churn from poor quality responses, ensures enterprise SLA compliance

Test Overview:
Validates QualityGateService integration with agent responses, including specificity,
actionability, and content scoring across different quality levels and failure scenarios.
Uses real QualityGateService components with minimal external dependency mocking.
"""

import asyncio
import pytest
import os
from typing import Dict, List, Any, Optional
from datetime import datetime, UTC
from unittest.mock import AsyncMock, patch

# Set testing environment before imports
os.environ["TESTING"] = "1"
os.environ["ENVIRONMENT"] = "testing"
os.environ["DATABASE_URL"] = "sqlite+aiosqlite:///:memory:"

from app.services.quality_gate_service import QualityGateService
from app.services.quality_gate.quality_gate_models import (
    ContentType, QualityLevel, QualityMetrics, ValidationResult
)
from app.redis_manager import RedisManager
from app.logging_config import central_logger

logger = central_logger.get_logger(__name__)


def mock_justified(reason: str):
    """Mock justification decorator per SPEC/testing.xml"""
    def decorator(func):
        func._mock_justification = reason
        return func
    return decorator


class TestQualityGateResponseValidation:
    """Integration test for quality gate response validation system"""

    @pytest.fixture
    @mock_justified("Redis is external dependency - focus on quality gate logic, not Redis infrastructure")
    async def redis_manager(self):
        """Create mocked Redis manager for testing"""
        redis_mock = AsyncMock(spec=RedisManager)
        redis_mock.set = AsyncMock()
        redis_mock.get = AsyncMock(return_value=None)
        redis_mock.delete = AsyncMock()
        return redis_mock

    @pytest.fixture
    async def quality_service(self, redis_manager):
        """Create quality gate service with dependencies"""
        return QualityGateService(redis_manager=redis_manager)

    @pytest.mark.asyncio
    async def test_agent_response_specificity_validation(self, quality_service):
        """Test quality gate validates agent response specificity scores"""
        test_responses = [
            {
                "content": "GPU memory optimized from 24GB to 16GB (33% reduction). Inference latency decreased 35ms. Cost savings: $2,400/month.",
                "expected_specificity_min": 0.6,
                "expected_quality": QualityLevel.GOOD
            },
            {
                "content": "Memory usage was reduced through optimization techniques.",
                "expected_specificity_max": 0.4,
                "expected_quality": QualityLevel.POOR
            },
            {
                "content": "Generally speaking, optimization might be beneficial.",
                "expected_specificity_max": 0.3,
                "expected_quality": QualityLevel.UNACCEPTABLE
            }
        ]

        validation_results = []
        for response in test_responses:
            result = await quality_service.validate_content(
                content=response["content"],
                content_type=ContentType.OPTIMIZATION,
                context={"test_type": "specificity_validation"}
            )

            # Verify validation completed
            assert isinstance(result, ValidationResult)
            assert hasattr(result.metrics, 'specificity_score')

            # Check specificity thresholds
            if "expected_specificity_min" in response:
                assert result.metrics.specificity_score >= response["expected_specificity_min"]
                assert result.metrics.quality_level.value in ["good", "excellent"]
            elif "expected_specificity_max" in response:
                assert result.metrics.specificity_score <= response["expected_specificity_max"]
                assert result.metrics.quality_level.value in ["poor", "unacceptable"]

            validation_results.append({
                "content_length": len(response["content"]),
                "specificity_score": result.metrics.specificity_score,
                "quality_level": result.metrics.quality_level.value,
                "validation_passed": result.passed
            })

        # Verify different responses produce different specificity scores
        scores = [r["specificity_score"] for r in validation_results]
        assert len(set(scores)) > 1, "Different content should produce different specificity scores"

        logger.info(f"Specificity validation completed for {len(test_responses)} responses")

    @pytest.mark.asyncio  
    async def test_agent_response_actionability_validation(self, quality_service):
        """Test quality gate validates agent response actionability"""
        actionability_tests = [
            {
                "content": "Step 1: Implement gradient checkpointing. Step 2: Deploy to production cluster. Step 3: Monitor memory usage metrics.",
                "expected_actionability_min": 0.7,
                "content_type": ContentType.ACTION_PLAN
            },
            {
                "content": "Database queries optimized using index analysis: CREATE INDEX idx_user_timestamp ON users(created_at); Expected 60% improvement.",
                "expected_actionability_min": 0.6,
                "content_type": ContentType.OPTIMIZATION
            },
            {
                "content": "Consider optimizing performance when possible.",
                "expected_actionability_max": 0.3,
                "content_type": ContentType.OPTIMIZATION
            }
        ]

        actionability_results = []
        for test in actionability_tests:
            result = await quality_service.validate_content(
                content=test["content"],
                content_type=test["content_type"],
                context={"test_type": "actionability_validation"}
            )

            # Verify actionability scoring
            assert hasattr(result.metrics, 'actionability_score')
            
            if "expected_actionability_min" in test:
                assert result.metrics.actionability_score >= test["expected_actionability_min"]
                assert result.passed == True
            elif "expected_actionability_max" in test:
                assert result.metrics.actionability_score <= test["expected_actionability_max"]
                assert result.passed == False

            actionability_results.append({
                "content_type": test["content_type"].value,
                "actionability_score": result.metrics.actionability_score,
                "validation_passed": result.passed
            })

        logger.info(f"Actionability validation completed for {len(actionability_tests)} responses")

    @pytest.mark.asyncio
    async def test_quality_level_classification_accuracy(self, quality_service):
        """Test quality gate accurately classifies responses into quality levels"""
        classification_tests = [
            {
                "content": "GPU cluster: 45% to 89% utilization (+44 pp). Memory: 32GB to 20GB (37.5% reduction). Savings: $8,400/month infrastructure cost.",
                "expected_levels": [QualityLevel.EXCELLENT, QualityLevel.GOOD],
                "should_pass": True
            },
            {
                "content": "Query performance improved from 850ms to 180ms (78.8% improvement) using B-tree indexing on user_id column.",
                "expected_levels": [QualityLevel.GOOD, QualityLevel.ACCEPTABLE],
                "should_pass": True
            },
            {
                "content": "Database performance was improved through various optimizations.",
                "expected_levels": [QualityLevel.POOR, QualityLevel.ACCEPTABLE],
                "should_pass": False
            },
            {
                "content": "Things work better now.",
                "expected_levels": [QualityLevel.UNACCEPTABLE, QualityLevel.POOR],
                "should_pass": False
            }
        ]

        classification_results = []
        for test in classification_tests:
            result = await quality_service.validate_content(
                content=test["content"],
                content_type=ContentType.OPTIMIZATION,
                context={"test_type": "classification_accuracy"}
            )

            # Verify classification accuracy
            assert result.metrics.quality_level in test["expected_levels"]
            assert result.passed == test["should_pass"]

            # Verify classification metadata
            assert result.metrics.overall_score >= 0.0
            assert result.metrics.overall_score <= 1.0

            classification_results.append({
                "expected_range": [level.value for level in test["expected_levels"]],
                "actual_level": result.metrics.quality_level.value,
                "overall_score": result.metrics.overall_score,
                "classification_correct": result.metrics.quality_level in test["expected_levels"]
            })

        # Verify all classifications were accurate
        correct_classifications = sum(1 for r in classification_results if r["classification_correct"])
        accuracy_rate = correct_classifications / len(classification_results)
        assert accuracy_rate >= 0.95, f"Classification accuracy {accuracy_rate:.2f} below 95% threshold"

        logger.info(f"Quality level classification accuracy: {accuracy_rate:.2f}")

    @pytest.mark.asyncio
    async def test_content_scoring_integration(self, quality_service):
        """Test integrated content scoring across multiple quality dimensions"""
        scoring_test_cases = [
            {
                "content": "Latency optimization: 200ms→95ms (52.5% improvement). Memory: 24GB→16GB (33% reduction). GPU utilization: 65%→89% (+24pp).",
                "expected_min_scores": {
                    "specificity": 0.7,
                    "actionability": 0.5,
                    "quantification": 0.8,
                    "overall": 0.6
                }
            },
            {
                "content": "Implement caching layer with Redis. Configure TTL=3600s. Expected 40% response time improvement.",
                "expected_min_scores": {
                    "specificity": 0.6,
                    "actionability": 0.7,
                    "quantification": 0.4,
                    "overall": 0.5
                }
            }
        ]

        integrated_scores = []
        for test_case in scoring_test_cases:
            result = await quality_service.validate_content(
                content=test_case["content"],
                content_type=ContentType.OPTIMIZATION,
                context={"test_type": "integrated_scoring"}
            )

            # Verify all scoring dimensions
            metrics = result.metrics
            expected = test_case["expected_min_scores"]

            assert metrics.specificity_score >= expected["specificity"]
            assert metrics.actionability_score >= expected["actionability"] 
            assert metrics.quantification_score >= expected["quantification"]
            assert metrics.overall_score >= expected["overall"]

            # Verify score consistency
            assert metrics.overall_score <= 1.0
            assert all(score >= 0.0 for score in [
                metrics.specificity_score,
                metrics.actionability_score,
                metrics.quantification_score
            ])

            integrated_scores.append({
                "content_preview": test_case["content"][:50] + "...",
                "specificity": metrics.specificity_score,
                "actionability": metrics.actionability_score,
                "quantification": metrics.quantification_score,
                "overall": metrics.overall_score
            })

        logger.info(f"Integrated scoring validated for {len(scoring_test_cases)} test cases")

    @pytest.mark.asyncio
    async def test_failure_scenario_handling(self, quality_service):
        """Test quality gate handles various failure scenarios appropriately"""
        failure_scenarios = [
            {
                "content": "",  # Empty content
                "scenario_type": "empty_content",
                "expected_result": "fail_with_error"
            },
            {
                "content": "a" * 10000,  # Extremely long content
                "scenario_type": "excessive_length",
                "expected_result": "process_or_truncate"
            },
            {
                "content": "Optimization optimization optimization optimization optimization.",
                "scenario_type": "circular_reasoning",
                "expected_result": "fail_with_suggestions"
            }
        ]

        failure_results = []
        for scenario in failure_scenarios:
            try:
                result = await quality_service.validate_content(
                    content=scenario["content"],
                    content_type=ContentType.OPTIMIZATION,
                    context={"test_type": "failure_scenario", "scenario": scenario["scenario_type"]}
                )

                # Verify failure handling
                assert isinstance(result, ValidationResult)

                if scenario["expected_result"] == "fail_with_error":
                    assert result.passed == False
                    assert len(result.metrics.issues) > 0
                elif scenario["expected_result"] == "process_or_truncate":
                    # Should process but may have lower quality
                    assert result.metrics.quality_level in [QualityLevel.POOR, QualityLevel.UNACCEPTABLE]
                elif scenario["expected_result"] == "fail_with_suggestions":
                    assert result.passed == False
                    assert result.retry_suggested == True
                    assert "circular" in str(result.metrics.issues).lower() or "repetitive" in str(result.metrics.issues).lower()

                failure_results.append({
                    "scenario": scenario["scenario_type"],
                    "processed_successfully": True,
                    "validation_passed": result.passed,
                    "issues_detected": len(result.metrics.issues)
                })

            except Exception as e:
                # Some scenarios may raise exceptions - capture for analysis
                failure_results.append({
                    "scenario": scenario["scenario_type"],
                    "processed_successfully": False,
                    "exception": str(e)
                })

        # Verify all scenarios were handled (either processed or gracefully failed)
        assert len(failure_results) == len(failure_scenarios)
        processed_count = sum(1 for r in failure_results if r["processed_successfully"])
        assert processed_count >= len(failure_scenarios) * 0.8  # At least 80% should process

        logger.info(f"Failure scenario handling validated for {len(failure_scenarios)} scenarios")

    @pytest.mark.asyncio
    async def test_quality_gate_metrics_collection(self, quality_service):
        """Test quality gate collects and aggregates validation metrics"""
        metrics_test_data = [
            ("GPU memory: 24GB→16GB (33% reduction)", ContentType.OPTIMIZATION),
            ("Database queries: 500ms→150ms (70% faster)", ContentType.DATA_ANALYSIS),
            ("Deploy model to 8 nodes with monitoring", ContentType.ACTION_PLAN),
            ("Analysis shows significant performance gains", ContentType.REPORT)
        ]

        collected_metrics = []
        for content, content_type in metrics_test_data:
            result = await quality_service.validate_content(
                content=content,
                content_type=content_type,
                context={"test_type": "metrics_collection"}
            )

            collected_metrics.append({
                "content_type": content_type.value,
                "overall_score": result.metrics.overall_score,
                "quality_level": result.metrics.quality_level.value,
                "word_count": result.metrics.word_count,
                "validation_passed": result.passed
            })

        # Test metrics aggregation
        for content_type in [ContentType.OPTIMIZATION, ContentType.DATA_ANALYSIS]:
            stats = await quality_service.get_quality_stats(content_type)
            
            if content_type.value in stats:
                type_stats = stats[content_type.value]
                
                # Verify metrics structure
                assert "count" in type_stats
                assert "avg_score" in type_stats
                assert "failure_rate" in type_stats
                assert "quality_distribution" in type_stats
                
                # Verify data integrity
                assert type_stats["count"] >= 1
                assert 0.0 <= type_stats["avg_score"] <= 1.0

        # Test cache functionality
        cache_stats = quality_service.get_cache_stats()
        assert cache_stats["cache_size"] >= 0
        assert cache_stats["metrics_history_size"] >= len(collected_metrics)

        logger.info(f"Metrics collection validated for {len(collected_metrics)} validations")

    @pytest.mark.asyncio
    async def test_concurrent_quality_validation(self, quality_service):
        """Test quality gate handles concurrent validation requests"""
        concurrent_test_data = [
            "GPU optimization: 45%→85% utilization (+40pp)",
            "Database indexing: 850ms→180ms (78.8% improvement)",
            "Memory allocation: 32GB→20GB (37.5% reduction)",
            "Cache implementation: 40% response time improvement",
            "Query optimization: B-tree indexing on primary keys"
        ] * 4  # 20 concurrent validations

        # Execute concurrent validations
        start_time = datetime.now(UTC)
        validation_tasks = [
            quality_service.validate_content(
                content=content,
                content_type=ContentType.OPTIMIZATION,
                context={"test_type": "concurrent_validation", "batch_id": i}
            )
            for i, content in enumerate(concurrent_test_data)
        ]

        concurrent_results = await asyncio.gather(*validation_tasks)
        end_time = datetime.now(UTC)

        # Verify concurrent processing
        assert len(concurrent_results) == len(concurrent_test_data)
        assert all(isinstance(result, ValidationResult) for result in concurrent_results)

        # Verify performance under load
        processing_time = (end_time - start_time).total_seconds()
        assert processing_time < 10.0  # Should complete within reasonable time

        # Verify result consistency
        passed_count = sum(1 for result in concurrent_results if result.passed)
        quality_levels = [result.metrics.quality_level.value for result in concurrent_results]
        unique_levels = set(quality_levels)

        assert passed_count > 0  # Some should pass
        assert len(unique_levels) > 1  # Should have varied quality assessment

        logger.info(f"Concurrent validation: {len(concurrent_results)} requests in {processing_time:.2f}s")

    @pytest.mark.asyncio
    async def test_enterprise_quality_standards_compliance(self, quality_service):
        """Test quality gate enforces enterprise-grade quality standards"""
        enterprise_responses = [
            {
                "content": "GPU cluster optimization: 52%→89% utilization (+37pp). Memory: 24GB→16GB (33% reduction). Cost savings: $8,400/month infrastructure reduction.",
                "tier": "enterprise_premium",
                "expected_pass": True
            },
            {
                "content": "Database query performance: 850ms→180ms (78.8% improvement). Throughput: 1,200→3,400 QPS (183% increase). Index optimization on user_id column.",
                "tier": "enterprise_standard", 
                "expected_pass": True
            },
            {
                "content": "Model deployment optimized through various techniques resulting in better performance.",
                "tier": "below_enterprise",
                "expected_pass": False
            }
        ]

        enterprise_results = []
        for response in enterprise_responses:
            # Test with strict enterprise mode
            result = await quality_service.validate_content(
                content=response["content"],
                content_type=ContentType.OPTIMIZATION,
                strict_mode=True,  # Enterprise-grade validation
                context={"test_type": "enterprise_compliance", "tier": response["tier"]}
            )

            enterprise_results.append({
                "tier": response["tier"],
                "expected_pass": response["expected_pass"],
                "actual_pass": result.passed,
                "quality_level": result.metrics.quality_level.value,
                "overall_score": result.metrics.overall_score,
                "meets_expectation": result.passed == response["expected_pass"]
            })

        # Verify enterprise compliance
        compliance_rate = sum(1 for r in enterprise_results if r["meets_expectation"]) / len(enterprise_results)
        assert compliance_rate >= 0.8, f"Enterprise compliance rate {compliance_rate:.2f} below 80% threshold"

        # Verify strict mode differentiation
        premium_scores = [r["overall_score"] for r in enterprise_results if r["tier"] == "enterprise_premium"]
        below_scores = [r["overall_score"] for r in enterprise_results if r["tier"] == "below_enterprise"]
        
        if premium_scores and below_scores:
            assert max(below_scores) < min(premium_scores), "Enterprise content should score higher than below-standard content"

        logger.info(f"Enterprise quality standards compliance: {compliance_rate:.2f}")