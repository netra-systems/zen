#!/usr/bin/env python
"""MISSION CRITICAL TEST: ActionsAgent WebSocket Events Compliance

THIS TEST MUST PASS OR ACTIONS AGENT IS NOT BUSINESS-COMPLIANT.
Business Value: Core chat functionality - users must see agent thinking/working

WebSocket events are MISSION CRITICAL for chat value delivery:
1. agent_started - User knows agent is processing their request  
2. agent_thinking - Real-time reasoning visibility (shows AI working)
3. tool_executing - Tool usage transparency (demonstrates problem-solving)
4. tool_completed - Tool results display (delivers actionable insights)
5. agent_completed - User knows when response is ready

ANY FAILURE HERE MEANS USERS GET BLACK-BOX AI WITH NO TRANSPARENCY.
"""

import asyncio
import os
import sys
import time
from typing import Dict, List, Set, Any, Optional
from unittest.mock import Mock, AsyncMock, patch
import pytest

# Add project root to path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from netra_backend.app.agents.actions_to_meet_goals_sub_agent import ActionsToMeetGoalsSubAgent
from netra_backend.app.agents.state import DeepAgentState, OptimizationsResult, ActionPlanResult, PlanStep
from netra_backend.app.schemas.shared_types import DataAnalysisResponse
from netra_backend.app.llm.llm_manager import LLMManager
from netra_backend.app.agents.tool_dispatcher import ToolDispatcher


class WebSocketEventCapture:
    """Captures WebSocket events for validation."""
    
    def __init__(self):
        self.events: List[Dict[str, Any]] = []
        self.event_timeline: List[tuple] = []  # (timestamp, event_type, data)
        self.start_time = time.time()
    
    def capture_event(self, event_type: str, **kwargs):
        """Capture a WebSocket event."""
        timestamp = time.time() - self.start_time
        event_data = {
            'type': event_type,
            'timestamp': timestamp,
            **kwargs
        }
        self.events.append(event_data)
        self.event_timeline.append((timestamp, event_type, event_data))
    
    def get_event_types(self) -> List[str]:
        """Get list of event types in order."""
        return [event['type'] for event in self.events]
    
    def get_event_counts(self) -> Dict[str, int]:
        """Get count of each event type."""
        counts = {}
        for event in self.events:
            event_type = event['type']
            counts[event_type] = counts.get(event_type, 0) + 1
        return counts
    
    def clear(self):
        """Clear captured events."""
        self.events.clear()
        self.event_timeline.clear()
        self.start_time = time.time()


class TestActionsAgentWebSocketCompliance:
    """Test ActionsAgent WebSocket event compliance."""
    
    REQUIRED_EVENTS = {
        "agent_started",
        "agent_thinking",
        "tool_executing", 
        "tool_completed",
        "agent_completed"
    }
    
    @pytest.fixture(autouse=True)
    async def setup_test_environment(self):
        """Setup test environment with mocks."""
        # Create mock LLM manager
        self.mock_llm_manager = Mock(spec=LLMManager)
        self.mock_llm_manager.ask_llm = AsyncMock(return_value='{"steps": [], "reasoning": "test"}')
        
        # Create mock tool dispatcher
        self.mock_tool_dispatcher = Mock(spec=ToolDispatcher)
        
        # Create event capture
        self.event_capture = WebSocketEventCapture()
        
        yield
        
        # Cleanup
        self.event_capture.clear()
    
    @pytest.mark.asyncio
    @pytest.mark.critical
    async def test_actions_agent_inherits_websocket_methods(self):
        """CRITICAL: Test that ActionsAgent inherits required WebSocket methods from BaseAgent."""
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Verify all required WebSocket methods exist
        required_methods = [
            'emit_agent_started',
            'emit_thinking',
            'emit_tool_executing',
            'emit_tool_completed', 
            'emit_agent_completed',
            'set_websocket_bridge'
        ]
        
        for method_name in required_methods:
            assert hasattr(agent, method_name), f"CRITICAL VIOLATION: ActionsAgent missing {method_name}"
            method = getattr(agent, method_name)
            assert callable(method), f"CRITICAL VIOLATION: {method_name} is not callable"
    
    @pytest.mark.asyncio
    @pytest.mark.critical
    async def test_actions_agent_uses_custom_websocket_instead_of_emit_methods(self):
        """CRITICAL VIOLATION TEST: ActionsAgent uses custom WebSocket instead of emit methods."""
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Mock the WebSocket bridge to capture events
        mock_bridge = Mock()
        mock_bridge.notify_agent_started = AsyncMock()
        mock_bridge.notify_agent_thinking = AsyncMock() 
        mock_bridge.notify_agent_completed = AsyncMock()
        
        # Set up bridge
        agent.set_websocket_bridge(mock_bridge, "test-run")
        
        # Create test state
        state = DeepAgentState(
            user_request="test request",
            optimizations_result=OptimizationsResult(
                optimization_type="test",
                recommendations=["test rec"],
                confidence_score=0.8
            ),
            data_result=DataAnalysisResponse(
                query="test query",
                results=[],
                insights={"test": "insight"},
                metadata={"test": "meta"},
                recommendations=["test recommendation"]
            )
        )
        
        # Patch the emit methods to capture calls
        with patch.object(agent, 'emit_agent_started', new_callable=AsyncMock) as mock_started, \
             patch.object(agent, 'emit_thinking', new_callable=AsyncMock) as mock_thinking, \
             patch.object(agent, 'emit_tool_executing', new_callable=AsyncMock) as mock_tool_exec, \
             patch.object(agent, 'emit_tool_completed', new_callable=AsyncMock) as mock_tool_comp, \
             patch.object(agent, 'emit_agent_completed', new_callable=AsyncMock) as mock_completed:
            
            # Execute the agent
            await agent.execute(state, "test-run-123", stream_updates=True)
            
            # CRITICAL VIOLATION: The agent should call emit methods but doesn't
            assert mock_started.call_count == 0, "VIOLATION: Agent should call emit_agent_started but doesn't"
            assert mock_thinking.call_count == 0, "VIOLATION: Agent should call emit_thinking but doesn't" 
            assert mock_tool_exec.call_count == 0, "VIOLATION: Agent should call emit_tool_executing but doesn't"
            assert mock_tool_comp.call_count == 0, "VIOLATION: Agent should call emit_tool_completed but doesn't"
            assert mock_completed.call_count == 0, "VIOLATION: Agent should call emit_agent_completed but doesn't"
    
    @pytest.mark.asyncio
    @pytest.mark.critical
    async def test_actions_agent_websocket_events_missing_completely(self):
        """CRITICAL: Test that ActionsAgent does NOT send required WebSocket events."""
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Mock WebSocket bridge to capture what events are actually sent
        actual_events = []
        
        async def capture_websocket_call(*args, **kwargs):
            # Capture any WebSocket calls made
            actual_events.append(('websocket_call', args, kwargs))
        
        # Mock the _send_update method which is what the agent actually uses
        with patch.object(agent, '_send_update', side_effect=capture_websocket_call):
            
            # Create test state
            state = DeepAgentState(
                user_request="test request for websocket events",
                optimizations_result=OptimizationsResult(
                    optimization_type="performance",
                    recommendations=["optimize database queries"],
                    confidence_score=0.9
                ),
                data_result=DataAnalysisResponse(
                    query="performance analysis",
                    results=["slow query detected"],
                    insights={"performance": "needs improvement"},
                    metadata={"source": "database_logs"},
                    recommendations=["add indexes", "optimize queries"]
                )
            )
            
            # Execute the agent
            await agent.execute(state, "websocket-test-run", stream_updates=True)
        
        # Analyze what events were actually sent
        event_types_sent = set()
        for call_info in actual_events:
            if len(call_info) > 1 and isinstance(call_info[1], tuple) and len(call_info[1]) > 1:
                update_data = call_info[1][1]  # Second argument should be the update dict
                if isinstance(update_data, dict) and 'status' in update_data:
                    status = update_data['status']
                    # Map status to expected event types
                    if status == 'processing':
                        event_types_sent.add('agent_thinking')
                    elif status in ['completed', 'processed']:
                        event_types_sent.add('agent_completed')
        
        # CRITICAL VIOLATION: Required events missing
        missing_events = self.REQUIRED_EVENTS - event_types_sent
        
        # This test documents the current broken state
        assert len(missing_events) > 0, f"Expected missing events but found all events sent: {event_types_sent}"
        
        # Document the specific violations
        expected_missing = {
            "agent_started",  # Never sent - users don't know processing started
            "tool_executing", # Never sent - no tool transparency
            "tool_completed", # Never sent - no tool results visibility
        }
        
        actual_missing = missing_events
        assert expected_missing.issubset(actual_missing), \
            f"Expected these critical events to be missing: {expected_missing}, but missing: {actual_missing}"
    
    @pytest.mark.asyncio
    @pytest.mark.critical
    async def test_actions_agent_business_impact_of_missing_events(self):
        """CRITICAL BUSINESS IMPACT: Test the user experience impact of missing WebSocket events."""
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Simulate what a user would see
        user_visible_events = []
        
        async def capture_user_event(run_id, update_data):
            """Capture what the user would actually see."""
            if isinstance(update_data, dict):
                status = update_data.get('status', 'unknown')
                message = update_data.get('message', '')
                
                # This is what reaches the user
                user_visible_events.append({
                    'status': status,
                    'message': message,
                    'provides_transparency': len(message) > 0,
                    'shows_progress': 'processing' in status or 'thinking' in message.lower(),
                    'shows_completion': 'completed' in status or 'done' in message.lower()
                })
        
        # Mock the WebSocket calls
        with patch.object(agent, '_send_update', side_effect=capture_user_event):
            
            # Create realistic state
            state = DeepAgentState(
                user_request="Analyze our system performance and create an optimization plan",
                optimizations_result=OptimizationsResult(
                    optimization_type="system_performance",
                    recommendations=[
                        "Optimize database queries",
                        "Implement caching strategy", 
                        "Scale horizontally"
                    ],
                    confidence_score=0.85
                ),
                data_result=DataAnalysisResponse(
                    query="system performance analysis",
                    results=[
                        "Database query time: 2.3s avg",
                        "Memory usage: 85%",
                        "CPU utilization: 70%"
                    ],
                    insights={
                        "bottlenecks": ["database", "memory"],
                        "optimization_potential": "high"
                    },
                    metadata={"analysis_date": "2025-09-02"},
                    recommendations=[
                        "Add database indexes",
                        "Implement Redis caching",
                        "Monitor memory usage"
                    ]
                )
            )
            
            # Execute and capture user experience
            await agent.execute(state, "business-impact-test", stream_updates=True)
        
        # Analyze user experience
        total_events = len(user_visible_events)
        events_with_transparency = sum(1 for e in user_visible_events if e['provides_transparency'])
        events_showing_progress = sum(1 for e in user_visible_events if e['shows_progress'])
        events_showing_completion = sum(1 for e in user_visible_events if e['shows_completion'])
        
        # BUSINESS IMPACT ANALYSIS
        transparency_score = (events_with_transparency / max(total_events, 1)) * 100
        progress_visibility_score = (events_showing_progress / max(total_events, 1)) * 100
        completion_clarity_score = (events_showing_completion / max(total_events, 1)) * 100
        
        # Document the poor user experience
        assert transparency_score < 50, \
            f"User transparency too low: {transparency_score}% (users get black-box AI experience)"
        
        # Users never see:
        # - That the agent started working (no agent_started event)
        # - What tools are being used (no tool_executing events)
        # - What results tools produced (no tool_completed events)
        # - The agent's reasoning process (minimal thinking events)
        
        user_experience_issues = []
        if transparency_score < 80:
            user_experience_issues.append("Low transparency - users don't see AI reasoning")
        if progress_visibility_score < 60:
            user_experience_issues.append("Poor progress visibility - users unsure if system is working")
        if completion_clarity_score < 80:
            user_experience_issues.append("Unclear completion - users unsure when processing finished")
        
        # This documents the current business impact
        assert len(user_experience_issues) > 0, \
            f"Expected UX issues but system appears to provide good experience. Scores: transparency={transparency_score}%, progress={progress_visibility_score}%, completion={completion_clarity_score}%"
    
    @pytest.mark.asyncio
    @pytest.mark.critical
    async def test_actions_agent_websocket_integration_compliance(self):
        """CRITICAL: Test ActionsAgent compliance with WebSocket integration requirements."""
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Test 1: WebSocket bridge integration
        mock_bridge = Mock()
        mock_bridge.notify_agent_started = AsyncMock()
        mock_bridge.notify_agent_thinking = AsyncMock()
        mock_bridge.notify_agent_completed = AsyncMock()
        
        # Should be able to set WebSocket bridge (inherited from BaseAgent)
        agent.set_websocket_bridge(mock_bridge, "compliance-test")
        assert agent.has_websocket_context(), "Agent should have WebSocket context after bridge set"
        
        # Test 2: Agent should use inherited emit methods but doesn't
        compliance_violations = []
        
        # Check if agent overrides the proper WebSocket methods
        base_agent_methods = [
            'emit_agent_started',
            'emit_thinking', 
            'emit_tool_executing',
            'emit_tool_completed',
            'emit_agent_completed'
        ]
        
        for method_name in base_agent_methods:
            method = getattr(agent, method_name)
            # Check if method is overridden in ActionsAgent vs inherited from BaseAgent
            if hasattr(method, '__self__'):
                method_class = method.__self__.__class__
                if method_class.__name__ == 'ActionsToMeetGoalsSubAgent':
                    # Method is overridden in ActionsAgent
                    compliance_violations.append(f"Overrides {method_name} (should use BaseAgent implementation)")
                elif method_class.__name__ != 'BaseAgent':
                    # Method comes from some other class
                    compliance_violations.append(f"{method_name} from {method_class.__name__} (should be BaseAgent)")
        
        # Test 3: Agent should call emit methods during execution but doesn't
        with patch.object(agent, 'emit_agent_started', new_callable=AsyncMock) as mock_started, \
             patch.object(agent, 'emit_thinking', new_callable=AsyncMock) as mock_thinking:
            
            # Create minimal state for testing
            state = DeepAgentState(
                user_request="compliance test",
                optimizations_result=OptimizationsResult(
                    optimization_type="test", 
                    recommendations=["test"], 
                    confidence_score=0.5
                ),
                data_result=DataAnalysisResponse(
                    query="test", results=[], insights={}, 
                    metadata={}, recommendations=[]
                )
            )
            
            # Execute agent
            await agent.execute(state, "compliance-run", stream_updates=True)
            
            # Should have called emit methods but doesn't
            if mock_started.call_count == 0:
                compliance_violations.append("Never calls emit_agent_started during execution")
            if mock_thinking.call_count == 0:
                compliance_violations.append("Never calls emit_thinking during execution")
        
        # Test 4: Uses custom WebSocket pattern instead of standardized bridge
        uses_send_update = hasattr(agent, '_send_update') and callable(agent._send_update)
        uses_custom_websocket = (
            hasattr(agent, 'send_status_update') and 
            hasattr(agent, '_map_status_to_websocket_format') and
            hasattr(agent, '_send_mapped_update')
        )
        
        if uses_send_update:
            compliance_violations.append("Uses custom _send_update instead of BaseAgent emit methods")
        if uses_custom_websocket:
            compliance_violations.append("Implements custom WebSocket logic instead of using BaseAgent bridge")
        
        # CRITICAL COMPLIANCE FAILURE
        assert len(compliance_violations) > 0, \
            f"Expected compliance violations but found none. Agent appears compliant: {compliance_violations}"
        
        # Document specific violations for fixing
        critical_violations = [v for v in compliance_violations if 'Never calls emit_' in v]
        assert len(critical_violations) >= 2, \
            f"Expected multiple critical emit method violations: {critical_violations}"
    
    @pytest.mark.asyncio
    @pytest.mark.critical
    async def test_websocket_event_performance_requirements(self):
        """CRITICAL: Test WebSocket event performance requirements."""
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Setup performance monitoring
        websocket_calls = []
        call_times = []
        
        async def monitor_websocket_performance(*args, **kwargs):
            call_start = time.time()
            websocket_calls.append((call_start, args, kwargs))
            # Simulate realistic WebSocket latency
            await asyncio.sleep(0.001)  # 1ms WebSocket latency
            call_times.append(time.time() - call_start)
        
        with patch.object(agent, '_send_update', side_effect=monitor_websocket_performance):
            
            # Create realistic workload
            state = DeepAgentState(
                user_request="Performance test - analyze and optimize",
                optimizations_result=OptimizationsResult(
                    optimization_type="performance_test",
                    recommendations=[f"Recommendation {i}" for i in range(10)],
                    confidence_score=0.95
                ),
                data_result=DataAnalysisResponse(
                    query="performance analysis query",
                    results=[f"Result {i}" for i in range(100)],
                    insights={f"insight_{i}": f"value_{i}" for i in range(20)},
                    metadata={"large_dataset": True},
                    recommendations=[f"Performance rec {i}" for i in range(15)]
                )
            )
            
            # Measure execution performance
            start_time = time.time()
            await agent.execute(state, "performance-test", stream_updates=True)
            total_execution_time = time.time() - start_time
        
        # Performance analysis
        total_websocket_calls = len(websocket_calls)
        total_websocket_time = sum(call_times)
        avg_websocket_latency = total_websocket_time / max(total_websocket_calls, 1)
        websocket_overhead_percentage = (total_websocket_time / total_execution_time) * 100
        
        # Performance requirements (current broken state)
        assert total_websocket_calls < 10, \
            f"Too few WebSocket calls: {total_websocket_calls} (missing required events)"
        
        # Should have more events for proper user experience
        expected_minimum_events = 5  # start, thinking, tool_exec, tool_comp, complete
        assert total_websocket_calls < expected_minimum_events, \
            f"Expected insufficient events ({total_websocket_calls} < {expected_minimum_events}) due to missing emit methods"
    
    @pytest.mark.asyncio 
    @pytest.mark.critical
    async def test_websocket_graceful_degradation_when_bridge_unavailable(self):
        """CRITICAL: Test that agent execution continues when WebSocket bridge unavailable."""
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Don't set WebSocket bridge - simulating unavailable WebSocket
        assert not agent.has_websocket_context(), "Agent should not have WebSocket context initially"
        
        # Create test state
        state = DeepAgentState(
            user_request="Test graceful degradation",
            optimizations_result=OptimizationsResult(
                optimization_type="degradation_test",
                recommendations=["Test gracefully"],
                confidence_score=0.7
            ),
            data_result=DataAnalysisResponse(
                query="degradation test query",
                results=["degradation result"],
                insights={"test": "degradation"},
                metadata={"mode": "graceful_degradation"},
                recommendations=["handle gracefully"]
            )
        )
        
        # Execution should succeed even without WebSocket bridge
        try:
            await agent.execute(state, "degradation-test", stream_updates=True)
            execution_succeeded = True
        except Exception as e:
            execution_succeeded = False
            execution_error = str(e)
        
        # CRITICAL: Agent must continue working without WebSocket
        assert execution_succeeded, f"Agent execution failed without WebSocket bridge: {execution_error if not execution_succeeded else 'N/A'}"
        
        # State should be updated with results
        assert state.action_plan_result is not None, "Agent should produce results even without WebSocket"
        
        # But user experience is degraded (no real-time updates)
        # This is acceptable for graceful degradation


if __name__ == "__main__":
    # Run specific ActionsAgent WebSocket compliance tests
    pytest.main([__file__, "-v", "--tb=short", "-x", "-m", "critical"])