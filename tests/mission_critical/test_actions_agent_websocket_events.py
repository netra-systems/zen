import os
import sys
import pytest
import asyncio
import time
from typing import Dict, Any, List
from unittest.mock import Mock, patch, AsyncMock

# Import SSOT WebSocket test utility
from test_framework.ssot.websocket_connection_test_utility import TestWebSocketConnection as WebSocketTestHelper

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from netra_backend.app.agents.actions_to_meet_goals_sub_agent import ActionsToMeetGoalsSubAgent
from netra_backend.app.schemas.agent_models import DeepAgentState
from netra_backend.app.agents.state import OptimizationsResult, ActionPlanResult, PlanStep
from netra_backend.app.schemas.shared_types import DataAnalysisResponse, PerformanceMetrics, ToolExecutionContext
from netra_backend.app.websocket_core.types import WebSocketMessage
from netra_backend.app.websocket_core.canonical_import_patterns import UnifiedWebSocketManager
from netra_backend.app.llm.llm_manager import LLMManager
from netra_backend.app.agents.tool_dispatcher import ToolDispatcher
from netra_backend.app.core.unified_error_handler import UnifiedErrorHandler
from netra_backend.app.db.database_manager import DatabaseManager
from netra_backend.app.clients.auth_client_core import AuthServiceClient
from shared.isolated_environment import get_env
from test_framework.ssot.websocket_connection_test_utility import TestWebSocketConnection


class WebSocketEventCapture:
    """Captures WebSocket events for validation."""
    
    def __init__(self):
        self.events: List[Dict[str, Any]] = []
        self.event_timeline: List[tuple] = []  # (timestamp, event_type, data)
        self.start_time = time.time()
    
    def capture_event(self, event_type: str, **kwargs):
        """Capture a WebSocket event."""
        timestamp = time.time() - self.start_time
        event_data = {
            'type': event_type,
            'timestamp': timestamp,
            **kwargs
        }
        self.events.append(event_data)
        self.event_timeline.append((timestamp, event_type, event_data))
    
    def get_event_types(self) -> List[str]:
        """Get list of event types in order."""
        return [event['type'] for event in self.events]
    
    def get_event_counts(self) -> Dict[str, int]:
        """Get count of each event type."""
        counts = {}
        for event in self.events:
            event_type = event['type']
            counts[event_type] = counts.get(event_type, 0) + 1
        return counts
    
    def clear(self):
        """Clear captured events."""
        self.events.clear()
        self.event_timeline.clear()
        self.start_time = time.time()


class ActionsAgentWebSocketComplianceTests:
    """Test ActionsAgent WebSocket event compliance."""
    
    REQUIRED_EVENTS = {
        "agent_started",
        "agent_thinking",
        "tool_executing",
        "tool_completed",
        "agent_completed"
    }
    
    @pytest.fixture(autouse=True)
    async def setup_test_environment(self):
        """Setup test environment with mocks."""
        # Create mock LLM manager
        self.mock_llm_manager = Mock(spec=LLMManager)
        self.mock_llm_manager.ask_llm = AsyncMock(return_value='{"steps": [], "reasoning": "test"}')
        
        # Create mock tool dispatcher
        self.mock_tool_dispatcher = Mock(spec=ToolDispatcher)
        
        # Create event capture
        self.event_capture = WebSocketEventCapture()
        
        yield
        
        # Cleanup
        self.event_capture.clear()
    
    @pytest.mark.asyncio
    @pytest.mark.critical
    async def test_actions_agent_inherits_websocket_methods(self):
        """CRITICAL: Test that ActionsAgent inherits required WebSocket methods from BaseAgent."""
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Verify all required WebSocket methods exist
        required_methods = [
            'emit_agent_started',
            'emit_thinking',
            'emit_tool_executing',
            'emit_tool_completed', 
            'emit_agent_completed',
            'set_websocket_bridge'
        ]
        
        for method_name in required_methods:
            assert hasattr(agent, method_name), f"CRITICAL VIOLATION: ActionsAgent missing {method_name}"
            method = getattr(agent, method_name)
            assert callable(method), f"CRITICAL VIOLATION: {method_name} is not callable"
    
    @pytest.mark.asyncio
    @pytest.mark.critical
    async def test_actions_agent_uses_custom_websocket_instead_of_emit_methods(self):
        "CRITICAL VIOLATION TEST: ActionsAgent uses custom WebSocket instead of emit methods."
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Mock the WebSocket bridge to capture events
        websocket = WebSocketTestHelper()  # Real WebSocket implementation
        
        # Create mock bridge
        mock_bridge = Mock()
        
        # Set up bridge
        agent.set_websocket_bridge(mock_bridge, "test-run")
        
        # Create test state
        state = DeepAgentState(
            user_request="test request",
            optimizations_result=OptimizationsResult(
                optimization_type="test",
                recommendations=["test rec"],
                confidence_score=0.8
            ),
            data_result=DataAnalysisResponse(
                analysis_id="test-analysis-1",
                status="completed",
                results={"test": "result"},
                metrics=PerformanceMetrics(duration_ms=100.0),
                created_at=time.time()
            )
        )
        
        # Patch the emit methods to capture calls
        with patch.object(agent, 'emit_agent_started', new_callable=AsyncMock) as mock_started, \
             patch.object(agent, 'emit_thinking', new_callable=AsyncMock) as mock_thinking, \
             patch.object(agent, 'emit_tool_executing', new_callable=AsyncMock) as mock_tool_exec, \
             patch.object(agent, 'emit_tool_completed', new_callable=AsyncMock) as mock_tool_comp, \
             patch.object(agent, 'emit_agent_completed', new_callable=AsyncMock) as mock_completed:
            
            # Execute the agent
            await agent.execute(state, "test-run-123", stream_updates=True)
            
            # CRITICAL VIOLATION: The agent should call emit methods but doesn't
            assert mock_started.call_count == 0, "VIOLATION: Agent should call emit_agent_started but doesn't"
            assert mock_thinking.call_count == 0, "VIOLATION: Agent should call emit_thinking but doesn't"
            assert mock_tool_exec.call_count == 0, "VIOLATION: Agent should call emit_tool_executing but doesn't"
            assert mock_tool_comp.call_count == 0, "VIOLATION: Agent should call emit_tool_completed but doesn't"
            assert mock_completed.call_count == 0, "VIOLATION: Agent should call emit_agent_completed but doesn't"
    
    @pytest.mark.asyncio
    @pytest.mark.critical
    async def test_actions_agent_websocket_events_missing_completely(self):
        "CRITICAL: Test that ActionsAgent does NOT send required WebSocket events."
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Mock WebSocket bridge to capture what events are actually sent
        actual_events = []
        
        async def capture_websocket_call(*args, **kwargs):
            # Capture any WebSocket calls made
            actual_events.append(('websocket_call', args, kwargs))
        
        # Mock the _send_update method which is what the agent actually uses
        with patch.object(agent, '_send_update', side_effect=capture_websocket_call):
            
            # Create test state
            state = DeepAgentState(
                user_request="test request for websocket events",
                optimizations_result=OptimizationsResult(
                    optimization_type="performance",
                    recommendations=["optimize database queries"],
                    confidence_score=0.9
                ),
                data_result=DataAnalysisResponse(
                    analysis_id="performance-analysis-1",
                    status="completed",
                    results={"queries": ["slow query detected"]},
                    metrics=PerformanceMetrics(duration_ms=250.0),
                    created_at=time.time()
                )
            )
            
            # Execute the agent
            await agent.execute(state, "websocket-test-run", stream_updates=True)
        
        # Analyze what events were actually sent
        event_types_sent = set()
        for call_info in actual_events:
            if len(call_info) > 1 and isinstance(call_info[1], tuple) and len(call_info[1]) > 1:
                update_data = call_info[1][1]  # Second argument should be the update dict
                if isinstance(update_data, dict) and 'status' in update_data:
                    status = update_data['status']
                    # Map status to expected event types
                    if status == 'processing':
                        event_types_sent.add('agent_thinking')
                    elif status in ['completed', 'processed']:
                        event_types_sent.add('agent_completed')
        
        # CRITICAL VIOLATION: Required events missing
        missing_events = self.REQUIRED_EVENTS - event_types_sent
        
        # This test documents the current broken state
        assert len(missing_events) > 0, fExpected missing events but found all events sent: {event_types_sent}"
        assert len(missing_events) > 0, fExpected missing events but found all events sent: {event_types_sent}"
        
        # Document the specific violations
        expected_missing = {
            "agent_started,  # Never sent - users don't know processing started"
            tool_executing, # Never sent - no tool transparency
            "tool_completed, # Never sent - no tool results visibility"
        }
        
        actual_missing = missing_events
        assert expected_missing.issubset(actual_missing), \
            fExpected these critical events to be missing: {expected_missing}, but missing: {actual_missing}
    
    @pytest.mark.asyncio
    @pytest.mark.critical
    async def test_actions_agent_business_impact_of_missing_events(self):
        CRITICAL BUSINESS IMPACT: Test the user experience impact of missing WebSocket events.""
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Simulate what a user would see
        user_visible_events = []
        
        async def capture_user_event(run_id, update_data):
            Capture what the user would actually see.""
            if isinstance(update_data, dict):
                status = update_data.get('status', 'unknown')
                message = update_data.get('message', '')
                
                # This is what reaches the user
                user_visible_events.append({
                    'status': status,
                    'message': message,
                    'provides_transparency': len(message) > 0,
                    'shows_progress': 'processing' in status or 'thinking' in message.lower(),
                    'shows_completion': 'completed' in status or 'done' in message.lower()
                }
        
        # Mock the WebSocket calls
        with patch.object(agent, '_send_update', side_effect=capture_user_event):
            
            # Create realistic state
            state = DeepAgentState(
                user_request=Analyze our system performance and create an optimization plan,
                optimizations_result=OptimizationsResult(
                    optimization_type=system_performance,"
                    optimization_type=system_performance,"
                    recommendations=[
                        "Optimize database queries,"
                        Implement caching strategy, 
                        "Scale horizontally"
                    ],
                    confidence_score=0.85
                ),
                data_result=DataAnalysisResponse(
                    analysis_id=system-performance-analysis-1,
                    status=completed,"
                    status=completed,"
                    results={
                        analysis_results": ["
                            Database query time: 2.3s avg,
                            Memory usage: 85%","
                            CPU utilization: 70%
                        ],
                        insights: {"
                        insights: {"
                            "bottlenecks: [database, memory],"
                            optimization_potential": "high
                        },
                        metadata: {analysis_date: 2025-9-2},"
                        metadata: {analysis_date: 2025-9-2},"
                        "recommendations: ["
                            Add database indexes,
                            "Implement Redis caching,"
                            Monitor memory usage
                        ]
                    },
                    metrics=PerformanceMetrics(duration_ms=1500.0),
                    created_at=time.time()
                )
            )
            
            # Execute and capture user experience
            await agent.execute(state, business-impact-test, stream_updates=True)"
            await agent.execute(state, business-impact-test, stream_updates=True)"
        
        # Analyze user experience
        total_events = len(user_visible_events)
        events_with_transparency = sum(1 for e in user_visible_events if e['provides_transparency')
        events_showing_progress = sum(1 for e in user_visible_events if e['shows_progress')
        events_showing_completion = sum(1 for e in user_visible_events if e['shows_completion')
        
        # BUSINESS IMPACT ANALYSIS
        transparency_score = (events_with_transparency / max(total_events, 1)) * 100
        progress_visibility_score = (events_showing_progress / max(total_events, 1)) * 100
        completion_clarity_score = (events_showing_completion / max(total_events, 1)) * 100
        
        # Document the poor user experience
        assert transparency_score < 50, \
            fUser transparency too low: {transparency_score}% (users get black-box AI experience)"
            fUser transparency too low: {transparency_score}% (users get black-box AI experience)"
        
        # Users never see:
        # - That the agent started working (no agent_started event)
        # - What tools are being used (no tool_executing events)
        # - What results tools produced (no tool_completed events)
        # - The agent's reasoning process (minimal thinking events)'
        
        user_experience_issues = []
        if transparency_score < 80:
            user_experience_issues.append(Low transparency - users don't see AI reasoning)'
        if progress_visibility_score < 60:
            user_experience_issues.append(Poor progress visibility - users unsure if system is working")"
        if completion_clarity_score < 80:
            user_experience_issues.append(Unclear completion - users unsure when processing finished)
        
        # This documents the current business impact
        assert len(user_experience_issues) > 0, \
            fExpected UX issues but system appears to provide good experience. Scores: transparency={transparency_score}%, progress={progress_visibility_score}%, completion={completion_clarity_score}%"
            fExpected UX issues but system appears to provide good experience. Scores: transparency={transparency_score}%, progress={progress_visibility_score}%, completion={completion_clarity_score}%"
    
    @pytest.mark.asyncio
    @pytest.mark.critical
    async def test_actions_agent_websocket_integration_compliance(self):
        "CRITICAL: Test ActionsAgent compliance with WebSocket integration requirements."
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Test 1: WebSocket bridge integration
        websocket = WebSocketTestHelper()  # Real WebSocket implementation
        
        # Create mock bridge
        mock_bridge = Mock()
        
        # Should be able to set WebSocket bridge (inherited from BaseAgent)
        agent.set_websocket_bridge(mock_bridge, compliance-test")"
        assert agent.has_websocket_context(), Agent should have WebSocket context after bridge set
        
        # Test 2: Agent should use inherited emit methods but doesn't'
        compliance_violations = []
        
        # Check if agent overrides the proper WebSocket methods
        base_agent_methods = [
            'emit_agent_started',
            'emit_thinking', 
            'emit_tool_executing',
            'emit_tool_completed',
            'emit_agent_completed'
        ]
        
        for method_name in base_agent_methods:
            method = getattr(agent, method_name)
            # Check if method is overridden in ActionsAgent vs inherited from BaseAgent
            if hasattr(method, '__self__'):
                method_class = method.__self__.__class__
                if method_class.__name__ == 'ActionsToMeetGoalsSubAgent':
                    # Method is overridden in ActionsAgent
                    compliance_violations.append(fOverrides {method_name} (should use BaseAgent implementation))"
                    compliance_violations.append(fOverrides {method_name} (should use BaseAgent implementation))"
                elif method_class.__name__ != 'BaseAgent':
                    # Method comes from some other class
                    compliance_violations.append(f"{method_name} from {method_class.__name__} (should be BaseAgent))"
        
        # Test 3: Agent should call emit methods during execution but doesn't'
        with patch.object(agent, 'emit_agent_started', new_callable=AsyncMock) as mock_started, \
             patch.object(agent, 'emit_thinking', new_callable=AsyncMock) as mock_thinking:
            
            # Create minimal state for testing
            state = DeepAgentState(
                user_request=compliance test,
                optimizations_result=OptimizationsResult(
                    optimization_type=test, "
                    optimization_type=test, "
                    recommendations=[test"],"
                    confidence_score=0.5
                ),
                data_result=DataAnalysisResponse(
                    analysis_id=compliance-test-1,
                    status=completed","
                    results={insights: {}, metadata: {}, recommendations: []},"
                    results={insights: {}, metadata: {}, recommendations: []},"
                    metrics=PerformanceMetrics(duration_ms=50.0),
                    created_at=time.time()
                )
            )
            
            # Execute agent
            await agent.execute(state, compliance-run", stream_updates=True)"
            
            # Should have called emit methods but doesn't'
            if mock_started.call_count == 0:
                compliance_violations.append(Never calls emit_agent_started during execution)
            if mock_thinking.call_count == 0:
                compliance_violations.append(Never calls emit_thinking during execution")"
        
        # Test 4: Uses custom WebSocket pattern instead of standardized bridge
        uses_send_update = hasattr(agent, '_send_update') and callable(agent._send_update)
        uses_custom_websocket = (
            hasattr(agent, 'send_status_update') and 
            hasattr(agent, '_map_status_to_websocket_format') and
            hasattr(agent, '_send_mapped_update')
        )
        
        if uses_send_update:
            compliance_violations.append(Uses custom _send_update instead of BaseAgent emit methods)
        if uses_custom_websocket:
            compliance_violations.append(Implements custom WebSocket logic instead of using BaseAgent bridge)"
            compliance_violations.append(Implements custom WebSocket logic instead of using BaseAgent bridge)"
        
        # CRITICAL COMPLIANCE FAILURE
        assert len(compliance_violations) > 0, \
            f"Expected compliance violations but found none. Agent appears compliant: {compliance_violations}"
        
        # Document specific violations for fixing
        critical_violations = [v for v in compliance_violations if 'Never calls emit_' in v]
        assert len(critical_violations) >= 2, \
            fExpected multiple critical emit method violations: {critical_violations}
    
    @pytest.mark.asyncio
    @pytest.mark.critical
    async def test_websocket_event_performance_requirements(self):
        CRITICAL: Test WebSocket event performance requirements.""
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Setup performance monitoring
        websocket_calls = []
        call_times = []
        
        async def monitor_websocket_performance(*args, **kwargs):
            call_start = time.time()
            websocket_calls.append((call_start, args, kwargs))
            # Simulate realistic WebSocket latency
            await asyncio.sleep(0.1)  # 1ms WebSocket latency
            call_times.append(time.time() - call_start)
        
        with patch.object(agent, '_send_update', side_effect=monitor_websocket_performance):
            
            # Create realistic workload
            state = DeepAgentState(
                user_request=Performance test - analyze and optimize,
                optimizations_result=OptimizationsResult(
                    optimization_type="performance_test,"
                    recommendations=[fRecommendation {i} for i in range(10)],
                    confidence_score=0.95
                ),
                data_result=DataAnalysisResponse(
                    analysis_id=performance-test-analysis-1,"
                    analysis_id=performance-test-analysis-1,"
                    status=completed","
                    results={
                        analysis_results: [fResult {i} for i in range(100)],
                        "insights: {finsight_{i}": fvalue_{i} for i in range(20)},
                        metadata: {large_dataset: True},
                        recommendations": [f"Performance rec {i} for i in range(15)]
                    },
                    metrics=PerformanceMetrics(duration_ms=2500.0),
                    created_at=time.time()
                )
            )
            
            # Measure execution performance
            start_time = time.time()
            await agent.execute(state, performance-test, stream_updates=True)
            total_execution_time = time.time() - start_time
        
        # Performance analysis
        total_websocket_calls = len(websocket_calls)
        total_websocket_time = sum(call_times)
        avg_websocket_latency = total_websocket_time / max(total_websocket_calls, 1)
        websocket_overhead_percentage = (total_websocket_time / total_execution_time) * 100
        
        # Performance requirements (current broken state)
        assert total_websocket_calls < 10, \
            fToo few WebSocket calls: {total_websocket_calls} (missing required events)
        
        # Should have more events for proper user experience
        expected_minimum_events = 5  # start, thinking, tool_exec, tool_comp, complete
        assert total_websocket_calls < expected_minimum_events, \
            fExpected insufficient events ({total_websocket_calls) < {expected_minimum_events) due to missing emit methods"
            fExpected insufficient events ({total_websocket_calls) < {expected_minimum_events) due to missing emit methods"
    
    @pytest.mark.asyncio 
    @pytest.mark.critical
    async def test_websocket_graceful_degradation_when_bridge_unavailable(self):
        "CRITICAL: Test that agent execution continues when WebSocket bridge unavailable."
        agent = ActionsToMeetGoalsSubAgent(self.mock_llm_manager, self.mock_tool_dispatcher)
        
        # Don't set WebSocket bridge - simulating unavailable WebSocket'
        assert not agent.has_websocket_context(), Agent should not have WebSocket context initially""
        
        # Create test state
        state = DeepAgentState(
            user_request=Test graceful degradation,
            optimizations_result=OptimizationsResult(
                optimization_type=degradation_test,"
                optimization_type=degradation_test,"
                recommendations=["Test gracefully],"
                confidence_score=0.7
            ),
            data_result=DataAnalysisResponse(
                analysis_id=degradation-test-analysis-1,
                status="completed,"
                results={
                    analysis_results: [degradation result],
                    insights: {"test: degradation"},
                    metadata: {mode: "graceful_degradation},"
                    recommendations: [handle gracefully]
                },
                metrics=PerformanceMetrics(duration_ms=75.0),
                created_at=time.time()
            )
        )
        
        # Execution should succeed even without WebSocket bridge
        try:
            await agent.execute(state, degradation-test, stream_updates=True)"
            await agent.execute(state, degradation-test, stream_updates=True)"
            execution_succeeded = True
        except Exception as e:
            execution_succeeded = False
            execution_error = str(e)
        
        # CRITICAL: Agent must continue working without WebSocket
        assert execution_succeeded, f"Agent execution failed without WebSocket bridge: {execution_error if not execution_succeeded else 'N/A'}"
        
        # State should be updated with results
        assert state.action_plan_result is not None, Agent should produce results even without WebSocket
        
        # But user experience is degraded (no real-time updates)
        # This is acceptable for graceful degradation


if __name__ == __main__":"
    # Run specific ActionsAgent WebSocket compliance tests
    # MIGRATED: Use SSOT unified test runner
    # python tests/unified_test_runner.py --category unit
    pass  # TODO: Replace with appropriate SSOT test execution

)))))))
}