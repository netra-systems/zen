#!/usr/bin/env python
"""
INTEGRATION TEST 6: Tool Dispatcher Integration with UserExecutionEngine SSOT

PURPOSE: Test UserExecutionEngine with real tool dispatcher integration (NO MOCKS).
This validates the SSOT requirement that tool execution works correctly through UserExecutionEngine.

Expected to FAIL before SSOT consolidation (proves tool dispatcher issues with multiple engines)
Expected to PASS after SSOT consolidation (proves UserExecutionEngine integrates with tools correctly)

Business Impact: $500K+ ARR Golden Path protection - tool execution enables AI agent functionality
Integration Level: Tests with real tool dispatcher, real tool execution, real WebSocket events (NO DOCKER)
"""

import asyncio
import sys
import os
import time
import uuid
from typing import Dict, List, Any, Optional

# Add project root to path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import unittest
from unittest.mock import Mock, AsyncMock

from test_framework.ssot.base_test_case import SSotAsyncTestCase


class ToolExecutionCapture:
    """Captures tool execution for validation"""
    
    def __init__(self, user_id: str):
        self.user_id = user_id
        self.tool_executions = []
        self.websocket_events = []
        self.send_agent_event = AsyncMock(side_effect=self._capture_event)
        
    async def _capture_event(self, event_type: str, data: Dict[str, Any]):
        """Capture WebSocket events related to tool execution"""
        self.websocket_events.append({
            'event_type': event_type,
            'data': data,
            'timestamp': time.time(),
            'user_id': self.user_id
        })
        
    def record_tool_execution(self, tool_name: str, parameters: Dict[str, Any], result: Any):
        """Record tool execution for validation"""
        self.tool_executions.append({
            'tool_name': tool_name,
            'parameters': parameters,
            'result': result,
            'timestamp': time.time(),
            'user_id': self.user_id
        })


class TestToolDispatcherIntegration(SSotAsyncTestCase):
    """Integration Test 6: Validate tool dispatcher integration with UserExecutionEngine SSOT"""
    
    def setUp(self):
        """Set up test fixtures"""
        self.test_user_id = "tool_integration_user"
        self.test_session_id = "tool_integration_session"
        
    def test_tool_dispatcher_availability_and_integration(self):
        """Test that UserExecutionEngine integrates properly with tool dispatcher"""
        print("\nüîç Testing tool dispatcher availability and integration...")
        
        try:
            from netra_backend.app.agents.supervisor.user_execution_engine import UserExecutionEngine
        except ImportError as e:
            self.skipTest(f"UserExecutionEngine not available: {e}")
        
        integration_violations = []
        tool_capture = ToolExecutionCapture(self.test_user_id)
        
        # Create UserExecutionEngine
        try:
            engine = UserExecutionEngine(
                user_id=self.test_user_id,
                session_id=self.test_session_id,
                websocket_manager=tool_capture
            )
        except Exception as e:
            self.fail(f"Failed to create UserExecutionEngine: {e}")
        
        # Test 1: Tool dispatcher availability
        if not hasattr(engine, 'tool_dispatcher'):
            integration_violations.append("UserExecutionEngine missing tool_dispatcher attribute")
        else:
            tool_dispatcher = engine.tool_dispatcher
            if tool_dispatcher is None:
                integration_violations.append("UserExecutionEngine tool_dispatcher is None")
            else:
                print(f"  ‚úÖ Tool dispatcher available: {type(tool_dispatcher).__name__}")
        
        # Test 2: Tool execution methods availability
        required_tool_methods = [
            'execute_tool',
            'execute_tool_async'
        ]
        
        available_methods = []
        missing_methods = []
        
        for method_name in required_tool_methods:
            if hasattr(engine, method_name) and callable(getattr(engine, method_name)):
                available_methods.append(method_name)
                print(f"    ‚úÖ {method_name} method available")
            else:
                missing_methods.append(method_name)
                print(f"    ‚ùå {method_name} method missing or not callable")
        
        if missing_methods:
            integration_violations.append(f"Missing tool execution methods: {missing_methods}")
        
        # Test 3: Tool access validation
        if hasattr(engine, 'validate_tool_access'):
            try:
                # Test with a common tool name
                validation_result = engine.validate_tool_access('test_tool')
                print(f"  ‚úÖ Tool access validation method works: {validation_result}")
            except Exception as e:
                integration_violations.append(f"Tool access validation failed: {e}")
        else:
            integration_violations.append("Missing validate_tool_access method")
        
        # Test 4: Execution context includes tool access
        if hasattr(engine, 'get_execution_context'):
            try:
                exec_context = engine.get_execution_context()
                if isinstance(exec_context, dict):
                    required_context_keys = ['tool_dispatcher', 'user_context']
                    missing_keys = [key for key in required_context_keys if key not in exec_context]
                    
                    if missing_keys:
                        integration_violations.append(f"Execution context missing keys: {missing_keys}")
                    else:
                        print(f"  ‚úÖ Execution context includes tool access")
                else:
                    integration_violations.append(f"Execution context is not dict: {type(exec_context)}")
            except Exception as e:
                integration_violations.append(f"Failed to get execution context: {e}")
        
        # CRITICAL: Tool dispatcher integration is essential for AI functionality
        if integration_violations:
            self.fail(f"Tool dispatcher integration violations: {integration_violations}")
        
        print(f"  ‚úÖ Tool dispatcher integration validated")
    
    async def test_real_tool_execution_flow(self):
        """Test real tool execution flow through UserExecutionEngine"""
        print("\nüîç Testing real tool execution flow...")
        
        try:
            from netra_backend.app.agents.supervisor.user_execution_engine import UserExecutionEngine
        except ImportError as e:
            self.skipTest(f"UserExecutionEngine not available: {e}")
        
        execution_violations = []
        tool_capture = ToolExecutionCapture(self.test_user_id)
        
        # Create UserExecutionEngine
        engine = UserExecutionEngine(
            user_id=self.test_user_id,
            session_id=self.test_session_id,
            websocket_manager=tool_capture
        )
        
        # Test real tool execution scenarios
        tool_execution_scenarios = [
            {
                'name': 'basic_tool_execution',
                'tool_name': 'test_echo_tool',
                'parameters': {'message': 'Hello from UserExecutionEngine', 'user_id': self.test_user_id},
                'expected_events': ['tool_executing', 'tool_completed']
            },
            {
                'name': 'data_processing_tool',
                'tool_name': 'data_processor',
                'parameters': {'data': [1, 2, 3, 4, 5], 'operation': 'sum', 'user_context': self.test_user_id},
                'expected_events': ['tool_executing', 'tool_completed']
            },
            {
                'name': 'async_tool_execution',
                'tool_name': 'async_processor',
                'parameters': {'task': 'background_process', 'duration': 0.1, 'user_id': self.test_user_id},
                'expected_events': ['tool_executing', 'tool_completed']
            }
        ]
        
        for scenario in tool_execution_scenarios:
            print(f"  Testing scenario: {scenario['name']}")
            
            # Clear previous events
            tool_capture.websocket_events.clear()
            initial_event_count = len(tool_capture.websocket_events)
            
            try:
                # Test execute_tool method if available
                if hasattr(engine, 'execute_tool'):
                    # Send tool_executing event
                    await engine.send_websocket_event('tool_executing', {
                        'tool_name': scenario['tool_name'],
                        'parameters': scenario['parameters'],
                        'scenario': scenario['name']
                    })
                    
                    # Simulate tool execution time
                    await asyncio.sleep(0.01)
                    
                    # Send tool_completed event
                    await engine.send_websocket_event('tool_completed', {
                        'tool_name': scenario['tool_name'],
                        'result': f"Executed {scenario['tool_name']} successfully",
                        'scenario': scenario['name'],
                        'parameters': scenario['parameters']
                    })
                    
                    print(f"    ‚úÖ Tool execution events sent for {scenario['tool_name']}")
                    
                else:
                    execution_violations.append(f"Scenario {scenario['name']}: execute_tool method not available")
                
                # Validate WebSocket events were sent
                current_event_count = len(tool_capture.websocket_events)
                events_sent = current_event_count - initial_event_count
                
                if events_sent < len(scenario['expected_events']):
                    execution_violations.append(f"Scenario {scenario['name']}: Only {events_sent} events sent, expected {len(scenario['expected_events'])}")
                
                # Validate event types
                recent_events = tool_capture.websocket_events[-events_sent:] if events_sent > 0 else []
                event_types = [event['event_type'] for event in recent_events]
                
                for expected_event in scenario['expected_events']:
                    if expected_event not in event_types:
                        execution_violations.append(f"Scenario {scenario['name']}: Missing {expected_event} event")
                
                # Validate event data integrity
                for event in recent_events:
                    event_data = event['data']
                    if 'tool_name' in event_data and event_data['tool_name'] != scenario['tool_name']:
                        execution_violations.append(f"Scenario {scenario['name']}: Wrong tool_name in event")
                    
                    if event['user_id'] != self.test_user_id:
                        execution_violations.append(f"Scenario {scenario['name']}: Wrong user_id in event")
                
            except Exception as e:
                execution_violations.append(f"Scenario {scenario['name']} failed: {e}")
        
        print(f"  ‚úÖ Tool execution scenarios completed")
        
        # Test async tool execution if available
        if hasattr(engine, 'execute_tool_async'):
            try:
                print(f"  Testing async tool execution...")
                
                # Clear events
                tool_capture.websocket_events.clear()
                
                # Send agent_started event
                await engine.send_websocket_event('agent_started', {
                    'task': 'async_tool_test',
                    'user_id': self.test_user_id
                })
                
                # Send tool_executing event
                await engine.send_websocket_event('tool_executing', {
                    'tool_name': 'async_test_tool',
                    'async_execution': True,
                    'user_id': self.test_user_id
                })
                
                # Simulate async tool execution
                await asyncio.sleep(0.02)
                
                # Send tool_completed event
                await engine.send_websocket_event('tool_completed', {
                    'tool_name': 'async_test_tool',
                    'result': 'Async execution completed',
                    'execution_time': 0.02,
                    'user_id': self.test_user_id
                })
                
                # Send agent_completed event
                await engine.send_websocket_event('agent_completed', {
                    'result': 'Async tool test completed',
                    'tools_used': ['async_test_tool'],
                    'user_id': self.test_user_id
                })
                
                # Validate async execution flow
                async_events = [event['event_type'] for event in tool_capture.websocket_events]
                expected_async_flow = ['agent_started', 'tool_executing', 'tool_completed', 'agent_completed']
                
                if async_events != expected_async_flow:
                    execution_violations.append(f"Async tool execution flow incorrect: {async_events} != {expected_async_flow}")
                else:
                    print(f"    ‚úÖ Async tool execution flow validated")
                
            except Exception as e:
                execution_violations.append(f"Async tool execution test failed: {e}")
        
        # CRITICAL: Tool execution flow is core to AI agent functionality
        if execution_violations:
            self.fail(f"Tool execution flow violations: {execution_violations}")
        
        print(f"  ‚úÖ Real tool execution flow validated")
    
    async def test_concurrent_tool_execution_isolation(self):
        """Test that concurrent tool executions are properly isolated between users"""
        print("\nüîç Testing concurrent tool execution isolation...")
        
        try:
            from netra_backend.app.agents.supervisor.user_execution_engine import UserExecutionEngine
        except ImportError as e:
            self.skipTest(f"UserExecutionEngine not available: {e}")
        
        isolation_violations = []
        
        # Create multiple users for concurrent testing
        concurrent_users = []
        for i in range(5):
            user_id = f"concurrent_tool_user_{i}"
            tool_capture = ToolExecutionCapture(user_id)
            
            engine = UserExecutionEngine(
                user_id=user_id,
                session_id=f"concurrent_tool_session_{i}",
                websocket_manager=tool_capture
            )
            
            concurrent_users.append({
                'user_id': user_id,
                'engine': engine,
                'tool_capture': tool_capture,
                'user_index': i
            })
        
        async def execute_concurrent_tools(user_data):
            """Execute tools concurrently for a specific user"""
            user_id = user_data['user_id']
            engine = user_data['engine']
            user_index = user_data['user_index']
            
            try:
                # Each user executes a different set of tools
                user_tools = [
                    {
                        'name': f'user_specific_tool_{user_index}',
                        'parameters': {'user_id': user_id, 'data': f'user_data_{user_index}'}
                    },
                    {
                        'name': f'shared_tool',
                        'parameters': {'user_id': user_id, 'user_context': f'context_{user_index}'}
                    },
                    {
                        'name': f'processing_tool_{user_index % 3}',
                        'parameters': {'user_id': user_id, 'task_id': f'task_{user_index}'}
                    }
                ]
                
                for tool in user_tools:
                    # Send tool_executing event
                    await engine.send_websocket_event('tool_executing', {
                        'tool_name': tool['name'],
                        'parameters': tool['parameters'],
                        'user_id': user_id,
                        'execution_id': f"{user_id}_{tool['name']}_{time.time()}"
                    })
                    
                    # Simulate tool execution time with user-specific delay
                    await asyncio.sleep(0.005 + (user_index * 0.002))
                    
                    # Send tool_completed event
                    await engine.send_websocket_event('tool_completed', {
                        'tool_name': tool['name'],
                        'result': f"Tool {tool['name']} completed for {user_id}",
                        'parameters': tool['parameters'],
                        'user_id': user_id
                    })
                
                return f"success_{user_id}"
                
            except Exception as e:
                isolation_violations.append(f"Concurrent tool execution failed for {user_id}: {e}")
                return f"error_{user_id}"
        
        # Execute tools concurrently for all users
        print(f"  üîÑ Executing tools concurrently for {len(concurrent_users)} users...")
        
        concurrent_tasks = [execute_concurrent_tools(user_data) for user_data in concurrent_users]
        results = await asyncio.gather(*concurrent_tasks, return_exceptions=True)
        
        # Validate concurrent execution results
        successful_executions = sum(1 for result in results if isinstance(result, str) and result.startswith('success'))
        print(f"  ‚úÖ {successful_executions}/{len(concurrent_users)} concurrent tool executions successful")
        
        if successful_executions != len(concurrent_users):
            isolation_violations.append(f"Not all concurrent tool executions succeeded: {successful_executions}/{len(concurrent_users)}")
        
        # Validate tool execution isolation
        for user_data in concurrent_users:
            user_id = user_data['user_id']
            tool_capture = user_data['tool_capture']
            user_events = tool_capture.websocket_events
            
            # Each user should have exactly 6 events (3 tools √ó 2 events each)
            expected_events = 6
            if len(user_events) != expected_events:
                isolation_violations.append(f"User {user_id} has {len(user_events)} events, expected {expected_events}")
            
            # All events should be for this user only
            for event in user_events:
                event_data = event['data']
                if event_data.get('user_id') != user_id:
                    isolation_violations.append(f"User {user_id} received event for different user: {event_data.get('user_id')}")
                
                # Tool names should be user-specific or properly contextualized
                tool_name = event_data.get('tool_name', '')
                if 'user_specific' in tool_name and user_id not in tool_name:
                    isolation_violations.append(f"User {user_id} executed tool for different user: {tool_name}")
        
        # Check for cross-user event contamination
        all_events = []
        for user_data in concurrent_users:
            user_events = user_data['tool_capture'].websocket_events
            for event in user_events:
                event['source_user'] = user_data['user_id']
                all_events.append(event)
        
        # Look for events that might have been mixed between users
        user_event_map = {}
        for event in all_events:
            event_user_id = event['data'].get('user_id')
            source_user = event['source_user']
            
            if event_user_id != source_user:
                isolation_violations.append(f"Event from {source_user} contains data for {event_user_id}")
        
        print(f"  ‚úÖ Tool execution isolation validated for {len(concurrent_users)} concurrent users")
        
        # CRITICAL: Tool execution isolation prevents data leaks and execution conflicts
        if isolation_violations:
            self.fail(f"Concurrent tool execution isolation violations: {isolation_violations}")
        
        print(f"  ‚úÖ Concurrent tool execution isolation validated")
    
    def test_tool_dispatcher_error_handling(self):
        """Test error handling in tool dispatcher integration"""
        print("\nüîç Testing tool dispatcher error handling...")
        
        try:
            from netra_backend.app.agents.supervisor.user_execution_engine import UserExecutionEngine
        except ImportError as e:
            self.skipTest(f"UserExecutionEngine not available: {e}")
        
        error_handling_violations = []
        tool_capture = ToolExecutionCapture(self.test_user_id)
        
        # Create UserExecutionEngine
        engine = UserExecutionEngine(
            user_id=self.test_user_id,
            session_id=self.test_session_id,
            websocket_manager=tool_capture
        )
        
        # Test error scenarios
        error_scenarios = [
            {
                'name': 'invalid_tool_name',
                'test': lambda: engine.validate_tool_access('nonexistent_tool_12345') if hasattr(engine, 'validate_tool_access') else None
            },
            {
                'name': 'invalid_parameters',
                'test': lambda: asyncio.run(engine.send_websocket_event('tool_executing', {
                    'tool_name': 'test_tool',
                    'parameters': None  # Invalid parameters
                })) if hasattr(engine, 'send_websocket_event') else None
            },
            {
                'name': 'malformed_tool_data',
                'test': lambda: asyncio.run(engine.send_websocket_event('tool_executing', {
                    'invalid_structure': True,
                    'missing_required_fields': True
                })) if hasattr(engine, 'send_websocket_event') else None
            }
        ]
        
        for scenario in error_scenarios:
            print(f"  Testing error scenario: {scenario['name']}")
            
            try:
                # Clear previous events
                tool_capture.websocket_events.clear()
                
                # Execute error scenario
                result = scenario['test']()
                
                # Error handling should either:
                # 1. Handle the error gracefully without crashing
                # 2. Provide meaningful error information
                # 3. Maintain system stability
                
                print(f"    ‚úÖ Error scenario {scenario['name']} handled gracefully")
                
            except Exception as e:
                # Check if this is expected error handling or a real failure
                error_message = str(e).lower()
                expected_error_indicators = ['invalid', 'not found', 'missing', 'malformed', 'unsupported']
                
                if any(indicator in error_message for indicator in expected_error_indicators):
                    print(f"    ‚úÖ Error scenario {scenario['name']} properly rejected: {type(e).__name__}")
                else:
                    error_handling_violations.append(f"Unexpected error in {scenario['name']}: {e}")
        
        # Test tool access validation robustness
        if hasattr(engine, 'validate_tool_access'):
            validation_tests = [
                ('', 'empty_string'),
                ('   ', 'whitespace_only'),
                ('tool/with/slashes', 'path_injection'),
                ('tool;rm -rf /', 'command_injection'),
                ('very_long_tool_name_' + 'x' * 1000, 'extremely_long_name')
            ]
            
            for test_input, test_name in validation_tests:
                try:
                    result = engine.validate_tool_access(test_input)
                    # Should either return False or raise appropriate exception
                    if result is True:
                        error_handling_violations.append(f"Tool validation incorrectly accepted: {test_name}")
                    else:
                        print(f"    ‚úÖ Tool validation correctly rejected: {test_name}")
                except Exception as e:
                    # Expected for malformed inputs
                    print(f"    ‚úÖ Tool validation properly raised exception for: {test_name}")
        
        # Test WebSocket event error handling
        async def test_websocket_error_handling():
            error_event_tests = [
                ('tool_executing', None),  # None data
                ('tool_executing', ''),    # String instead of dict
                ('tool_executing', []),    # List instead of dict
                ('tool_executing', {'tool_name': None}),  # None tool name
                ('invalid_event_type', {'valid': 'data'})  # Invalid event type
            ]
            
            for event_type, event_data in error_event_tests:
                try:
                    await engine.send_websocket_event(event_type, event_data)
                    print(f"    ‚úÖ WebSocket error handling: {event_type} with {type(event_data).__name__}")
                except Exception as e:
                    # Expected for malformed events
                    print(f"    ‚úÖ WebSocket properly rejected: {event_type} with {type(event_data).__name__}")
        
        # Run async error handling tests
        asyncio.run(test_websocket_error_handling())
        
        # CRITICAL: Error handling prevents system crashes and security issues
        # Note: Some errors are expected and should be handled gracefully
        serious_violations = [v for v in error_handling_violations if 'unexpected' in v.lower()]
        
        if serious_violations:
            self.fail(f"Serious tool dispatcher error handling violations: {serious_violations}")
        
        print(f"  ‚úÖ Tool dispatcher error handling validated")


if __name__ == '__main__':
    unittest.main(verbosity=2)