"""
Performance & Load Testing: Agent Execution Performance Under Concurrent Load

Business Value Justification (BVJ):
- Segment: All (Free, Early, Mid, Enterprise)
- Business Goal: Ensure AI agents can handle concurrent user requests without performance degradation
- Value Impact: Multiple users can simultaneously use AI agents with consistent response quality
- Strategic Impact: Agent concurrency is critical for platform scalability and revenue growth

CRITICAL: This test validates agent execution performance, resource utilization efficiency,
and response quality consistency under concurrent load scenarios.
"""

import asyncio
import pytest
import time
import statistics
import json
import uuid
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor

from test_framework.base_integration_test import BaseIntegrationTest
from test_framework.ssot.e2e_auth_helper import E2EAuthHelper, create_authenticated_user_context
from test_framework.real_services_test_fixtures import real_services_fixture
from shared.isolated_environment import get_env


@dataclass
class AgentPerformanceMetrics:
    """Agent execution performance metrics."""
    agent_name: str
    total_executions: int
    successful_executions: int
    failed_executions: int
    average_execution_time: float
    p95_execution_time: float
    p99_execution_time: float
    min_execution_time: float
    max_execution_time: float
    concurrent_executions_peak: int
    throughput_executions_per_second: float
    memory_usage_mb: float
    errors: List[str] = field(default_factory=list)
    execution_details: List[Dict[str, Any]] = field(default_factory=list)


@dataclass
class AgentConcurrencyTestResult:
    """Results from agent concurrency testing."""
    test_name: str
    concurrent_agents: int
    total_duration: float
    overall_success_rate: float
    agent_metrics: List[AgentPerformanceMetrics]
    resource_contention_detected: bool
    scalability_bottlenecks: List[str]


class TestAgentExecutionConcurrentPerformance(BaseIntegrationTest):
    """Test agent execution performance under concurrent load."""
    
    def _simulate_agent_work(self, complexity_level: str = "medium") -> Dict[str, Any]:
        """Simulate agent work with different complexity levels."""
        work_configs = {
            "simple": {"iterations": 10, "data_size": 100, "processing_time": 0.01},
            "medium": {"iterations": 50, "data_size": 500, "processing_time": 0.05},
            "complex": {"iterations": 100, "data_size": 1000, "processing_time": 0.1}
        }
        
        config = work_configs.get(complexity_level, work_configs["medium"])
        
        # Simulate computational work
        result_data = []
        for i in range(config["iterations"]):
            # Simulate data processing
            data_item = {
                "id": i,
                "data": "x" * config["data_size"],
                "timestamp": time.time(),
                "processed": True
            }
            result_data.append(data_item)
        
        # Simulate processing time
        time.sleep(config[\"processing_time\"])
        
        return {
            \"complexity\": complexity_level,
            \"iterations\": config[\"iterations\"],
            \"data_items\": len(result_data),
            \"processing_time\": config[\"processing_time\"],
            \"result_summary\": f\"Processed {len(result_data)} items at {complexity_level} complexity\"
        }
    
    @pytest.mark.integration
    @pytest.mark.performance
    @pytest.mark.real_services
    async def test_concurrent_agent_execution_performance(self, real_services_fixture):
        \"\"\"
        Test agent execution performance with concurrent users.
        
        Performance SLA:
        - Agent execution time: <5s (p95)
        - Success rate: >95% under 20 concurrent executions
        - Memory usage: <50MB per concurrent agent
        - No resource contention blocking
        \"\"\"
        concurrent_agents = 20
        executions_per_agent = 5
        
        db = real_services_fixture[\"db\"]
        redis = real_services_fixture[\"redis\"]
        
        agent_performance_results = []
        
        async def simulate_agent_execution(agent_id: int, execution_count: int) -> AgentPerformanceMetrics:
            \"\"\"Simulate a single agent's execution lifecycle.\"\"\"
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            initial_memory = process.memory_info().rss / 1024 / 1024
            
            # Create authenticated user context for the agent
            user_context = await create_authenticated_user_context(
                user_email=f\"agent_perf_{agent_id}@example.com\",
                environment=\"test\",
                permissions=[\"read\", \"write\", \"execute\"],
                websocket_enabled=True
            )
            
            execution_times = []
            execution_details = []
            errors = []
            successful_executions = 0
            
            for execution_id in range(execution_count):
                execution_start = time.time()
                
                try:
                    # Simulate agent initialization and context setup\n                    agent_context = {\n                        \"agent_id\": agent_id,\n                        \"execution_id\": execution_id,\n                        \"user_context\": user_context,\n                        \"request_id\": str(uuid.uuid4()),\n                        \"start_time\": execution_start\n                    }\n                    \n                    # Store agent state in Redis (simulating real agent pattern)\n                    agent_state_key = f\"agent_state:{agent_id}:{execution_id}\"\n                    await redis.set(\n                        agent_state_key,\n                        json.dumps({\n                            \"agent_id\": agent_id,\n                            \"user_id\": str(user_context.user_id),\n                            \"status\": \"executing\",\n                            \"started_at\": execution_start\n                        }),\n                        ex=300  # 5 minute expiry\n                    )\n                    \n                    # Simulate agent work with varying complexity\n                    complexity = [\"simple\", \"medium\", \"complex\"][execution_id % 3]\n                    work_result = self._simulate_agent_work(complexity)\n                    \n                    # Simulate database interaction (agent storing results)\n                    await db.execute(\n                        \"SELECT $1 as agent_id, $2 as execution_id, $3 as result_summary\",\n                        agent_id, execution_id, work_result[\"result_summary\"]\n                    )\n                    \n                    # Update agent state to completed\n                    await redis.set(\n                        agent_state_key,\n                        json.dumps({\n                            \"agent_id\": agent_id,\n                            \"user_id\": str(user_context.user_id),\n                            \"status\": \"completed\",\n                            \"started_at\": execution_start,\n                            \"completed_at\": time.time(),\n                            \"result\": work_result\n                        }),\n                        ex=300\n                    )\n                    \n                    execution_time = time.time() - execution_start\n                    execution_times.append(execution_time)\n                    successful_executions += 1\n                    \n                    execution_details.append({\n                        \"execution_id\": execution_id,\n                        \"execution_time\": execution_time,\n                        \"complexity\": complexity,\n                        \"success\": True\n                    })\n                    \n                    # Cleanup agent state\n                    await redis.delete(agent_state_key)\n                    \n                except Exception as e:\n                    execution_time = time.time() - execution_start\n                    execution_times.append(execution_time)\n                    errors.append(f\"Agent {agent_id} execution {execution_id} failed: {str(e)}\")\n                    \n                    execution_details.append({\n                        \"execution_id\": execution_id,\n                        \"execution_time\": execution_time,\n                        \"error\": str(e),\n                        \"success\": False\n                    })\n            \n            final_memory = process.memory_info().rss / 1024 / 1024\n            memory_usage = final_memory - initial_memory\n            \n            # Calculate performance metrics\n            total_executions = len(execution_times)\n            failed_executions = total_executions - successful_executions\n            \n            metrics = AgentPerformanceMetrics(\n                agent_name=f\"Agent_{agent_id}\",\n                total_executions=total_executions,\n                successful_executions=successful_executions,\n                failed_executions=failed_executions,\n                average_execution_time=statistics.mean(execution_times) if execution_times else 0,\n                p95_execution_time=0,\n                p99_execution_time=0,\n                min_execution_time=min(execution_times) if execution_times else 0,\n                max_execution_time=max(execution_times) if execution_times else 0,\n                concurrent_executions_peak=1,  # Single agent, so peak is 1\n                throughput_executions_per_second=0,\n                memory_usage_mb=memory_usage,\n                errors=errors,\n                execution_details=execution_details\n            )\n            \n            # Calculate percentiles\n            if execution_times:\n                execution_times.sort()\n                p95_index = int(len(execution_times) * 0.95)\n                p99_index = int(len(execution_times) * 0.99)\n                \n                metrics.p95_execution_time = execution_times[p95_index]\n                metrics.p99_execution_time = execution_times[p99_index]\n                \n                # Calculate throughput\n                total_agent_time = sum(execution_times)\n                metrics.throughput_executions_per_second = successful_executions / total_agent_time if total_agent_time > 0 else 0\n            \n            return metrics\n        \n        # Execute concurrent agent simulations\n        print(f\"🤖 Starting {concurrent_agents} concurrent agent executions...\")\n        concurrent_start = time.time()\n        \n        agent_tasks = [\n            simulate_agent_execution(agent_id, executions_per_agent)\n            for agent_id in range(concurrent_agents)\n        ]\n        \n        agent_results = await asyncio.gather(*agent_tasks, return_exceptions=True)\n        \n        concurrent_duration = time.time() - concurrent_start\n        \n        # Filter successful agent results\n        successful_agent_metrics = [\n            result for result in agent_results\n            if isinstance(result, AgentPerformanceMetrics)\n        ]\n        \n        failed_agents = len(agent_results) - len(successful_agent_metrics)\n        \n        # Calculate overall metrics\n        total_executions = sum(m.total_executions for m in successful_agent_metrics)\n        total_successful = sum(m.successful_executions for m in successful_agent_metrics)\n        total_failed = sum(m.failed_executions for m in successful_agent_metrics)\n        \n        overall_success_rate = total_successful / total_executions if total_executions > 0 else 0\n        \n        # Aggregate execution times for overall percentiles\n        all_execution_times = []\n        for metrics in successful_agent_metrics:\n            for detail in metrics.execution_details:\n                if detail.get(\"success\", False):\n                    all_execution_times.append(detail[\"execution_time\"])\n        \n        overall_p95 = 0\n        overall_p99 = 0\n        if all_execution_times:\n            all_execution_times.sort()\n            p95_index = int(len(all_execution_times) * 0.95)\n            p99_index = int(len(all_execution_times) * 0.99)\n            overall_p95 = all_execution_times[p95_index]\n            overall_p99 = all_execution_times[p99_index]\n        \n        # Memory usage analysis\n        total_memory_usage = sum(m.memory_usage_mb for m in successful_agent_metrics)\n        average_memory_per_agent = total_memory_usage / len(successful_agent_metrics) if successful_agent_metrics else 0\n        \n        # Performance assertions\n        assert len(successful_agent_metrics) >= concurrent_agents * 0.90, f\"Too many failed agents: {failed_agents}/{concurrent_agents}\"\n        assert overall_success_rate >= 0.95, f\"Overall success rate {overall_success_rate:.3f} below 95% SLA\"\n        assert overall_p95 <= 5.0, f\"P95 execution time {overall_p95:.2f}s exceeds 5s SLA\"\n        assert average_memory_per_agent <= 50.0, f\"Average memory per agent {average_memory_per_agent:.1f}MB exceeds 50MB SLA\"\n        \n        print(f\"✅ Concurrent Agent Execution Performance Results:\")\n        print(f\"   Concurrent agents: {concurrent_agents}\")\n        print(f\"   Successful agents: {len(successful_agent_metrics)}\")\n        print(f\"   Failed agents: {failed_agents}\")\n        print(f\"   Total executions: {total_executions}\")\n        print(f\"   Successful executions: {total_successful}\")\n        print(f\"   Failed executions: {total_failed}\")\n        print(f\"   Overall success rate: {overall_success_rate:.3f}\")\n        print(f\"   Overall P95 execution time: {overall_p95:.2f}s\")\n        print(f\"   Overall P99 execution time: {overall_p99:.2f}s\")\n        print(f\"   Average memory per agent: {average_memory_per_agent:.1f}MB\")\n        print(f\"   Total memory usage: {total_memory_usage:.1f}MB\")\n        print(f\"   Test duration: {concurrent_duration:.2f}s\")\n    \n    @pytest.mark.integration\n    @pytest.mark.performance\n    @pytest.mark.real_services\n    async def test_agent_resource_contention_detection(self, real_services_fixture):\n        \"\"\"
        Test agent performance under resource contention scenarios.
        
        Performance SLA:
        - Graceful degradation under resource pressure
        - No deadlocks or resource starvation
        - Fair resource allocation across agents\n        \"\"\"\n        db = real_services_fixture[\"db\"]\n        redis = real_services_fixture[\"redis\"]\n        \n        # Create resource contention by having many agents compete for limited resources\n        high_contention_agents = 50\n        resource_intensive_operations = 10\n        \n        contention_metrics = {\n            \"successful_agents\": 0,\n            \"failed_agents\": 0,\n            \"resource_timeouts\": 0,\n            \"deadlock_events\": 0,\n            \"average_wait_time\": 0,\n            \"max_wait_time\": 0,\n            \"fairness_variance\": 0\n        }\n        \n        agent_completion_times = []\n        \n        async def resource_contention_agent(agent_id: int) -> Dict[str, Any]:\n            \"\"\"Agent that competes for shared resources.\"\"\"            \n            agent_start = time.time()\n            operations_completed = 0\n            wait_times = []\n            errors = []\n            \n            try:\n                user_context = await create_authenticated_user_context(\n                    user_email=f\"contention_agent_{agent_id}@example.com\",\n                    environment=\"test\"\n                )\n                \n                for op_id in range(resource_intensive_operations):\n                    operation_start = time.time()\n                    \n                    try:\n                        # Simulate resource-intensive operations\n                        \n                        # 1. Database resource contention\n                        db_wait_start = time.time()\n                        await db.execute(\n                            \"SELECT COUNT(*) FROM generate_series(1, $1) WHERE generate_series % $2 = 0\",\n                            1000,  # Computationally expensive\n                            agent_id % 10 + 1\n                        )\n                        db_wait_time = time.time() - db_wait_start\n                        wait_times.append(db_wait_time)\n                        \n                        # 2. Redis resource contention\n                        redis_key = f\"contention_resource_{op_id % 5}\"  # Shared keys to create contention\n                        \n                        # Atomic increment with potential contention\n                        await redis.incr(redis_key)\n                        current_value = await redis.get(redis_key)\n                        \n                        # Simulate processing based on shared resource state\n                        if current_value:\n                            processing_time = min(0.1, int(current_value) * 0.001)\n                            await asyncio.sleep(processing_time)\n                        \n                        operations_completed += 1\n                        \n                    except asyncio.TimeoutError:\n                        contention_metrics[\"resource_timeouts\"] += 1\n                        errors.append(f\"Timeout in agent {agent_id} operation {op_id}\")\n                    except Exception as e:\n                        errors.append(f\"Error in agent {agent_id} operation {op_id}: {str(e)}\")\n                \n                agent_duration = time.time() - agent_start\n                agent_completion_times.append(agent_duration)\n                \n                return {\n                    \"agent_id\": agent_id,\n                    \"success\": len(errors) == 0,\n                    \"operations_completed\": operations_completed,\n                    \"total_duration\": agent_duration,\n                    \"average_wait_time\": statistics.mean(wait_times) if wait_times else 0,\n                    \"max_wait_time\": max(wait_times) if wait_times else 0,\n                    \"errors\": errors\n                }\n                \n            except Exception as e:\n                contention_metrics[\"failed_agents\"] += 1\n                return {\n                    \"agent_id\": agent_id,\n                    \"success\": False,\n                    \"operations_completed\": operations_completed,\n                    \"total_duration\": time.time() - agent_start,\n                    \"error\": str(e)\n                }\n        \n        # Execute high contention test\n        print(f\"⚔️ Starting resource contention test with {high_contention_agents} agents...\")\n        contention_start = time.time()\n        \n        contention_tasks = [\n            resource_contention_agent(agent_id)\n            for agent_id in range(high_contention_agents)\n        ]\n        \n        contention_results = await asyncio.gather(*contention_tasks, return_exceptions=True)\n        \n        contention_duration = time.time() - contention_start\n        \n        # Analyze contention results\n        successful_results = [r for r in contention_results if isinstance(r, dict) and r.get(\"success\", False)]\n        failed_results = [r for r in contention_results if isinstance(r, dict) and not r.get(\"success\", False)]\n        exceptions = [r for r in contention_results if not isinstance(r, dict)]\n        \n        contention_metrics[\"successful_agents\"] = len(successful_results)\n        contention_metrics[\"failed_agents\"] = len(failed_results) + len(exceptions)\n        \n        # Calculate fairness metrics\n        if successful_results:\n            completion_times = [r[\"total_duration\"] for r in successful_results]\n            wait_times = [r[\"average_wait_time\"] for r in successful_results]\n            \n            contention_metrics[\"average_wait_time\"] = statistics.mean(wait_times)\n            contention_metrics[\"max_wait_time\"] = max([r[\"max_wait_time\"] for r in successful_results])\n            \n            # Fairness variance (lower is better)\n            contention_metrics[\"fairness_variance\"] = statistics.stdev(completion_times) if len(completion_times) > 1 else 0\n        \n        success_rate = contention_metrics[\"successful_agents\"] / high_contention_agents\n        \n        # Resource contention assertions (more lenient than normal operations)\n        assert success_rate >= 0.80, f\"Resource contention success rate {success_rate:.3f} below 80% threshold\"\n        assert contention_metrics[\"resource_timeouts\"] <= high_contention_agents * 0.2, f\"Too many resource timeouts: {contention_metrics['resource_timeouts']}\"\n        assert contention_metrics[\"deadlock_events\"] == 0, f\"Deadlock events detected: {contention_metrics['deadlock_events']}\"\n        \n        # Fairness check - completion times shouldn't vary too much\n        assert contention_metrics[\"fairness_variance\"] <= 5.0, f\"Fairness variance {contention_metrics['fairness_variance']:.2f}s too high\"\n        \n        print(f\"✅ Agent Resource Contention Test Results:\")\n        print(f\"   Test duration: {contention_duration:.2f}s\")\n        print(f\"   Successful agents: {contention_metrics['successful_agents']}/{high_contention_agents}\")\n        print(f\"   Failed agents: {contention_metrics['failed_agents']}\")\n        print(f\"   Success rate: {success_rate:.3f}\")\n        print(f\"   Resource timeouts: {contention_metrics['resource_timeouts']}\")\n        print(f\"   Average wait time: {contention_metrics['average_wait_time']:.3f}s\")\n        print(f\"   Max wait time: {contention_metrics['max_wait_time']:.3f}s\")\n        print(f\"   Fairness variance: {contention_metrics['fairness_variance']:.2f}s\")\n        \n        # Cleanup shared resources\n        for i in range(resource_intensive_operations):\n            try:\n                await redis.delete(f\"contention_resource_{i % 5}\")\n            except Exception:\n                pass\n    \n    @pytest.mark.integration\n    @pytest.mark.performance\n    @pytest.mark.real_services\n    async def test_agent_scalability_bottleneck_identification(self, real_services_fixture):\n        \"\"\"
        Test agent scalability and identify performance bottlenecks.\n        \n        Performance SLA:\n        - Linear or sub-linear scaling up to 30 concurrent agents\n        - Identify bottlenecks before system degradation\n        - Maintain >90% efficiency at scale\n        \"\"\"\n        db = real_services_fixture[\"db\"]\n        redis = real_services_fixture[\"redis\"]\n        \n        # Test different scales to identify bottlenecks\n        scale_tests = [5, 10, 20, 30]\n        scalability_results = []\n        \n        for scale in scale_tests:\n            print(f\"📈 Testing scalability at {scale} concurrent agents...\")\n            \n            scale_start = time.time()\n            agent_metrics = []\n            \n            async def scalability_test_agent(agent_id: int) -> Dict[str, Any]:\n                \"\"\"Lightweight agent for scalability testing.\"\"\"\n                agent_start = time.time()\n                operations = 3  # Reduced for scalability testing\n                \n                try:\n                    user_context = await create_authenticated_user_context(\n                        user_email=f\"scale_{scale}_agent_{agent_id}@example.com\",\n                        environment=\"test\"\n                    )\n                    \n                    for i in range(operations):\n                        # Simple database operation\n                        await db.execute(\"SELECT $1 as agent_id, $2 as operation\", agent_id, i)\n                        \n                        # Simple Redis operation\n                        await redis.set(f\"scale_test:{scale}:{agent_id}:{i}\", f\"data_{i}\", ex=60)\n                        \n                        # Minimal processing\n                        await asyncio.sleep(0.01)\n                    \n                    return {\n                        \"agent_id\": agent_id,\n                        \"execution_time\": time.time() - agent_start,\n                        \"operations_completed\": operations,\n                        \"success\": True\n                    }\n                    \n                except Exception as e:\n                    return {\n                        \"agent_id\": agent_id,\n                        \"execution_time\": time.time() - agent_start,\n                        \"operations_completed\": 0,\n                        \"success\": False,\n                        \"error\": str(e)\n                    }\n            \n            # Execute scalability test for current scale\n            scale_tasks = [\n                scalability_test_agent(agent_id)\n                for agent_id in range(scale)\n            ]\n            \n            scale_agent_results = await asyncio.gather(*scale_tasks, return_exceptions=True)\n            scale_duration = time.time() - scale_start\n            \n            # Analyze scale results\n            successful_agents = [r for r in scale_agent_results if isinstance(r, dict) and r.get(\"success\", False)]\n            \n            if successful_agents:\n                execution_times = [r[\"execution_time\"] for r in successful_agents]\n                avg_execution_time = statistics.mean(execution_times)\n                max_execution_time = max(execution_times)\n                \n                scale_result = {\n                    \"scale\": scale,\n                    \"successful_agents\": len(successful_agents),\n                    \"total_agents\": scale,\n                    \"success_rate\": len(successful_agents) / scale,\n                    \"total_duration\": scale_duration,\n                    \"average_execution_time\": avg_execution_time,\n                    \"max_execution_time\": max_execution_time,\n                    \"throughput_agents_per_second\": len(successful_agents) / scale_duration,\n                    \"efficiency\": len(successful_agents) / scale  # Perfect efficiency would be 1.0\n                }\n            else:\n                scale_result = {\n                    \"scale\": scale,\n                    \"successful_agents\": 0,\n                    \"total_agents\": scale,\n                    \"success_rate\": 0,\n                    \"total_duration\": scale_duration,\n                    \"average_execution_time\": 0,\n                    \"max_execution_time\": 0,\n                    \"throughput_agents_per_second\": 0,\n                    \"efficiency\": 0\n                }\n            \n            scalability_results.append(scale_result)\n            \n            # Cleanup scale test data\n            for agent_id in range(scale):\n                for i in range(3):\n                    try:\n                        await redis.delete(f\"scale_test:{scale}:{agent_id}:{i}\")\n                    except Exception:\n                        pass\n        \n        # Analyze scalability trends\n        efficiencies = [r[\"efficiency\"] for r in scalability_results]\n        throughputs = [r[\"throughput_agents_per_second\"] for r in scalability_results]\n        \n        # Check for scalability bottlenecks\n        bottlenecks = []\n        \n        # Efficiency degradation check\n        for i in range(1, len(efficiencies)):\n            efficiency_drop = efficiencies[i-1] - efficiencies[i]\n            if efficiency_drop > 0.1:  # 10% efficiency drop\n                bottlenecks.append(f\"Efficiency bottleneck between {scalability_results[i-1]['scale']} and {scalability_results[i]['scale']} agents\")\n        \n        # Throughput scaling check\n        expected_linear_scaling = True\n        for i in range(1, len(throughputs)):\n            scale_ratio = scalability_results[i][\"scale\"] / scalability_results[i-1][\"scale\"]\n            throughput_ratio = throughputs[i] / throughputs[i-1] if throughputs[i-1] > 0 else 0\n            \n            if throughput_ratio < scale_ratio * 0.7:  # Less than 70% of expected scaling\n                bottlenecks.append(f\"Throughput bottleneck at {scalability_results[i]['scale']} agents\")\n                expected_linear_scaling = False\n        \n        # Overall scalability assertions\n        final_efficiency = efficiencies[-1] if efficiencies else 0\n        assert final_efficiency >= 0.90, f\"Final efficiency {final_efficiency:.3f} below 90% at {scale_tests[-1]} agents\"\n        \n        min_success_rate = min([r[\"success_rate\"] for r in scalability_results])\n        assert min_success_rate >= 0.95, f\"Minimum success rate {min_success_rate:.3f} below 95% across scales\"\n        \n        print(f\"✅ Agent Scalability Test Results:\")\n        for result in scalability_results:\n            print(f\"   Scale {result['scale']:2d}: {result['successful_agents']:2d}/{result['total_agents']:2d} agents, \"\n                  f\"efficiency: {result['efficiency']:.3f}, throughput: {result['throughput_agents_per_second']:.1f} agents/s\")\n        \n        print(f\"   Scalability Analysis:\")\n        print(f\"     Final efficiency: {final_efficiency:.3f}\")\n        print(f\"     Linear scaling: {'Yes' if expected_linear_scaling else 'No'}\")\n        print(f\"     Bottlenecks identified: {len(bottlenecks)}\")\n        \n        if bottlenecks:\n            for bottleneck in bottlenecks:\n                print(f\"       - {bottleneck}\")\n        \n        return {\n            \"scalability_results\": scalability_results,\n            \"bottlenecks\": bottlenecks,\n            \"final_efficiency\": final_efficiency,\n            \"linear_scaling\": expected_linear_scaling\n        }"