"""
Integration Tests for Agent Response Quality and Business Value Delivery

Business Value Justification (BVJ):
- Segment: All (Free, Early, Mid, Enterprise)
- Business Goal: User Experience & Platform Value - Ensure agents deliver high-quality, actionable responses
- Value Impact: Validates agents provide substantive business solutions (90% of platform value)
- Strategic Impact: $500K+ ARR protection - Response quality directly impacts user satisfaction and retention

This module tests agent response quality with emphasis on:
1. Response content quality and actionability
2. Business problem-solving capability
3. Response formatting and structure
4. Context awareness and personalization
5. Multi-agent coordination for complex responses

CRITICAL REQUIREMENTS per CLAUDE.md:
- NO MOCKS - use real agent systems and LLM responses
- Focus on business value and actionable insights
- Test response quality across different agent types
- Validate response structure and formatting
- Ensure responses solve real business problems
- Test context retention and personalization
"""

import asyncio
import json
import re
import time
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Set, Tuple
from dataclasses import dataclass
import pytest

# SSOT imports following established patterns
from test_framework.ssot.base_test_case import SSotAsyncTestCase
from shared.isolated_environment import get_env

# Agent response infrastructure - REAL SERVICES ONLY
from netra_backend.app.agents.supervisor.user_execution_engine import UserExecutionEngine
from netra_backend.app.agents.supervisor.execution_context import (
    AgentExecutionContext,
    AgentExecutionResult
)
from netra_backend.app.services.user_execution_context import UserExecutionContext
from netra_backend.app.agents.supervisor.agent_instance_factory import get_agent_instance_factory
from netra_backend.app.services.agent_websocket_bridge import AgentWebSocketBridge
from netra_backend.app.websocket_core.unified_emitter import UnifiedWebSocketEmitter
from netra_backend.app.tools.enhanced_dispatcher import EnhancedToolDispatcher
from netra_backend.app.agents.base_agent import BaseAgent


@dataclass
class ResponseQualityMetrics:
    """Metrics for evaluating agent response quality."""
    
    # Content quality metrics
    response_length: int = 0
    word_count: int = 0
    sentence_count: int = 0
    paragraph_count: int = 0
    
    # Business value metrics
    actionable_items_count: int = 0
    recommendations_count: int = 0
    insights_count: int = 0
    data_points_count: int = 0
    
    # Structure quality metrics
    has_introduction: bool = False
    has_summary: bool = False
    has_structured_format: bool = False
    has_clear_sections: bool = False
    
    # Context quality metrics
    addresses_user_question: bool = False
    uses_domain_context: bool = False
    personalizes_response: bool = False
    maintains_conversation_context: bool = False
    
    # Technical quality metrics
    response_time: float = 0.0
    tool_usage_count: int = 0
    error_indicators_count: int = 0
    
    def calculate_quality_score(self) -> float:
        """Calculate overall quality score (0-100)."""
        score = 0
        
        # Content quality (25 points)
        if self.word_count >= 50:
            score += min(25, self.word_count / 20)
        
        # Business value (30 points)
        business_value = (
            self.actionable_items_count * 3 +
            self.recommendations_count * 3 +
            self.insights_count * 2 +
            self.data_points_count * 1
        )
        score += min(30, business_value)
        
        # Structure quality (20 points)
        structure_score = 0
        if self.has_structured_format:
            structure_score += 5
        if self.has_clear_sections:
            structure_score += 5
        if self.has_introduction:
            structure_score += 5
        if self.has_summary:
            structure_score += 5
        score += structure_score
        
        # Context quality (15 points)
        context_score = 0
        if self.addresses_user_question:
            context_score += 5
        if self.uses_domain_context:
            context_score += 4
        if self.personalizes_response:
            context_score += 3
        if self.maintains_conversation_context:
            context_score += 3
        score += context_score
        
        # Technical quality (10 points)
        technical_score = 10
        if self.response_time > 30:
            technical_score -= 5
        if self.error_indicators_count > 0:
            technical_score -= self.error_indicators_count * 2
        score += max(0, technical_score)
        
        return min(100, max(0, score))


class ResponseQualityAnalyzer:
    """Analyzes agent response quality using various metrics."""
    
    def __init__(self):
        # Keywords for identifying business value elements
        self.actionable_keywords = [
            'recommend', 'suggest', 'should', 'could', 'implement', 'optimize',
            'improve', 'reduce', 'increase', 'deploy', 'configure', 'action'
        ]
        
        self.insight_keywords = [
            'analysis', 'finding', 'insight', 'trend', 'pattern', 'correlation',
            'impact', 'conclusion', 'observation', 'discovery'
        ]
        
        self.recommendation_keywords = [
            'recommendation', 'advice', 'guidance', 'best practice', 'solution',
            'approach', 'strategy', 'plan', 'proposal'
        ]
        
        self.error_keywords = [
            'error', 'failed', 'unable', 'cannot', 'issue', 'problem',
            'exception', 'timeout', 'unavailable'
        ]
    
    def analyze_response(self, response: str, user_message: str, response_time: float, tools_used: List[str] = None) -> ResponseQualityMetrics:
        """Analyze response quality and return metrics."""
        if not response:
            return ResponseQualityMetrics()
        
        metrics = ResponseQualityMetrics()
        response_lower = response.lower()
        
        # Basic content metrics
        metrics.response_length = len(response)
        metrics.word_count = len(response.split())
        metrics.sentence_count = len([s for s in response.split('.') if s.strip()])
        metrics.paragraph_count = len([p for p in response.split('\n\n') if p.strip()])
        metrics.response_time = response_time
        metrics.tool_usage_count = len(tools_used or [])
        
        # Business value analysis
        metrics.actionable_items_count = self._count_keywords(response_lower, self.actionable_keywords)
        metrics.recommendations_count = self._count_keywords(response_lower, self.recommendation_keywords)
        metrics.insights_count = self._count_keywords(response_lower, self.insight_keywords)
        metrics.error_indicators_count = self._count_keywords(response_lower, self.error_keywords)
        
        # Data points (numbers, percentages, metrics)
        metrics.data_points_count = len(re.findall(r'\d+(?:\.\d+)?%?|\$\d+(?:,\d{3})*(?:\.\d{2})?', response))
        
        # Structure analysis
        metrics.has_introduction = self._has_introduction(response)
        metrics.has_summary = self._has_summary(response)
        metrics.has_structured_format = self._has_structured_format(response)
        metrics.has_clear_sections = self._has_clear_sections(response)
        
        # Context analysis
        metrics.addresses_user_question = self._addresses_user_question(response, user_message)
        metrics.uses_domain_context = self._uses_domain_context(response)
        metrics.personalizes_response = self._personalizes_response(response)
        
        return metrics
    
    def _count_keywords(self, text: str, keywords: List[str]) -> int:
        """Count occurrences of keywords in text."""
        count = 0
        for keyword in keywords:
            count += text.count(keyword)
        return count
    
    def _has_introduction(self, response: str) -> bool:
        """Check if response has an introduction."""
        intro_indicators = ['based on', 'analysis shows', 'here is', 'let me', 'i\'ll help']
        first_sentence = response.lower().split('.')[0] if '.' in response else response.lower()
        return any(indicator in first_sentence for indicator in intro_indicators)
    
    def _has_summary(self, response: str) -> bool:
        """Check if response has a summary or conclusion."""
        summary_indicators = ['in summary', 'to summarize', 'in conclusion', 'overall', 'key takeaway']
        return any(indicator in response.lower() for indicator in summary_indicators)
    
    def _has_structured_format(self, response: str) -> bool:
        """Check if response has structured formatting."""
        # Look for lists, bullets, numbering
        structure_indicators = ['\n-', '\nâ€¢', '\n*', '1.', '2.', '3.', '\n##', '\n###']
        return any(indicator in response for indicator in structure_indicators)
    
    def _has_clear_sections(self, response: str) -> bool:
        """Check if response has clear sections."""
        # Look for section headers or clear topic transitions
        return '\n\n' in response and len(response.split('\n\n')) >= 2
    
    def _addresses_user_question(self, response: str, user_message: str) -> bool:
        """Check if response addresses the user's question."""
        if not user_message:
            return False
        
        # Extract key terms from user message
        user_keywords = set(word.lower() for word in re.findall(r'\w+', user_message) if len(word) > 3)
        response_keywords = set(word.lower() for word in re.findall(r'\w+', response) if len(word) > 3)
        
        # Check overlap
        overlap = len(user_keywords & response_keywords)
        return overlap >= min(3, len(user_keywords) // 2)
    
    def _uses_domain_context(self, response: str) -> bool:
        """Check if response uses domain-specific context."""
        domain_indicators = [
            'system', 'performance', 'optimization', 'cost', 'efficiency',
            'infrastructure', 'analysis', 'metrics', 'data', 'report'
        ]
        return sum(1 for indicator in domain_indicators if indicator in response.lower()) >= 2
    
    def _personalizes_response(self, response: str) -> bool:
        """Check if response is personalized."""
        personal_indicators = ['your', 'you', 'specific', 'custom', 'based on your']
        return any(indicator in response.lower() for indicator in personal_indicators)


class TestResponseQualityIntegration(SSotAsyncTestCase):
    """Integration tests for agent response quality and business value delivery."""
    
    async def async_setup_method(self, method=None):
        """Set up test environment with real agent response infrastructure."""
        await super().async_setup_method(method)
        
        # Initialize environment
        self.env = get_env()
        self.set_env_var("TESTING", "true")
        self.set_env_var("TEST_MODE", "response_quality")
        
        # Initialize response quality analyzer
        self.quality_analyzer = ResponseQualityAnalyzer()
        
        # Initialize agent infrastructure
        self.agent_factory = None
        self.execution_engine = None
        self.tool_dispatcher = None
        
        # Quality metrics tracking
        self.quality_metrics = {
            'responses_analyzed': 0,
            'average_quality_score': 0.0,
            'high_quality_responses': 0,
            'business_value_responses': 0,
            'response_times': []
        }
        
        await self._initialize_response_infrastructure()
    
    async def _initialize_response_infrastructure(self):
        """Initialize real agent infrastructure for response testing."""
        try:
            # Initialize agent factory
            self.agent_factory = get_agent_instance_factory()
            
            # Initialize tool dispatcher
            self.tool_dispatcher = EnhancedToolDispatcher()
            
            # Initialize execution engine
            self.execution_engine = UserExecutionEngine(
                tool_dispatcher=self.tool_dispatcher
            )
            
            self.record_metric("response_infrastructure_init_success", True)
            
        except Exception as e:
            self.record_metric("response_infrastructure_init_error", str(e))
            raise
    
    async def _execute_and_analyze_response(
        self,
        agent_name: str,
        user_message: str,
        user_context: UserExecutionContext,
        tools_available: List[str] = None
    ) -> Tuple[AgentExecutionResult, ResponseQualityMetrics]:
        """Execute agent and analyze response quality."""
        # Create execution context
        execution_context = AgentExecutionContext(
            agent_name=agent_name,
            user_message=user_message,
            user_context=user_context,
            tools_available=tools_available or [],
            execution_id=f"quality_exec_{uuid.uuid4().hex[:8]}"
        )
        
        # Execute agent
        start_time = time.time()
        result = await self.execution_engine.execute_agent(execution_context)
        execution_time = time.time() - start_time
        
        # Analyze response quality
        response_content = result.agent_response if result else ""
        tools_used = result.tools_used if result else []
        
        quality_metrics = self.quality_analyzer.analyze_response(
            response_content,
            user_message,
            execution_time,
            tools_used
        )
        
        return result, quality_metrics
    
    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_triage_agent_response_quality(self):
        """
        Test triage agent response quality for system analysis queries.
        
        Business Value: Validates triage agent provides high-quality initial
        analysis and routing decisions for user queries.
        """
        # Create user context
        user_context = UserExecutionContext(
            user_id=f"triage_user_{uuid.uuid4().hex[:8]}",
            thread_id=f"triage_thread_{uuid.uuid4().hex[:8]}",
            run_id=f"triage_run_{uuid.uuid4().hex[:8]}",
            session_metadata={"test_type": "triage_quality"}
        )
        
        # Test various triage scenarios
        test_scenarios = [
            {
                "message": "Our application is running slowly and users are complaining about response times",
                "expected_elements": ["performance", "analysis", "recommendations"],
                "min_quality_score": 70
            },
            {
                "message": "I need to optimize our cloud costs - they've increased 50% this month",
                "expected_elements": ["cost", "optimization", "analysis"],
                "min_quality_score": 70
            },
            {
                "message": "Help me understand why our database queries are taking so long",
                "expected_elements": ["database", "queries", "performance"],
                "min_quality_score": 65
            }
        ]
        
        quality_scores = []
        response_analysis = []
        
        for scenario in test_scenarios:
            # Execute triage agent
            result, quality_metrics = await self._execute_and_analyze_response(
                agent_name="triage_agent",
                user_message=scenario["message"],
                user_context=user_context,
                tools_available=["system_analyzer", "performance_monitor"]
            )
            
            # Validate execution succeeded
            self.assertIsNotNone(result, f"Triage agent should execute successfully for: {scenario['message'][:50]}...")
            self.assertTrue(result.success or result.partial_success, "Triage execution should succeed")
            
            # Validate response content
            self.assertIsNotNone(result.agent_response, "Triage agent should provide response")
            self.assertGreater(len(result.agent_response), 50, "Response should be substantial")
            
            # Calculate quality score
            quality_score = quality_metrics.calculate_quality_score()
            quality_scores.append(quality_score)
            
            # Validate quality score
            self.assertGreater(
                quality_score, 
                scenario["min_quality_score"], 
                f"Quality score {quality_score} should exceed {scenario['min_quality_score']} for: {scenario['message'][:50]}..."
            )
            
            # Validate expected elements are present
            response_lower = result.agent_response.lower()
            for element in scenario["expected_elements"]:
                self.assertIn(element, response_lower, 
                            f"Response should contain '{element}' for query about {element}")
            
            # Business value validation
            self.assertGreater(quality_metrics.actionable_items_count, 0, 
                             "Triage response should contain actionable items")
            
            response_analysis.append({
                "scenario": scenario["message"][:30] + "...",
                "quality_score": quality_score,
                "word_count": quality_metrics.word_count,
                "actionable_items": quality_metrics.actionable_items_count,
                "response_time": quality_metrics.response_time
            })
        
        # Overall quality validation
        avg_quality_score = sum(quality_scores) / len(quality_scores)
        self.assertGreater(avg_quality_score, 65, "Average triage quality should exceed 65")
        
        # Performance validation
        avg_response_time = sum(m.response_time for _, m in [
            await self._execute_and_analyze_response("triage_agent", s["message"], user_context)
            for s in test_scenarios[:1]  # Test one for timing
        ]) / 1
        
        self.record_metric("triage_avg_quality_score", avg_quality_score)
        self.record_metric("triage_scenarios_tested", len(test_scenarios))
        self.record_metric("triage_response_analysis", response_analysis)
    
    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_optimization_agent_business_value_delivery(self):
        """
        Test optimization agent delivers high business value in responses.
        
        Business Value: Validates optimization agent provides actionable
        cost savings and performance improvements with measurable impact.
        """
        # Create user context for optimization testing
        user_context = UserExecutionContext(
            user_id=f"opt_user_{uuid.uuid4().hex[:8]}",
            thread_id=f"opt_thread_{uuid.uuid4().hex[:8]}",
            run_id=f"opt_run_{uuid.uuid4().hex[:8]}",
            session_metadata={"test_type": "optimization_business_value"}
        )
        
        # Test optimization scenarios with expected business value
        optimization_scenarios = [
            {
                "message": "Optimize our AWS infrastructure - we're spending $10,000/month",
                "expected_business_value": {
                    "cost_savings": True,
                    "specific_recommendations": True,
                    "quantified_impact": True
                },
                "required_elements": ["cost", "savings", "AWS", "recommendations"],
                "min_quality_score": 75
            },
            {
                "message": "Our API response times are 2-3 seconds, need to improve performance",
                "expected_business_value": {
                    "performance_improvements": True,
                    "actionable_steps": True,
                    "measurable_outcomes": True
                },
                "required_elements": ["performance", "response time", "optimization", "improvement"],
                "min_quality_score": 70
            }
        ]
        
        business_value_scores = []
        optimization_results = []
        
        for scenario in optimization_scenarios:
            # Execute optimization agent
            result, quality_metrics = await self._execute_and_analyze_response(
                agent_name="apex_optimizer_agent",
                user_message=scenario["message"],
                user_context=user_context,
                tools_available=["cost_analyzer", "performance_optimizer", "system_analyzer"]
            )
            
            # Validate execution
            self.assertIsNotNone(result)
            self.assertTrue(result.success or result.partial_success)
            
            # Validate business value elements
            response_content = result.agent_response
            self.assertIsNotNone(response_content)
            
            # Check for required elements
            response_lower = response_content.lower()
            for element in scenario["required_elements"]:
                self.assertIn(element, response_lower, 
                            f"Optimization response should mention '{element}'")
            
            # Business value analysis
            business_value_score = 0
            
            # Check for cost savings indicators
            if scenario["expected_business_value"]["cost_savings"]:
                cost_indicators = ["save", "reduce cost", "optimize spending", "cost reduction"]
                if any(indicator in response_lower for indicator in cost_indicators):
                    business_value_score += 25
            
            # Check for specific recommendations
            if scenario["expected_business_value"]["specific_recommendations"]:
                if quality_metrics.recommendations_count >= 2:
                    business_value_score += 25
            
            # Check for quantified impact
            if scenario["expected_business_value"]["quantified_impact"]:
                if quality_metrics.data_points_count >= 1:
                    business_value_score += 25
            
            # Check for actionable steps
            if scenario["expected_business_value"].get("actionable_steps"):
                if quality_metrics.actionable_items_count >= 3:
                    business_value_score += 25
            
            business_value_scores.append(business_value_score)
            
            # Quality score validation
            quality_score = quality_metrics.calculate_quality_score()
            self.assertGreater(quality_score, scenario["min_quality_score"], 
                             f"Optimization quality score should exceed {scenario['min_quality_score']}")
            
            # Validate substantial content
            self.assertGreater(quality_metrics.word_count, 100, 
                             "Optimization response should be comprehensive")
            
            optimization_results.append({
                "scenario": scenario["message"][:40] + "...",
                "business_value_score": business_value_score,
                "quality_score": quality_score,
                "recommendations": quality_metrics.recommendations_count,
                "actionable_items": quality_metrics.actionable_items_count,
                "data_points": quality_metrics.data_points_count
            })
        
        # Overall business value validation
        avg_business_value = sum(business_value_scores) / len(business_value_scores)
        self.assertGreater(avg_business_value, 60, 
                         "Average business value score should exceed 60")
        
        self.record_metric("optimization_avg_business_value", avg_business_value)
        self.record_metric("optimization_results", optimization_results)
        self.record_metric("optimization_business_value_validated", True)
    
    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_multi_agent_response_coordination(self):
        """
        Test response quality in multi-agent coordination scenarios.
        
        Business Value: Validates complex problems requiring multiple agents
        produce coordinated, high-quality responses.
        """
        # Create user context for multi-agent testing
        user_context = UserExecutionContext(
            user_id=f"multi_user_{uuid.uuid4().hex[:8]}",
            thread_id=f"multi_thread_{uuid.uuid4().hex[:8]}",
            run_id=f"multi_run_{uuid.uuid4().hex[:8]}",
            session_metadata={"test_type": "multi_agent_coordination"}
        )
        
        # Complex scenario requiring multiple agent types
        complex_query = (
            "We need a comprehensive analysis of our system: "
            "performance issues are causing customer complaints, "
            "costs are rising, and we need actionable recommendations "
            "with a detailed implementation plan."
        )
        
        # Execute multiple agents in sequence to simulate coordination
        agent_sequence = [
            {
                "agent": "triage_agent",
                "focus": "Initial analysis and problem identification",
                "tools": ["system_analyzer"]
            },
            {
                "agent": "data_helper_agent", 
                "focus": "Data gathering and performance analysis",
                "tools": ["performance_monitor", "data_processor"]
            },
            {
                "agent": "apex_optimizer_agent",
                "focus": "Optimization recommendations and implementation plan",
                "tools": ["cost_analyzer", "performance_optimizer"]
            }
        ]
        
        agent_responses = []
        cumulative_context = complex_query
        
        for step in agent_sequence:
            # Execute agent with cumulative context
            result, quality_metrics = await self._execute_and_analyze_response(
                agent_name=step["agent"],
                user_message=cumulative_context,
                user_context=user_context,
                tools_available=step["tools"]
            )
            
            # Validate execution
            self.assertIsNotNone(result, f"{step['agent']} should execute successfully")
            self.assertTrue(result.success or result.partial_success)
            
            # Quality validation
            quality_score = quality_metrics.calculate_quality_score()
            self.assertGreater(quality_score, 60, 
                             f"{step['agent']} quality score should exceed 60")
            
            # Content validation
            response_content = result.agent_response
            self.assertIsNotNone(response_content)
            self.assertGreater(len(response_content), 75, 
                             f"{step['agent']} should provide substantial response")
            
            # Context continuity validation
            self.assertTrue(quality_metrics.addresses_user_question,
                          f"{step['agent']} should address the user question")
            
            agent_responses.append({
                "agent": step["agent"],
                "quality_score": quality_score,
                "word_count": quality_metrics.word_count,
                "actionable_items": quality_metrics.actionable_items_count,
                "recommendations": quality_metrics.recommendations_count,
                "response_preview": response_content[:100] + "..."
            })
            
            # Add this agent's response to cumulative context for next agent
            cumulative_context += f"\n\nPrevious analysis from {step['agent']}: {response_content[:200]}..."
        
        # Validate coordination effectiveness
        total_actionable_items = sum(r["actionable_items"] for r in agent_responses)
        total_recommendations = sum(r["recommendations"] for r in agent_responses)
        avg_quality = sum(r["quality_score"] for r in agent_responses) / len(agent_responses)
        
        # Multi-agent coordination should produce substantial results
        self.assertGreater(total_actionable_items, 5, 
                         "Multi-agent coordination should produce multiple actionable items")
        self.assertGreater(total_recommendations, 3, 
                         "Multi-agent coordination should produce multiple recommendations")
        self.assertGreater(avg_quality, 65, 
                         "Average quality across agents should exceed 65")
        
        # Validate complementary responses (each agent should add value)
        for i, response in enumerate(agent_responses):
            self.assertGreater(response["quality_score"], 55, 
                             f"Agent {response['agent']} should meet minimum quality threshold")
            
            # Each agent should contribute meaningfully
            self.assertTrue(
                response["actionable_items"] > 0 or response["recommendations"] > 0,
                f"Agent {response['agent']} should contribute actionable value"
            )
        
        self.record_metric("multi_agent_avg_quality", avg_quality)
        self.record_metric("multi_agent_total_actionable_items", total_actionable_items)
        self.record_metric("multi_agent_total_recommendations", total_recommendations)
        self.record_metric("multi_agent_responses", agent_responses)
        self.record_metric("multi_agent_coordination_success", True)
    
    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_response_personalization_and_context_awareness(self):
        """
        Test agent response personalization and context awareness.
        
        Business Value: Validates agents provide personalized, context-aware
        responses that enhance user experience and engagement.
        """
        # Create user contexts with different profiles
        user_profiles = [
            {
                "user_id": f"tech_user_{uuid.uuid4().hex[:8]}",
                "profile": "technical_lead",
                "metadata": {
                    "role": "Technical Lead",
                    "experience": "Senior",
                    "preferences": "detailed_technical_analysis",
                    "context": "enterprise_infrastructure"
                },
                "query": "Optimize our microservices architecture for better performance"
            },
            {
                "user_id": f"biz_user_{uuid.uuid4().hex[:8]}",
                "profile": "business_manager", 
                "metadata": {
                    "role": "Business Manager",
                    "experience": "Business",
                    "preferences": "high_level_summary_with_roi",
                    "context": "cost_optimization"
                },
                "query": "Reduce our cloud spending while maintaining service quality"
            }
        ]
        
        personalization_results = []
        
        for profile in user_profiles:
            # Create personalized user context
            user_context = UserExecutionContext(
                user_id=profile["user_id"],
                thread_id=f"personal_thread_{uuid.uuid4().hex[:8]}",
                run_id=f"personal_run_{uuid.uuid4().hex[:8]}",
                session_metadata={
                    "test_type": "personalization",
                    **profile["metadata"]
                }
            )
            
            # Execute agent with personalized context
            result, quality_metrics = await self._execute_and_analyze_response(
                agent_name="apex_optimizer_agent",
                user_message=profile["query"],
                user_context=user_context,
                tools_available=["system_analyzer", "cost_analyzer", "performance_optimizer"]
            )
            
            # Validate execution
            self.assertIsNotNone(result)
            self.assertTrue(result.success or result.partial_success)
            
            # Analyze response for personalization
            response_content = result.agent_response
            self.assertIsNotNone(response_content)
            
            # Validate context awareness
            self.assertTrue(quality_metrics.addresses_user_question,
                          "Response should address user's specific question")
            
            # Profile-specific validation
            response_lower = response_content.lower()
            
            if profile["profile"] == "technical_lead":
                # Technical responses should be detailed and technical
                technical_terms = ["architecture", "performance", "optimization", "system", "technical"]
                technical_matches = sum(1 for term in technical_terms if term in response_lower)
                self.assertGreater(technical_matches, 2, 
                                 "Technical user should receive technical response")
                
                # Should have detailed recommendations
                self.assertGreater(quality_metrics.recommendations_count, 2,
                                 "Technical user should receive detailed recommendations")
                
            elif profile["profile"] == "business_manager":
                # Business responses should focus on ROI and high-level benefits
                business_terms = ["cost", "savings", "roi", "business", "value", "benefit"]
                business_matches = sum(1 for term in business_terms if term in response_lower)
                self.assertGreater(business_matches, 2,
                                 "Business user should receive business-focused response")
                
                # Should have cost/benefit analysis
                self.assertGreater(quality_metrics.data_points_count, 0,
                                 "Business user should receive quantified information")
            
            # Quality validation
            quality_score = quality_metrics.calculate_quality_score()
            self.assertGreater(quality_score, 65, 
                             f"Personalized response quality should exceed 65 for {profile['profile']}")
            
            # Personalization validation
            self.assertTrue(quality_metrics.personalizes_response,
                          "Response should show personalization elements")
            
            personalization_results.append({
                "profile": profile["profile"],
                "quality_score": quality_score,
                "personalizes_response": quality_metrics.personalizes_response,
                "addresses_question": quality_metrics.addresses_user_question,
                "uses_domain_context": quality_metrics.uses_domain_context,
                "word_count": quality_metrics.word_count,
                "actionable_items": quality_metrics.actionable_items_count
            })
        
        # Validate differentiation between profiles
        tech_result = personalization_results[0]
        biz_result = personalization_results[1]
        
        # Responses should be personalized differently
        self.assertNotEqual(tech_result["word_count"], biz_result["word_count"],
                          "Technical and business responses should differ in length")
        
        # Both should be high quality but potentially different styles
        for result in personalization_results:
            self.assertTrue(result["personalizes_response"],
                          f"{result['profile']} should receive personalized response")
            self.assertTrue(result["addresses_question"],
                          f"{result['profile']} response should address their question")
            self.assertGreater(result["quality_score"], 60,
                             f"{result['profile']} should receive quality response")
        
        self.record_metric("personalization_results", personalization_results)
        self.record_metric("personalization_validated", True)
        avg_personalization_quality = sum(r["quality_score"] for r in personalization_results) / len(personalization_results)
        self.record_metric("avg_personalization_quality", avg_personalization_quality)
    
    @pytest.mark.integration
    @pytest.mark.real_services
    async def test_response_formatting_and_structure_quality(self):
        """
        Test response formatting and structure quality across different scenarios.
        
        Business Value: Ensures responses are well-structured and readable,
        improving user comprehension and action-ability.
        """
        # Create user context for formatting tests
        user_context = UserExecutionContext(
            user_id=f"format_user_{uuid.uuid4().hex[:8]}",
            thread_id=f"format_thread_{uuid.uuid4().hex[:8]}",
            run_id=f"format_run_{uuid.uuid4().hex[:8]}",
            session_metadata={"test_type": "formatting_quality"}
        )
        
        # Test scenarios that should produce well-structured responses
        formatting_scenarios = [
            {
                "agent": "data_helper_agent",
                "query": "Generate a comprehensive report on system performance with key metrics and recommendations",
                "expected_structure": {
                    "has_sections": True,
                    "has_lists": True,
                    "has_summary": True,
                    "min_paragraphs": 3
                }
            },
            {
                "agent": "apex_optimizer_agent", 
                "query": "Create an optimization plan for reducing costs with step-by-step implementation",
                "expected_structure": {
                    "has_numbered_steps": True,
                    "has_recommendations": True,
                    "has_introduction": True,
                    "min_word_count": 150
                }
            }
        ]
        
        formatting_results = []
        
        for scenario in formatting_scenarios:
            # Execute agent
            result, quality_metrics = await self._execute_and_analyze_response(
                agent_name=scenario["agent"],
                user_message=scenario["query"],
                user_context=user_context,
                tools_available=["system_analyzer", "performance_monitor", "report_generator"]
            )
            
            # Validate execution
            self.assertIsNotNone(result)
            self.assertTrue(result.success or result.partial_success)
            
            response_content = result.agent_response
            self.assertIsNotNone(response_content)
            
            # Structure validation
            expected = scenario["expected_structure"]
            
            if expected.get("has_sections"):
                self.assertTrue(quality_metrics.has_clear_sections,
                              f"{scenario['agent']} should provide clear sections")
            
            if expected.get("has_lists"):
                self.assertTrue(quality_metrics.has_structured_format,
                              f"{scenario['agent']} should use structured formatting")
            
            if expected.get("has_summary"):
                self.assertTrue(quality_metrics.has_summary,
                              f"{scenario['agent']} should provide summary")
            
            if expected.get("has_introduction"):
                self.assertTrue(quality_metrics.has_introduction,
                              f"{scenario['agent']} should provide introduction")
            
            if expected.get("min_paragraphs"):
                self.assertGreaterEqual(quality_metrics.paragraph_count, expected["min_paragraphs"],
                                      f"{scenario['agent']} should have at least {expected['min_paragraphs']} paragraphs")
            
            if expected.get("min_word_count"):
                self.assertGreaterEqual(quality_metrics.word_count, expected["min_word_count"],
                                      f"{scenario['agent']} should have at least {expected['min_word_count']} words")
            
            # General formatting quality
            quality_score = quality_metrics.calculate_quality_score()
            self.assertGreater(quality_score, 65,
                             f"{scenario['agent']} formatting quality should exceed 65")
            
            # Readability checks
            avg_sentence_length = quality_metrics.word_count / max(quality_metrics.sentence_count, 1)
            self.assertLess(avg_sentence_length, 30,
                          f"{scenario['agent']} should maintain readable sentence length")
            
            formatting_results.append({
                "agent": scenario["agent"],
                "quality_score": quality_score,
                "has_structured_format": quality_metrics.has_structured_format,
                "has_clear_sections": quality_metrics.has_clear_sections,
                "has_introduction": quality_metrics.has_introduction,
                "has_summary": quality_metrics.has_summary,
                "paragraph_count": quality_metrics.paragraph_count,
                "avg_sentence_length": avg_sentence_length
            })
        
        # Overall formatting quality validation
        avg_formatting_quality = sum(r["quality_score"] for r in formatting_results) / len(formatting_results)
        self.assertGreater(avg_formatting_quality, 65,
                         "Average formatting quality should exceed 65")
        
        # Structure consistency validation
        structured_responses = sum(1 for r in formatting_results if r["has_structured_format"])
        self.assertGreater(structured_responses, 0,
                         "At least one response should have structured formatting")
        
        self.record_metric("formatting_results", formatting_results)
        self.record_metric("avg_formatting_quality", avg_formatting_quality)
        self.record_metric("structured_responses_count", structured_responses)
        self.record_metric("formatting_quality_validated", True)
    
    async def async_teardown_method(self, method=None):
        """Clean up response quality infrastructure and log comprehensive metrics."""
        try:
            # Calculate final quality metrics
            all_metrics = self.get_all_metrics()
            
            # Clean up infrastructure
            if self.execution_engine:
                await self.execution_engine.cleanup()
            
            # Log comprehensive response quality analysis
            if all_metrics:
                print(f"\nResponse Quality Integration Test Results:")
                print(f"=" * 60)
                
                # Quality scores
                quality_keys = [k for k in all_metrics.keys() if 'quality' in k and 'score' in k]
                if quality_keys:
                    print(f"Quality Scores:")
                    for key in quality_keys:
                        print(f"  {key}: {all_metrics[key]:.2f}")
                
                # Business value metrics  
                business_keys = [k for k in all_metrics.keys() if 'business' in k or 'actionable' in k]
                if business_keys:
                    print(f"\nBusiness Value Metrics:")
                    for key in business_keys:
                        print(f"  {key}: {all_metrics[key]}")
                
                # Performance metrics
                perf_keys = [k for k in all_metrics.keys() if 'time' in k or 'count' in k]
                if perf_keys:
                    print(f"\nPerformance Metrics:")
                    for key in perf_keys:
                        value = all_metrics[key]
                        if isinstance(value, float):
                            print(f"  {key}: {value:.3f}")
                        else:
                            print(f"  {key}: {value}")
                
                print(f"=" * 60)
            
            self.record_metric("response_quality_test_completed", True)
            
        except Exception as e:
            self.record_metric("test_cleanup_error", str(e))
        
        await super().async_teardown_method(method)