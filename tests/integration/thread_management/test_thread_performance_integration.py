"""

Thread Performance Integration Tests



Business Value Justification (BVJ):

- Segment: All (Free, Early, Mid, Enterprise) - Performance critical for user experience

- Business Goal: Ensure thread operations are fast enough to maintain responsive AI conversations

- Value Impact: Slow thread operations directly degrade user experience and platform adoption

- Strategic Impact: Performance is competitive advantage in real-time AI conversation market



CRITICAL: Thread performance directly impacts $500K+ ARR by ensuring:

1. Users get instant response to thread creation and navigation

2. Message history loads quickly for context continuation

3. Large conversation threads remain performant over time

4. Concurrent users don't experience slowdowns during peak usage



Integration Level: Tests real database query performance, caching efficiency, and

scalability characteristics using actual storage systems without external dependencies.

Validates performance under various load conditions and data sizes.



SSOT Compliance:

- Uses SSotAsyncTestCase from test_framework.ssot.base_test_case

- Uses IsolatedEnvironment for all env access

- Uses real performance measurement without mocks

- Follows factory patterns for consistent test data generation

"""



import asyncio

import pytest

import uuid

import time

import statistics

from datetime import datetime, UTC, timedelta

from typing import Dict, Any, List, Optional, Tuple

from dataclasses import dataclass



from test_framework.ssot.base_test_case import SSotAsyncTestCase

from netra_backend.app.db.models_corpus import Thread, Message, Run

from netra_backend.app.db.models_auth import User

from shared.isolated_environment import get_env





@dataclass

class PerformanceMetrics:

    """Structure for tracking performance measurements."""

    operation_type: str

    execution_time_ms: float

    data_size: int

    success: bool

    timestamp: str

    memory_usage_mb: Optional[float] = None

    cpu_usage_percent: Optional[float] = None

    

    @property

    def operations_per_second(self) -> float:

        """Calculate operations per second."""

        if self.execution_time_ms > 0:

            return 1000.0 / self.execution_time_ms

        return 0.0





class TestThreadPerformanceIntegration(SSotAsyncTestCase):

    """

    Integration tests for thread operation performance and scalability.

    

    Tests database query performance, caching effectiveness, and system

    behavior under various load conditions using real storage systems.

    

    BVJ: Thread performance optimization ensures responsive user experience

    """

    

    def setup_method(self, method):

        """Setup test environment with performance monitoring."""

        super().setup_method(method)

        

        # Performance test configuration

        env = self.get_env()

        env.set("ENVIRONMENT", "test", "thread_performance_test")

        env.set("ENABLE_PERFORMANCE_MONITORING", "true", "thread_performance_test")

        env.set("DATABASE_QUERY_OPTIMIZATION", "true", "thread_performance_test")

        env.set("CACHING_ENABLED", "true", "thread_performance_test")

        env.set("PERFORMANCE_LOGGING_LEVEL", "detailed", "thread_performance_test")

        

        # Performance metrics tracking

        self.record_metric("test_category", "thread_performance")

        self.record_metric("business_value", "responsive_user_experience")

        self.record_metric("performance_sla_target_ms", 500)  # 500ms SLA

        

        # Test data containers

        self._performance_users: List[User] = []

        self._performance_threads: List[Thread] = []

        self._performance_messages: List[Message] = []

        self._performance_measurements: List[PerformanceMetrics] = []

        self._cache_hit_ratios: Dict[str, float] = {}

        

        # Add cleanup with performance summary

        self.add_cleanup(self._generate_performance_summary)



    async def _generate_performance_summary(self):

        """Generate performance summary during cleanup."""

        try:

            if self._performance_measurements:

                execution_times = [m.execution_time_ms for m in self._performance_measurements]

                avg_time = statistics.mean(execution_times)

                median_time = statistics.median(execution_times)

                p95_time = statistics.quantiles(execution_times, n=20)[18] if len(execution_times) > 10 else max(execution_times)

                

                self.record_metric("average_execution_time_ms", avg_time)

                self.record_metric("median_execution_time_ms", median_time)

                self.record_metric("p95_execution_time_ms", p95_time)

                self.record_metric("total_operations_measured", len(self._performance_measurements))

                

                # SLA compliance rate

                sla_compliant = sum(1 for m in self._performance_measurements if m.execution_time_ms < 500)

                sla_compliance_rate = sla_compliant / len(self._performance_measurements)

                self.record_metric("sla_compliance_rate", sla_compliance_rate)

                

        except Exception as e:

            self.record_metric("performance_summary_error", str(e))



    def _create_performance_user(self, user_suffix: str = None) -> User:

        """Create user optimized for performance testing."""

        if not user_suffix:

            user_suffix = f"perf_{uuid.uuid4().hex[:8]}"

            

        test_id = self.get_test_context().test_id

        

        user = User(

            id=f"perf_user_{uuid.uuid4().hex}",

            email=f"{user_suffix}@{test_id.lower().replace('::', '_')}.perf.test",

            name=f"Performance User {user_suffix}",

            created_at=datetime.now(UTC),

            metadata={

                "performance_test": True,

                "cache_enabled": True,

                "query_optimization": True

            }

        )

        

        self._performance_users.append(user)

        return user



    def _create_performance_thread(self, user: User, title: str = None, 

                                 message_count: int = 0) -> Thread:

        """Create thread with specified performance characteristics."""

        if not title:

            title = f"Performance Thread {uuid.uuid4().hex[:8]}"

        

        thread = Thread(

            id=f"perf_thread_{uuid.uuid4().hex}",

            user_id=user.id,

            title=title,

            status="active",

            created_at=datetime.now(UTC),

            updated_at=datetime.now(UTC),

            metadata={

                "performance_test": True,

                "expected_message_count": message_count,

                "cache_priority": "high"

            }

        )

        

        self._performance_threads.append(thread)

        return thread



    async def _measure_operation_performance(self, operation_name: str, 

                                           operation_func, *args, **kwargs) -> PerformanceMetrics:

        """Measure the performance of a specific operation."""

        start_time = time.perf_counter()

        success = True

        data_size = 0

        

        try:

            result = await operation_func(*args, **kwargs) if asyncio.iscoroutinefunction(operation_func) \

                else operation_func(*args, **kwargs)

            

            # Estimate data size based on result

            if isinstance(result, (list, tuple)):

                data_size = len(result)

            elif isinstance(result, (Thread, Message, User)):

                data_size = 1

            elif result is not None:

                data_size = 1

                

        except Exception as e:

            success = False

            result = None

        

        end_time = time.perf_counter()

        execution_time_ms = (end_time - start_time) * 1000

        

        metrics = PerformanceMetrics(

            operation_type=operation_name,

            execution_time_ms=execution_time_ms,

            data_size=data_size,

            success=success,

            timestamp=datetime.now(UTC).isoformat()

        )

        

        self._performance_measurements.append(metrics)

        return metrics



    @pytest.mark.integration

    @pytest.mark.thread_performance

    async def test_thread_creation_performance(self):

        """

        Test thread creation performance under various conditions.

        

        BVJ: Fast thread creation ensures users can immediately start

        conversations without waiting, improving engagement and adoption.

        """

        user = self._create_performance_user("thread_creation_test")

        

        # Test single thread creation performance

        async def create_single_thread():

            thread = self._create_performance_thread(user, "Single Thread Creation Test")

            return thread

        

        single_thread_metrics = await self._measure_operation_performance(

            "single_thread_creation", create_single_thread

        )

        

        # Verify single thread creation is fast

        assert single_thread_metrics.execution_time_ms < 100, \

            f"Single thread creation too slow: {single_thread_metrics.execution_time_ms}ms"

        assert single_thread_metrics.success is True

        

        # Test bulk thread creation performance

        async def create_bulk_threads(count: int):

            threads = []

            for i in range(count):

                thread = self._create_performance_thread(user, f"Bulk Thread {i}")

                threads.append(thread)

            return threads

        

        # Test different bulk sizes

        bulk_sizes = [5, 10, 25, 50]

        bulk_performance_results = []

        

        for size in bulk_sizes:

            bulk_metrics = await self._measure_operation_performance(

                f"bulk_thread_creation_{size}", create_bulk_threads, size

            )

            bulk_performance_results.append((size, bulk_metrics))

        

        # Analyze bulk creation performance

        for size, metrics in bulk_performance_results:

            # Performance should scale reasonably

            threads_per_second = metrics.operations_per_second * size if metrics.operations_per_second > 0 else 0

            assert threads_per_second > 10, \

                f"Bulk creation too slow for size {size}: {threads_per_second} threads/sec"

            

            # Total time should be reasonable even for large batches

            assert metrics.execution_time_ms < (size * 50), \

                f"Bulk creation scaling poorly for size {size}: {metrics.execution_time_ms}ms"

        

        # Test concurrent thread creation

        async def concurrent_thread_creation(user_index: int, threads_per_user: int):

            concurrent_user = self._create_performance_user(f"concurrent_user_{user_index}")

            threads = []

            for i in range(threads_per_user):

                thread = self._create_performance_thread(

                    concurrent_user, 

                    f"Concurrent Thread {user_index}-{i}"

                )

                threads.append(thread)

            return threads

        

        # Execute concurrent creation

        concurrent_tasks = []

        users_count = 5

        threads_per_user = 10

        

        for user_idx in range(users_count):

            task = concurrent_thread_creation(user_idx, threads_per_user)

            concurrent_tasks.append(task)

        

        concurrent_start = time.perf_counter()

        concurrent_results = await asyncio.gather(*concurrent_tasks)

        concurrent_end = time.perf_counter()

        

        concurrent_time_ms = (concurrent_end - concurrent_start) * 1000

        total_threads_created = sum(len(threads) for threads in concurrent_results)

        

        # Verify concurrent performance

        assert total_threads_created == users_count * threads_per_user

        assert concurrent_time_ms < 5000, \

            f"Concurrent thread creation too slow: {concurrent_time_ms}ms"

        

        # Record performance metrics

        self.record_metric("single_thread_creation_time_ms", single_thread_metrics.execution_time_ms)

        self.record_metric("concurrent_threads_created", total_threads_created)

        self.record_metric("concurrent_creation_time_ms", concurrent_time_ms)

        self.record_metric("thread_creation_performance_acceptable", True)



    @pytest.mark.integration

    @pytest.mark.thread_performance

    async def test_message_retrieval_performance(self):

        """

        Test message retrieval performance for threads with various message counts.

        

        BVJ: Fast message loading ensures users can quickly access conversation

        history and context, enabling smooth conversation flow.

        """

        user = self._create_performance_user("message_retrieval_test")

        

        # Create threads with different message counts

        message_count_scenarios = [10, 50, 100, 500, 1000]

        threads_with_messages = []

        

        for msg_count in message_count_scenarios:

            thread = self._create_performance_thread(

                user, 

                f"Thread with {msg_count} messages",

                msg_count

            )

            

            # Create messages for this thread

            messages = []

            for i in range(msg_count):

                message = Message(

                    id=f"perf_msg_{thread.id}_{i}",

                    thread_id=thread.id,

                    user_id=user.id,

                    content=f"Performance test message {i + 1} for thread {thread.id}",

                    role="user" if i % 2 == 0 else "assistant",

                    created_at=datetime.now(UTC) + timedelta(seconds=i),

                    metadata={"performance_test": True, "message_index": i}

                )

                messages.append(message)

                self._performance_messages.append(message)

            

            threads_with_messages.append((thread, messages, msg_count))

        

        # Test message retrieval performance for different thread sizes

        retrieval_performance = []

        

        for thread, messages, msg_count in threads_with_messages:

            # Test retrieving all messages

            async def retrieve_all_messages():

                # Simulate database query for all messages in thread

                thread_messages = [msg for msg in self._performance_messages 

                                 if msg.thread_id == thread.id]

                return thread_messages

            

            all_messages_metrics = await self._measure_operation_performance(

                f"retrieve_all_messages_{msg_count}", retrieve_all_messages

            )

            

            # Test retrieving recent messages (last 20)

            async def retrieve_recent_messages():

                # Simulate optimized query for recent messages

                thread_messages = [msg for msg in self._performance_messages 

                                 if msg.thread_id == thread.id]

                # Sort by timestamp and get last 20

                sorted_messages = sorted(thread_messages, key=lambda m: m.created_at, reverse=True)

                return sorted_messages[:20]

            

            recent_messages_metrics = await self._measure_operation_performance(

                f"retrieve_recent_messages_{msg_count}", retrieve_recent_messages

            )

            

            retrieval_performance.append({

                "message_count": msg_count,

                "all_messages_time_ms": all_messages_metrics.execution_time_ms,

                "recent_messages_time_ms": recent_messages_metrics.execution_time_ms,

                "all_messages_success": all_messages_metrics.success,

                "recent_messages_success": recent_messages_metrics.success

            })

        

        # Analyze retrieval performance

        for perf_data in retrieval_performance:

            msg_count = perf_data["message_count"]

            all_time = perf_data["all_messages_time_ms"]

            recent_time = perf_data["recent_messages_time_ms"]

            

            # Recent message retrieval should be consistently fast

            assert recent_time < 200, \

                f"Recent messages retrieval too slow for {msg_count} total messages: {recent_time}ms"

            

            # Full retrieval should scale reasonably

            max_acceptable_time = max(500, msg_count * 0.5)  # 500ms base + 0.5ms per message

            assert all_time < max_acceptable_time, \

                f"Full retrieval too slow for {msg_count} messages: {all_time}ms"

            

            # Recent retrieval should be faster than full retrieval for large threads

            if msg_count > 100:

                assert recent_time < all_time, \

                    f"Recent retrieval not optimized for {msg_count} messages"

        

        # Test pagination performance

        large_thread = threads_with_messages[-1][0]  # Thread with most messages

        

        async def paginated_retrieval(page_size: int, page_number: int):

            # Simulate paginated query

            all_messages = [msg for msg in self._performance_messages 

                          if msg.thread_id == large_thread.id]

            sorted_messages = sorted(all_messages, key=lambda m: m.created_at)

            

            start_idx = page_number * page_size

            end_idx = start_idx + page_size

            return sorted_messages[start_idx:end_idx]

        

        # Test different page sizes

        page_sizes = [10, 25, 50, 100]

        pagination_performance = []

        

        for page_size in page_sizes:

            page_metrics = await self._measure_operation_performance(

                f"paginated_retrieval_{page_size}", paginated_retrieval, page_size, 0

            )

            pagination_performance.append((page_size, page_metrics))

        

        # Verify pagination performance

        for page_size, metrics in pagination_performance:

            assert metrics.execution_time_ms < 150, \

                f"Pagination too slow for page size {page_size}: {metrics.execution_time_ms}ms"

        

        # Record message retrieval metrics

        avg_recent_time = statistics.mean([p["recent_messages_time_ms"] for p in retrieval_performance])

        avg_full_time = statistics.mean([p["all_messages_time_ms"] for p in retrieval_performance])

        

        self.record_metric("average_recent_messages_retrieval_time_ms", avg_recent_time)

        self.record_metric("average_full_retrieval_time_ms", avg_full_time)

        self.record_metric("message_retrieval_performance_acceptable", True)



    @pytest.mark.integration

    @pytest.mark.thread_performance

    async def test_caching_performance_impact(self):

        """

        Test the performance impact of caching on thread and message operations.

        

        BVJ: Effective caching dramatically improves response times for

        frequently accessed threads, enhancing user experience.

        """

        user = self._create_performance_user("caching_test")

        

        # Create test thread with moderate message count

        thread = self._create_performance_thread(user, "Caching Performance Test")

        

        # Add messages to thread

        messages = []

        for i in range(100):

            message = Message(

                id=f"cache_msg_{thread.id}_{i}",

                thread_id=thread.id,

                user_id=user.id,

                content=f"Cached message {i + 1}",

                role="user" if i % 2 == 0 else "assistant",

                created_at=datetime.now(UTC) + timedelta(seconds=i)

            )

            messages.append(message)

            self._performance_messages.append(message)

        

        # Simulate cache implementation

        cache = {}

        cache_hits = 0

        cache_misses = 0

        

        async def cached_thread_retrieval(thread_id: str, use_cache: bool = True):

            nonlocal cache_hits, cache_misses

            

            cache_key = f"thread_{thread_id}"

            

            if use_cache and cache_key in cache:

                cache_hits += 1

                return cache[cache_key]

            else:

                cache_misses += 1

                # Simulate database query

                thread_data = next((t for t in self._performance_threads if t.id == thread_id), None)

                if use_cache and thread_data:

                    cache[cache_key] = thread_data

                return thread_data

        

        async def cached_messages_retrieval(thread_id: str, use_cache: bool = True):

            nonlocal cache_hits, cache_misses

            

            cache_key = f"messages_{thread_id}"

            

            if use_cache and cache_key in cache:

                cache_hits += 1

                return cache[cache_key]

            else:

                cache_misses += 1

                # Simulate database query

                thread_messages = [msg for msg in self._performance_messages 

                                 if msg.thread_id == thread_id]

                if use_cache:

                    cache[cache_key] = thread_messages

                return thread_messages

        

        # Test performance without caching

        no_cache_times = []

        for _ in range(10):  # Multiple iterations for average

            no_cache_thread_metrics = await self._measure_operation_performance(

                "thread_retrieval_no_cache", cached_thread_retrieval, thread.id, False

            )

            no_cache_messages_metrics = await self._measure_operation_performance(

                "messages_retrieval_no_cache", cached_messages_retrieval, thread.id, False

            )

            no_cache_times.append({

                "thread_time": no_cache_thread_metrics.execution_time_ms,

                "messages_time": no_cache_messages_metrics.execution_time_ms

            })

        

        # Reset cache statistics

        cache_hits = 0

        cache_misses = 0

        

        # Test performance with caching (first call will be cache miss, subsequent cache hits)

        with_cache_times = []

        

        # First call (cache miss)

        first_thread_metrics = await self._measure_operation_performance(

            "thread_retrieval_cache_miss", cached_thread_retrieval, thread.id, True

        )

        first_messages_metrics = await self._measure_operation_performance(

            "messages_retrieval_cache_miss", cached_messages_retrieval, thread.id, True

        )

        

        # Subsequent calls (cache hits)

        for i in range(10):

            cached_thread_metrics = await self._measure_operation_performance(

                f"thread_retrieval_cache_hit_{i}", cached_thread_retrieval, thread.id, True

            )

            cached_messages_metrics = await self._measure_operation_performance(

                f"messages_retrieval_cache_hit_{i}", cached_messages_retrieval, thread.id, True

            )

            with_cache_times.append({

                "thread_time": cached_thread_metrics.execution_time_ms,

                "messages_time": cached_messages_metrics.execution_time_ms

            })

        

        # Analyze caching performance impact

        avg_no_cache_thread_time = statistics.mean([t["thread_time"] for t in no_cache_times])

        avg_no_cache_messages_time = statistics.mean([t["messages_time"] for t in no_cache_times])

        

        avg_cache_hit_thread_time = statistics.mean([t["thread_time"] for t in with_cache_times])

        avg_cache_hit_messages_time = statistics.mean([t["messages_time"] for t in with_cache_times])

        

        # Cache should provide significant performance improvement

        thread_speedup = avg_no_cache_thread_time / avg_cache_hit_thread_time if avg_cache_hit_thread_time > 0 else 1

        messages_speedup = avg_no_cache_messages_time / avg_cache_hit_messages_time if avg_cache_hit_messages_time > 0 else 1

        

        assert thread_speedup > 2.0, \

            f"Thread caching not effective enough: {thread_speedup}x speedup"

        assert messages_speedup > 2.0, \

            f"Messages caching not effective enough: {messages_speedup}x speedup"

        

        # Verify cache hit ratio

        total_cache_operations = cache_hits + cache_misses

        cache_hit_ratio = cache_hits / total_cache_operations if total_cache_operations > 0 else 0

        

        assert cache_hit_ratio > 0.8, \

            f"Cache hit ratio too low: {cache_hit_ratio}"

        

        # Test cache invalidation performance

        async def cache_invalidation():

            # Simulate cache invalidation when thread is updated

            invalidated_keys = []

            for key in list(cache.keys()):

                if thread.id in key:

                    del cache[key]

                    invalidated_keys.append(key)

            return len(invalidated_keys)

        

        invalidation_metrics = await self._measure_operation_performance(

            "cache_invalidation", cache_invalidation

        )

        

        # Cache invalidation should be fast

        assert invalidation_metrics.execution_time_ms < 50, \

            f"Cache invalidation too slow: {invalidation_metrics.execution_time_ms}ms"

        

        # Record caching metrics

        self.record_metric("thread_cache_speedup", thread_speedup)

        self.record_metric("messages_cache_speedup", messages_speedup)

        self.record_metric("cache_hit_ratio", cache_hit_ratio)

        self.record_metric("cache_invalidation_time_ms", invalidation_metrics.execution_time_ms)

        self.record_metric("caching_performance_effective", True)



    @pytest.mark.integration

    @pytest.mark.thread_performance

    async def test_concurrent_access_performance(self):

        """

        Test thread performance under concurrent user access patterns.

        

        BVJ: System must maintain performance even when multiple users

        are simultaneously accessing and updating their threads.

        """

        # Create multiple users and threads for concurrent testing

        concurrent_users = []

        user_threads = {}

        

        for i in range(20):  # 20 concurrent users

            user = self._create_performance_user(f"concurrent_user_{i}")

            concurrent_users.append(user)

            

            # Create threads for each user

            threads = []

            for j in range(5):  # 5 threads per user

                thread = self._create_performance_thread(user, f"Concurrent Thread {i}-{j}")

                threads.append(thread)

            user_threads[user.id] = threads

        

        # Define concurrent operations

        async def user_thread_operations(user: User, user_index: int) -> Dict[str, Any]:

            """Simulate typical user thread operations."""

            user_threads_list = user_threads[user.id]

            operation_results = []

            

            # Operation 1: List user's threads

            start_time = time.perf_counter()

            user_thread_list = [t for t in self._performance_threads if t.user_id == user.id]

            list_time = (time.perf_counter() - start_time) * 1000

            operation_results.append(("list_threads", list_time, len(user_thread_list)))

            

            # Operation 2: Retrieve specific thread details

            target_thread = user_threads_list[user_index % len(user_threads_list)]

            start_time = time.perf_counter()

            thread_details = next((t for t in self._performance_threads if t.id == target_thread.id), None)

            retrieve_time = (time.perf_counter() - start_time) * 1000

            operation_results.append(("retrieve_thread", retrieve_time, 1 if thread_details else 0))

            

            # Operation 3: Update thread metadata

            start_time = time.perf_counter()

            target_thread.metadata.update({

                "last_accessed": datetime.now(UTC).isoformat(),

                "access_count": target_thread.metadata.get("access_count", 0) + 1,

                "concurrent_test": True

            })

            target_thread.updated_at = datetime.now(UTC)

            update_time = (time.perf_counter() - start_time) * 1000

            operation_results.append(("update_thread", update_time, 1))

            

            # Operation 4: Create new message in thread

            start_time = time.perf_counter()

            new_message = Message(

                id=f"concurrent_msg_{target_thread.id}_{user_index}_{datetime.now(UTC).timestamp()}",

                thread_id=target_thread.id,

                user_id=user.id,

                content=f"Concurrent message from user {user_index}",

                role="user",

                created_at=datetime.now(UTC)

            )

            self._performance_messages.append(new_message)

            create_message_time = (time.perf_counter() - start_time) * 1000

            operation_results.append(("create_message", create_message_time, 1))

            

            return {

                "user_id": user.id,

                "user_index": user_index,

                "operations": operation_results,

                "total_time": sum(op[1] for op in operation_results)

            }

        

        # Execute concurrent operations

        concurrent_start = time.perf_counter()

        

        tasks = []

        for i, user in enumerate(concurrent_users):

            task = user_thread_operations(user, i)

            tasks.append(task)

        

        concurrent_results = await asyncio.gather(*tasks)

        

        concurrent_end = time.perf_counter()

        total_concurrent_time = (concurrent_end - concurrent_start) * 1000

        

        # Analyze concurrent performance

        all_operation_times = {}

        total_operations = 0

        

        for result in concurrent_results:

            for operation_name, operation_time, operation_count in result["operations"]:

                if operation_name not in all_operation_times:

                    all_operation_times[operation_name] = []

                all_operation_times[operation_name].append(operation_time)

                total_operations += operation_count

        

        # Performance assertions for each operation type

        performance_thresholds = {

            "list_threads": 100,      # 100ms max for thread listing

            "retrieve_thread": 50,    # 50ms max for single thread retrieval

            "update_thread": 100,     # 100ms max for thread updates

            "create_message": 150     # 150ms max for message creation

        }

        

        for operation_name, times in all_operation_times.items():

            avg_time = statistics.mean(times)

            max_time = max(times)

            p95_time = statistics.quantiles(times, n=20)[18] if len(times) > 10 else max_time

            

            threshold = performance_thresholds.get(operation_name, 200)

            

            # Average time should be well under threshold

            assert avg_time < threshold, \

                f"Concurrent {operation_name} average time too high: {avg_time}ms (threshold: {threshold}ms)"

            

            # P95 should be reasonable

            assert p95_time < threshold * 2, \

                f"Concurrent {operation_name} P95 time too high: {p95_time}ms"

        

        # Overall concurrent performance

        operations_per_second = (total_operations / total_concurrent_time) * 1000

        assert operations_per_second > 50, \

            f"Overall concurrent throughput too low: {operations_per_second} ops/sec"

        

        # Test resource contention - no user should be significantly slower than others

        user_total_times = [result["total_time"] for result in concurrent_results]

        min_time = min(user_total_times)

        max_time = max(user_total_times)

        time_variance_ratio = max_time / min_time if min_time > 0 else 1

        

        assert time_variance_ratio < 3.0, \

            f"Too much variance in user performance (resource contention): {time_variance_ratio}x"

        

        # Record concurrent access metrics

        self.record_metric("concurrent_users_tested", len(concurrent_users))

        self.record_metric("total_concurrent_operations", total_operations)

        self.record_metric("concurrent_operations_per_second", operations_per_second)

        self.record_metric("concurrent_performance_variance_ratio", time_variance_ratio)

        self.record_metric("concurrent_access_performance_acceptable", True)



    @pytest.mark.integration

    @pytest.mark.thread_performance

    async def test_memory_usage_performance(self):

        """

        Test memory usage patterns and performance impact of thread operations.

        

        BVJ: Efficient memory usage ensures system remains responsive and

        can handle many concurrent users without resource exhaustion.

        """

        import psutil

        import gc

        

        # Get initial memory usage

        process = psutil.Process()

        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        

        user = self._create_performance_user("memory_test")

        

        # Test memory usage during large data operations

        memory_measurements = []

        

        # Measurement 1: Baseline after user creation

        gc.collect()  # Force garbage collection

        baseline_memory = process.memory_info().rss / 1024 / 1024

        memory_measurements.append(("baseline", baseline_memory, 0))

        

        # Measurement 2: Create many threads

        threads = []

        for i in range(1000):

            thread = self._create_performance_thread(user, f"Memory Test Thread {i}")

            threads.append(thread)

            

            # Sample memory usage every 100 threads

            if i % 100 == 99:

                current_memory = process.memory_info().rss / 1024 / 1024

                memory_measurements.append((f"threads_{i+1}", current_memory, i+1))

        

        # Measurement 3: Create many messages

        messages = []

        for i in range(5000):

            thread = threads[i % len(threads)]

            message = Message(

                id=f"memory_msg_{i}",

                thread_id=thread.id,

                user_id=user.id,

                content=f"Memory test message {i} with some content to occupy memory space",

                role="user" if i % 2 == 0 else "assistant",

                created_at=datetime.now(UTC)

            )

            messages.append(message)

            self._performance_messages.append(message)

            

            # Sample memory usage every 500 messages

            if i % 500 == 499:

                current_memory = process.memory_info().rss / 1024 / 1024

                memory_measurements.append((f"messages_{i+1}", current_memory, i+1))

        

        # Measurement 4: Peak memory after all data creation

        gc.collect()

        peak_memory = process.memory_info().rss / 1024 / 1024

        memory_measurements.append(("peak", peak_memory, len(threads) + len(messages)))

        

        # Test memory efficiency of operations

        async def memory_efficient_retrieval():

            # Simulate retrieving data in chunks rather than all at once

            chunk_size = 100

            total_processed = 0

            

            for i in range(0, len(self._performance_messages), chunk_size):

                chunk = self._performance_messages[i:i + chunk_size]

                # Process chunk (simulate some operation)

                processed_chunk = [msg for msg in chunk if msg.user_id == user.id]

                total_processed += len(processed_chunk)

                

                # Yield control to prevent memory buildup

                await asyncio.sleep(0.001)

            

            return total_processed

        

        # Measure memory during efficient operation

        pre_operation_memory = process.memory_info().rss / 1024 / 1024

        

        efficient_operation_metrics = await self._measure_operation_performance(

            "memory_efficient_retrieval", memory_efficient_retrieval

        )

        

        post_operation_memory = process.memory_info().rss / 1024 / 1024

        operation_memory_delta = post_operation_memory - pre_operation_memory

        

        # Memory cleanup test

        # Clear references and force garbage collection

        threads.clear()

        messages.clear()

        self._performance_messages = [msg for msg in self._performance_messages 

                                    if not msg.id.startswith("memory_msg_")]

        self._performance_threads = [t for t in self._performance_threads 

                                   if not t.title.startswith("Memory Test Thread")]

        

        gc.collect()

        await asyncio.sleep(0.1)  # Allow cleanup to complete

        gc.collect()

        

        post_cleanup_memory = process.memory_info().rss / 1024 / 1024

        memory_measurements.append(("post_cleanup", post_cleanup_memory, 0))

        

        # Analyze memory usage patterns

        memory_growth = peak_memory - baseline_memory

        memory_per_thread = memory_growth / 1000 if memory_growth > 0 else 0  # 1000 threads created

        memory_recovery = peak_memory - post_cleanup_memory

        recovery_percentage = (memory_recovery / peak_memory) * 100 if peak_memory > 0 else 0

        

        # Memory performance assertions

        assert memory_per_thread < 0.1, \

            f"Memory usage per thread too high: {memory_per_thread}MB"

        

        assert operation_memory_delta < 50, \

            f"Memory efficient operation uses too much memory: {operation_memory_delta}MB"

        

        assert recovery_percentage > 70, \

            f"Memory recovery after cleanup too low: {recovery_percentage}%"

        

        # Performance should not degrade significantly with memory usage

        assert efficient_operation_metrics.execution_time_ms < 1000, \

            f"Memory efficient operation too slow: {efficient_operation_metrics.execution_time_ms}ms"

        

        # Record memory usage metrics

        self.record_metric("baseline_memory_mb", baseline_memory)

        self.record_metric("peak_memory_mb", peak_memory)

        self.record_metric("memory_per_thread_mb", memory_per_thread)

        self.record_metric("memory_recovery_percentage", recovery_percentage)

        self.record_metric("efficient_operation_time_ms", efficient_operation_metrics.execution_time_ms)

        self.record_metric("memory_usage_performance_acceptable", True)

