"""
Comprehensive Configuration Drift Monitoring Test Suite - MISSION CRITICAL VALIDATION

This test suite validates the complete configuration drift monitoring system
to ensure it prevents recurrence of the WebSocket authentication failures
that affected $120K+ MRR staging environment.

Test Coverage:
1. E2E OAuth simulation key consistency detection
2. JWT secret alignment monitoring between services
3. WebSocket authentication configuration coherence validation
4. Real-time drift detection with business impact calculation
5. Alert system integration with multiple channels
6. Automated remediation trigger capabilities
7. Executive escalation for high-impact incidents

CRITICAL VALIDATION: These tests must pass to ensure the monitoring system
can detect and prevent the specific configuration drift patterns that caused
the original authentication cascade failures.
"""

import asyncio
import json
import pytest
import time
import logging
from datetime import datetime, timezone
from typing import Dict, Any, List
from unittest.mock import patch, MagicMock, AsyncMock

# Import the monitoring components
from netra_backend.app.monitoring.configuration_drift_monitor import (
    ConfigurationDriftMonitor,
    E2EOAuthSimulationKeyValidator,
    JWTSecretAlignmentValidator,
    WebSocketConfigurationValidator,
    ConfigurationDrift,
    DriftSeverity,
    ConfigurationScope
)
from netra_backend.app.monitoring.configuration_drift_alerts import (
    ConfigurationDriftAlerting,
    AlertChannel,
    AlertRule
)
from netra_backend.app.monitoring.unified_configuration_monitoring import (
    UnifiedConfigurationMonitoring,
    get_unified_configuration_monitoring
)
from shared.isolated_environment import get_env

# Configure test logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TestE2EOAuthSimulationKeyValidator:
    """Test E2E OAuth simulation key consistency validation."""
    
    @pytest.fixture
    def oauth_validator(self):
        """Create OAuth validator for testing."""
        return E2EOAuthSimulationKeyValidator()
    
    @pytest.mark.asyncio
    async def test_missing_e2e_oauth_key_detection(self, oauth_validator):
        """Test detection of missing E2E OAuth simulation key."""
        with patch.object(oauth_validator.env, 'get') as mock_env_get:
            # Simulate missing E2E OAuth simulation key
            mock_env_get.side_effect = lambda key, default=None: {
                "ENVIRONMENT": "staging",
                "E2E_OAUTH_SIMULATION_KEY": None
            }.get(key, default)
            
            result = await oauth_validator.validate_key_consistency()
            
            assert result["drift_detected"] is True
            assert result["business_impact"] == "high"
            assert result["key_available"] is False
            assert len(result["drift_details"]) == 1
            
            drift_detail = result["drift_details"][0]
            assert drift_detail["severity"] == "high"
            assert drift_detail["business_impact_mrr"] == 50000.0
            assert drift_detail["config_key"] == "E2E_OAUTH_SIMULATION_KEY"
            assert "e2e_authentication_bypass_failure" in drift_detail["cascade_risk"]
            
            logger.info("âœ… Missing E2E OAuth key detection test passed")
    
    @pytest.mark.asyncio
    async def test_short_e2e_oauth_key_detection(self, oauth_validator):
        """Test detection of too-short E2E OAuth simulation key."""
        with patch.object(oauth_validator.env, 'get') as mock_env_get:
            # Simulate short E2E OAuth simulation key
            mock_env_get.side_effect = lambda key, default=None: {
                "ENVIRONMENT": "staging", 
                "E2E_OAUTH_SIMULATION_KEY": "short-key"  # Only 9 chars
            }.get(key, default)
            
            result = await oauth_validator.validate_key_consistency()
            
            assert result["drift_detected"] is True
            assert result["business_impact"] == "moderate"
            assert result["key_available"] is True
            assert result["key_length"] == 9
            
            drift_detail = result["drift_details"][0]
            assert drift_detail["severity"] == "moderate"
            assert drift_detail["business_impact_mrr"] == 10000.0
            assert "weak_e2e_authentication" in drift_detail["cascade_risk"]
            
            logger.info("âœ… Short E2E OAuth key detection test passed")
    
    @pytest.mark.asyncio
    async def test_valid_e2e_oauth_key(self, oauth_validator):
        """Test validation with properly configured E2E OAuth key."""
        with patch.object(oauth_validator.env, 'get') as mock_env_get:
            # Simulate valid E2E OAuth simulation key
            mock_env_get.side_effect = lambda key, default=None: {
                "ENVIRONMENT": "staging",
                "E2E_OAUTH_SIMULATION_KEY": "test-e2e-oauth-bypass-key-for-testing-only-unified-2025"
            }.get(key, default)
            
            # Mock successful E2E OAuth functionality test
            with patch.object(oauth_validator, '_test_e2e_oauth_simulation') as mock_test:
                mock_test.return_value = {"success": True}
                
                result = await oauth_validator.validate_key_consistency()
                
                assert result["drift_detected"] is False
                assert result["business_impact"] == "none"
                assert result["key_available"] is True
                assert result["key_length"] == 59
                
                logger.info("âœ… Valid E2E OAuth key test passed")
    
    @pytest.mark.asyncio
    async def test_e2e_oauth_functionality_failure(self, oauth_validator):
        """Test detection when E2E OAuth key is present but non-functional."""
        with patch.object(oauth_validator.env, 'get') as mock_env_get:
            mock_env_get.side_effect = lambda key, default=None: {
                "ENVIRONMENT": "staging",
                "E2E_OAUTH_SIMULATION_KEY": "test-e2e-oauth-bypass-key-for-testing-only-unified-2025"
            }.get(key, default)
            
            # Mock failed E2E OAuth functionality test
            with patch.object(oauth_validator, '_test_e2e_oauth_simulation') as mock_test:
                mock_test.return_value = {
                    "success": False,
                    "error": "Auth service returned 401"
                }
                
                result = await oauth_validator.validate_key_consistency()
                
                assert result["drift_detected"] is True
                assert result["business_impact"] == "critical"
                
                # Should have 2 drift details - original config plus functionality failure
                assert len(result["drift_details"]) >= 1
                critical_drift = next((d for d in result["drift_details"] if d["severity"] == "critical"), None)
                assert critical_drift is not None
                assert critical_drift["business_impact_mrr"] == 120000.0
                assert "websocket_authentication_failure" in critical_drift["cascade_risk"]
                
                logger.info("âœ… E2E OAuth functionality failure detection test passed")


class TestJWTSecretAlignmentValidator:
    """Test JWT secret alignment validation."""
    
    @pytest.fixture
    def jwt_validator(self):
        """Create JWT validator for testing."""
        return JWTSecretAlignmentValidator()
    
    @pytest.mark.asyncio
    async def test_missing_jwt_secret_detection(self, jwt_validator):
        """Test detection of missing JWT secret."""
        with patch.object(jwt_validator.env, 'get') as mock_env_get:
            mock_env_get.side_effect = lambda key, default=None: {
                "ENVIRONMENT": "staging",
                "JWT_SECRET_KEY": None
            }.get(key, default)
            
            result = await jwt_validator.validate_jwt_secret_alignment()
            
            assert result["drift_detected"] is True
            assert result["business_impact"] == "critical"
            assert result["secret_available"] is False
            
            drift_detail = result["drift_details"][0]
            assert drift_detail["severity"] == "critical"
            assert drift_detail["business_impact_mrr"] == 120000.0
            assert drift_detail["config_key"] == "JWT_SECRET_KEY"
            assert "complete_authentication_failure" in drift_detail["cascade_risk"]
            
            logger.info("âœ… Missing JWT secret detection test passed")
    
    @pytest.mark.asyncio
    async def test_short_jwt_secret_detection(self, jwt_validator):
        """Test detection of too-short JWT secret."""
        with patch.object(jwt_validator.env, 'get') as mock_env_get:
            mock_env_get.side_effect = lambda key, default=None: {
                "ENVIRONMENT": "staging",
                "JWT_SECRET_KEY": "short_secret"  # Only 12 chars
            }.get(key, default)
            
            # Mock successful JWT functionality test
            with patch.object(jwt_validator, '_test_jwt_functionality') as mock_test:
                mock_test.return_value = {"success": True}
                
                result = await jwt_validator.validate_jwt_secret_alignment()
                
                assert result["drift_detected"] is True
                assert result["business_impact"] == "high"
                assert result["secret_available"] is True
                assert result["secret_length"] == 12
                
                drift_detail = result["drift_details"][0]
                assert drift_detail["severity"] == "high"
                assert drift_detail["business_impact_mrr"] == 50000.0
                assert "weak_jwt_security" in drift_detail["cascade_risk"]
                
                logger.info("âœ… Short JWT secret detection test passed")
    
    @pytest.mark.asyncio
    async def test_jwt_functionality_failure(self, jwt_validator):
        """Test detection when JWT secret is present but functionality fails."""
        with patch.object(jwt_validator.env, 'get') as mock_env_get:
            mock_env_get.side_effect = lambda key, default=None: {
                "ENVIRONMENT": "staging",
                "JWT_SECRET_KEY": "this-is-a-valid-length-jwt-secret-key-for-testing-purposes"
            }.get(key, default)
            
            # Mock failed JWT functionality test
            with patch.object(jwt_validator, '_test_jwt_functionality') as mock_test:
                mock_test.return_value = {
                    "success": False,
                    "error": "Auth service unhealthy: degraded"
                }
                
                result = await jwt_validator.validate_jwt_secret_alignment()
                
                assert result["drift_detected"] is True
                assert result["business_impact"] == "critical"
                
                # Find the critical drift for functionality failure
                critical_drift = next((d for d in result["drift_details"] if d["severity"] == "critical"), None)
                assert critical_drift is not None
                assert critical_drift["business_impact_mrr"] == 120000.0
                assert "jwt_validation_failure" in critical_drift["cascade_risk"]
                
                logger.info("âœ… JWT functionality failure detection test passed")


class TestWebSocketConfigurationValidator:
    """Test WebSocket authentication configuration validation."""
    
    @pytest.fixture
    def websocket_validator(self):
        """Create WebSocket configuration validator for testing."""
        return WebSocketConfigurationValidator()
    
    @pytest.mark.asyncio
    async def test_missing_websocket_url_detection(self, websocket_validator):
        """Test detection of missing WebSocket URL configuration."""
        with patch.object(websocket_validator.env, 'get') as mock_env_get:
            mock_env_get.side_effect = lambda key, default=None: {
                "ENVIRONMENT": "staging",
                "NEXT_PUBLIC_WS_URL": None,
                "NEXT_PUBLIC_WEBSOCKET_URL": None
            }.get(key, default)
            
            # Mock failed WebSocket auth integration test
            with patch.object(websocket_validator, '_test_websocket_auth_integration') as mock_test:
                mock_test.return_value = {"success": False}
            with patch.object(websocket_validator, '_validate_websocket_cors_config') as mock_cors:
                mock_cors.return_value = {"drift_detected": False}
                
                result = await websocket_validator.validate_websocket_config_coherence()
                
                assert result["drift_detected"] is True
                assert result["business_impact"] == "critical"
                
                # Check for missing WebSocket URL drift
                url_drift = next((d for d in result["drift_details"] if d["config_key"] == "NEXT_PUBLIC_WS_URL"), None)
                assert url_drift is not None
                assert url_drift["severity"] == "critical"
                assert url_drift["business_impact_mrr"] == 120000.0
                assert "no_websocket_connection" in url_drift["cascade_risk"]
                
                logger.info("âœ… Missing WebSocket URL detection test passed")
    
    @pytest.mark.asyncio
    async def test_websocket_auth_integration_failure(self, websocket_validator):
        """Test detection of WebSocket authentication integration failure."""
        with patch.object(websocket_validator.env, 'get') as mock_env_get:
            mock_env_get.side_effect = lambda key, default=None: {
                "ENVIRONMENT": "staging",
                "NEXT_PUBLIC_WS_URL": "wss://api.staging.netrasystems.ai",
                "WEBSOCKET_ENABLED": "true"
            }.get(key, default)
            
            # Mock failed WebSocket auth integration test
            with patch.object(websocket_validator, '_test_websocket_auth_integration') as mock_test:
                mock_test.return_value = {
                    "success": False,
                    "error": "Auth service: degraded, WebSocket: enabled"
                }
            with patch.object(websocket_validator, '_validate_websocket_cors_config') as mock_cors:
                mock_cors.return_value = {"drift_detected": False}
                
                result = await websocket_validator.validate_websocket_config_coherence()
                
                assert result["drift_detected"] is True
                assert result["business_impact"] == "high"
                
                # Check for WebSocket auth integration drift
                auth_drift = next((d for d in result["drift_details"] if "INTEGRATION" in d["config_key"]), None)
                assert auth_drift is not None
                assert auth_drift["severity"] == "high"
                assert auth_drift["business_impact_mrr"] == 80000.0
                assert "websocket_auth_failure" in auth_drift["cascade_risk"]
                
                logger.info("âœ… WebSocket auth integration failure detection test passed")


class TestConfigurationDriftMonitor:
    """Test the comprehensive configuration drift monitor."""
    
    @pytest.fixture
    def drift_monitor(self):
        """Create configuration drift monitor for testing.""" 
        return ConfigurationDriftMonitor()
    
    @pytest.mark.asyncio
    async def test_comprehensive_drift_detection(self, drift_monitor):
        """Test comprehensive configuration drift detection across all validators."""
        # Mock all validators to return drift
        with patch.object(drift_monitor.e2e_oauth_validator, 'validate_key_consistency') as mock_oauth:
            mock_oauth.return_value = {
                "drift_detected": True,
                "drift_details": [{
                    "config_key": "E2E_OAUTH_SIMULATION_KEY",
                    "scope": "authentication",
                    "severity": "high",
                    "current_value": "<MISSING>",
                    "expected_value": "<REQUIRED>",
                    "business_impact_mrr": 50000.0,
                    "cascade_risk": ["e2e_authentication_bypass_failure"],
                    "remediation_priority": 2,
                    "detection_timestamp": time.time()
                }]
            }
            
            with patch.object(drift_monitor.jwt_secret_validator, 'validate_jwt_secret_alignment') as mock_jwt:
                mock_jwt.return_value = {
                    "drift_detected": True,
                    "drift_details": [{
                        "config_key": "JWT_SECRET_KEY",
                        "scope": "authentication",
                        "severity": "critical",
                        "current_value": "<MISSING>",
                        "expected_value": "<REQUIRED>",
                        "business_impact_mrr": 120000.0,
                        "cascade_risk": ["complete_authentication_failure"],
                        "remediation_priority": 1,
                        "detection_timestamp": time.time()
                    }]
                }
                
                with patch.object(drift_monitor.websocket_config_validator, 'validate_websocket_config_coherence') as mock_ws:
                    mock_ws.return_value = {
                        "drift_detected": True,
                        "drift_details": [{
                            "config_key": "NEXT_PUBLIC_WS_URL",
                            "scope": "websocket",
                            "severity": "critical",
                            "current_value": "<MISSING>",
                            "expected_value": "<REQUIRED>", 
                            "business_impact_mrr": 120000.0,
                            "cascade_risk": ["no_websocket_connection"],
                            "remediation_priority": 1,
                            "detection_timestamp": time.time()
                        }]
                    }
                    
                    # Execute health check
                    result = await drift_monitor.check_health()
                    
                    assert result.success is False  # Critical drift detected
                    assert result.health_score == 0.0  # Critical issues = unhealthy
                    assert result.status == "critical"
                    
                    # Check drift detection summary
                    drift_summary = result.details["drift_detection_summary"]
                    assert drift_summary["total_drift_detected"] is True
                    assert drift_summary["critical_drift_count"] == 2  # JWT + WebSocket
                    assert drift_summary["total_drift_count"] == 3
                    assert drift_summary["total_business_impact_mrr"] == 290000.0  # 50K + 120K + 120K
                    
                    # Check alert status
                    alert_status = result.details["alert_status"]
                    assert alert_status["level"] == "critical"
                    assert alert_status["should_alert"] is True
                    assert alert_status["escalate_to_executives"] is True  # >100K impact
                    assert alert_status["immediate_action_required"] is True
                    
                    # Check remediation recommendations
                    recommendations = result.details["remediation_recommendations"]
                    assert len(recommendations) == 3
                    
                    # Check business impact analysis
                    business_impact = result.details["business_impact_analysis"]
                    assert business_impact["impact_level"] == "critical"
                    assert business_impact["total_mrr_at_risk"] == 290000.0
                    assert "complete_service_disruption" in business_impact["impact_categories"]
                    
                    logger.info("âœ… Comprehensive drift detection test passed")
    
    @pytest.mark.asyncio
    async def test_no_drift_detection(self, drift_monitor):
        """Test behavior when no drift is detected."""
        # Mock all validators to return no drift
        with patch.object(drift_monitor.e2e_oauth_validator, 'validate_key_consistency') as mock_oauth:
            mock_oauth.return_value = {"drift_detected": False}
            
            with patch.object(drift_monitor.jwt_secret_validator, 'validate_jwt_secret_alignment') as mock_jwt:
                mock_jwt.return_value = {"drift_detected": False}
                
                with patch.object(drift_monitor.websocket_config_validator, 'validate_websocket_config_coherence') as mock_ws:
                    mock_ws.return_value = {"drift_detected": False}
                    
                    result = await drift_monitor.check_health()
                    
                    assert result.success is True
                    assert result.health_score == 1.0
                    assert result.status == "healthy"
                    
                    drift_summary = result.details["drift_detection_summary"]
                    assert drift_summary["total_drift_detected"] is False
                    assert drift_summary["total_business_impact_mrr"] == 0.0
                    
                    alert_status = result.details["alert_status"]
                    assert alert_status["level"] == "none"
                    assert alert_status["should_alert"] is False
                    
                    logger.info("âœ… No drift detection test passed")


class TestConfigurationDriftAlerting:
    """Test configuration drift alerting system."""
    
    @pytest.fixture
    def drift_alerting(self):
        """Create configuration drift alerting system for testing."""
        return ConfigurationDriftAlerting()
    
    @pytest.mark.asyncio
    async def test_critical_drift_alerting(self, drift_alerting):
        """Test alerting for critical configuration drift."""
        # Create mock drift result with critical impact
        drift_result = {
            "drift_detection_summary": {
                "total_drift_detected": True,
                "critical_drift_count": 1,
                "total_business_impact_mrr": 120000.0
            },
            "detected_drifts": [{
                "config_key": "JWT_SECRET_KEY",
                "severity": "critical",
                "business_impact_mrr": 120000.0,
                "environment": "staging",
                "cascade_risk": ["complete_authentication_failure"]
            }],
            "critical_drifts": [{
                "config_key": "JWT_SECRET_KEY",
                "severity": "critical",
                "business_impact_mrr": 120000.0,
                "environment": "staging"
            }]
        }
        
        # Mock alert channel sending
        with patch.object(drift_alerting, '_send_slack_alert') as mock_slack:
            mock_slack.return_value = {"channel": "slack", "success": True}
        with patch.object(drift_alerting, '_send_pagerduty_alert') as mock_pagerduty:
            mock_pagerduty.return_value = {"channel": "pagerduty", "success": True}
        with patch.object(drift_alerting, '_send_email_alert') as mock_email:
            mock_email.return_value = {"channel": "email", "success": True}
        with patch.object(drift_alerting, '_trigger_automated_remediation') as mock_remediation:
            mock_remediation.return_value = {"remediation_id": "test_remediation", "status": "pending"}
        with patch.object(drift_alerting, '_trigger_executive_escalation') as mock_executive:
            mock_executive.return_value = {"alert_id": "executive_escalation", "severity": "critical"}
            
        result = await drift_alerting.process_drift_detection(drift_result)
        
        assert result["alerts_triggered"] >= 1
        assert result["remediation_actions"] >= 1
        assert result["status"] == "processed"
        assert result["total_business_impact"] == 120000.0
        assert result["critical_drift_count"] == 1
        
        # Verify executive escalation was triggered for high impact
        mock_executive.assert_called_once()
        
        logger.info("âœ… Critical drift alerting test passed")
    
    @pytest.mark.asyncio
    async def test_alert_throttling(self, drift_alerting):
        """Test alert throttling to prevent spam."""
        # Create drift result
        drift_result = {
            "drift_detection_summary": {
                "total_drift_detected": True,
                "total_business_impact_mrr": 10000.0
            },
            "detected_drifts": [{
                "config_key": "TEST_CONFIG",
                "severity": "moderate",
                "business_impact_mrr": 10000.0,
                "environment": "staging"
            }],
            "critical_drifts": []
        }
        
        # Add fake alert history to trigger throttling
        fake_alert = {
            "timestamp": time.time(),
            "alert_details": [{"config_key": "TEST_CONFIG"}]
        }
        # Simulate max alerts already sent in last hour
        drift_alerting.alert_history = [fake_alert] * 5
        
        with patch.object(drift_alerting, '_send_slack_alert') as mock_slack:
            result = await drift_alerting.process_drift_detection(drift_result)
            
            # Should not send additional alerts due to throttling
            mock_slack.assert_not_called()
            assert result["alerts_triggered"] == 0
            
            logger.info("âœ… Alert throttling test passed")


class TestUnifiedConfigurationMonitoring:
    """Test the unified configuration monitoring orchestration."""
    
    @pytest.fixture
    def unified_monitoring(self):
        """Create unified configuration monitoring for testing."""
        return UnifiedConfigurationMonitoring(monitoring_interval_seconds=1)  # Short interval for testing
    
    @pytest.mark.asyncio
    async def test_monitoring_startup_and_shutdown(self, unified_monitoring):
        """Test monitoring system startup and shutdown."""
        # Mock the monitoring cycle to complete quickly
        with patch.object(unified_monitoring, '_perform_monitoring_cycle') as mock_cycle:
            mock_cycle.return_value = MagicMock(
                cycle_id="test_cycle",
                start_timestamp=time.time(),
                end_timestamp=time.time() + 1,
                drift_detected=False,
                status="completed",
                business_impact_mrr=0.0,
                alerts_triggered=0,
                remediation_actions=0
            )
            
            # Start monitoring
            start_result = await unified_monitoring.start_continuous_monitoring()
            
            assert start_result["status"] == "started"
            assert unified_monitoring.is_monitoring is True
            assert start_result["total_mrr_protected"] == 120000.0
            
            # Wait a brief moment for monitoring loop to execute
            await asyncio.sleep(0.1)
            
            # Stop monitoring
            stop_result = await unified_monitoring.stop_monitoring()
            
            assert stop_result["status"] == "stopped"
            assert unified_monitoring.is_monitoring is False
            assert "final_statistics" in stop_result
            
            logger.info("âœ… Monitoring startup and shutdown test passed")
    
    @pytest.mark.asyncio
    async def test_immediate_drift_check(self, unified_monitoring):
        """Test immediate configuration drift check."""
        with patch.object(unified_monitoring, '_perform_monitoring_cycle') as mock_cycle:
            mock_cycle.return_value = MagicMock(
                cycle_id="immediate_test",
                start_timestamp=time.time(),
                end_timestamp=time.time() + 2,
                drift_detected=True,
                status="completed",
                business_impact_mrr=50000.0,
                alerts_triggered=2,
                remediation_actions=1,
                error=None
            )
            
            result = await unified_monitoring.perform_immediate_drift_check()
            
            assert result["check_type"] == "immediate"
            assert result["drift_detected"] is True
            assert result["business_impact_mrr"] == 50000.0
            assert result["alerts_triggered"] == 2
            assert result["remediation_actions"] == 1
            assert result["status"] == "completed"
            assert result["execution_time_seconds"] == 2
            
            logger.info("âœ… Immediate drift check test passed")
    
    @pytest.mark.asyncio
    async def test_drift_history_retrieval(self, unified_monitoring):
        """Test drift history retrieval and analysis."""
        # Add some fake monitoring history
        current_time = time.time()
        fake_cycles = [
            MagicMock(
                cycle_id=f"cycle_{i}",
                start_timestamp=current_time - (i * 3600),  # i hours ago
                drift_detected=i % 2 == 0,  # Every other cycle has drift
                business_impact_mrr=10000.0 if i % 2 == 0 else 0.0,
                alerts_triggered=1 if i % 2 == 0 else 0,
                remediation_actions=1 if i % 2 == 0 else 0,
                status="completed"
            )
            for i in range(10)
        ]
        unified_monitoring.monitoring_history = fake_cycles
        
        # Get 24 hours of history
        history = await unified_monitoring.get_drift_history(hours=24)
        
        assert history["time_period_hours"] == 24
        assert history["total_cycles"] == len(fake_cycles)
        assert history["drift_incidents"] == 5  # Half of 10 cycles
        assert history["total_business_impact_mrr"] == 50000.0  # 5 incidents * 10K each
        assert history["total_alerts_triggered"] == 5
        assert len(history["incidents"]) == 5
        
        logger.info("âœ… Drift history retrieval test passed")


class TestIntegrationScenarios:
    """Test complete integration scenarios representing real-world drift patterns."""
    
    @pytest.mark.asyncio
    async def test_websocket_authentication_failure_scenario(self):
        """
        Test the complete scenario that caused the original WebSocket authentication failures.
        
        This test simulates:
        1. E2E OAuth simulation key missing from staging
        2. JWT secret mismatch between services
        3. WebSocket URL configuration drift
        4. Complete alerting and remediation pipeline
        """
        # Create monitoring system
        unified_monitoring = UnifiedConfigurationMonitoring(monitoring_interval_seconds=1)
        
        # Mock environment to simulate the problematic configuration state
        with patch('shared.isolated_environment.get_env') as mock_get_env:
            mock_env = MagicMock()
            mock_env.get.side_effect = lambda key, default=None: {
                "ENVIRONMENT": "staging",
                # Simulate the exact problem scenario
                "E2E_OAUTH_SIMULATION_KEY": None,  # Missing key
                "JWT_SECRET_KEY": "short",  # Too short
                "NEXT_PUBLIC_WS_URL": None,  # Missing WebSocket URL
                "WEBSOCKET_ENABLED": "true"
            }.get(key, default)
            mock_get_env.return_value = mock_env
            
            # Mock auth service to be unhealthy
            with patch('netra_backend.app.services.unified_authentication_service.get_unified_auth_service') as mock_auth:
                mock_auth_service = AsyncMock()
                mock_auth_service.health_check.return_value = {"status": "degraded"}
                mock_auth_service.authenticate_websocket.return_value = (
                    MagicMock(success=False, error="Auth validation failed"),
                    None
                )
                mock_auth.return_value = mock_auth_service
                
                # Execute immediate drift check
                result = await unified_monitoring.perform_immediate_drift_check()
                
                # Verify the complete failure scenario is detected
                assert result["drift_detected"] is True
                assert result["status"] == "completed"
                
                # The business impact should be substantial due to multiple critical issues
                assert result["business_impact_mrr"] > 100000.0
                
                # Multiple alerts should be triggered
                assert result["alerts_triggered"] > 0
                
                logger.info(f"âœ… WebSocket authentication failure scenario test passed - ${result['business_impact_mrr']:,.0f} MRR impact detected")
    
    @pytest.mark.asyncio
    async def test_configuration_drift_prevention_success(self):
        """
        Test successful prevention scenario where monitoring detects and remediates drift.
        
        This test simulates:
        1. Early detection of configuration drift
        2. Successful automated remediation  
        3. Validation that remediation resolved the issues
        4. Business impact mitigation measurement
        """
        unified_monitoring = UnifiedConfigurationMonitoring(monitoring_interval_seconds=1)
        
        # Mock initially problematic then fixed environment
        config_states = [
            # Initial state: drift detected
            {
                "ENVIRONMENT": "staging",
                "E2E_OAUTH_SIMULATION_KEY": "short",  # Too short initially
                "JWT_SECRET_KEY": "this-is-a-valid-length-jwt-secret-key-for-testing",
                "NEXT_PUBLIC_WS_URL": "wss://api.staging.netrasystems.ai"
            },
            # After remediation: drift resolved
            {
                "ENVIRONMENT": "staging", 
                "E2E_OAUTH_SIMULATION_KEY": "test-e2e-oauth-bypass-key-for-testing-only-unified-2025",
                "JWT_SECRET_KEY": "this-is-a-valid-length-jwt-secret-key-for-testing",
                "NEXT_PUBLIC_WS_URL": "wss://api.staging.netrasystems.ai"
            }
        ]
        
        config_call_count = 0
        
        def mock_env_get(key, default=None):
            nonlocal config_call_count
            state_index = min(config_call_count // 10, len(config_states) - 1)  # Switch state after 10 calls
            config_call_count += 1
            return config_states[state_index].get(key, default)
        
        with patch('shared.isolated_environment.get_env') as mock_get_env:
            mock_env = MagicMock()
            mock_env.get.side_effect = mock_env_get
            mock_get_env.return_value = mock_env
            
            # Mock auth service as healthy
            with patch('netra_backend.app.services.unified_authentication_service.get_unified_auth_service') as mock_auth:
                mock_auth_service = AsyncMock()
                mock_auth_service.health_check.return_value = {"status": "healthy"}
                mock_auth.return_value = mock_auth_service
                
                # First check: detect drift
                initial_result = await unified_monitoring.perform_immediate_drift_check()
                
                # Should detect initial drift
                assert initial_result["drift_detected"] is True
                initial_impact = initial_result["business_impact_mrr"]
                assert initial_impact > 0
                
                # Simulate some remediation time
                await asyncio.sleep(0.1)
                
                # Second check: after remediation
                final_result = await unified_monitoring.perform_immediate_drift_check()
                
                # Should show improvement or resolution
                final_impact = final_result["business_impact_mrr"]
                
                # Business impact should be reduced or eliminated
                assert final_impact <= initial_impact
                
                logger.info(f"âœ… Configuration drift prevention test passed - Impact reduced from ${initial_impact:,.0f} to ${final_impact:,.0f} MRR")


@pytest.mark.asyncio
async def test_configuration_monitoring_system_integration():
    """
    Comprehensive integration test for the entire configuration monitoring system.
    
    This test validates:
    1. All components work together correctly
    2. End-to-end drift detection and alerting
    3. Business impact calculation accuracy
    4. System resilience and error handling
    """
    logger.info("ðŸš€ Starting comprehensive configuration monitoring system integration test")
    
    # Get the unified monitoring instance
    monitoring_system = get_unified_configuration_monitoring()
    
    # Verify system initialization
    status = await monitoring_system.get_current_status()
    assert "monitoring_active" in status
    assert "total_mrr_protected" in status
    assert status["total_mrr_protected"] == 120000.0
    
    # Test immediate drift check capability
    immediate_check = await monitoring_system.perform_immediate_drift_check()
    assert "check_type" in immediate_check
    assert immediate_check["check_type"] == "immediate"
    assert "drift_detected" in immediate_check
    assert "business_impact_mrr" in immediate_check
    
    # Test drift history functionality
    history = await monitoring_system.get_drift_history(hours=1)
    assert "time_period_hours" in history
    assert history["time_period_hours"] == 1
    assert "total_cycles" in history
    assert "incidents" in history
    
    logger.info("âœ… Comprehensive configuration monitoring system integration test passed")


if __name__ == "__main__":
    """Run all configuration drift monitoring tests."""
    import sys
    
    async def run_all_tests():
        """Run all test classes and methods."""
        test_results = []
        
        # Test E2E OAuth Validation
        oauth_tests = TestE2EOAuthSimulationKeyValidator()
        oauth_validator = E2EOAuthSimulationKeyValidator()
        
        try:
            await oauth_tests.test_missing_e2e_oauth_key_detection(oauth_validator)
            await oauth_tests.test_short_e2e_oauth_key_detection(oauth_validator)
            await oauth_tests.test_valid_e2e_oauth_key(oauth_validator)
            await oauth_tests.test_e2e_oauth_functionality_failure(oauth_validator)
            test_results.append("âœ… E2E OAuth Validation Tests: PASSED")
        except Exception as e:
            test_results.append(f"âŒ E2E OAuth Validation Tests: FAILED - {e}")
        
        # Test JWT Secret Validation
        jwt_tests = TestJWTSecretAlignmentValidator()
        jwt_validator = JWTSecretAlignmentValidator()
        
        try:
            await jwt_tests.test_missing_jwt_secret_detection(jwt_validator)
            await jwt_tests.test_short_jwt_secret_detection(jwt_validator)
            await jwt_tests.test_jwt_functionality_failure(jwt_validator)
            test_results.append("âœ… JWT Secret Validation Tests: PASSED")
        except Exception as e:
            test_results.append(f"âŒ JWT Secret Validation Tests: FAILED - {e}")
        
        # Test WebSocket Configuration Validation
        ws_tests = TestWebSocketConfigurationValidator()
        ws_validator = WebSocketConfigurationValidator()
        
        try:
            await ws_tests.test_missing_websocket_url_detection(ws_validator)
            await ws_tests.test_websocket_auth_integration_failure(ws_validator)
            test_results.append("âœ… WebSocket Configuration Validation Tests: PASSED")
        except Exception as e:
            test_results.append(f"âŒ WebSocket Configuration Validation Tests: FAILED - {e}")
        
        # Test Configuration Drift Monitor
        monitor_tests = TestConfigurationDriftMonitor()
        drift_monitor = ConfigurationDriftMonitor()
        
        try:
            await monitor_tests.test_comprehensive_drift_detection(drift_monitor)
            await monitor_tests.test_no_drift_detection(drift_monitor)
            test_results.append("âœ… Configuration Drift Monitor Tests: PASSED")
        except Exception as e:
            test_results.append(f"âŒ Configuration Drift Monitor Tests: FAILED - {e}")
        
        # Test Configuration Drift Alerting
        alert_tests = TestConfigurationDriftAlerting()
        drift_alerting = ConfigurationDriftAlerting()
        
        try:
            await alert_tests.test_critical_drift_alerting(drift_alerting)
            await alert_tests.test_alert_throttling(drift_alerting)
            test_results.append("âœ… Configuration Drift Alerting Tests: PASSED")
        except Exception as e:
            test_results.append(f"âŒ Configuration Drift Alerting Tests: FAILED - {e}")
        
        # Test Unified Configuration Monitoring
        unified_tests = TestUnifiedConfigurationMonitoring()
        unified_monitoring = UnifiedConfigurationMonitoring(monitoring_interval_seconds=1)
        
        try:
            await unified_tests.test_monitoring_startup_and_shutdown(unified_monitoring)
            await unified_tests.test_immediate_drift_check(unified_monitoring)
            await unified_tests.test_drift_history_retrieval(unified_monitoring)
            test_results.append("âœ… Unified Configuration Monitoring Tests: PASSED")
        except Exception as e:
            test_results.append(f"âŒ Unified Configuration Monitoring Tests: FAILED - {e}")
        
        # Test Integration Scenarios
        integration_tests = TestIntegrationScenarios()
        
        try:
            await integration_tests.test_websocket_authentication_failure_scenario()
            await integration_tests.test_configuration_drift_prevention_success()
            test_results.append("âœ… Integration Scenario Tests: PASSED")
        except Exception as e:
            test_results.append(f"âŒ Integration Scenario Tests: FAILED - {e}")
        
        # Test System Integration
        try:
            await test_configuration_monitoring_system_integration()
            test_results.append("âœ… System Integration Test: PASSED")
        except Exception as e:
            test_results.append(f"âŒ System Integration Test: FAILED - {e}")
        
        # Print results
        print("\n" + "="*80)
        print("CONFIGURATION DRIFT MONITORING COMPREHENSIVE TEST RESULTS")
        print("="*80)
        
        passed_count = len([r for r in test_results if "âœ…" in r])
        total_count = len(test_results)
        
        for result in test_results:
            print(result)
        
        print("="*80)
        print(f"SUMMARY: {passed_count}/{total_count} test suites passed")
        
        if passed_count == total_count:
            print("ðŸŽ‰ ALL TESTS PASSED - Configuration drift monitoring system is ready for deployment!")
            return True
        else:
            print("âŒ SOME TESTS FAILED - Review and fix issues before deployment")
            return False
    
    # Run the test suite
    success = asyncio.run(run_all_tests())
    sys.exit(0 if success else 1)