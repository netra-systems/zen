"""

Test module split from original file

Generated by auto_fix_test_violations.py

"""



import json

import pytest

import time

from shared.isolated_environment import IsolatedEnvironment



import psutil





@pytest.mark.e2e

async def test_rapid_message_suite_performance_benchmark(rapid_message_sender):

    """

    Performance benchmark for rapid message succession capabilities.

    Provides baseline metrics for regression testing.

    """

    benchmark_results = {

        "start_time": time.time(),

        "message_throughput": {},

        "latency_metrics": {},

        "memory_usage": {},

        "connection_stability": {},

    }



    # Benchmark message throughput

    throughput_messages = []

    for i in range(100):

        message = {

            "type": "user_message",

            "content": f"Throughput test message {i}",

            "message_id": f"throughput-{i}",

            "timestamp": time.time(),

        }

        throughput_messages.append(message)



    # Measure sending throughput

    start_time = time.perf_counter()

    results = await rapid_message_sender.send_rapid_burst(

        throughput_messages, burst_interval=0.01

    )

    send_duration = time.perf_counter() - start_time



    # Measure response latency

    responses = await rapid_message_sender.receive_responses(

        expected_count=100, timeout=30.0

    )



    total_duration = time.perf_counter() - start_time



    # Calculate metrics

    messages_per_second = len(throughput_messages) / send_duration

    response_ratio = len(responses) / len(throughput_messages)



    benchmark_results["message_throughput"] = {

        "messages_sent": len(throughput_messages),

        "send_duration": send_duration,

        "messages_per_second": messages_per_second,

        "responses_received": len(responses),

        "response_ratio": response_ratio,

        "total_duration": total_duration,

    }



    # Latency analysis

    if responses:

        latencies = []

        for response in responses:

            send_time = response.get("timestamp", 0)

            receive_time = response.get("received_time", 0)

            if send_time and receive_time:

                latency = receive_time - send_time

                latencies.append(latency)



        if latencies:

            benchmark_results["latency_metrics"] = {

                "count": len(latencies),

                "avg_latency": sum(latencies) / len(latencies),

                "min_latency": min(latencies),

                "max_latency": max(latencies),

                "p95_latency": (

                    sorted(latencies)[int(len(latencies) * 0.95)]

                    if len(latencies) > 20

                    else max(latencies)

                ),

            }



    # Memory usage

    memory_info = psutil.Process().memory_info()

    benchmark_results["memory_usage"] = {

        "rss_mb": memory_info.rss / (1024 * 1024),

        "vms_mb": memory_info.vms / (1024 * 1024),

    }



    benchmark_results["total_benchmark_duration"] = (

        time.time() - benchmark_results["start_time"]

    )



    # Performance assertions

    assert (

        messages_per_second >= RAPID_MESSAGE_TEST_CONFIG["min_throughput"]

    ), f"Throughput below threshold: {

        messages_per_second:.1f} < {

        RAPID_MESSAGE_TEST_CONFIG['min_throughput']}"



    if benchmark_results.get("latency_metrics", {}).get("avg_latency"):

        avg_latency = benchmark_results["latency_metrics"]["avg_latency"]

        assert (

            avg_latency <= RAPID_MESSAGE_TEST_CONFIG["max_message_latency"]

        ), f"Average latency too high: {

            avg_latency:.3f}s > {

            RAPID_MESSAGE_TEST_CONFIG['max_message_latency']}s"



    logger.info(

        f"Rapid message benchmark completed: {

            json.dumps(

                benchmark_results,

                indent=2)}"

    )



    return benchmark_results

