"""
Test module split from original file
Generated by auto_fix_test_violations.py
"""

import json
import time

import psutil


async def test_rapid_message_suite_performance_benchmark(rapid_message_sender):
    """
    Performance benchmark for rapid message succession capabilities.
    Provides baseline metrics for regression testing.
    """
    benchmark_results = {
        "start_time": time.time(),
        "message_throughput": {},
        "latency_metrics": {},
        "memory_usage": {},
        "connection_stability": {},
    }

    # Benchmark message throughput
    throughput_messages = []
    for i in range(100):
        message = {
            "type": "user_message",
            "content": f"Throughput test message {i}",
            "message_id": f"throughput-{i}",
            "timestamp": time.time(),
        }
        throughput_messages.append(message)

    # Measure sending throughput
    start_time = time.perf_counter()
    results = await rapid_message_sender.send_rapid_burst(
        throughput_messages, burst_interval=0.01
    )
    send_duration = time.perf_counter() - start_time

    # Measure response latency
    responses = await rapid_message_sender.receive_responses(
        expected_count=100, timeout=30.0
    )

    total_duration = time.perf_counter() - start_time

    # Calculate metrics
    messages_per_second = len(throughput_messages) / send_duration
    response_ratio = len(responses) / len(throughput_messages)

    benchmark_results["message_throughput"] = {
        "messages_sent": len(throughput_messages),
        "send_duration": send_duration,
        "messages_per_second": messages_per_second,
        "responses_received": len(responses),
        "response_ratio": response_ratio,
        "total_duration": total_duration,
    }

    # Latency analysis
    if responses:
        latencies = []
        for response in responses:
            send_time = response.get("timestamp", 0)
            receive_time = response.get("received_time", 0)
            if send_time and receive_time:
                latency = receive_time - send_time
                latencies.append(latency)

        if latencies:
            benchmark_results["latency_metrics"] = {
                "count": len(latencies),
                "avg_latency": sum(latencies) / len(latencies),
                "min_latency": min(latencies),
                "max_latency": max(latencies),
                "p95_latency": (
                    sorted(latencies)[int(len(latencies) * 0.95)]
                    if len(latencies) > 20
                    else max(latencies)
                ),
            }

    # Memory usage
    memory_info = psutil.Process().memory_info()
    benchmark_results["memory_usage"] = {
        "rss_mb": memory_info.rss / (1024 * 1024),
        "vms_mb": memory_info.vms / (1024 * 1024),
    }

    benchmark_results["total_benchmark_duration"] = (
        time.time() - benchmark_results["start_time"]
    )

    # Performance assertions
    assert (
        messages_per_second >= RAPID_MESSAGE_TEST_CONFIG["min_throughput"]
    ), f"Throughput below threshold: {
        messages_per_second:.1f} < {
        RAPID_MESSAGE_TEST_CONFIG['min_throughput']}"

    if benchmark_results.get("latency_metrics", {}).get("avg_latency"):
        avg_latency = benchmark_results["latency_metrics"]["avg_latency"]
        assert (
            avg_latency <= RAPID_MESSAGE_TEST_CONFIG["max_message_latency"]
        ), f"Average latency too high: {
            avg_latency:.3f}s > {
            RAPID_MESSAGE_TEST_CONFIG['max_message_latency']}s"

    logger.info(
        f"Rapid message benchmark completed: {
            json.dumps(
                benchmark_results,
                indent=2)}"
    )

    return benchmark_results
