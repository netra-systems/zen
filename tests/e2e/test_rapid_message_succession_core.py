"""
Test module split from original file
Generated by auto_fix_test_violations.py
"""

import asyncio
import json
import random
import time
import uuid
from dataclasses import field
from typing import Any, Dict, List, Optional

import httpx
import pytest

# Import required classes
from tests.e2e.utils.rapid_message_sender import RapidMessageSender

class MessageSequenceEntry:
    """Tracking entry for message sequence validation."""

    sequence_id: int
    message_id: str
    content: str
    timestamp: float
    sent_time: Optional[float] = None
    received_time: Optional[float] = None
    processed_time: Optional[float] = None
    status: str = "pending"
    response_data: Optional[Dict] = None

class MessageBurstResult:
    """Result tracking for message burst operations."""

    total_sent: int = 0
    total_received: int = 0
    total_failed: int = 0
    total_duplicates: int = 0
    avg_latency: float = 0.0
    max_latency: float = 0.0
    sequence_violations: List[str] = field(default_factory=list)
    memory_growth: int = 0
    queue_overflows: int = 0

class MessageSequenceValidator:
    """Advanced message sequence integrity validator."""

    def __init__(self):
        self.expected_sequences = []
        self.received_sequences = []
        self.violations = []
        self.duplicate_tracker = set()

    def track_expected_sequence(
        self, sequence_id: int, message_id: str, content: str = ""
    ):
        """Track expected message sequence."""
        entry = MessageSequenceEntry(
            sequence_id=sequence_id,
            message_id=message_id,
            content=content,
            timestamp=time.time(),
        )
        self.expected_sequences.append(entry)
        return entry

    def track_received_sequence(
        self, sequence_id: int, message_id: str, response: Dict
    ):
        """Track received message sequence."""
        entry = MessageSequenceEntry(
            sequence_id=sequence_id,
            message_id=message_id,
            content=response.get("content", ""),
            timestamp=time.time(),
            received_time=time.time(),
            response_data=response,
        )
        self.received_sequences.append(entry)
        return entry

    def detect_duplicate_processing(self, message_id: str) -> bool:
        """Detect if message was already processed."""
        if message_id in self.duplicate_tracker:
            return True
        self.duplicate_tracker.add(message_id)
        return False

    def validate_sequence_integrity(self) -> Dict[str, Any]:
        """Comprehensive sequence integrity validation."""
        validation_result = {
            "total_expected": len(self.expected_sequences),
            "total_received": len(self.received_sequences),
            "missing_sequences": [],
            "duplicate_sequences": [],
            "out_of_order_sequences": [],
            "sequence_gaps": [],
            "violations": [],
        }

        # Check for missing sequences
        expected_ids = {seq.sequence_id for seq in self.expected_sequences}
        received_ids = {seq.sequence_id for seq in self.received_sequences}

        validation_result["missing_sequences"] = list(expected_ids - received_ids)

        # Check for duplicates
        received_sequence_ids = [seq.sequence_id for seq in self.received_sequences]
        duplicates = [
            sid for sid in received_sequence_ids if received_sequence_ids.count(sid) > 1
        ]
        validation_result["duplicate_sequences"] = list(set(duplicates))

        # Check for ordering violations
        sorted_received = sorted(
            self.received_sequences, key=lambda x: x.received_time or 0
        )
        sequence_order = [seq.sequence_id for seq in sorted_received]
        expected_order = sorted(sequence_order)

        if sequence_order != expected_order:
            validation_result["out_of_order_sequences"] = sequence_order
            validation_result["violations"].append(
                "Message ordering violation detected"
            )

        # Check for sequence gaps
        if expected_ids:
            min_seq = min(expected_ids)
            max_seq = max(expected_ids)
            expected_range = set(range(min_seq, max_seq + 1))
            gaps = expected_range - received_ids
            validation_result["sequence_gaps"] = list(gaps)

        # Compile violations
        if validation_result["missing_sequences"]:
            validation_result["violations"].append(
                f"Missing sequences: {
                    validation_result['missing_sequences']}"
            )

        if validation_result["duplicate_sequences"]:
            validation_result["violations"].append(
                f"Duplicate sequences: {
                    validation_result['duplicate_sequences']}"
            )

        if validation_result["sequence_gaps"]:
            validation_result["violations"].append(
                f"Sequence gaps: {validation_result['sequence_gaps']}"
            )

        return validation_result

    def assert_no_sequence_violations(self):
        """Assert no sequence violations occurred."""
        validation_result = self.validate_sequence_integrity()
        violations = validation_result["violations"]

        assert len(violations) == 0, f"Sequence violations detected: {violations}"
        return validation_result

class TestBurstMessageIdempotencyEnforcement:
    """Test Case 2: Burst Message Idempotency Enforcement"""

    @pytest.mark.asyncio
    @pytest.mark.e2e
    async def test_burst_message_idempotency_enforcement(:
        self, rapid_message_sender, message_sequence_validator, agent_state_monitor
    ):
        """
        Scenario: User sends same message multiple times rapidly due to UI lag
        Expected: Only one processing per unique message, duplicates rejected gracefully
        """
        await agent_state_monitor.capture_state_snapshot(
            "idempotency_test_start", rapid_message_sender
        )

        # Create base message with unique ID
        base_message = {
            "type": "user_message",
            "content": "Analyze the Q3 sales data trends",
            "message_id": f"unique-message-{uuid.uuid4().hex[:8]}",
            "timestamp": time.time(),
            "requires_processing": True,
        }

        message_sequence_validator.track_expected_sequence(
            0, base_message["message_id"], base_message["content"]
        )

        # Send same message 10 times rapidly (simulating UI issues)
        duplicate_messages = []
        for i in range(10):
            message_copy = base_message.copy()
            message_copy["duplicate_attempt"] = i
            duplicate_messages.append(message_copy)

        # Send all duplicates with minimal delay variations
        start_time = time.perf_counter()
        results = await rapid_message_sender.send_rapid_burst(
            duplicate_messages,
            burst_interval=random.uniform(0, 0.05),  # 0-50ms variations
        )

        # Collect responses with extended timeout for processing
        responses = await rapid_message_sender.receive_responses(
            expected_count=10, timeout=15.0  # Allow for rejection messages
        )

        # Analyze responses
        processed_responses = []
        duplicate_rejections = []

        for response in responses:
            response_type = response.get("type", "unknown")
            message_id = response.get("message_id", response.get("correlation_id"))

            if (
                response_type in ["ai_response", "processed"]
                and message_id == base_message["message_id"]
            ):
                processed_responses.append(response)
                message_sequence_validator.track_received_sequence(
                    0, message_id, response
                )
            elif (
                response_type in ["duplicate_rejected", "error"]
                and "duplicate" in response.get("message", "").lower()
            ):
                duplicate_rejections.append(response)

        # Validation: Only one should be processed
        assert (
            len(processed_responses) <= 1
        ), f"Multiple messages processed: {
            len(processed_responses)} (expected: 0 or 1)"

        # Should have rejection messages for duplicates
        assert (
            len(duplicate_rejections) >= 8
        ), f"Insufficient duplicate rejections: {
            len(duplicate_rejections)} (expected: â‰¥8)"

        # Verify no duplicate processing in agent state
        agent_state = await rapid_message_sender.get_agent_state()
        if isinstance(agent_state, dict):
            processed_message_ids = agent_state.get("processed_message_ids", [])
            if base_message["message_id"] in processed_message_ids:
                count = processed_message_ids.count(base_message["message_id"])
                assert (
                    count == 1
                ), f"Message processed {count} times despite idempotency"

        await agent_state_monitor.capture_state_snapshot(
            "idempotency_test_end", rapid_message_sender
        )

        logger.info(
            f"Idempotency test completed: {
                len(processed_responses)} processed, {
                len(duplicate_rejections)} rejected"
        )

class TestQueueOverflowBackpressureHandling:
    """Test Case 3: Queue Overflow and Backpressure Handling"""

    @pytest.mark.asyncio
    @pytest.mark.e2e
    async def test_queue_overflow_backpressure_handling(:
        self, rapid_message_sender, message_sequence_validator, agent_state_monitor
    ):
        """
        Scenario: User sends messages faster than system can process
        Expected: Graceful backpressure, message queuing, no loss of critical messages
        """
        await agent_state_monitor.capture_state_snapshot(
            "queue_test_start", rapid_message_sender
        )

        # Test with high message volume to trigger queue limits
        message_count = RAPID_MESSAGE_TEST_CONFIG["queue_capacity_threshold"] + 50
        messages = []

        for i in range(message_count):
            priority = "high" if i % 10 == 0 else "normal"
            message = {
                "type": "user_message",
                "content": f"Process data batch {i}",
                "message_id": f"batch-{i}",
                "priority": priority,
                "timestamp": time.time(),
                "sequence_id": i,
            }
            messages.append(message)
            message_sequence_validator.track_expected_sequence(i, message["message_id"])

        # Send messages as fast as possible
        start_time = time.perf_counter()
        # 10ms intervals
        results = await rapid_message_sender.send_rapid_burst(
            messages, burst_interval=0.01
        )
        send_duration = time.perf_counter() - start_time

        # Monitor queue state during processing
        queue_monitoring_task = asyncio.create_task(
            self._monitor_queue_state(rapid_message_sender, duration=30.0)
        )

        # Collect responses with extended timeout
        responses = await rapid_message_sender.receive_responses(
            expected_count=min(
                message_count, RAPID_MESSAGE_TEST_CONFIG["queue_capacity_threshold"]
            ),
            timeout=45.0,
        )

        queue_states = await queue_monitoring_task

        # Analysis
        successful_sends = len([r for r in results if r.get("status") == "sent"])
        processed_responses = len(
            [r for r in responses if r.get("type") in ["ai_response", "processed"]]
        )
        queue_full_responses = len(
            [r for r in responses if r.get("type") == "queue_full"]
        )

        # Validation: Should handle backpressure gracefully
        assert (
            queue_full_responses > 0 or processed_responses < message_count
        ), "No backpressure signals detected despite queue overflow"

        assert successful_sends > 0, "No messages successfully sent"

        # Verify high-priority message preservation
        high_priority_responses = [
            r
            for r in responses
            if r.get("priority") == "high"
            and r.get("type") in ["ai_response", "processed"]
        ]

        high_priority_sent = len([m for m in messages if m["priority"] == "high"])
        if high_priority_sent > 0:
            preservation_ratio = len(high_priority_responses) / high_priority_sent
            assert (
                preservation_ratio >= 0.8
            ), f"High priority preservation too low: {preservation_ratio:.2f}"

        # Verify queue management
        if queue_states:
            max_queue_size = max(state.get("queue_size", 0) for state in queue_states)
            assert (
                max_queue_size
                <= RAPID_MESSAGE_TEST_CONFIG["queue_capacity_threshold"] * 1.1
            ), f"Queue capacity exceeded: {max_queue_size}"

        await agent_state_monitor.capture_state_snapshot(
            "queue_test_end", rapid_message_sender
        )

        logger.info(
            f"Queue overflow test: {successful_sends} sent, {processed_responses} processed, {queue_full_responses} backpressure signals"
        )

    async def _monitor_queue_state(
        self, sender: RapidMessageSender, duration: float
    ) -> List[Dict]:
        """Monitor queue state over time."""
        states = []
        end_time = time.time() + duration

        while time.time() < end_time:
            try:
                # Request queue state
                state_request = {"type": "get_queue_state", "timestamp": time.time()}

                await sender.connection.send(json.dumps(state_request))

                # Wait for response
                try:
                    response = await asyncio.wait_for(
                        sender.connection.recv(), timeout=2.0
                    )
                    state_data = json.loads(response)
                    if state_data.get("type") == "queue_state":
                        states.append(state_data)
                except asyncio.TimeoutError:
                    pass

                await asyncio.sleep(1.0)  # Monitor every second

            except Exception as e:
                logger.warning(f"Queue monitoring error: {e}")
                break

        return states

async def test_user_token():
    """Create test user and return auth token."""
    if E2E_TEST_CONFIG["test_mode"] == "real":
        # Use real authentication service
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{E2E_TEST_CONFIG['auth_service_url']}/auth/test-user",
                    json={"email": f"test-{uuid.uuid4().hex[:8]}@example.com"},
                    timeout=10.0,
                )
                if response.status_code == 200:
                    token_data = response.json()
                    return {
                        "user_id": token_data["user_id"],
                        "token": token_data["token"],
                        "email": token_data["email"],
                    }
        except Exception as e:
            logger.warning(f"Real auth failed, using mock: {e}")

    # Fallback to mock authentication
    test_user_id = f"test-user-{uuid.uuid4().hex[:8]}"
    return {
        "user_id": test_user_id,
        "token": f"mock-token-{uuid.uuid4().hex}",
        "email": f"{test_user_id}@example.com",
    }
