"""
Staging Backend Configuration Audit Failures - Comprehensive Infrastructure Validation

Business Value Justification (BVJ):
- Segment: Platform/Internal
- Business Goal: Infrastructure Reliability, Deployment Validation, Production Readiness
- Value Impact: Prevents complete platform unavailability by catching configuration drift
- Strategic Impact: Ensures staging accurately validates production readiness requirements

EXPECTED TO FAIL: These tests replicate the comprehensive backend configuration issues from staging audit:

## CRITICAL INFRASTRUCTURE FAILURES MATRIX

### 1. Auth Service Configuration Cascade (100% Authentication Breakdown)
- DATABASE_URL undefined/misconfigured → Auth service startup failure
- JWT_SECRET_KEY mismatch → Cross-service authentication broken  
- Service account credentials invalid → OAuth completely non-functional
- REDIS_URL fallback masking → Session persistence broken

### 2. External Service Dependencies (Analytics & Cache System Failure)
- ClickHouse unreachable → Health checks 503, deployment validation fails
- Redis connection failure → Performance 5-10x degradation, sessions lost
- Inappropriate fallback modes → Staging doesn't validate production requirements

### 3. Environment Detection and Validation (Development Behavior in Staging)
- Staging environment not properly detected → Development fallbacks allowed
- Strict validation disabled → Infrastructure issues masked
- Silent failures → Problems not visible until production

### 4. Health Check and Deployment Validation (Release Pipeline Blocked)
- /health/ready returns 503 → GCP Cloud Run deployment fails
- External service timeouts → Monitoring alerts false positives
- Service readiness != operational capability → Deployment gate failures

## TEST-DRIVEN CORRECTION (TDC) APPROACH

These tests follow TDC methodology:
1. **Define Discrepancy**: Document exact gap between expected and actual behavior
2. **Create Failing Test**: Write test that exposes the specific configuration issue
3. **Enable Surgical Fix**: Provide detailed error categorization for targeted fixes
4. **Prevent Regression**: Ensure tests continue to validate after fixes

## ENVIRONMENT REQUIREMENTS

- Must run in staging environment (@pytest.mark.env("staging"))  
- Requires comprehensive staging configuration validation
- Tests infrastructure provisioning and service dependencies
- Validates environment-specific behavior enforcement

## BUSINESS IMPACT ANALYSIS

Configuration failures compound exponentially:
- Single missing env var → Service degradation → Cascade failures → Platform unavailability
- Staging validation gaps → Production failures → Revenue loss → Customer impact
- Infrastructure drift → Deployment failures → Release pipeline blocks → Development velocity loss
"""

import asyncio
import json
import os
import socket
import sys
import time
import pytest
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse

import httpx

# Core system imports using absolute paths per SPEC/import_management_architecture.xml
from shared.isolated_environment import IsolatedEnvironment
from test_framework.environment_markers import env_requires, staging_only


@dataclass
class ConfigurationAuditResult:
    """Comprehensive result container for configuration audit findings."""
    category: str
    severity: str  # CRITICAL, HIGH, MEDIUM, LOW
    config_key: str
    expected_value: Optional[str]
    actual_value: Optional[str]
    validation_error: str
    business_impact: str
    fix_recommendation: str
    environment_source: str = "unknown"


@dataclass
class ServiceDependencyValidation:
    """Result container for external service dependency validation."""
    service_name: str
    dependency_type: str  # required, optional, fallback_ok
    connectivity_status: str  # connected, timeout, refused, dns_failure
    configuration_status: str  # valid, missing, invalid
    fallback_status: str  # disabled, enabled, inappropriate
    business_impact: str
    staging_requirement: str


@pytest.mark.e2e
class TestStagingBackendConfigurationAuditFailures:
    """Comprehensive configuration audit test suite for staging backend service failures."""

    def setup_method(self):
        """Setup isolated test environment for configuration audit."""
        self.env = IsolatedEnvironment()
        self.env.enable_isolation_mode()
        self.start_time = time.time()
        self.audit_results: List[ConfigurationAuditResult] = []
        
    def teardown_method(self):
        """Clean up test environment and report audit findings."""
        if hasattr(self, 'env'):
            self.env.reset_to_original()
        
        # Report audit summary if there are results
        if hasattr(self, 'audit_results') and self.audit_results:
            self._generate_audit_summary()

    @pytest.mark.env("staging")
    @pytest.mark.critical
    @pytest.mark.e2e
    def test_comprehensive_backend_configuration_cascade_failure_analysis(self):
        """
        EXPECTED TO FAIL - CRITICAL CONFIGURATION CASCADE ISSUE
        
        Issue: Multiple configuration failures compound to create complete backend service breakdown
        Expected: All critical configuration properly loaded and validated for staging
        Actual: Configuration cascade failures prevent service from reaching operational state
        
        Cascade Pattern: Missing env vars → Wrong defaults → Connection failures → Service degradation → Platform unavailability
        """
        # Define comprehensive configuration requirements for staging backend
        critical_config_matrix = {
            'authentication': {
                'DATABASE_URL': {
                    'required': True,
                    'validation': lambda v: v.startswith(('postgresql://', 'postgres://')) and 'staging' in v and 'localhost' not in v,
                    'business_impact': 'Complete authentication breakdown',
                    'staging_requirement': 'Must point to staging database with Cloud SQL or staging host'
                },
                'JWT_SECRET_KEY': {
                    'required': True,
                    'validation': lambda v: len(v) >= 32 and v != 'dev-secret-key',
                    'business_impact': 'Cross-service authentication failure',
                    'staging_requirement': 'Must be production-strength secret (32+ chars)'
                },
                'AUTH_SERVICE_URL': {
                    'required': True,
                    'validation': lambda v: 'staging' in v and 'localhost' not in v,
                    'business_impact': 'Service-to-service auth completely broken',
                    'staging_requirement': 'Must point to staging auth service'
                }
            },
            'external_services': {
                'REDIS_URL': {
                    'required': True,
                    'validation': lambda v: v.startswith('redis://') and 'localhost' not in v,
                    'business_impact': 'Cache and session degradation',
                    'staging_requirement': 'Must point to staging Redis, not localhost'
                },
                'CLICKHOUSE_HOST': {
                    'required': True,
                    'validation': lambda v: 'staging' in v and 'localhost' not in v,
                    'business_impact': 'Analytics system failure, health checks 503',
                    'staging_requirement': 'Must point to staging ClickHouse'
                },
                'CLICKHOUSE_PORT': {
                    'required': True,
                    'validation': lambda v: v == '8123',
                    'business_impact': 'ClickHouse connectivity failure',
                    'staging_requirement': 'Must be standard ClickHouse port 8123'
                }
            },
            'environment_enforcement': {
                'NETRA_ENVIRONMENT': {
                    'required': True,
                    'validation': lambda v: v == 'staging',
                    'business_impact': 'Development behavior in staging',
                    'staging_requirement': 'Must be "staging" to enforce staging behavior'
                },
                'REDIS_FALLBACK_ENABLED': {
                    'required': True,
                    'validation': lambda v: v.lower() == 'false',
                    'business_impact': 'Infrastructure issues masked',
                    'staging_requirement': 'Must be false to catch Redis provisioning gaps'
                },
                'FAIL_FAST_ON_SERVICE_ERRORS': {
                    'required': True,
                    'validation': lambda v: v.lower() == 'true',
                    'business_impact': 'Silent service degradation',
                    'staging_requirement': 'Must be true to catch external service issues'
                }
            }
        }
        
        # Run comprehensive configuration audit
        all_failures = []
        
        for category, config_vars in critical_config_matrix.items():
            category_failures = []
            
            for var_name, requirements in config_vars.items():
                value = self.env.get(var_name)
                
                # Check if required variable is missing
                if requirements['required'] and value is None:
                    failure = ConfigurationAuditResult(
                        category=category,
                        severity="CRITICAL",
                        config_key=var_name,
                        expected_value="<configured>",
                        actual_value=None,
                        validation_error=f"Required variable missing",
                        business_impact=requirements['business_impact'],
                        fix_recommendation=f"Set {var_name}: {requirements['staging_requirement']}",
                        environment_source=self.env.get("NETRA_ENVIRONMENT", "unknown")
                    )
                    category_failures.append(failure)
                    continue
                
                # Check if value is empty
                if requirements['required'] and value == "":
                    failure = ConfigurationAuditResult(
                        category=category,
                        severity="CRITICAL",
                        config_key=var_name,
                        expected_value="<non-empty>",
                        actual_value="",
                        validation_error="Required variable is empty",
                        business_impact=requirements['business_impact'],
                        fix_recommendation=f"Configure {var_name}: {requirements['staging_requirement']}",
                        environment_source=self.env.get("NETRA_ENVIRONMENT", "unknown")
                    )
                    category_failures.append(failure)
                    continue
                
                # Run staging-specific validation
                if value and not requirements['validation'](value):
                    failure = ConfigurationAuditResult(
                        category=category,
                        severity="CRITICAL",
                        config_key=var_name,
                        expected_value="<valid_staging_value>",
                        actual_value=value,
                        validation_error=f"Validation failed: {requirements['staging_requirement']}",
                        business_impact=requirements['business_impact'],
                        fix_recommendation=f"Update {var_name}: {requirements['staging_requirement']}",
                        environment_source=self.env.get("NETRA_ENVIRONMENT", "unknown")
                    )
                    category_failures.append(failure)
            
            if category_failures:
                all_failures.extend(category_failures)
        
        # Report comprehensive configuration audit failures
        if all_failures:
            self.audit_results.extend(all_failures)
            
            # Group failures by category for reporting
            failures_by_category = {}
            for failure in all_failures:
                if failure.category not in failures_by_category:
                    failures_by_category[failure.category] = []
                failures_by_category[failure.category].append(failure)
            
            # Generate detailed failure report
            failure_report_sections = []
            for category, failures in failures_by_category.items():
                section_header = f"\n{category.upper()} CONFIGURATION FAILURES:"
                failure_details = []
                for failure in failures:
                    failure_details.append(
                        f"  - {failure.config_key}: {failure.validation_error}\n"
                        f"    Expected: {failure.expected_value}, Got: {failure.actual_value}\n"
                        f"    Business Impact: {failure.business_impact}\n"
                        f"    Fix: {failure.fix_recommendation}"
                    )
                failure_report_sections.append(section_header + "\n" + "\n".join(failure_details))
            
            failure_report = "\n".join(failure_report_sections)
            
            assert False, (
                f"CRITICAL BACKEND CONFIGURATION CASCADE FAILURES:\n{failure_report}\n\n"
                f"COMPOUND IMPACT ANALYSIS:\n"
                f"  - {len([f for f in all_failures if f.category == 'authentication'])} authentication failures "
                f"→ 100% auth breakdown\n"
                f"  - {len([f for f in all_failures if f.category == 'external_services'])} external service failures "
                f"→ Analytics and cache broken\n"
                f"  - {len([f for f in all_failures if f.category == 'environment_enforcement'])} enforcement failures "
                f"→ Development behavior in staging\n\n"
                f"These {len(all_failures)} configuration failures compound to create complete "
                f"backend service breakdown, preventing production deployment validation."
            )

    @pytest.mark.env("staging")
    @pytest.mark.critical
    @pytest.mark.e2e
    async def test_service_dependency_validation_external_service_requirements(self):
        """
        EXPECTED TO FAIL - CRITICAL SERVICE DEPENDENCY VALIDATION ISSUE
        
        Issue: Backend service reports healthy despite external service dependency failures
        Expected: Service health accurately reflects all critical dependency status
        Actual: Service health checks pass while dependencies are failing
        
        Validation Gap: Service readiness != operational capability with external dependencies
        """
        # Define critical external service dependencies for backend
        critical_dependencies = {
            'auth_service': {
                'type': 'required',
                'test_endpoint': '/health',
                'expected_response_time_ms': 1000,
                'fallback_acceptable': False,
                'business_impact': 'Authentication completely broken'
            },
            'postgres_database': {
                'type': 'required',
                'test_method': 'database_connection',
                'expected_response_time_ms': 2000,
                'fallback_acceptable': False,
                'business_impact': 'Data persistence failure'
            },
            'redis_cache': {
                'type': 'required_in_staging',
                'test_method': 'redis_ping',
                'expected_response_time_ms': 500,
                'fallback_acceptable': False,  # Should be False in staging
                'business_impact': 'Performance degradation, session loss'
            },
            'clickhouse_analytics': {
                'type': 'required_in_staging',
                'test_endpoint': '/',
                'expected_response_time_ms': 3000,
                'fallback_acceptable': False,  # Should be False in staging
                'business_impact': 'Analytics broken, health checks fail'
            }
        }
        
        dependency_failures = []
        
        for service_name, requirements in critical_dependencies.items():
            validation_result = await self._validate_service_dependency(service_name, requirements)
            
            if not validation_result.connectivity_status == "connected":
                dependency_failures.append(validation_result)
            
            # Check fallback configuration appropriateness
            if (requirements['fallback_acceptable'] is False and 
                validation_result.fallback_status == "enabled"):
                dependency_failures.append(ServiceDependencyValidation(
                    service_name=service_name,
                    dependency_type=requirements['type'],
                    connectivity_status=validation_result.connectivity_status,
                    configuration_status="fallback_misconfigured",
                    fallback_status="inappropriately_enabled",
                    business_impact=f"Infrastructure issues masked for {service_name}",
                    staging_requirement="Fallback should be disabled in staging"
                ))
        
        # Report service dependency validation failures
        if dependency_failures:
            failure_report = []
            for failure in dependency_failures:
                failure_report.append(
                    f"  - {failure.service_name} ({failure.dependency_type}):\n"
                    f"    Connectivity: {failure.connectivity_status}\n"
                    f"    Configuration: {failure.configuration_status}\n" 
                    f"    Fallback: {failure.fallback_status}\n"
                    f"    Business Impact: {failure.business_impact}\n"
                    f"    Staging Requirement: {failure.staging_requirement}"
                )
            
            failure_summary = "\n".join(failure_report)
            
            assert False, (
                f"CRITICAL SERVICE DEPENDENCY VALIDATION FAILURES:\n{failure_summary}\n\n"
                f"These external service dependency issues prevent backend service from "
                f"reaching fully operational state. Service may report healthy while critical "
                f"functionality is degraded or broken.\n\n"
                f"DEPLOYMENT IMPACT:\n"
                f"  - Health checks may pass while service is actually degraded\n"
                f"  - Deployment validation provides false positives\n"
                f"  - Production failures not caught by staging validation\n"
                f"  - User-facing functionality broken despite healthy service status"
            )

    @pytest.mark.env("staging")
    @pytest.mark.critical
    @pytest.mark.e2e
    def test_environment_behavior_enforcement_staging_vs_development_drift(self):
        """
        EXPECTED TO FAIL - CRITICAL ENVIRONMENT BEHAVIOR ENFORCEMENT ISSUE
        
        Issue: Staging environment behaves like development with inappropriate fallbacks
        Expected: Staging enforces production-like strict validation and fail-fast behavior
        Actual: Staging allows development fallbacks masking production readiness issues
        
        Drift Pattern: Dev (permissive) != Staging (should be strict) != Prod (ultra-strict)
        Anti-Pattern: Staging validation gaps allow issues that break production
        """
        # Test environment detection accuracy
        environment_detection_methods = {
            'NETRA_ENVIRONMENT': self.env.get("NETRA_ENVIRONMENT"),
            'K_SERVICE': self.env.get("K_SERVICE"),  # Cloud Run
            'GOOGLE_CLOUD_PROJECT': self.env.get("GOOGLE_CLOUD_PROJECT"),  # GCP
            'GCP_PROJECT': self.env.get("GCP_PROJECT"),
            'GAE_SERVICE': self.env.get("GAE_SERVICE"),  # App Engine
            'DATABASE_URL_INDICATOR': 'staging' in self.env.get("DATABASE_URL", "").lower()
        }
        
        # At least one method should detect staging
        staging_detected = any([
            environment_detection_methods['NETRA_ENVIRONMENT'] == 'staging',
            environment_detection_methods['K_SERVICE'] is not None,
            environment_detection_methods['GOOGLE_CLOUD_PROJECT'] is not None,
            environment_detection_methods['DATABASE_URL_INDICATOR']
        ])
        
        assert staging_detected, (
            f"CRITICAL STAGING DETECTION FAILURE: No staging environment indicators found.\n"
            f"Detection methods: {json.dumps(environment_detection_methods, indent=2)}\n\n"
            f"Without proper staging detection, service uses development behavior patterns "
            f"with inappropriate fallbacks, masking production readiness issues."
        )
        
        # Test staging behavior enforcement configuration
        if staging_detected:
            staging_enforcement_config = {
                'STRICT_VALIDATION_MODE': {
                    'expected': 'true',
                    'impact': 'Validation gaps allow invalid configurations'
                },
                'FAIL_FAST_ON_MISSING_SERVICES': {
                    'expected': 'true', 
                    'impact': 'Silent service degradation masking issues'
                },
                'ALLOW_LOCALHOST_FALLBACK': {
                    'expected': 'false',
                    'impact': 'Development fallbacks hide infrastructure gaps'
                },
                'REQUIRE_EXTERNAL_SERVICES': {
                    'expected': 'true',
                    'impact': 'Optional services should be required in staging'
                },
                'ENABLE_DEVELOPMENT_FALLBACKS': {
                    'expected': 'false',
                    'impact': 'Development patterns inappropriate in staging'
                }
            }
            
            enforcement_failures = []
            
            for var_name, requirements in staging_enforcement_config.items():
                actual_value = self.env.get(var_name, "undefined")
                expected_value = requirements['expected']
                
                if actual_value != expected_value:
                    enforcement_failures.append(
                        f"{var_name}: Expected '{expected_value}', got '{actual_value}' "
                        f"- {requirements['impact']}"
                    )
            
            if enforcement_failures:
                failure_report = "\n".join(f"  - {failure}" for failure in enforcement_failures)
                assert False, (
                    f"CRITICAL STAGING BEHAVIOR ENFORCEMENT FAILURES:\n{failure_report}\n\n"
                    f"Staging environment detected but enforcement configuration allows development behavior. "
                    f"This creates dangerous staging/production drift where issues pass staging validation "
                    f"but break production deployment.\n\n"
                    f"PRODUCTION RISK:\n"
                    f"  - Staging gives false confidence in production readiness\n"
                    f"  - Infrastructure issues hidden by inappropriate fallbacks\n"
                    f"  - Production failures not caught during staging validation\n"
                    f"  - Customer-facing outages from unvalidated configuration"
                )

    @pytest.mark.env("staging")
    @pytest.mark.critical
    @pytest.mark.e2e
    async def test_health_endpoint_external_service_validation_503_failure_analysis(self):
        """
        EXPECTED TO FAIL - CRITICAL HEALTH ENDPOINT VALIDATION ISSUE
        
        Issue: /health/ready returns 503 due to external service connectivity failures
        Expected: Health endpoint returns 200 when all required dependencies accessible
        Actual: External service timeouts cause 503 responses blocking deployment validation
        
        Health Gap: Service startup success != service operational readiness
        Deployment Impact: GCP Cloud Run deployment validation fails, blocking releases
        """
        # Test backend health endpoint comprehensive validation
        backend_url = self.env.get("BACKEND_URL", "http://localhost:8000")
        health_endpoints = [
            '/health/',
            '/health/ready', 
            '/health/live'
        ]
        
        health_failures = []
        
        for endpoint in health_endpoints:
            full_url = f"{backend_url}{endpoint}"
            
            start_time = time.time()
            try:
                async with httpx.AsyncClient(timeout=15.0) as client:
                    response = await client.get(full_url)
                    response_time = time.time() - start_time
                    
                    if response.status_code == 200:
                        # Health check passed - validate response details
                        try:
                            health_data = response.json()
                            
                            # Check external service status in health response
                            if "services" in health_data:
                                for service_name, service_status in health_data["services"].items():
                                    if service_name in ["clickhouse", "redis", "auth_service"]:
                                        service_healthy = service_status.get("healthy", False)
                                        if not service_healthy:
                                            health_failures.append({
                                                'endpoint': endpoint,
                                                'issue': f'External service {service_name} unhealthy',
                                                'details': service_status,
                                                'response_time': response_time
                                            })
                            
                            print(f"SUCCESS: {endpoint} health check passed in {response_time:.2f}s")
                            
                        except Exception as parse_error:
                            health_failures.append({
                                'endpoint': endpoint,
                                'issue': 'Health response parsing failed',
                                'details': {'error': str(parse_error), 'response': response.text[:200]},
                                'response_time': response_time
                            })
                            
                    elif response.status_code == 503:
                        # Expected failure - health check failing due to external services
                        try:
                            error_data = response.json()
                        except:
                            error_data = {'raw_response': response.text}
                        
                        health_failures.append({
                            'endpoint': endpoint,
                            'issue': f'Health check returned 503 Service Unavailable',
                            'details': error_data,
                            'response_time': response_time,
                            'status_code': 503
                        })
                        
                    else:
                        # Unexpected status code
                        health_failures.append({
                            'endpoint': endpoint,
                            'issue': f'Unexpected status code {response.status_code}',
                            'details': {'response': response.text[:200]},
                            'response_time': response_time,
                            'status_code': response.status_code
                        })
                        
            except httpx.TimeoutException:
                response_time = time.time() - start_time
                health_failures.append({
                    'endpoint': endpoint,
                    'issue': 'Health endpoint timeout',
                    'details': {'timeout_seconds': response_time},
                    'response_time': response_time,
                    'status_code': 0
                })
                
            except Exception as e:
                response_time = time.time() - start_time
                health_failures.append({
                    'endpoint': endpoint,
                    'issue': f'Health endpoint connection failed',
                    'details': {'error': str(e)},
                    'response_time': response_time,
                    'status_code': 0
                })
        
        # Report health endpoint failures
        if health_failures:
            failure_report = []
            for failure in health_failures:
                failure_report.append(
                    f"  - {failure['endpoint']}: {failure['issue']} "
                    f"({failure['response_time']:.2f}s)"
                )
                if failure.get('status_code') == 503:
                    failure_report.append(f"    503 Details: {json.dumps(failure['details'], indent=6)}")
            
            failure_summary = "\n".join(failure_report)
            
            assert False, (
                f"CRITICAL HEALTH ENDPOINT VALIDATION FAILURES:\n{failure_summary}\n\n"
                f"Health endpoint failures indicate external service dependency issues "
                f"that will block deployment validation and production releases.\n\n"
                f"DEPLOYMENT IMPACT:\n"
                f"  - GCP Cloud Run deployment validation fails\n"
                f"  - Kubernetes readiness probes fail\n"
                f"  - Load balancer removes service from rotation\n"
                f"  - Monitoring systems generate false alerts\n"
                f"  - Release pipeline blocked preventing feature delivery"
            )

    @pytest.mark.env("staging")
    @pytest.mark.critical
    @pytest.mark.e2e
    def test_configuration_source_validation_secret_manager_vs_environment(self):
        """
        EXPECTED TO FAIL - HIGH CONFIGURATION SOURCE ISSUE
        
        Issue: Configuration loaded from wrong source or source priority incorrect
        Expected: Staging configuration loaded from GCP Secret Manager with env var fallback
        Actual: Configuration source priority allows development values to override staging
        
        Source Priority Gap: Dev env vars override staging secrets
        """
        # Test configuration source detection and priority
        config_source_vars = {
            'CONFIG_SOURCE_PRIORITY': {
                'expected': 'secret_manager,environment,defaults',
                'description': 'Secret Manager should have highest priority in staging'
            },
            'USE_SECRET_MANAGER': {
                'expected': 'true',
                'description': 'GCP Secret Manager should be enabled in staging'
            },
            'ALLOW_ENV_OVERRIDE_SECRETS': {
                'expected': 'false',
                'description': 'Environment variables should not override secrets in staging'
            },
            'SECRET_MANAGER_PROJECT': {
                'required': True,
                'validation': lambda v: v is not None and 'staging' in v,
                'description': 'Should point to staging GCP project'
            }
        }
        
        source_validation_failures = []
        
        for var_name, requirements in config_source_vars.items():
            value = self.env.get(var_name)
            
            if 'expected' in requirements and value != requirements['expected']:
                source_validation_failures.append(
                    f"{var_name}: Expected '{requirements['expected']}', got '{value}' - {requirements['description']}"
                )
            
            if requirements.get('required') and value is None:
                source_validation_failures.append(
                    f"{var_name}: MISSING - {requirements['description']}"
                )
            
            if 'validation' in requirements and value and not requirements['validation'](value):
                source_validation_failures.append(
                    f"{var_name}: INVALID - '{value}' - {requirements['description']}"
                )
        
        # Test actual configuration loading behavior
        test_config_keys = ['DATABASE_URL', 'REDIS_URL', 'JWT_SECRET_KEY']
        for key in test_config_keys:
            # Check if value comes from appropriate source
            value = self.env.get(key)
            source = self.env.get_variable_source(key) if hasattr(self.env, 'get_variable_source') else "unknown"
            
            if value and source == "environment" and "dev" in value.lower():
                source_validation_failures.append(
                    f"{key}: Development value from environment overriding staging secrets - '{value}'"
                )
        
        # Report configuration source failures
        if source_validation_failures:
            failure_report = "\n".join(f"  - {failure}" for failure in source_validation_failures)
            assert False, (
                f"CRITICAL CONFIGURATION SOURCE VALIDATION FAILURES:\n{failure_report}\n\n"
                f"Configuration source priority issues cause staging to load development values "
                f"instead of production-like configuration from GCP Secret Manager.\n\n"
                f"This creates dangerous configuration drift where staging validation passes "
                f"with development configuration but production fails with actual secrets."
            )

    # ===================================================================
    # SERVICE DEPENDENCY VALIDATION HELPER METHODS
    # ===================================================================
    
    async def _validate_service_dependency(
        self, 
        service_name: str, 
        requirements: Dict[str, Any]
    ) -> ServiceDependencyValidation:
        """Validate individual service dependency connectivity and configuration."""
        
        if service_name == "auth_service":
            return await self._validate_auth_service_dependency(requirements)
        elif service_name == "postgres_database":
            return await self._validate_postgres_dependency(requirements)
        elif service_name == "redis_cache":
            return await self._validate_redis_dependency(requirements)
        elif service_name == "clickhouse_analytics":
            return await self._validate_clickhouse_dependency(requirements)
        else:
            return ServiceDependencyValidation(
                service_name=service_name,
                dependency_type=requirements['type'],
                connectivity_status="unknown",
                configuration_status="unknown",
                fallback_status="unknown",
                business_impact="unknown_service",
                staging_requirement="unknown"
            )
    
    async def _validate_auth_service_dependency(self, requirements: Dict[str, Any]) -> ServiceDependencyValidation:
        """Validate auth service dependency."""
        auth_url = self.env.get("AUTH_SERVICE_URL", "http://localhost:8080")
        
        if "localhost" in auth_url:
            return ServiceDependencyValidation(
                service_name="auth_service",
                dependency_type=requirements['type'],
                connectivity_status="localhost_fallback",
                configuration_status="invalid",
                fallback_status="inappropriate",
                business_impact=requirements['business_impact'],
                staging_requirement="Must point to staging auth service"
            )
        
        # Test auth service connectivity
        try:
            health_url = f"{auth_url}/health"
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(health_url)
                
                if response.status_code == 200:
                    return ServiceDependencyValidation(
                        service_name="auth_service",
                        dependency_type=requirements['type'],
                        connectivity_status="connected",
                        configuration_status="valid",
                        fallback_status="disabled",
                        business_impact="auth_service_operational",
                        staging_requirement="accessible"
                    )
                else:
                    return ServiceDependencyValidation(
                        service_name="auth_service",
                        dependency_type=requirements['type'],
                        connectivity_status="unhealthy",
                        configuration_status="service_error",
                        fallback_status="unknown",
                        business_impact=requirements['business_impact'],
                        staging_requirement=f"Health check should return 200, got {response.status_code}"
                    )
                    
        except Exception as e:
            return ServiceDependencyValidation(
                service_name="auth_service",
                dependency_type=requirements['type'],
                connectivity_status="connection_failed",
                configuration_status="network_error",
                fallback_status="unknown",
                business_impact=requirements['business_impact'],
                staging_requirement=f"Should be accessible, got error: {e}"
            )
    
    async def _validate_postgres_dependency(self, requirements: Dict[str, Any]) -> ServiceDependencyValidation:
        """Validate PostgreSQL database dependency."""
        database_url = self.env.get("DATABASE_URL")
        
        if not database_url:
            return ServiceDependencyValidation(
                service_name="postgres_database",
                dependency_type=requirements['type'],
                connectivity_status="not_configured",
                configuration_status="missing",
                fallback_status="unknown",
                business_impact=requirements['business_impact'],
                staging_requirement="DATABASE_URL must be configured"
            )
        
        if "localhost" in database_url:
            return ServiceDependencyValidation(
                service_name="postgres_database",
                dependency_type=requirements['type'],
                connectivity_status="localhost_fallback",
                configuration_status="invalid",
                fallback_status="inappropriate",
                business_impact=requirements['business_impact'],
                staging_requirement="Must use staging database, not localhost"
            )
        
        # Test database connectivity
        try:
            import psycopg2
            from urllib.parse import urlparse
            
            parsed = urlparse(database_url)
            conn_params = {
                'host': parsed.hostname,
                'port': parsed.port or 5432,
                'database': parsed.path.lstrip('/'),
                'user': parsed.username,
                'password': parsed.password
            }
            
            # Add SSL for staging
            if "staging" in database_url or "cloudsql" in database_url:
                conn_params['sslmode'] = 'require'
            
            conn = psycopg2.connect(**conn_params)
            conn.close()
            
            return ServiceDependencyValidation(
                service_name="postgres_database",
                dependency_type=requirements['type'],
                connectivity_status="connected",
                configuration_status="valid",
                fallback_status="disabled",
                business_impact="database_operational",
                staging_requirement="accessible"
            )
            
        except Exception as e:
            return ServiceDependencyValidation(
                service_name="postgres_database",
                dependency_type=requirements['type'],
                connectivity_status="connection_failed",
                configuration_status="credentials_invalid",
                fallback_status="unknown",
                business_impact=requirements['business_impact'],
                staging_requirement=f"Database should be accessible, got error: {e}"
            )
    
    async def _validate_redis_dependency(self, requirements: Dict[str, Any]) -> ServiceDependencyValidation:
        """Validate Redis cache dependency."""
        redis_url = self.env.get("REDIS_URL")
        redis_fallback = self.env.get("REDIS_FALLBACK_ENABLED", "true").lower() == "true"
        
        if not redis_url:
            return ServiceDependencyValidation(
                service_name="redis_cache",
                dependency_type=requirements['type'],
                connectivity_status="not_configured",
                configuration_status="missing",
                fallback_status="enabled" if redis_fallback else "disabled",
                business_impact=requirements['business_impact'],
                staging_requirement="REDIS_URL must be configured"
            )
        
        # Test Redis connectivity
        try:
            parsed = urlparse(redis_url)
            host = parsed.hostname
            port = parsed.port or 6379
            
            sock = socket.create_connection((host, port), timeout=3.0)
            sock.close()
            
            return ServiceDependencyValidation(
                service_name="redis_cache",
                dependency_type=requirements['type'],
                connectivity_status="connected",
                configuration_status="valid",
                fallback_status="disabled" if not redis_fallback else "enabled",
                business_impact="cache_operational",
                staging_requirement="accessible"
            )
            
        except Exception as e:
            return ServiceDependencyValidation(
                service_name="redis_cache", 
                dependency_type=requirements['type'],
                connectivity_status="connection_failed",
                configuration_status="network_error",
                fallback_status="enabled" if redis_fallback else "disabled",
                business_impact=requirements['business_impact'],
                staging_requirement=f"Redis should be accessible, got error: {e}"
            )
    
    async def _validate_clickhouse_dependency(self, requirements: Dict[str, Any]) -> ServiceDependencyValidation:
        """Validate ClickHouse analytics dependency."""
        clickhouse_host = self.env.get("CLICKHOUSE_HOST", "clickhouse.staging.netrasystems.ai")
        clickhouse_port = int(self.env.get("CLICKHOUSE_PORT", "8123"))
        
        # Test ClickHouse connectivity
        try:
            sock = socket.create_connection((clickhouse_host, clickhouse_port), timeout=5.0)
            sock.close()
            
            return ServiceDependencyValidation(
                service_name="clickhouse_analytics",
                dependency_type=requirements['type'],
                connectivity_status="connected",
                configuration_status="valid",
                fallback_status="disabled",
                business_impact="analytics_operational",
                staging_requirement="accessible"
            )
            
        except Exception as e:
            return ServiceDependencyValidation(
                service_name="clickhouse_analytics",
                dependency_type=requirements['type'], 
                connectivity_status="connection_failed",
                configuration_status="network_error",
                fallback_status="unknown",
                business_impact=requirements['business_impact'],
                staging_requirement=f"ClickHouse should be accessible, got error: {e}"
            )
    
    def _generate_audit_summary(self):
        """Generate comprehensive audit summary for reporting."""
        if not self.audit_results:
            return
        
        # Group results by severity and category
        by_severity = {}
        by_category = {}
        
        for result in self.audit_results:
            if result.severity not in by_severity:
                by_severity[result.severity] = []
            by_severity[result.severity].append(result)
            
            if result.category not in by_category:
                by_category[result.category] = []
            by_category[result.category].append(result)
        
        print("\n" + "="*80)
        print("STAGING BACKEND CONFIGURATION AUDIT SUMMARY")
        print("="*80)
        
        for severity in ["CRITICAL", "HIGH", "MEDIUM", "LOW"]:
            if severity in by_severity:
                print(f"\n{severity} ISSUES ({len(by_severity[severity])}):")
                for result in by_severity[severity]:
                    print(f"  - {result.config_key}: {result.validation_error}")
        
        print(f"\nTotal Issues: {len(self.audit_results)}")
        print(f"Test Duration: {time.time() - self.start_time:.2f}s")


# ===================================================================
# STANDALONE COMPREHENSIVE AUDIT TEST
# ===================================================================

@pytest.mark.env("staging")
@pytest.mark.critical
@pytest.mark.e2e
async def test_staging_backend_comprehensive_configuration_audit():
    """
    STANDALONE COMPREHENSIVE TEST - Complete Backend Configuration Audit
    
    EXPECTED TO FAIL: Comprehensive validation of all backend configuration issues
    Purpose: Single test to validate complete staging backend configuration readiness
    """
    env = IsolatedEnvironment()
    env.enable_isolation_mode()
    
    try:
        # Quick comprehensive validation
        issues = []
        
        # Database configuration
        database_url = env.get("DATABASE_URL")
        if not database_url or "localhost" in database_url:
            issues.append("DATABASE_URL: Missing or using localhost")
        
        # Redis configuration
        redis_url = env.get("REDIS_URL") 
        redis_fallback = env.get("REDIS_FALLBACK_ENABLED", "true").lower() == "true"
        if not redis_url or "localhost" in redis_url or redis_fallback:
            issues.append("REDIS: Missing, localhost, or fallback enabled")
        
        # ClickHouse configuration
        clickhouse_host = env.get("CLICKHOUSE_HOST")
        if not clickhouse_host or "localhost" in clickhouse_host:
            issues.append("CLICKHOUSE: Missing or using localhost")
        
        # Environment enforcement
        netra_env = env.get("NETRA_ENVIRONMENT")
        if netra_env != "staging":
            issues.append(f"ENVIRONMENT: Should be 'staging', got '{netra_env}'")
        
        if issues:
            assert False, f"CRITICAL BACKEND CONFIGURATION ISSUES: {', '.join(issues)}"
        
        print("SUCCESS: Comprehensive backend configuration audit passed")
        
    finally:
        env.reset_to_original()


if __name__ == "__main__":
    """Direct execution for rapid testing during development."""
    print("Running comprehensive backend configuration audit...")
    
    # Environment snapshot
    env = IsolatedEnvironment()
    print(f"Environment: {env.get('NETRA_ENVIRONMENT', 'unknown')}")
    print(f"Database: {env.get('DATABASE_URL', 'NOT_SET')[:50]}...")
    print(f"Redis: {env.get('REDIS_URL', 'NOT_SET')}")
    print(f"ClickHouse: {env.get('CLICKHOUSE_HOST', 'NOT_SET')}")
    print(f"Auth Service: {env.get('AUTH_SERVICE_URL', 'NOT_SET')}")
    
    # Run comprehensive audit
    try:
        asyncio.run(test_staging_backend_comprehensive_configuration_audit())
    except Exception as e:
        print(f"Comprehensive audit failed: {e}")
    
    print("Backend configuration audit completed.")