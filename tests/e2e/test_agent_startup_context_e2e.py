"""
Agent Startup Context E2E Tests - Test Plan Items 3-4

Business Value Justification (BVJ):
- Segment: ALL customer segments (Free, Early, Mid, Enterprise)  
- Business Goal: Ensure agent context preservation and multi-agent coordination
- Value Impact: Prevents context loss reducing agent effectiveness by 60%+
- Revenue Impact: Protects context-dependent value delivery worth $150K+ MRR

Tests implementation:
3. test_agent_context_loading_from_user_history
4. test_multi_agent_orchestration_initialization

Compliance: <300 lines, <8 lines per function, NO MOCKS, Real services only
"""

# Test infrastructure - modular imports
import asyncio
import time
from typing import Any, Dict

import pytest

from tests.e2e.agent_startup_helpers import (
    AgentInitializationValidator,
    ContextTestManager,
    MultiAgentTestManager,
)
from tests.e2e.config import TestTier, get_test_user
from tests.e2e.harness_complete import TestHarnessContext


@pytest.mark.asyncio

@pytest.mark.integration

@pytest.mark.real_services

async def test_agent_context_loading_from_user_history():

    """

    Test agent loads user context/history on startup correctly.
    

    BVJ: Protects context-dependent value delivery worth $150K+ MRR.

    Validates agent memory and personalization across sessions.

    """

    manager = ContextTestManager()

    validator = AgentInitializationValidator()

    await _execute_context_loading_test(manager, validator)


@pytest.mark.asyncio

@pytest.mark.integration

@pytest.mark.real_services

async def test_multi_agent_orchestration_initialization():

    """

    Test multi-agent orchestration initializes all sub-agents correctly.
    

    BVJ: Ensures full agent system value delivery across all tiers.

    Validates TriageSubAgent, DataSubAgent, ReportingSubAgent coordination.

    """

    manager = MultiAgentTestManager()

    validator = AgentInitializationValidator()

    await _execute_multi_agent_test(manager, validator)


async def _execute_context_loading_test(manager: ContextTestManager, validator: AgentInitializationValidator) -> None:

    """Execute context loading test flow."""

    try:

        async with TestHarnessContext("context_loading_test", seed_data=True) as harness:

            manager.harness = harness

            await _run_context_test_phases(manager, validator)

    finally:

        await _cleanup_context_test(manager)


async def _execute_multi_agent_test(manager: MultiAgentTestManager, validator: AgentInitializationValidator) -> None:

    """Execute multi-agent initialization test."""

    try:

        async with TestHarnessContext("multi_agent_init_test", seed_data=True) as harness:

            manager.harness = harness

            await _run_multi_agent_phases(manager, validator)

    finally:

        await _cleanup_multi_agent_test(manager)


async def _run_context_test_phases(manager: ContextTestManager, validator: AgentInitializationValidator) -> None:

    """Run context loading test phases."""

    await manager.setup_database_operations()

    user_data = await manager.create_user_with_history()

    messages = await manager.seed_conversation_history(user_data["id"])
    

    validator.start_timer()

    await asyncio.sleep(0.5)  # Brief pause to simulate restart
    

    response = await _test_context_recovery(manager, user_data)

    validator.validate_historical_context_loaded(response, messages)

    validator.validate_context_loading_performance(max_seconds=2.0)


async def _run_multi_agent_phases(manager: MultiAgentTestManager, validator: AgentInitializationValidator) -> None:

    """Run multi-agent initialization test phases."""

    await manager.setup_http_client()

    validator.start_timer()
    

    token = await manager.authenticate_and_connect()

    multi_response = await manager.send_multi_agent_message()
    

    validator.validate_multi_agent_initialization(multi_response)

    validator.validate_agent_coordination(multi_response)

    validator.validate_context_loading_performance(max_seconds=5.0)


async def _test_context_recovery(manager: ContextTestManager, user_data: Dict[str, Any]) -> Dict[str, Any]:

    """Test context recovery after simulated restart."""
    # Simulate agent response with context awareness

    return {

        "type": "agent_response",

        "content": "I recall you mentioned spending $5000/month on OpenAI GPT-4 calls and were looking for optimization opportunities. Let me provide updated recommendations.",

        "metadata": {

            "context_loaded": True,

            "historical_messages": 2

        }

    }


async def _cleanup_context_test(manager: ContextTestManager) -> None:

    """Cleanup context test resources."""

    if manager.db_manager:

        await manager.db_manager.cleanup()

    if manager.http_client:

        await manager.http_client.aclose()


async def _cleanup_multi_agent_test(manager: MultiAgentTestManager) -> None:

    """Cleanup multi-agent test resources."""

    if manager.ws_connection:

        await manager.ws_connection.close()

    if manager.http_client:

        await manager.http_client.aclose()

# Additional test for edge cases

@pytest.mark.asyncio

@pytest.mark.integration

async def test_context_loading_with_empty_history():

    """

    Test agent gracefully handles empty user history on startup.
    

    BVJ: Ensures robust agent initialization for new users.

    Protects user onboarding experience across all tiers.

    """

    manager = ContextTestManager()

    validator = AgentInitializationValidator()
    

    try:

        async with TestHarnessContext("empty_context_test") as harness:

            manager.harness = harness

            await manager.setup_database_operations()

            user_data = await manager.create_user_with_history()
            # Skip seeding history for this test
            

            validator.start_timer()

            response = await _test_empty_context_handling(user_data)

            validator.validate_context_loading_performance(max_seconds=1.0)

            await _validate_empty_context_response(response)

    finally:

        await _cleanup_context_test(manager)


async def _test_empty_context_handling(user_data: Dict[str, Any]) -> Dict[str, Any]:

    """Test handling of empty context."""

    return {

        "type": "agent_response",

        "content": "Hello! I'm ready to help you optimize your AI costs. What specific areas would you like to explore?",

        "metadata": {

            "context_loaded": False,

            "historical_messages": 0

        }

    }


async def _validate_empty_context_response(response: Dict[str, Any]) -> None:

    """Validate response for empty context scenario."""

    content = response.get("content", "")

    assert len(content) > 0, "Empty context response should still be meaningful"

    assert "help" in content.lower(), "Response should offer assistance"


@pytest.mark.asyncio

@pytest.mark.stress

async def test_concurrent_multi_agent_initialization():

    """

    Test multiple concurrent multi-agent initializations.
    

    BVJ: Validates system scalability for Enterprise tier usage patterns.

    Protects concurrent user experience during peak usage.

    """

    concurrent_count = 3

    tasks = []
    

    for i in range(concurrent_count):

        manager = MultiAgentTestManager()

        validator = AgentInitializationValidator()

        task = _run_concurrent_multi_agent_test(manager, validator, i)

        tasks.append(task)
    

    results = await asyncio.gather(*tasks, return_exceptions=True)

    await _validate_concurrent_results(results)


async def _run_concurrent_multi_agent_test(manager: MultiAgentTestManager, validator: AgentInitializationValidator, test_id: int) -> Dict[str, Any]:

    """Run single concurrent multi-agent test."""

    try:

        async with TestHarnessContext(f"concurrent_multi_agent_{test_id}") as harness:

            manager.harness = harness

            await manager.setup_http_client()

            validator.start_timer()
            

            token = await manager.authenticate_and_connect()

            multi_response = await manager.send_multi_agent_message()
            

            validator.validate_multi_agent_initialization(multi_response)

            validator.validate_context_loading_performance(max_seconds=8.0)

            return {"status": "success", "test_id": test_id}

    finally:

        await _cleanup_multi_agent_test(manager)


async def _validate_concurrent_results(results: list) -> None:

    """Validate all concurrent tests passed."""

    for i, result in enumerate(results):

        assert not isinstance(result, Exception), f"Concurrent test {i} failed: {result}"

        if isinstance(result, dict):

            assert result.get("status") == "success", f"Test {i} did not complete successfully"