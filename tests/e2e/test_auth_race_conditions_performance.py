"""

Test module split from original file

Generated by auto_fix_test_violations.py

"""



import json

import pytest

import os

import sys

import time

from shared.isolated_environment import IsolatedEnvironment



import psutil



# Add auth_service to path for imports





@pytest.mark.e2e

async def test_race_condition_suite_performance_benchmark(isolated_auth_environment):

    """

    Performance benchmark for the entire race condition test suite.

    This test measures the overall performance characteristics and

    provides baseline metrics for future regression testing.

    """

    start_time = time.perf_counter()



    # Execute a subset of race condition tests for benchmarking

    detector = RaceConditionDetector()

    executor = ConcurrentExecutor()



    auth_service = isolated_auth_environment["auth_service"]



    # Benchmark token generation

    token_start = time.perf_counter()

    tokens = []

    for i in range(1000):

        token = auth_service.jwt_handler.create_access_token(

            f"user-{i}", f"user{i}@example.com"

        )

        tokens.append(token)

    token_duration = time.perf_counter() - token_start



    # Benchmark session operations

    session_start = time.perf_counter()

    sessions = []

    for i in range(100):

        session_id = auth_service.session_manager.create_session(

            f"user-{i}", {"device": f"device-{i}"}

        )

        sessions.append(session_id)

    session_duration = time.perf_counter() - session_start



    total_duration = time.perf_counter() - start_time



    # Generate benchmark report

    benchmark_report = {

        "total_duration": total_duration,

        "token_generation": {

            "count": len(tokens),

            "duration": token_duration,

            "tokens_per_second": len(tokens) / token_duration,

        },

        "session_creation": {

            "count": len(sessions),

            "duration": session_duration,

            "sessions_per_second": len(sessions) / session_duration,

        },

        "memory_usage_mb": psutil.Process().memory_info().rss / (1024 * 1024),

    }



    logger.info(

        f"Race condition test suite benchmark: {

            json.dumps(

                benchmark_report,

                indent=2)}"

    )



    # Validate performance expectations

    assert (

        benchmark_report["token_generation"]["tokens_per_second"] > 1000

    ), "Token generation performance below expectation"



    assert (

        benchmark_report["session_creation"]["sessions_per_second"] > 100

    ), "Session creation performance below expectation"



    return benchmark_report

