"""

Test module split from original file

Generated by auto_fix_test_violations.py

"""



import asyncio

import gc

import hashlib

import json

import logging

import os

import random

import secrets

import statistics

import threading

import time

import uuid

from concurrent.futures import ThreadPoolExecutor, as_completed

from contextlib import asynccontextmanager

from dataclasses import dataclass, field

from datetime import datetime, timedelta, timezone

from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union

from shared.isolated_environment import IsolatedEnvironment



import asyncpg

import psutil

import pytest

import redis.asyncio as redis



from netra_backend.app.logging_config import central_logger

from tests.e2e.conftest import E2E_CONFIG, E2EEnvironmentValidator





class ConcurrencyTestMetrics:

    """Tracks metrics for concurrent tool execution testing."""

    total_operations: int = 0

    successful_operations: int = 0

    failed_operations: int = 0

    deadlock_count: int = 0

    retry_count: int = 0

    conflict_count: int = 0

    avg_response_time_ms: float = 0.0

    p95_response_time_ms: float = 0.0

    p99_response_time_ms: float = 0.0

    max_response_time_ms: float = 0.0

    transaction_abort_count: int = 0

    resource_exhaustion_count: int = 0

    data_consistency_violations: int = 0

    response_times: List[float] = field(default_factory=list)

    start_time: Optional[datetime] = None

    end_time: Optional[datetime] = None

    

    def add_operation(self, success: bool, response_time_ms: float, 

                     deadlock: bool = False, retry: bool = False, 

                     conflict: bool = False) -> None:

        """Record an operation result."""

        self.total_operations += 1

        if success:

            self.successful_operations += 1

        else:

            self.failed_operations += 1

            

        if deadlock:

            self.deadlock_count += 1

        if retry:

            self.retry_count += 1

        if conflict:

            self.conflict_count += 1

            

        self.response_times.append(response_time_ms)

        

        # Update response time statistics

        if self.response_times:

            self.avg_response_time_ms = statistics.mean(self.response_times)

            sorted_times = sorted(self.response_times)

            self.p95_response_time_ms = statistics.quantiles(sorted_times, n=20)[18]  # 95th percentile

            self.p99_response_time_ms = statistics.quantiles(sorted_times, n=100)[98]  # 99th percentile

            self.max_response_time_ms = max(self.response_times)

    

    @property

    def success_rate(self) -> float:

        """Calculate operation success rate."""

        if self.total_operations == 0:

            return 0.0

        return self.successful_operations / self.total_operations

    

    @property

    def conflict_rate(self) -> float:

        """Calculate transaction conflict rate."""

        if self.total_operations == 0:

            return 0.0

        return self.conflict_count / self.total_operations

    

    @property

    def deadlock_rate(self) -> float:

        """Calculate deadlock occurrence rate."""

        if self.total_operations == 0:

            return 0.0

        return self.deadlock_count / self.total_operations



class TestConcurrentToolConflictFramework:

    """Test framework for concurrent tool execution conflict scenarios."""

    

    def __init__(self):

        self.db_pool = None

        self.redis_client = None

        self.test_data = {}

        self.metrics = ConcurrencyTestMetrics()

        self.agent_states = {}

        

    async def setup(self) -> None:

        """Initialize test framework components."""

        logger.info("Setting up concurrent tool conflict test framework")

        

        # Initialize database connection pool

        try:

            self.db_pool = await asyncpg.create_pool(

                E2E_CONFIG["postgres_url"],

                min_size=5,

                max_size=TEST_CONFIG["max_connections"]

            )

        except Exception as e:

            logger.warning(f"Could not connect to PostgreSQL: {e}. Using SQLite for testing.")

            self.db_pool = None

        

        # Initialize Redis client

        try:

            self.redis_client = redis.from_url(E2E_CONFIG["redis_url"])

            await self.redis_client.ping()

        except Exception as e:

            logger.warning(f"Could not connect to Redis: {e}. Skipping Redis tests.")

            self.redis_client = None

        

        # Create test data

        await self._create_test_data()

        

        # Setup transaction isolation configuration

        await self._setup_transaction_isolation()

        

    async def teardown(self) -> None:

        """Clean up test framework resources."""

        logger.info("Tearing down concurrent tool conflict test framework")

        

        if self.redis_client:

            await self.redis_client.close()

            

        if self.db_pool:

            # Clean up database connections

            await self.db_pool.close()

            

    async def _create_test_data(self) -> None:

        """Create test data for conflict scenarios."""

        logger.info("Creating test data for concurrent conflict scenarios")

        

        # Create test users with optimization credits

        self.test_data["users"] = []

        for i in range(TEST_CONFIG["test_data_size"]):

            user_data = {

                "id": f"test_user_{i}",

                "optimization_credits": 1000,

                "version": 1,

                "created_at": datetime.now(timezone.utc)

            }

            self.test_data["users"].append(user_data)

            

        # Create test agent configurations

        self.test_data["agent_configs"] = []

        for i in range(100):

            config_data = {

                "id": f"agent_config_{i}",

                "configuration": {"timeout": 30, "max_retries": 3},

                "version": 1,

                "last_modified": datetime.now(timezone.utc)

            }

            self.test_data["agent_configs"].append(config_data)

            

        # Create shared resource pool

        self.test_data["resource_pool"] = {

            "max_connections": TEST_CONFIG["max_connections"],

            "current_connections": 0,

            "waiting_queue": []

        }

        

    async def _setup_transaction_isolation(self) -> None:

        """Configure database transaction isolation levels for testing."""

        logger.info("Setting up transaction isolation configuration")

        

        # Test different isolation levels

        isolation_levels = ["READ_COMMITTED", "REPEATABLE_READ", "SERIALIZABLE"]

        self.test_data["isolation_levels"] = isolation_levels

        

    async def _cleanup_database_connections(self) -> None:

        """Clean up database connections and reset state."""

        logger.info("Cleaning up database connections")

        

        if self.db_pool:

            try:

                # Clean up test tables if they exist

                async with self.db_pool.acquire() as conn:

                    await conn.execute("DROP TABLE IF EXISTS test_users CASCADE")

                    await conn.execute("DROP TABLE IF EXISTS test_agent_configurations CASCADE") 

                    await conn.execute("DROP TABLE IF EXISTS test_resources CASCADE")

            except Exception as e:

                logger.warning(f"Error during database cleanup: {e}")



class ConcurrentCreditDeductionTool:

    """Tool that deducts optimization credits with transaction isolation."""

    

    name = "concurrent_credit_deduction"

    description = "Deducts optimization credits from user account with conflict handling"

    

    def __init__(self, db_pool, test_metrics: ConcurrencyTestMetrics):

        self.db_pool = db_pool

        self.test_metrics = test_metrics

        

    async def _execute(self, user_id: str, credits_to_deduct: int, 

                      isolation_level: str = "READ_COMMITTED") -> Dict[str, Any]:

        """Execute credit deduction with specified isolation level."""

        start_time = time.time()

        

        try:

            result = await self._deduct_credits_with_isolation(

                user_id, credits_to_deduct, isolation_level

            )

            

            response_time = (time.time() - start_time) * 1000

            self.test_metrics.add_operation(

                success=True, 

                response_time_ms=response_time,

                conflict=result.get("had_conflict", False),

                retry=result.get("retry_count", 0) > 0

            )

            

            return result

            

        except Exception as e:

            response_time = (time.time() - start_time) * 1000

            is_deadlock = "deadlock" in str(e).lower()

            

            self.test_metrics.add_operation(

                success=False, 

                response_time_ms=response_time,

                deadlock=is_deadlock

            )

            

            raise e

            

    async def _deduct_credits_with_isolation(self, user_id: str, credits_to_deduct: int,

                                           isolation_level: str) -> Dict[str, Any]:

        """Perform credit deduction with optimistic locking and retry logic."""

        max_retries = TEST_CONFIG["transaction_retry_limit"]

        retry_count = 0

        had_conflict = False

        

        if not self.db_pool:

            # Fallback to mock operation if no database

            await asyncio.sleep(random.uniform(0.01, 0.1))

            return {

                "success": True,

                "previous_credits": 1000,

                "new_credits": 1000 - credits_to_deduct,

                "credits_deducted": credits_to_deduct,

                "retry_count": 0,

                "had_conflict": False,

                "mock_operation": True

            }

        

        while retry_count <= max_retries:

            try:

                async with self.db_pool.acquire() as conn:

                    async with conn.transaction(isolation=isolation_level):

                        # Read current user data with version

                        user_query = """

                        SELECT optimization_credits, version 

                        FROM test_users 

                        WHERE id = $1 

                        FOR UPDATE

                        """

                        user_row = await conn.fetchrow(user_query, user_id)

                        

                        if not user_row:

                            raise ValueError(f"User {user_id} not found")

                            

                        current_credits = user_row["optimization_credits"]

                        current_version = user_row["version"]

                        

                        if current_credits < credits_to_deduct:

                            raise ValueError(f"Insufficient credits: {current_credits} < {credits_to_deduct}")

                        

                        # Simulate processing time to increase conflict probability

                        await asyncio.sleep(random.uniform(0.01, 0.05))

                        

                        # Attempt optimistic update

                        new_credits = current_credits - credits_to_deduct

                        new_version = current_version + 1

                        

                        update_query = """

                        UPDATE test_users 

                        SET optimization_credits = $1, version = $2, updated_at = $3

                        WHERE id = $4 AND version = $5

                        """

                        

                        result = await conn.execute(

                            update_query, 

                            new_credits, 

                            new_version, 

                            datetime.now(timezone.utc),

                            user_id, 

                            current_version

                        )

                        

                        if result == "UPDATE 0":

                            # Version conflict detected

                            had_conflict = True

                            retry_count += 1

                            if retry_count <= max_retries:

                                # Exponential backoff before retry

                                await asyncio.sleep(random.uniform(0.01, 0.1) * (2 ** retry_count))

                                continue

                            else:

                                raise Exception("Version conflict: Maximum retries exceeded")

                        

                        # Success

                        return {

                            "success": True,

                            "previous_credits": current_credits,

                            "new_credits": new_credits,

                            "credits_deducted": credits_to_deduct,

                            "retry_count": retry_count,

                            "had_conflict": had_conflict

                        }

                        

            except asyncpg.exceptions.SerializationError:

                # Serialization failure - retry

                had_conflict = True

                retry_count += 1

                if retry_count <= max_retries:

                    await asyncio.sleep(random.uniform(0.01, 0.1) * (2 ** retry_count))

                    continue

                else:

                    raise

                    

            except asyncpg.exceptions.DeadlockDetectedError:

                # Deadlock detected - retry after brief delay

                retry_count += 1

                if retry_count <= max_retries:

                    await asyncio.sleep(random.uniform(0.001, 0.01))

                    continue

                else:

                    raise

        

        raise Exception("Transaction failed after maximum retries")



@pytest.mark.e2e

class TestConcurrentToolExecutionConflicts:

    """Test suite for concurrent tool execution conflicts."""

    

    @pytest.fixture(autouse=True)

    async def setup_test_framework(self):

        """Setup test framework for each test."""

        self.framework = ConcurrentToolConflictTestFramework()

        await self.framework.setup()

        yield

        await self.framework.teardown()

    

    @pytest.mark.e2e

    async def test_database_record_modification_conflicts(self):

        """Test Case 1: Validate transaction isolation when multiple agents modify the same database record."""

        logger.info("=== Test Case 1: Database Record Modification Conflicts ===")

        

        test_user_id = "test_user_conflicts_1"

        initial_credits = 1000

        concurrent_agents = 10

        credits_per_deduction = 50

        

        # Create test user

        await self._create_test_user(test_user_id, initial_credits)

        

        # Create concurrent credit deduction tool

        credit_tool = ConcurrentCreditDeductionTool(

            self.framework.db_pool, 

            self.framework.metrics

        )

        

        # Test different isolation levels

        isolation_levels = ["READ_COMMITTED", "REPEATABLE_READ", "SERIALIZABLE"]

        

        for isolation_level in isolation_levels:

            logger.info(f"Testing isolation level: {isolation_level}")

            

            # Reset user credits for each isolation level test

            await self._reset_user_credits(test_user_id, initial_credits)

            

            # Create concurrent tasks

            tasks = []

            for i in range(concurrent_agents):

                task = credit_tool._execute(

                    user_id=test_user_id,

                    credits_to_deduct=credits_per_deduction,

                    isolation_level=isolation_level

                )

                tasks.append(task)

            

            # Execute all tasks concurrently

            start_time = time.time()

            results = await asyncio.gather(*tasks, return_exceptions=True)

            execution_time = time.time() - start_time

            

            # Analyze results

            successful_deductions = [r for r in results if isinstance(r, dict) and r.get("success")]

            failed_operations = [r for r in results if isinstance(r, Exception)]

            

            logger.info(f"Isolation level {isolation_level} results:")

            logger.info(f"  Successful operations: {len(successful_deductions)}")

            logger.info(f"  Failed operations: {len(failed_operations)}")

            logger.info(f"  Execution time: {execution_time:.2f}s")

            

            # Validate final state consistency

            final_credits = await self._get_user_credits(test_user_id)

            expected_credits = initial_credits - (len(successful_deductions) * credits_per_deduction)

            

            assert final_credits == expected_credits, (

                f"Credit calculation mismatch: expected {expected_credits}, got {final_credits}"

            )

            

            # Performance assertions

            assert execution_time < TEST_CONFIG["performance_threshold_ms"] / 1000, (

                f"Execution time {execution_time}s exceeded threshold"

            )

            

            # Conflict rate should be reasonable for higher isolation levels

            if isolation_level == "SERIALIZABLE":

                assert self.framework.metrics.conflict_rate <= TEST_CONFIG["conflict_rate_threshold"], (

                    f"Conflict rate {self.framework.metrics.conflict_rate} exceeded threshold"

                )

    

    @pytest.mark.e2e

    async def test_agent_tool_resource_pool_exhaustion(self):

        """Test Case 2: Test behavior when concurrent tool executions exhaust shared resource pools."""

        logger.info("=== Test Case 2: Agent Tool Resource Pool Exhaustion ===")

        

        # Configure limited connection pool

        max_connections = 5

        concurrent_agents = 15

        query_duration = 2.0  # seconds

        

        # Create resource pool exhaustion test

        connection_requests = []

        acquisition_times = []

        

        async def long_running_query(agent_id: str) -> Dict[str, Any]:

            """Simulate long-running database query."""

            start_time = time.time()

            

            try:

                if not self.framework.db_pool:

                    # Fallback simulation if no database

                    acquisition_time = time.time() - start_time

                    acquisition_times.append(acquisition_time)

                    await asyncio.sleep(query_duration)

                    return {

                        "success": True,

                        "agent_id": agent_id,

                        "acquisition_time": acquisition_time,

                        "result": 1,

                        "mock_operation": True

                    }

                

                async with self.framework.db_pool.acquire() as conn:

                    acquisition_time = time.time() - start_time

                    acquisition_times.append(acquisition_time)

                    

                    # Simulate analytical query

                    await asyncio.sleep(query_duration)

                    

                    # Simple query to validate connection

                    result = await conn.fetchval("SELECT 1")

                    

                    return {

                        "success": True,

                        "agent_id": agent_id,

                        "acquisition_time": acquisition_time,

                        "result": result

                    }

                    

            except Exception as e:

                acquisition_time = time.time() - start_time

                acquisition_times.append(acquisition_time)

                return {

                    "success": False,

                    "agent_id": agent_id,

                    "acquisition_time": acquisition_time,

                    "error": str(e)

                }

        

        # Execute concurrent long-running queries

        tasks = [long_running_query(f"agent_{i}") for i in range(concurrent_agents)]

        

        start_time = time.time()

        results = await asyncio.gather(*tasks, return_exceptions=True)

        total_execution_time = time.time() - start_time

        

        # Analyze results

        successful_queries = [r for r in results if isinstance(r, dict) and r.get("success")]

        failed_queries = [r for r in results if isinstance(r, dict) and not r.get("success")]

        

        logger.info(f"Resource pool exhaustion test results:")

        logger.info(f"  Successful queries: {len(successful_queries)}")

        logger.info(f"  Failed queries: {len(failed_queries)}")

        logger.info(f"  Total execution time: {total_execution_time:.2f}s")

        logger.info(f"  Average acquisition time: {statistics.mean(acquisition_times):.3f}s")

        

        # Validate graceful degradation

        assert len(successful_queries) > 0, "No queries succeeded - system completely failed"

        

        # Validate that system handled pool exhaustion gracefully

        # Some queries should succeed even with limited pool

        success_rate = len(successful_queries) / len(results)

        assert success_rate >= 0.5, f"Success rate {success_rate} too low - poor pool management"

        

        # Validate no connection leaks

        # This would be monitored through pool metrics in real implementation

    

    @pytest.mark.e2e

    async def test_optimistic_locking_version_conflicts(self):

        """Test Case 3: Validate optimistic concurrency control using version-based locking."""

        logger.info("=== Test Case 3: Optimistic Locking Version Conflicts ===")

        

        config_id = "test_config_optimistic"

        concurrent_agents = 15

        

        # Create test configuration with version field

        await self._create_test_config(config_id, {"timeout": 30, "retries": 3}, version=1)

        

        async def update_config_field(agent_id: str, field_name: str, new_value: Any) -> Dict[str, Any]:

            """Update a specific configuration field with optimistic locking."""

            max_retries = 5

            retry_count = 0

            

            while retry_count <= max_retries:

                try:

                    async with self.framework.db_manager.get_connection() as conn:

                        async with conn.transaction():

                            # Read current configuration and version

                            config_query = """

                            SELECT configuration, version 

                            FROM agent_configurations 

                            WHERE id = $1

                            """

                            config_row = await conn.fetchrow(config_query, config_id)

                            

                            if not config_row:

                                raise ValueError(f"Configuration {config_id} not found")

                            

                            current_config = config_row["configuration"]

                            current_version = config_row["version"]

                            

                            # Modify configuration

                            new_config = current_config.copy()

                            new_config[field_name] = new_value

                            

                            # Simulate processing time

                            await asyncio.sleep(random.uniform(0.01, 0.1))

                            

                            # Attempt optimistic update

                            update_query = """

                            UPDATE agent_configurations 

                            SET configuration = $1, version = $2, updated_at = $3

                            WHERE id = $4 AND version = $5

                            """

                            

                            result = await conn.execute(

                                update_query,

                                json.dumps(new_config),

                                current_version + 1,

                                datetime.now(timezone.utc),

                                config_id,

                                current_version

                            )

                            

                            if result == "UPDATE 0":

                                # Version conflict - retry with exponential backoff

                                retry_count += 1

                                await asyncio.sleep(random.uniform(0.01, 0.1) * (2 ** retry_count))

                                continue

                            

                            return {

                                "success": True,

                                "agent_id": agent_id,

                                "field_updated": field_name,

                                "new_value": new_value,

                                "retry_count": retry_count,

                                "final_version": current_version + 1

                            }

                            

                except Exception as e:

                    retry_count += 1

                    if retry_count <= max_retries:

                        await asyncio.sleep(random.uniform(0.01, 0.1) * (2 ** retry_count))

                        continue

                    else:

                        return {

                            "success": False,

                            "agent_id": agent_id,

                            "error": str(e),

                            "retry_count": retry_count

                        }

            

            return {

                "success": False,

                "agent_id": agent_id,

                "error": "Maximum retries exceeded",

                "retry_count": retry_count

            }

        

        # Create concurrent update tasks for different fields

        tasks = []

        field_updates = [

            ("timeout", random.randint(10, 60)),

            ("retries", random.randint(1, 10)),

            ("batch_size", random.randint(10, 100)),

            ("cache_ttl", random.randint(60, 3600))

        ]

        

        for i in range(concurrent_agents):

            field_name, new_value = random.choice(field_updates)

            task = update_config_field(f"agent_{i}", field_name, new_value)

            tasks.append(task)

        

        # Execute concurrent updates

        start_time = time.time()

        results = await asyncio.gather(*tasks, return_exceptions=True)

        execution_time = time.time() - start_time

        

        # Analyze results

        successful_updates = [r for r in results if isinstance(r, dict) and r.get("success")]

        failed_updates = [r for r in results if isinstance(r, dict) and not r.get("success")]

        

        logger.info(f"Optimistic locking test results:")

        logger.info(f"  Successful updates: {len(successful_updates)}")

        logger.info(f"  Failed updates: {len(failed_updates)}")

        logger.info(f"  Execution time: {execution_time:.2f}s")

        

        # Validate that at least some updates succeeded

        assert len(successful_updates) > 0, "No updates succeeded"

        

        # Validate final configuration integrity

        final_config = await self._get_config(config_id)

        assert final_config is not None, "Configuration was corrupted"

        

        # Validate version progression

        assert final_config["version"] > 1, "Version was not incremented"

    

    @pytest.mark.e2e

    async def test_deadlock_detection_and_recovery_cascade(self):

        """Test Case 5: Test system behavior during complex deadlock scenarios."""

        logger.info("=== Test Case 5: Deadlock Detection and Recovery Cascade ===")

        

        # Create multiple resources for deadlock scenario

        resource_ids = ["resource_1", "resource_2", "resource_3"]

        

        for resource_id in resource_ids:

            await self._create_test_resource(resource_id, {"value": 100, "locked_by": None})

        

        async def create_deadlock_scenario(agent_id: str, resource_order: List[str]) -> Dict[str, Any]:

            """Create intentional deadlock by acquiring resources in different orders."""

            acquired_resources = []

            start_time = time.time()

            

            try:

                async with self.framework.db_manager.get_connection() as conn:

                    async with conn.transaction():

                        for resource_id in resource_order:

                            # Lock resource

                            lock_query = """

                            UPDATE test_resources 

                            SET locked_by = $1, locked_at = $2

                            WHERE id = $3 AND locked_by IS NULL

                            """

                            

                            result = await conn.execute(

                                lock_query,

                                agent_id,

                                datetime.now(timezone.utc),

                                resource_id

                            )

                            

                            if result == "UPDATE 0":

                                # Resource already locked - potential deadlock

                                await asyncio.sleep(random.uniform(0.1, 0.5))  # Wait and retry

                                

                            acquired_resources.append(resource_id)

                            

                            # Simulate work on resource

                            await asyncio.sleep(random.uniform(0.05, 0.2))

                        

                        # Work completed successfully

                        execution_time = time.time() - start_time

                        return {

                            "success": True,

                            "agent_id": agent_id,

                            "acquired_resources": acquired_resources,

                            "execution_time": execution_time

                        }

                        

            except asyncpg.exceptions.DeadlockDetectedError as e:

                execution_time = time.time() - start_time

                return {

                    "success": False,

                    "agent_id": agent_id,

                    "error": "deadlock_detected",

                    "acquired_resources": acquired_resources,

                    "execution_time": execution_time,

                    "deadlock_details": str(e)

                }

                

            except Exception as e:

                execution_time = time.time() - start_time

                return {

                    "success": False,

                    "agent_id": agent_id,

                    "error": str(e),

                    "acquired_resources": acquired_resources,

                    "execution_time": execution_time

                }

        

        # Create deadlock scenarios with different resource acquisition orders

        deadlock_scenarios = [

            (["resource_1", "resource_2", "resource_3"], "forward"),

            (["resource_3", "resource_2", "resource_1"], "reverse"),

            (["resource_2", "resource_1", "resource_3"], "mixed1"),

            (["resource_2", "resource_3", "resource_1"], "mixed2")

        ]

        

        tasks = []

        for i, (resource_order, scenario_type) in enumerate(deadlock_scenarios * 3):  # Run multiple times

            task = create_deadlock_scenario(f"agent_{i}_{scenario_type}", resource_order)

            tasks.append(task)

        

        # Execute potential deadlock scenarios

        start_time = time.time()

        results = await asyncio.gather(*tasks, return_exceptions=True)

        total_execution_time = time.time() - start_time

        

        # Analyze results

        successful_operations = [r for r in results if isinstance(r, dict) and r.get("success")]

        deadlock_detected = [r for r in results if isinstance(r, dict) and r.get("error") == "deadlock_detected"]

        other_failures = [r for r in results if isinstance(r, dict) and not r.get("success") and r.get("error") != "deadlock_detected"]

        

        logger.info(f"Deadlock detection test results:")

        logger.info(f"  Successful operations: {len(successful_operations)}")

        logger.info(f"  Deadlocks detected: {len(deadlock_detected)}")

        logger.info(f"  Other failures: {len(other_failures)}")

        logger.info(f"  Total execution time: {total_execution_time:.2f}s")

        

        # Validate deadlock detection

        if len(deadlock_detected) > 0:

            # Deadlocks were detected and handled

            avg_detection_time = statistics.mean([r["execution_time"] for r in deadlock_detected])

            logger.info(f"  Average deadlock detection time: {avg_detection_time:.3f}s")

            

            # Validate detection time is within threshold

            assert avg_detection_time < TEST_CONFIG["deadlock_timeout_ms"] / 1000, (

                f"Deadlock detection time {avg_detection_time}s exceeded threshold"

            )

        

        # Validate system continued operating

        assert len(successful_operations) > 0, "No operations succeeded - system completely deadlocked"

        

        # Clean up resources

        await self._cleanup_test_resources(resource_ids)

    

    @pytest.mark.e2e

    async def test_tool_execution_queue_management_under_load(self):

        """Test Case 7: Test tool execution queue behavior under high concurrent load."""

        logger.info("=== Test Case 7: Tool Execution Queue Management Under Load ===")

        

        queue_capacity = 50

        concurrent_requests = 100

        processing_delay = 0.1  # seconds per request

        

        # Create simple tool execution queue simulator

        execution_queue = asyncio.Queue(maxsize=queue_capacity)

        completed_executions = []

        queue_overflow_count = 0

        

        async def queue_processor():

            """Process tool execution requests from queue."""

            while True:

                try:

                    request = await asyncio.wait_for(execution_queue.get(), timeout=1.0)

                    

                    # Simulate tool execution time

                    await asyncio.sleep(processing_delay)

                    

                    # Mark request as completed

                    request["completed_at"] = datetime.now(timezone.utc)

                    request["success"] = True

                    completed_executions.append(request)

                    

                    execution_queue.task_done()

                    

                except asyncio.TimeoutError:

                    break  # No more requests in queue

                except Exception as e:

                    logger.error(f"Queue processor error: {e}")

                    break

        

        async def submit_tool_request(request_id: str, priority: int = 1) -> Dict[str, Any]:

            """Submit tool execution request to queue."""

            request = {

                "id": request_id,

                "priority": priority,

                "submitted_at": datetime.now(timezone.utc),

                "completed_at": None,

                "success": False

            }

            

            try:

                # Try to add to queue (non-blocking)

                execution_queue.put_nowait(request)

                return {"queued": True, "request_id": request_id}

                

            except asyncio.QueueFull:

                nonlocal queue_overflow_count

                queue_overflow_count += 1

                return {"queued": False, "request_id": request_id, "error": "queue_full"}

        

        # Start queue processor

        processor_task = asyncio.create_task(queue_processor())

        

        # Submit concurrent tool execution requests

        tasks = []

        for i in range(concurrent_requests):

            priority = 1 if i < 20 else 2  # Some high-priority requests

            task = submit_tool_request(f"request_{i}", priority)

            tasks.append(task)

        

        # Submit all requests concurrently

        start_time = time.time()

        submission_results = await asyncio.gather(*tasks)

        

        # Wait for queue processing to complete

        await execution_queue.join()

        processor_task.cancel()

        

        total_processing_time = time.time() - start_time

        

        # Analyze results

        queued_requests = [r for r in submission_results if r.get("queued")]

        overflow_requests = [r for r in submission_results if not r.get("queued")]

        

        logger.info(f"Queue management test results:")

        logger.info(f"  Requests submitted: {concurrent_requests}")

        logger.info(f"  Requests queued: {len(queued_requests)}")

        logger.info(f"  Queue overflows: {len(overflow_requests)}")

        logger.info(f"  Requests completed: {len(completed_executions)}")

        logger.info(f"  Total processing time: {total_processing_time:.2f}s")

        

        # Validate queue management

        assert len(queued_requests) <= queue_capacity, "Queue accepted more requests than capacity"

        assert queue_overflow_count == len(overflow_requests), "Overflow count mismatch"

        assert len(completed_executions) == len(queued_requests), "Not all queued requests were processed"

        

        # Validate processing order (would be priority-based in real implementation)

        completion_times = [r["completed_at"] for r in completed_executions]

        assert len(set(completion_times)) == len(completion_times), "Duplicate completion times detected"

    

    # Helper methods for test data management

    

    async def _create_test_user(self, user_id: str, initial_credits: int) -> None:

        """Create a test user with specified credits."""

        if not self.framework.db_pool:

            return  # Skip if no database connection

            

        async with self.framework.db_pool.acquire() as conn:

            # Create table if it doesn't exist

            await conn.execute("""

                CREATE TABLE IF NOT EXISTS test_users (

                    id TEXT PRIMARY KEY,

                    optimization_credits INTEGER NOT NULL,

                    version INTEGER NOT NULL DEFAULT 1,

                    created_at TIMESTAMP WITH TIME ZONE NOT NULL,

                    updated_at TIMESTAMP WITH TIME ZONE NOT NULL

                )

            """)

            

            await conn.execute("""

                INSERT INTO test_users (id, optimization_credits, version, created_at, updated_at)

                VALUES ($1, $2, 1, $3, $3)

                ON CONFLICT (id) DO UPDATE SET

                optimization_credits = $2, version = 1, updated_at = $3

            """, user_id, initial_credits, datetime.now(timezone.utc))

    

    async def _reset_user_credits(self, user_id: str, credits: int) -> None:

        """Reset user credits to specified amount."""

        if not self.framework.db_pool:

            return  # Skip if no database connection

            

        async with self.framework.db_pool.acquire() as conn:

            await conn.execute("""

                UPDATE test_users 

                SET optimization_credits = $1, version = 1, updated_at = $2

                WHERE id = $3

            """, credits, datetime.now(timezone.utc), user_id)

    

    async def _get_user_credits(self, user_id: str) -> int:

        """Get current user credits."""

        if not self.framework.db_pool:

            return 1000  # Mock value if no database connection

            

        async with self.framework.db_pool.acquire() as conn:

            result = await conn.fetchval(

                "SELECT optimization_credits FROM test_users WHERE id = $1", 

                user_id

            )

            return result or 0

    

    async def _create_test_config(self, config_id: str, config_data: Dict[str, Any], version: int = 1) -> None:

        """Create a test configuration record."""

        if not self.framework.db_pool:

            return  # Skip if no database connection

            

        async with self.framework.db_pool.acquire() as conn:

            # Create table if it doesn't exist

            await conn.execute("""

                CREATE TABLE IF NOT EXISTS test_agent_configurations (

                    id TEXT PRIMARY KEY,

                    configuration JSONB NOT NULL,

                    version INTEGER NOT NULL DEFAULT 1,

                    created_at TIMESTAMP WITH TIME ZONE NOT NULL,

                    updated_at TIMESTAMP WITH TIME ZONE NOT NULL

                )

            """)

            

            await conn.execute("""

                INSERT INTO test_agent_configurations (id, configuration, version, created_at, updated_at)

                VALUES ($1, $2, $3, $4, $4)

                ON CONFLICT (id) DO UPDATE SET

                configuration = $2, version = $3, updated_at = $4

            """, config_id, json.dumps(config_data), version, datetime.now(timezone.utc))

    

    async def _get_config(self, config_id: str) -> Optional[Dict[str, Any]]:

        """Get configuration by ID."""

        if not self.framework.db_pool:

            return {"configuration": {"timeout": 30}, "version": 1}  # Mock config

            

        async with self.framework.db_pool.acquire() as conn:

            row = await conn.fetchrow("""

                SELECT configuration, version FROM test_agent_configurations WHERE id = $1

            """, config_id)

            

            if row:

                return {

                    "configuration": json.loads(row["configuration"]),

                    "version": row["version"]

                }

            return None

    

    async def _create_test_resource(self, resource_id: str, resource_data: Dict[str, Any]) -> None:

        """Create a test resource for deadlock scenarios."""

        if not self.framework.db_pool:

            return  # Skip if no database connection

            

        async with self.framework.db_pool.acquire() as conn:

            # Create table if it doesn't exist

            await conn.execute("""

                CREATE TABLE IF NOT EXISTS test_resources (

                    id TEXT PRIMARY KEY,

                    data JSONB NOT NULL,

                    locked_by TEXT,

                    locked_at TIMESTAMP WITH TIME ZONE,

                    created_at TIMESTAMP WITH TIME ZONE NOT NULL

                )

            """)

            

            await conn.execute("""

                INSERT INTO test_resources (id, data, locked_by, locked_at, created_at)

                VALUES ($1, $2, $3, $4, $5)

                ON CONFLICT (id) DO UPDATE SET

                data = $2, locked_by = $3, locked_at = $4

            """, 

            resource_id, 

            json.dumps(resource_data), 

            resource_data.get("locked_by"),

            resource_data.get("locked_at"),

            datetime.now(timezone.utc))

    

    async def _cleanup_test_resources(self, resource_ids: List[str]) -> None:

        """Clean up test resources."""

        if not self.framework.db_pool:

            return  # Skip if no database connection

            

        async with self.framework.db_pool.acquire() as conn:

            for resource_id in resource_ids:

                await conn.execute("DELETE FROM test_resources WHERE id = $1", resource_id)

