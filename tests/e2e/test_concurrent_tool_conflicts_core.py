"""
Test module split from original file
Generated by auto_fix_test_violations.py
"""

import asyncio
import gc
import hashlib
import json
import logging
import os
import random
import secrets
import statistics
import threading
import time
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union

import asyncpg
import psutil
import pytest
import redis.asyncio as redis

from netra_backend.app.logging_config import central_logger
from tests.e2e.conftest import E2E_CONFIG, E2EEnvironmentValidator


class ConcurrencyTestMetrics:
    """Tracks metrics for concurrent tool execution testing."""
    total_operations: int = 0
    successful_operations: int = 0
    failed_operations: int = 0
    deadlock_count: int = 0
    retry_count: int = 0
    conflict_count: int = 0
    avg_response_time_ms: float = 0.0
    p95_response_time_ms: float = 0.0
    p99_response_time_ms: float = 0.0
    max_response_time_ms: float = 0.0
    transaction_abort_count: int = 0
    resource_exhaustion_count: int = 0
    data_consistency_violations: int = 0
    response_times: List[float] = field(default_factory=list)
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    
    def add_operation(self, success: bool, response_time_ms: float, 
                     deadlock: bool = False, retry: bool = False, 
                     conflict: bool = False) -> None:
        """Record an operation result."""
        self.total_operations += 1
        if success:
            self.successful_operations += 1
        else:
            self.failed_operations += 1
            
        if deadlock:
            self.deadlock_count += 1
        if retry:
            self.retry_count += 1
        if conflict:
            self.conflict_count += 1
            
        self.response_times.append(response_time_ms)
        
        # Update response time statistics
        if self.response_times:
            self.avg_response_time_ms = statistics.mean(self.response_times)
            sorted_times = sorted(self.response_times)
            self.p95_response_time_ms = statistics.quantiles(sorted_times, n=20)[18]  # 95th percentile
            self.p99_response_time_ms = statistics.quantiles(sorted_times, n=100)[98]  # 99th percentile
            self.max_response_time_ms = max(self.response_times)
    
    @property
    def success_rate(self) -> float:
        """Calculate operation success rate."""
        if self.total_operations == 0:
            return 0.0
        return self.successful_operations / self.total_operations
    
    @property
    def conflict_rate(self) -> float:
        """Calculate transaction conflict rate."""
        if self.total_operations == 0:
            return 0.0
        return self.conflict_count / self.total_operations
    
    @property
    def deadlock_rate(self) -> float:
        """Calculate deadlock occurrence rate."""
        if self.total_operations == 0:
            return 0.0
        return self.deadlock_count / self.total_operations

class TestConcurrentToolConflictFramework:
    """Test framework for concurrent tool execution conflict scenarios."""
    
    def __init__(self):
        self.db_pool = None
        self.redis_client = None
        self.test_data = {}
        self.metrics = ConcurrencyTestMetrics()
        self.agent_states = {}
        
    async def setup(self) -> None:
        """Initialize test framework components."""
        logger.info("Setting up concurrent tool conflict test framework")
        
        # Initialize database connection pool
        try:
            self.db_pool = await asyncpg.create_pool(
                E2E_CONFIG["postgres_url"],
                min_size=5,
                max_size=TEST_CONFIG["max_connections"]
            )
        except Exception as e:
            logger.warning(f"Could not connect to PostgreSQL: {e}. Using SQLite for testing.")
            self.db_pool = None
        
        # Initialize Redis client
        try:
            self.redis_client = redis.from_url(E2E_CONFIG["redis_url"])
            await self.redis_client.ping()
        except Exception as e:
            logger.warning(f"Could not connect to Redis: {e}. Skipping Redis tests.")
            self.redis_client = None
        
        # Create test data
        await self._create_test_data()
        
        # Setup transaction isolation configuration
        await self._setup_transaction_isolation()
        
    async def teardown(self) -> None:
        """Clean up test framework resources."""
        logger.info("Tearing down concurrent tool conflict test framework")
        
        if self.redis_client:
            await self.redis_client.close()
            
        if self.db_pool:
            # Clean up database connections
            await self.db_pool.close()
            
    async def _create_test_data(self) -> None:
        """Create test data for conflict scenarios."""
        logger.info("Creating test data for concurrent conflict scenarios")
        
        # Create test users with optimization credits
        self.test_data["users"] = []
        for i in range(TEST_CONFIG["test_data_size"]):
            user_data = {
                "id": f"test_user_{i}",
                "optimization_credits": 1000,
                "version": 1,
                "created_at": datetime.now(timezone.utc)
            }
            self.test_data["users"].append(user_data)
            
        # Create test agent configurations
        self.test_data["agent_configs"] = []
        for i in range(100):
            config_data = {
                "id": f"agent_config_{i}",
                "configuration": {"timeout": 30, "max_retries": 3},
                "version": 1,
                "last_modified": datetime.now(timezone.utc)
            }
            self.test_data["agent_configs"].append(config_data)
            
        # Create shared resource pool
        self.test_data["resource_pool"] = {
            "max_connections": TEST_CONFIG["max_connections"],
            "current_connections": 0,
            "waiting_queue": []
        }
        
    async def _setup_transaction_isolation(self) -> None:
        """Configure database transaction isolation levels for testing."""
        logger.info("Setting up transaction isolation configuration")
        
        # Test different isolation levels
        isolation_levels = ["READ_COMMITTED", "REPEATABLE_READ", "SERIALIZABLE"]
        self.test_data["isolation_levels"] = isolation_levels
        
    async def _cleanup_database_connections(self) -> None:
        """Clean up database connections and reset state."""
        logger.info("Cleaning up database connections")
        
        if self.db_pool:
            try:
                # Clean up test tables if they exist
                async with self.db_pool.acquire() as conn:
                    await conn.execute("DROP TABLE IF EXISTS test_users CASCADE")
                    await conn.execute("DROP TABLE IF EXISTS test_agent_configurations CASCADE") 
                    await conn.execute("DROP TABLE IF EXISTS test_resources CASCADE")
            except Exception as e:
                logger.warning(f"Error during database cleanup: {e}")

class ConcurrentCreditDeductionTool:
    """Tool that deducts optimization credits with transaction isolation."""
    
    name = "concurrent_credit_deduction"
    description = "Deducts optimization credits from user account with conflict handling"
    
    def __init__(self, db_pool, test_metrics: ConcurrencyTestMetrics):
        self.db_pool = db_pool
        self.test_metrics = test_metrics
        
    async def _execute(self, user_id: str, credits_to_deduct: int, 
                      isolation_level: str = "READ_COMMITTED") -> Dict[str, Any]:
        """Execute credit deduction with specified isolation level."""
        start_time = time.time()
        
        try:
            result = await self._deduct_credits_with_isolation(
                user_id, credits_to_deduct, isolation_level
            )
            
            response_time = (time.time() - start_time) * 1000
            self.test_metrics.add_operation(
                success=True, 
                response_time_ms=response_time,
                conflict=result.get("had_conflict", False),
                retry=result.get("retry_count", 0) > 0
            )
            
            return result
            
        except Exception as e:
            response_time = (time.time() - start_time) * 1000
            is_deadlock = "deadlock" in str(e).lower()
            
            self.test_metrics.add_operation(
                success=False, 
                response_time_ms=response_time,
                deadlock=is_deadlock
            )
            
            raise e
            
    async def _deduct_credits_with_isolation(self, user_id: str, credits_to_deduct: int,
                                           isolation_level: str) -> Dict[str, Any]:
        """Perform credit deduction with optimistic locking and retry logic."""
        max_retries = TEST_CONFIG["transaction_retry_limit"]
        retry_count = 0
        had_conflict = False
        
        if not self.db_pool:
            # Fallback to mock operation if no database
            await asyncio.sleep(random.uniform(0.01, 0.1))
            return {
                "success": True,
                "previous_credits": 1000,
                "new_credits": 1000 - credits_to_deduct,
                "credits_deducted": credits_to_deduct,
                "retry_count": 0,
                "had_conflict": False,
                "mock_operation": True
            }
        
        while retry_count <= max_retries:
            try:
                async with self.db_pool.acquire() as conn:
                    async with conn.transaction(isolation=isolation_level):
                        # Read current user data with version
                        user_query = """
                        SELECT optimization_credits, version 
                        FROM test_users 
                        WHERE id = $1 
                        FOR UPDATE
                        """
                        user_row = await conn.fetchrow(user_query, user_id)
                        
                        if not user_row:
                            raise ValueError(f"User {user_id} not found")
                            
                        current_credits = user_row["optimization_credits"]
                        current_version = user_row["version"]
                        
                        if current_credits < credits_to_deduct:
                            raise ValueError(f"Insufficient credits: {current_credits} < {credits_to_deduct}")
                        
                        # Simulate processing time to increase conflict probability
                        await asyncio.sleep(random.uniform(0.01, 0.05))
                        
                        # Attempt optimistic update
                        new_credits = current_credits - credits_to_deduct
                        new_version = current_version + 1
                        
                        update_query = """
                        UPDATE test_users 
                        SET optimization_credits = $1, version = $2, updated_at = $3
                        WHERE id = $4 AND version = $5
                        """
                        
                        result = await conn.execute(
                            update_query, 
                            new_credits, 
                            new_version, 
                            datetime.now(timezone.utc),
                            user_id, 
                            current_version
                        )
                        
                        if result == "UPDATE 0":
                            # Version conflict detected
                            had_conflict = True
                            retry_count += 1
                            if retry_count <= max_retries:
                                # Exponential backoff before retry
                                await asyncio.sleep(random.uniform(0.01, 0.1) * (2 ** retry_count))
                                continue
                            else:
                                raise Exception("Version conflict: Maximum retries exceeded")
                        
                        # Success
                        return {
                            "success": True,
                            "previous_credits": current_credits,
                            "new_credits": new_credits,
                            "credits_deducted": credits_to_deduct,
                            "retry_count": retry_count,
                            "had_conflict": had_conflict
                        }
                        
            except asyncpg.exceptions.SerializationError:
                # Serialization failure - retry
                had_conflict = True
                retry_count += 1
                if retry_count <= max_retries:
                    await asyncio.sleep(random.uniform(0.01, 0.1) * (2 ** retry_count))
                    continue
                else:
                    raise
                    
            except asyncpg.exceptions.DeadlockDetectedError:
                # Deadlock detected - retry after brief delay
                retry_count += 1
                if retry_count <= max_retries:
                    await asyncio.sleep(random.uniform(0.001, 0.01))
                    continue
                else:
                    raise
        
        raise Exception("Transaction failed after maximum retries")

@pytest.mark.e2e
class TestConcurrentToolExecutionConflicts:
    """Test suite for concurrent tool execution conflicts."""
    
    @pytest.fixture(autouse=True)
    async def setup_test_framework(self):
        """Setup test framework for each test."""
        self.framework = ConcurrentToolConflictTestFramework()
        await self.framework.setup()
        yield
        await self.framework.teardown()
    
    @pytest.mark.e2e
    async def test_database_record_modification_conflicts(self):
        """Test Case 1: Validate transaction isolation when multiple agents modify the same database record."""
        logger.info("=== Test Case 1: Database Record Modification Conflicts ===")
        
        test_user_id = "test_user_conflicts_1"
        initial_credits = 1000
        concurrent_agents = 10
        credits_per_deduction = 50
        
        # Create test user
        await self._create_test_user(test_user_id, initial_credits)
        
        # Create concurrent credit deduction tool
        credit_tool = ConcurrentCreditDeductionTool(
            self.framework.db_pool, 
            self.framework.metrics
        )
        
        # Test different isolation levels
        isolation_levels = ["READ_COMMITTED", "REPEATABLE_READ", "SERIALIZABLE"]
        
        for isolation_level in isolation_levels:
            logger.info(f"Testing isolation level: {isolation_level}")
            
            # Reset user credits for each isolation level test
            await self._reset_user_credits(test_user_id, initial_credits)
            
            # Create concurrent tasks
            tasks = []
            for i in range(concurrent_agents):
                task = credit_tool._execute(
                    user_id=test_user_id,
                    credits_to_deduct=credits_per_deduction,
                    isolation_level=isolation_level
                )
                tasks.append(task)
            
            # Execute all tasks concurrently
            start_time = time.time()
            results = await asyncio.gather(*tasks, return_exceptions=True)
            execution_time = time.time() - start_time
            
            # Analyze results
            successful_deductions = [r for r in results if isinstance(r, dict) and r.get("success")]
            failed_operations = [r for r in results if isinstance(r, Exception)]
            
            logger.info(f"Isolation level {isolation_level} results:")
            logger.info(f"  Successful operations: {len(successful_deductions)}")
            logger.info(f"  Failed operations: {len(failed_operations)}")
            logger.info(f"  Execution time: {execution_time:.2f}s")
            
            # Validate final state consistency
            final_credits = await self._get_user_credits(test_user_id)
            expected_credits = initial_credits - (len(successful_deductions) * credits_per_deduction)
            
            assert final_credits == expected_credits, (
                f"Credit calculation mismatch: expected {expected_credits}, got {final_credits}"
            )
            
            # Performance assertions
            assert execution_time < TEST_CONFIG["performance_threshold_ms"] / 1000, (
                f"Execution time {execution_time}s exceeded threshold"
            )
            
            # Conflict rate should be reasonable for higher isolation levels
            if isolation_level == "SERIALIZABLE":
                assert self.framework.metrics.conflict_rate <= TEST_CONFIG["conflict_rate_threshold"], (
                    f"Conflict rate {self.framework.metrics.conflict_rate} exceeded threshold"
                )
    
    @pytest.mark.e2e
    async def test_agent_tool_resource_pool_exhaustion(self):
        """Test Case 2: Test behavior when concurrent tool executions exhaust shared resource pools."""
        logger.info("=== Test Case 2: Agent Tool Resource Pool Exhaustion ===")
        
        # Configure limited connection pool
        max_connections = 5
        concurrent_agents = 15
        query_duration = 2.0  # seconds
        
        # Create resource pool exhaustion test
        connection_requests = []
        acquisition_times = []
        
        async def long_running_query(agent_id: str) -> Dict[str, Any]:
            """Simulate long-running database query."""
            start_time = time.time()
            
            try:
                if not self.framework.db_pool:
                    # Fallback simulation if no database
                    acquisition_time = time.time() - start_time
                    acquisition_times.append(acquisition_time)
                    await asyncio.sleep(query_duration)
                    return {
                        "success": True,
                        "agent_id": agent_id,
                        "acquisition_time": acquisition_time,
                        "result": 1,
                        "mock_operation": True
                    }
                
                async with self.framework.db_pool.acquire() as conn:
                    acquisition_time = time.time() - start_time
                    acquisition_times.append(acquisition_time)
                    
                    # Simulate analytical query
                    await asyncio.sleep(query_duration)
                    
                    # Simple query to validate connection
                    result = await conn.fetchval("SELECT 1")
                    
                    return {
                        "success": True,
                        "agent_id": agent_id,
                        "acquisition_time": acquisition_time,
                        "result": result
                    }
                    
            except Exception as e:
                acquisition_time = time.time() - start_time
                acquisition_times.append(acquisition_time)
                return {
                    "success": False,
                    "agent_id": agent_id,
                    "acquisition_time": acquisition_time,
                    "error": str(e)
                }
        
        # Execute concurrent long-running queries
        tasks = [long_running_query(f"agent_{i}") for i in range(concurrent_agents)]
        
        start_time = time.time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_execution_time = time.time() - start_time
        
        # Analyze results
        successful_queries = [r for r in results if isinstance(r, dict) and r.get("success")]
        failed_queries = [r for r in results if isinstance(r, dict) and not r.get("success")]
        
        logger.info(f"Resource pool exhaustion test results:")
        logger.info(f"  Successful queries: {len(successful_queries)}")
        logger.info(f"  Failed queries: {len(failed_queries)}")
        logger.info(f"  Total execution time: {total_execution_time:.2f}s")
        logger.info(f"  Average acquisition time: {statistics.mean(acquisition_times):.3f}s")
        
        # Validate graceful degradation
        assert len(successful_queries) > 0, "No queries succeeded - system completely failed"
        
        # Validate that system handled pool exhaustion gracefully
        # Some queries should succeed even with limited pool
        success_rate = len(successful_queries) / len(results)
        assert success_rate >= 0.5, f"Success rate {success_rate} too low - poor pool management"
        
        # Validate no connection leaks
        # This would be monitored through pool metrics in real implementation
    
    @pytest.mark.e2e
    async def test_optimistic_locking_version_conflicts(self):
        """Test Case 3: Validate optimistic concurrency control using version-based locking."""
        logger.info("=== Test Case 3: Optimistic Locking Version Conflicts ===")
        
        config_id = "test_config_optimistic"
        concurrent_agents = 15
        
        # Create test configuration with version field
        await self._create_test_config(config_id, {"timeout": 30, "retries": 3}, version=1)
        
        async def update_config_field(agent_id: str, field_name: str, new_value: Any) -> Dict[str, Any]:
            """Update a specific configuration field with optimistic locking."""
            max_retries = 5
            retry_count = 0
            
            while retry_count <= max_retries:
                try:
                    async with self.framework.db_manager.get_connection() as conn:
                        async with conn.transaction():
                            # Read current configuration and version
                            config_query = """
                            SELECT configuration, version 
                            FROM agent_configurations 
                            WHERE id = $1
                            """
                            config_row = await conn.fetchrow(config_query, config_id)
                            
                            if not config_row:
                                raise ValueError(f"Configuration {config_id} not found")
                            
                            current_config = config_row["configuration"]
                            current_version = config_row["version"]
                            
                            # Modify configuration
                            new_config = current_config.copy()
                            new_config[field_name] = new_value
                            
                            # Simulate processing time
                            await asyncio.sleep(random.uniform(0.01, 0.1))
                            
                            # Attempt optimistic update
                            update_query = """
                            UPDATE agent_configurations 
                            SET configuration = $1, version = $2, updated_at = $3
                            WHERE id = $4 AND version = $5
                            """
                            
                            result = await conn.execute(
                                update_query,
                                json.dumps(new_config),
                                current_version + 1,
                                datetime.now(timezone.utc),
                                config_id,
                                current_version
                            )
                            
                            if result == "UPDATE 0":
                                # Version conflict - retry with exponential backoff
                                retry_count += 1
                                await asyncio.sleep(random.uniform(0.01, 0.1) * (2 ** retry_count))
                                continue
                            
                            return {
                                "success": True,
                                "agent_id": agent_id,
                                "field_updated": field_name,
                                "new_value": new_value,
                                "retry_count": retry_count,
                                "final_version": current_version + 1
                            }
                            
                except Exception as e:
                    retry_count += 1
                    if retry_count <= max_retries:
                        await asyncio.sleep(random.uniform(0.01, 0.1) * (2 ** retry_count))
                        continue
                    else:
                        return {
                            "success": False,
                            "agent_id": agent_id,
                            "error": str(e),
                            "retry_count": retry_count
                        }
            
            return {
                "success": False,
                "agent_id": agent_id,
                "error": "Maximum retries exceeded",
                "retry_count": retry_count
            }
        
        # Create concurrent update tasks for different fields
        tasks = []
        field_updates = [
            ("timeout", random.randint(10, 60)),
            ("retries", random.randint(1, 10)),
            ("batch_size", random.randint(10, 100)),
            ("cache_ttl", random.randint(60, 3600))
        ]
        
        for i in range(concurrent_agents):
            field_name, new_value = random.choice(field_updates)
            task = update_config_field(f"agent_{i}", field_name, new_value)
            tasks.append(task)
        
        # Execute concurrent updates
        start_time = time.time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        execution_time = time.time() - start_time
        
        # Analyze results
        successful_updates = [r for r in results if isinstance(r, dict) and r.get("success")]
        failed_updates = [r for r in results if isinstance(r, dict) and not r.get("success")]
        
        logger.info(f"Optimistic locking test results:")
        logger.info(f"  Successful updates: {len(successful_updates)}")
        logger.info(f"  Failed updates: {len(failed_updates)}")
        logger.info(f"  Execution time: {execution_time:.2f}s")
        
        # Validate that at least some updates succeeded
        assert len(successful_updates) > 0, "No updates succeeded"
        
        # Validate final configuration integrity
        final_config = await self._get_config(config_id)
        assert final_config is not None, "Configuration was corrupted"
        
        # Validate version progression
        assert final_config["version"] > 1, "Version was not incremented"
    
    @pytest.mark.e2e
    async def test_deadlock_detection_and_recovery_cascade(self):
        """Test Case 5: Test system behavior during complex deadlock scenarios."""
        logger.info("=== Test Case 5: Deadlock Detection and Recovery Cascade ===")
        
        # Create multiple resources for deadlock scenario
        resource_ids = ["resource_1", "resource_2", "resource_3"]
        
        for resource_id in resource_ids:
            await self._create_test_resource(resource_id, {"value": 100, "locked_by": None})
        
        async def create_deadlock_scenario(agent_id: str, resource_order: List[str]) -> Dict[str, Any]:
            """Create intentional deadlock by acquiring resources in different orders."""
            acquired_resources = []
            start_time = time.time()
            
            try:
                async with self.framework.db_manager.get_connection() as conn:
                    async with conn.transaction():
                        for resource_id in resource_order:
                            # Lock resource
                            lock_query = """
                            UPDATE test_resources 
                            SET locked_by = $1, locked_at = $2
                            WHERE id = $3 AND locked_by IS NULL
                            """
                            
                            result = await conn.execute(
                                lock_query,
                                agent_id,
                                datetime.now(timezone.utc),
                                resource_id
                            )
                            
                            if result == "UPDATE 0":
                                # Resource already locked - potential deadlock
                                await asyncio.sleep(random.uniform(0.1, 0.5))  # Wait and retry
                                
                            acquired_resources.append(resource_id)
                            
                            # Simulate work on resource
                            await asyncio.sleep(random.uniform(0.05, 0.2))
                        
                        # Work completed successfully
                        execution_time = time.time() - start_time
                        return {
                            "success": True,
                            "agent_id": agent_id,
                            "acquired_resources": acquired_resources,
                            "execution_time": execution_time
                        }
                        
            except asyncpg.exceptions.DeadlockDetectedError as e:
                execution_time = time.time() - start_time
                return {
                    "success": False,
                    "agent_id": agent_id,
                    "error": "deadlock_detected",
                    "acquired_resources": acquired_resources,
                    "execution_time": execution_time,
                    "deadlock_details": str(e)
                }
                
            except Exception as e:
                execution_time = time.time() - start_time
                return {
                    "success": False,
                    "agent_id": agent_id,
                    "error": str(e),
                    "acquired_resources": acquired_resources,
                    "execution_time": execution_time
                }
        
        # Create deadlock scenarios with different resource acquisition orders
        deadlock_scenarios = [
            (["resource_1", "resource_2", "resource_3"], "forward"),
            (["resource_3", "resource_2", "resource_1"], "reverse"),
            (["resource_2", "resource_1", "resource_3"], "mixed1"),
            (["resource_2", "resource_3", "resource_1"], "mixed2")
        ]
        
        tasks = []
        for i, (resource_order, scenario_type) in enumerate(deadlock_scenarios * 3):  # Run multiple times
            task = create_deadlock_scenario(f"agent_{i}_{scenario_type}", resource_order)
            tasks.append(task)
        
        # Execute potential deadlock scenarios
        start_time = time.time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_execution_time = time.time() - start_time
        
        # Analyze results
        successful_operations = [r for r in results if isinstance(r, dict) and r.get("success")]
        deadlock_detected = [r for r in results if isinstance(r, dict) and r.get("error") == "deadlock_detected"]
        other_failures = [r for r in results if isinstance(r, dict) and not r.get("success") and r.get("error") != "deadlock_detected"]
        
        logger.info(f"Deadlock detection test results:")
        logger.info(f"  Successful operations: {len(successful_operations)}")
        logger.info(f"  Deadlocks detected: {len(deadlock_detected)}")
        logger.info(f"  Other failures: {len(other_failures)}")
        logger.info(f"  Total execution time: {total_execution_time:.2f}s")
        
        # Validate deadlock detection
        if len(deadlock_detected) > 0:
            # Deadlocks were detected and handled
            avg_detection_time = statistics.mean([r["execution_time"] for r in deadlock_detected])
            logger.info(f"  Average deadlock detection time: {avg_detection_time:.3f}s")
            
            # Validate detection time is within threshold
            assert avg_detection_time < TEST_CONFIG["deadlock_timeout_ms"] / 1000, (
                f"Deadlock detection time {avg_detection_time}s exceeded threshold"
            )
        
        # Validate system continued operating
        assert len(successful_operations) > 0, "No operations succeeded - system completely deadlocked"
        
        # Clean up resources
        await self._cleanup_test_resources(resource_ids)
    
    @pytest.mark.e2e
    async def test_tool_execution_queue_management_under_load(self):
        """Test Case 7: Test tool execution queue behavior under high concurrent load."""
        logger.info("=== Test Case 7: Tool Execution Queue Management Under Load ===")
        
        queue_capacity = 50
        concurrent_requests = 100
        processing_delay = 0.1  # seconds per request
        
        # Create simple tool execution queue simulator
        execution_queue = asyncio.Queue(maxsize=queue_capacity)
        completed_executions = []
        queue_overflow_count = 0
        
        async def queue_processor():
            """Process tool execution requests from queue."""
            while True:
                try:
                    request = await asyncio.wait_for(execution_queue.get(), timeout=1.0)
                    
                    # Simulate tool execution time
                    await asyncio.sleep(processing_delay)
                    
                    # Mark request as completed
                    request["completed_at"] = datetime.now(timezone.utc)
                    request["success"] = True
                    completed_executions.append(request)
                    
                    execution_queue.task_done()
                    
                except asyncio.TimeoutError:
                    break  # No more requests in queue
                except Exception as e:
                    logger.error(f"Queue processor error: {e}")
                    break
        
        async def submit_tool_request(request_id: str, priority: int = 1) -> Dict[str, Any]:
            """Submit tool execution request to queue."""
            request = {
                "id": request_id,
                "priority": priority,
                "submitted_at": datetime.now(timezone.utc),
                "completed_at": None,
                "success": False
            }
            
            try:
                # Try to add to queue (non-blocking)
                execution_queue.put_nowait(request)
                return {"queued": True, "request_id": request_id}
                
            except asyncio.QueueFull:
                nonlocal queue_overflow_count
                queue_overflow_count += 1
                return {"queued": False, "request_id": request_id, "error": "queue_full"}
        
        # Start queue processor
        processor_task = asyncio.create_task(queue_processor())
        
        # Submit concurrent tool execution requests
        tasks = []
        for i in range(concurrent_requests):
            priority = 1 if i < 20 else 2  # Some high-priority requests
            task = submit_tool_request(f"request_{i}", priority)
            tasks.append(task)
        
        # Submit all requests concurrently
        start_time = time.time()
        submission_results = await asyncio.gather(*tasks)
        
        # Wait for queue processing to complete
        await execution_queue.join()
        processor_task.cancel()
        
        total_processing_time = time.time() - start_time
        
        # Analyze results
        queued_requests = [r for r in submission_results if r.get("queued")]
        overflow_requests = [r for r in submission_results if not r.get("queued")]
        
        logger.info(f"Queue management test results:")
        logger.info(f"  Requests submitted: {concurrent_requests}")
        logger.info(f"  Requests queued: {len(queued_requests)}")
        logger.info(f"  Queue overflows: {len(overflow_requests)}")
        logger.info(f"  Requests completed: {len(completed_executions)}")
        logger.info(f"  Total processing time: {total_processing_time:.2f}s")
        
        # Validate queue management
        assert len(queued_requests) <= queue_capacity, "Queue accepted more requests than capacity"
        assert queue_overflow_count == len(overflow_requests), "Overflow count mismatch"
        assert len(completed_executions) == len(queued_requests), "Not all queued requests were processed"
        
        # Validate processing order (would be priority-based in real implementation)
        completion_times = [r["completed_at"] for r in completed_executions]
        assert len(set(completion_times)) == len(completion_times), "Duplicate completion times detected"
    
    # Helper methods for test data management
    
    async def _create_test_user(self, user_id: str, initial_credits: int) -> None:
        """Create a test user with specified credits."""
        if not self.framework.db_pool:
            return  # Skip if no database connection
            
        async with self.framework.db_pool.acquire() as conn:
            # Create table if it doesn't exist
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS test_users (
                    id TEXT PRIMARY KEY,
                    optimization_credits INTEGER NOT NULL,
                    version INTEGER NOT NULL DEFAULT 1,
                    created_at TIMESTAMP WITH TIME ZONE NOT NULL,
                    updated_at TIMESTAMP WITH TIME ZONE NOT NULL
                )
            """)
            
            await conn.execute("""
                INSERT INTO test_users (id, optimization_credits, version, created_at, updated_at)
                VALUES ($1, $2, 1, $3, $3)
                ON CONFLICT (id) DO UPDATE SET
                optimization_credits = $2, version = 1, updated_at = $3
            """, user_id, initial_credits, datetime.now(timezone.utc))
    
    async def _reset_user_credits(self, user_id: str, credits: int) -> None:
        """Reset user credits to specified amount."""
        if not self.framework.db_pool:
            return  # Skip if no database connection
            
        async with self.framework.db_pool.acquire() as conn:
            await conn.execute("""
                UPDATE test_users 
                SET optimization_credits = $1, version = 1, updated_at = $2
                WHERE id = $3
            """, credits, datetime.now(timezone.utc), user_id)
    
    async def _get_user_credits(self, user_id: str) -> int:
        """Get current user credits."""
        if not self.framework.db_pool:
            return 1000  # Mock value if no database connection
            
        async with self.framework.db_pool.acquire() as conn:
            result = await conn.fetchval(
                "SELECT optimization_credits FROM test_users WHERE id = $1", 
                user_id
            )
            return result or 0
    
    async def _create_test_config(self, config_id: str, config_data: Dict[str, Any], version: int = 1) -> None:
        """Create a test configuration record."""
        if not self.framework.db_pool:
            return  # Skip if no database connection
            
        async with self.framework.db_pool.acquire() as conn:
            # Create table if it doesn't exist
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS test_agent_configurations (
                    id TEXT PRIMARY KEY,
                    configuration JSONB NOT NULL,
                    version INTEGER NOT NULL DEFAULT 1,
                    created_at TIMESTAMP WITH TIME ZONE NOT NULL,
                    updated_at TIMESTAMP WITH TIME ZONE NOT NULL
                )
            """)
            
            await conn.execute("""
                INSERT INTO test_agent_configurations (id, configuration, version, created_at, updated_at)
                VALUES ($1, $2, $3, $4, $4)
                ON CONFLICT (id) DO UPDATE SET
                configuration = $2, version = $3, updated_at = $4
            """, config_id, json.dumps(config_data), version, datetime.now(timezone.utc))
    
    async def _get_config(self, config_id: str) -> Optional[Dict[str, Any]]:
        """Get configuration by ID."""
        if not self.framework.db_pool:
            return {"configuration": {"timeout": 30}, "version": 1}  # Mock config
            
        async with self.framework.db_pool.acquire() as conn:
            row = await conn.fetchrow("""
                SELECT configuration, version FROM test_agent_configurations WHERE id = $1
            """, config_id)
            
            if row:
                return {
                    "configuration": json.loads(row["configuration"]),
                    "version": row["version"]
                }
            return None
    
    async def _create_test_resource(self, resource_id: str, resource_data: Dict[str, Any]) -> None:
        """Create a test resource for deadlock scenarios."""
        if not self.framework.db_pool:
            return  # Skip if no database connection
            
        async with self.framework.db_pool.acquire() as conn:
            # Create table if it doesn't exist
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS test_resources (
                    id TEXT PRIMARY KEY,
                    data JSONB NOT NULL,
                    locked_by TEXT,
                    locked_at TIMESTAMP WITH TIME ZONE,
                    created_at TIMESTAMP WITH TIME ZONE NOT NULL
                )
            """)
            
            await conn.execute("""
                INSERT INTO test_resources (id, data, locked_by, locked_at, created_at)
                VALUES ($1, $2, $3, $4, $5)
                ON CONFLICT (id) DO UPDATE SET
                data = $2, locked_by = $3, locked_at = $4
            """, 
            resource_id, 
            json.dumps(resource_data), 
            resource_data.get("locked_by"),
            resource_data.get("locked_at"),
            datetime.now(timezone.utc))
    
    async def _cleanup_test_resources(self, resource_ids: List[str]) -> None:
        """Clean up test resources."""
        if not self.framework.db_pool:
            return  # Skip if no database connection
            
        async with self.framework.db_pool.acquire() as conn:
            for resource_id in resource_ids:
                await conn.execute("DELETE FROM test_resources WHERE id = $1", resource_id)
