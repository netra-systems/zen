"""Latency Target Validation - Phase 6 Unified System Testing

Performance-critical latency testing for user-perceived responsiveness.
Validates first byte time, WebSocket latency, auth response time, and API P99 latency.

Business Value Justification (BVJ):
1. Segment: All customer tiers (Free â†’ Enterprise conversion)
2. Business Goal: Speed drives user satisfaction and prevents churn
3. Value Impact: Fast response times increase conversion rates by 15-25%
4. Revenue Impact: Latency issues can cause 30%+ user abandonment

LATENCY TARGETS:
- First Byte Time: < 100ms (Time to first response)
- WebSocket Latency: < 50ms (Real-time communication)
- Auth Response Time: < 500ms (Login/token validation)
- API P99 Latency: < 200ms (99th percentile API responses)

ARCHITECTURAL COMPLIANCE:
- File size: <300 lines enforced through modular design
- Function size: <8 lines each (MANDATORY)
- Real latency measurement with high precision
- Performance regression detection
"""

import asyncio
import statistics
import time
from typing import Any, Dict, List, Tuple
from unittest.mock import AsyncMock

import pytest

from tests.e2e.config import TestUser, UnifiedTestConfig

# Import MockWebSocket from the actual location
try:
    from netra_backend.app.tests.services.test_ws_connection_mocks import MockWebSocket
    # Create dummy classes for missing ones
    class WebSocketBuilder:
        def build(self):
            return MockWebSocket()
except ImportError:
    # Fallback if even this doesn't work
    class MockWebSocket:
        def __init__(self, user_id=None):
            self.user_id = user_id
            self.sent_messages = []
    
    class WebSocketBuilder:
        def build(self):
            return MockWebSocket()
from netra_backend.app.logging_config import central_logger
from tests.e2e.jwt_token_helpers import JWTTestHelper

logger = central_logger.get_logger(__name__)


class LatencyMeasurer:
    """High-precision latency measurement for performance validation."""
    
    def __init__(self):
        self.measurements: Dict[str, List[float]] = {}
        self.targets = self._get_targets()
    
    def _get_targets(self) -> Dict[str, float]:
        """Get latency targets in milliseconds."""
        return {
            "first_byte": 100.0, "websocket": 50.0,
            "auth_response": 500.0, "api_p99": 200.0
        }
    
    async def measure_latency(self, op: str, func, *args, **kwargs) -> Tuple[Any, float]:
        """Measure operation latency with microsecond precision."""
        start_ns = time.perf_counter_ns()
        result = await func(*args, **kwargs)
        latency_ms = (time.perf_counter_ns() - start_ns) / 1_000_000
        self._record(op, latency_ms)
        return result, latency_ms
    
    def _record(self, operation: str, latency_ms: float) -> None:
        """Record latency measurement for analysis."""
        if operation not in self.measurements:
            self.measurements[operation] = []
        self.measurements[operation].append(latency_ms)


class LatencyStats:
    """Calculate latency statistics for SLA validation."""
    
    @staticmethod
    def calc_percentiles(measurements: List[float]) -> Dict[str, float]:
        """Calculate P50, P95, P99 percentiles."""
        if not measurements:
            return {"p50": 0.0, "p95": 0.0, "p99": 0.0}
        sorted_vals = sorted(measurements)
        return LatencyStats._build_percentile_dict(sorted_vals)
    
    @staticmethod
    def _build_percentile_dict(sorted_vals: List[float]) -> Dict[str, float]:
        """Build percentile dictionary from sorted values."""
        return {
            "p50": LatencyStats._percentile(sorted_vals, 50),
            "p95": LatencyStats._percentile(sorted_vals, 95),
            "p99": LatencyStats._percentile(sorted_vals, 99),
            "avg": statistics.mean(sorted_vals)
        }
    
    @staticmethod
    def _percentile(sorted_values: List[float], percentile: int) -> float:
        """Calculate specific percentile value."""
        if not sorted_values:
            return 0.0
        idx = (percentile / 100.0) * (len(sorted_values) - 1)
        lower, upper = int(idx), min(int(idx) + 1, len(sorted_values) - 1)
        weight = idx - lower
        return sorted_values[lower] * (1 - weight) + sorted_values[upper] * weight


@pytest.fixture
def latency_measurer():
    """Create latency measurement fixture."""
    return LatencyMeasurer()


@pytest.fixture
@pytest.mark.e2e
def test_config():
    """Create unified test configuration fixture."""
    return UnifiedTestConfig()


@pytest.fixture
def mock_http_client():
    """Create mock HTTP client for API latency tests."""
    # Mock: Generic component isolation for controlled unit testing
    mock_client = AsyncMock()
    # Mock: Async component isolation for testing without real async operations
    mock_client.get = AsyncMock(return_value={"status": 200, "data": "response"})
    # Mock: Async component isolation for testing without real async operations
    mock_client.post = AsyncMock(return_value={"status": 200, "data": "created"})
    return mock_client


@pytest.mark.e2e
class TestFirstByteTime:
    """Test first byte time meets < 100ms target."""
    
    @pytest.mark.e2e
    async def test_first_byte_time(self, latency_measurer, mock_http_client):
        """Test API first byte time meets 100ms target (BVJ: User perception critical)."""
        await self._run_measurements(latency_measurer, mock_http_client)
        self._validate_stats(latency_measurer)
    
    async def _run_measurements(self, measurer, client):
        """Run first byte time measurements."""
        for i in range(10):
            result, _ = await measurer.measure_latency(
                "first_byte", self._sim_first_byte, client
            )
            assert result is not None
    
    def _validate_stats(self, measurer: LatencyMeasurer) -> None:
        """Validate first byte time statistics against targets."""
        stats = LatencyStats.calc_percentiles(measurer.measurements["first_byte"])
        target = measurer.targets["first_byte"]
        assert stats["p95"] < target, f"First byte P95 {stats['p95']:.1f}ms exceeds {target}ms"
        assert stats["avg"] < target * 0.7, f"Average should be well below target"
    
    async def _sim_first_byte(self, http_client) -> Dict[str, Any]:
        """Simulate first byte response with realistic server processing."""
        await asyncio.sleep(0.03 + (0.02 * asyncio.get_event_loop().time() % 0.02))
        return await http_client.get("/api/health")


@pytest.mark.e2e
class TestWebSocketLatency:
    """Test WebSocket real-time communication latency < 50ms."""
    
    @pytest.mark.e2e
    async def test_websocket_message_latency(self, latency_measurer):
        """Test WebSocket message latency meets 50ms target (BVJ: Real-time experience)."""
        websocket = await self._setup_websocket("latency_test")
        await self._run_ws_measurements(latency_measurer, websocket)
        self._validate_ws_stats(latency_measurer)
    
    async def _setup_websocket(self, user_id: str) -> MockWebSocket:
        """Setup WebSocket for latency testing."""
        websocket = WebSocketBuilder().with_user_id(user_id).build()
        await websocket.accept()
        return websocket
    
    async def _run_ws_measurements(self, measurer, websocket):
        """Run WebSocket latency measurements."""
        for i in range(15):
            result, _ = await measurer.measure_latency(
                "websocket", self._sim_ws_exchange, websocket, f"msg_{i}"
            )
            assert result is not None
    
    def _validate_ws_stats(self, measurer: LatencyMeasurer) -> None:
        """Validate WebSocket latency statistics."""
        stats = LatencyStats.calc_percentiles(measurer.measurements["websocket"])
        target = measurer.targets["websocket"]
        assert stats["p95"] < target, f"WebSocket P95 {stats['p95']:.1f}ms exceeds {target}ms"
    
    async def _sim_ws_exchange(self, websocket: MockWebSocket, message: str) -> Dict[str, Any]:
        """Simulate WebSocket message exchange."""
        await asyncio.sleep(0.015 + (0.01 * asyncio.get_event_loop().time() % 0.01))
        await websocket.send_json({"type": "message", "content": message})
        return {"sent": message, "timestamp": time.time()}


@pytest.mark.e2e
class TestAuthResponseTime:
    """Test authentication response time < 500ms."""
    
    @pytest.mark.e2e
    async def test_auth_token_validation_latency(self, latency_measurer, test_config):
        """Test auth token validation meets 500ms target (BVJ: Login experience)."""
        test_user = test_config.users["free"]
        await self._run_auth_measurements(latency_measurer, test_user)
        self._validate_auth_stats(latency_measurer)
    
    async def _run_auth_measurements(self, measurer, user):
        """Run authentication latency measurements."""
        for i in range(8):
            result, _ = await measurer.measure_latency(
                "auth_response", self._sim_auth_validation, user
            )
            assert result["valid"] is True
    
    def _validate_auth_stats(self, measurer: LatencyMeasurer) -> None:
        """Validate auth latency statistics."""
        stats = LatencyStats.calc_percentiles(measurer.measurements["auth_response"])
        target = measurer.targets["auth_response"]
        assert stats["p95"] < target, f"Auth P95 {stats['p95']:.1f}ms exceeds {target}ms"
        assert stats["avg"] < target * 0.6, f"Auth average should be well below target"
    
    async def _sim_auth_validation(self, user: TestUser) -> Dict[str, Any]:
        """Simulate auth token validation with database lookup."""
        await asyncio.sleep(0.08 + (0.04 * asyncio.get_event_loop().time() % 0.05))
        token = create_test_token(user.id)
        return {"valid": True, "user_id": user.id, "token": token}


@pytest.mark.e2e
class TestApiP99Latency:
    """Test API P99 latency meets < 200ms target."""
    
    @pytest.mark.e2e
    async def test_api_p99_latency_target(self, latency_measurer, mock_http_client):
        """Test API P99 latency meets 200ms target (BVJ: Enterprise SLA compliance)."""
        await self._run_api_measurements(latency_measurer, mock_http_client)
        self._validate_api_stats(latency_measurer)
    
    async def _run_api_measurements(self, measurer, client):
        """Run API P99 latency measurements."""
        for i in range(50):  # Need larger sample for P99
            result, _ = await measurer.measure_latency(
                "api_p99", self._sim_api_call, client, f"/api/endpoint_{i % 5}"
            )
            assert result is not None
    
    def _validate_api_stats(self, measurer: LatencyMeasurer) -> None:
        """Validate API P99 latency statistics."""
        stats = LatencyStats.calc_percentiles(measurer.measurements["api_p99"])
        target = measurer.targets["api_p99"]
        assert stats["p99"] < target, f"API P99 {stats['p99']:.1f}ms exceeds {target}ms"
        assert stats["p95"] < target * 0.75, f"P95 should be well below P99 target"
    
    async def _sim_api_call(self, http_client, endpoint: str) -> Dict[str, Any]:
        """Simulate API call with realistic processing time."""
        processing_time = 0.04 + (0.03 * hash(endpoint) % 10 / 10)
        await asyncio.sleep(processing_time)
        return await http_client.get(endpoint)
    
    @pytest.mark.e2e
    async def test_database_query_latency(self, latency_measurer):
        """Test database query latency contributes properly to P99."""
        await self._run_db_measurements(latency_measurer)
        self._validate_db_stats(latency_measurer)
    
    async def _run_db_measurements(self, measurer):
        """Run database query measurements."""
        for i in range(30):
            result, _ = await measurer.measure_latency(
                "db_query", self._sim_database_query, f"SELECT * FROM table_{i % 3}"
            )
            assert result["rows"] > 0
    
    def _validate_db_stats(self, measurer: LatencyMeasurer) -> None:
        """Validate database query statistics."""
        stats = LatencyStats.calc_percentiles(measurer.measurements["db_query"])
        assert stats["p99"] < 150, f"DB query P99 {stats['p99']:.1f}ms impacts API latency"
    
    async def _sim_database_query(self, query: str) -> Dict[str, Any]:
        """Simulate database query with variable execution time."""
        base_time = 0.03
        complexity_factor = len(query) * 0.0001
        await asyncio.sleep(base_time + complexity_factor)
        return {"rows": 100, "query": query, "execution_time_ms": base_time * 1000}


@pytest.mark.performance
@pytest.mark.asyncio
@pytest.mark.e2e
async def test_latency_target_summary(latency_measurer):
    """Summary test validating all latency targets are met."""
    failed_targets = _check_all_targets(latency_measurer)
    assert not failed_targets, f"Failed latency targets: {', '.join(failed_targets)}"


def _check_all_targets(measurer):
    """Check all latency targets and return failures."""
    failed = []
    for operation, target in measurer.targets.items():
        if operation in measurer.measurements:
            failed.extend(_check_single_target(measurer, operation, target))
    return failed


def _check_single_target(measurer, operation, target):
    """Check single target and return failure if exceeded."""
    stats = LatencyStats.calc_percentiles(measurer.measurements[operation])
    metric = stats["p95"] if operation != "api_p99" else stats["p99"]
    return [f"{operation}: {metric:.1f}ms > {target}ms"] if metric > target else []


if __name__ == "__main__":
    # Execute latency validation when run directly
    import subprocess
    subprocess.run([
        "python", "-m", "pytest", __file__,
        "-v", "--tb=short", "-x"
    ])
