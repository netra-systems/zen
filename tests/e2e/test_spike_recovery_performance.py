"""
Test module split from original file
Generated by auto_fix_test_violations.py
"""

import asyncio
import gc
import json
import logging
import os
import random
import statistics
import sys
import threading
import time
import uuid
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import asynccontextmanager
from datetime import datetime, timedelta, timezone
from typing import Any, AsyncGenerator, Dict, List, Optional, Tuple
from unittest.mock import AsyncMock, MagicMock, patch

import aiohttp
import httpx
import psutil
import pytest
import websockets

# Import required classes from test_spike_recovery_core
from tests.e2e.test_spike_recovery_core import SpikeLoadGenerator, SpikeLoadMetrics

async def test_spike_testing_performance_benchmark(load_generator: SpikeLoadGenerator:
                                                 spike_metrics: SpikeLoadMetrics):
    """
    Performance benchmark for spike testing capabilities.
    Establishes baseline metrics for regression testing.
    """
    logger.info("Starting Spike Testing Performance Benchmark")
    
    # Benchmark various spike scenarios
    benchmark_results = {}
    
    # Benchmark 1: Login spike performance
    start_time = time.perf_counter()
    spike_result = await load_generator.generate_thundering_herd_spike()
    benchmark_results['login_spike'] = {
        'duration': time.perf_counter() - start_time,
        'success_rate': spike_result['success_rate'],
        'requests_per_second': spike_result['requests_per_second']
    }
    
    # Benchmark 2: WebSocket avalanche performance
    start_time = time.perf_counter()
    ws_result = await load_generator.generate_websocket_avalanche()
    benchmark_results['websocket_avalanche'] = {
        'duration': time.perf_counter() - start_time,
        'success_rate': ws_result['success_rate'],
        'connections_per_second': ws_result['total_attempts'] / ws_result['avalanche_duration']
    }
    
    # Benchmark 3: Recovery performance
    start_time = time.perf_counter()
    recovery_time = await load_generator.measure_recovery_time()
    benchmark_results['recovery'] = {
        'recovery_duration': recovery_time,
        'measurement_duration': time.perf_counter() - start_time
    }
    
    # Generate comprehensive benchmark report
    final_summary = spike_metrics.get_performance_summary()
    benchmark_results['overall'] = final_summary
    
    logger.info(f"Spike Testing Benchmark Results: {json.dumps(benchmark_results, indent=2)}")
    
    # Validate benchmark expectations
    assert benchmark_results['login_spike']['success_rate'] >= 0.95, \
        "Login spike benchmark below expectations"
    
    assert benchmark_results['websocket_avalanche']['success_rate'] >= 0.90, \
        "WebSocket avalanche benchmark below expectations"
    
    assert benchmark_results['recovery']['recovery_duration'] <= 30.0, \
        "Recovery time benchmark exceeded"
    
    return benchmark_results
