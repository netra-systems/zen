
# PERFORMANCE: Lazy loading for mission critical tests

# PERFORMANCE: Lazy loading for mission critical tests
_lazy_imports = {}

def lazy_import(module_path: str, component: str = None):
    """Lazy import pattern for performance optimization"""
    if module_path not in _lazy_imports:
        try:
            module = __import__(module_path, fromlist=[component] if component else [])
            if component:
                _lazy_imports[module_path] = getattr(module, component)
            else:
                _lazy_imports[module_path] = module
        except ImportError as e:
            print(f"Warning: Failed to lazy load {module_path}: {e}")
            _lazy_imports[module_path] = None
    
    return _lazy_imports[module_path]

_lazy_imports = {}

def lazy_import(module_path: str, component: str = None):
    """Lazy import pattern for performance optimization"""
    if module_path not in _lazy_imports:
        try:
            module = __import__(module_path, fromlist=[component] if component else [])
            if component:
                _lazy_imports[module_path] = getattr(module, component)
            else:
                _lazy_imports[module_path] = module
        except ImportError as e:
            print(f"Warning: Failed to lazy load {module_path}: {e}")
            _lazy_imports[module_path] = None
    
    return _lazy_imports[module_path]

"""
WebSocket Race Conditions Golden Path E2E Tests

Business Value Justification (BVJ):
- Segment: All (Free, Early, Mid, Enterprise)
- Business Goal: Validate complete user chat experience works without race conditions
- Value Impact: Ensures core chat value delivery is reliable, protecting $500K+ ARR
- Strategic/Revenue Impact: Protects primary business value delivery mechanism

ROOT CAUSE ADDRESSED:
- Complete end-to-end validation of race condition fixes in production-like environment
- WebSocket 1011 errors in Cloud Run environments causing user experience failures
- Missing business-critical WebSocket events breaking user chat interactions  
- Service dependency race conditions affecting system reliability

CRITICAL E2E TESTING FOCUS:
1. Complete golden path chat sessions without race conditions
2. Multi-user concurrent chat sessions with proper isolation
3. Real agent workflow timing and WebSocket event delivery
4. Chat session recovery after timing issues
5. Business value delivery validation under stress conditions

FULL SYSTEM REQUIREMENTS (Real Everything):
- Full Docker stack (Backend, Auth, PostgreSQL, Redis)
- Real LLM integration for agent responses
- Real WebSocket connections with authentication
- Real agent execution with tool dispatching
- Real business value delivery validation

This E2E test ensures the complete user chat experience works reliably
in production-like conditions, validating that race condition fixes protect
the core business value delivery mechanism.
"""

import asyncio
import json
import time
import uuid
from datetime import datetime, timezone, timedelta
from typing import Any, Dict, List, Optional, Set, Tuple
from unittest.mock import patch

import pytest
import websockets
from websockets import ConnectionClosedError, InvalidStatusCode

# SSOT imports following CLAUDE.md absolute import requirements
from test_framework.common_imports import *  # PERFORMANCE: Consolidated imports
# CONSOLIDATED: from test_framework.common_imports import *  # PERFORMANCE: Consolidated imports
# CONSOLIDATED: # CONSOLIDATED: from test_framework.ssot.base_test_case import SSotAsyncTestCase
# CONSOLIDATED: # CONSOLIDATED: from test_framework.real_services_test_fixtures import real_services_fixture
# CONSOLIDATED: from test_framework.ssot.e2e_auth_helper import (
#     E2EWebSocketAuthHelper,
#     E2EAuthConfig,
#     AuthenticatedUser,
#     create_authenticated_user_context
# )
# CONSOLIDATED: # CONSOLIDATED: from test_framework.websocket_helpers import WebSocketTestClient
# CONSOLIDATED: # CONSOLIDATED: from test_framework.ssot.websocket_golden_path_helpers import WebSocketGoldenPathValidator
from shared.isolated_environment import get_env
from shared.types.core_types import UserID, ThreadID, ensure_user_id

# Import real agent components for full system testing
from netra_backend.app.agents.supervisor_ssot import SupervisorAgent
from netra_backend.app.agents.supervisor.agent_registry import AgentRegistry
from netra_backend.app.services.agent_websocket_bridge import AgentWebSocketBridge
from netra_backend.app.core.tools.unified_tool_dispatcher import UnifiedToolDispatcher
from netra_backend.app.services.user_execution_context import UserExecutionContext


@pytest.mark.e2e
class TestWebSocketRaceConditionsGoldenPath(SSotAsyncTestCase):

    def create_user_context(self) -> UserExecutionContext:
        """Create isolated user execution context for golden path tests"""
        return UserExecutionContext.from_request(
            user_id="test_user",
            thread_id="test_thread",
            run_id="test_run"
        )

    """
    E2E tests for WebSocket race conditions in complete golden path scenarios.
    
    Tests complete user chat experience with REAL services, REAL LLM,
    and REAL agent execution. NO MOCKS per CLAUDE.md requirements.
    
    CRITICAL: These tests validate that race condition fixes work in
    production-like environment with complete business value delivery.
    """
    
    # Required WebSocket events for complete business value delivery
    CRITICAL_WEBSOCKET_EVENTS = [
        "agent_started",    # User sees AI began processing their problem
        "agent_thinking",   # Real-time reasoning visibility
        "tool_executing",   # Tool usage transparency  
        "tool_completed",   # Tool results display
        "agent_completed"   # User knows when valuable response is ready
    ]
    
    @pytest.fixture(autouse=True)
    async def setup_e2e_environment(self, real_services_fixture):
        """Set up full E2E environment for golden path race condition testing."""
        self.services = real_services_fixture
        
        # Initialize authentication with staging-compatible config
        self.auth_config = E2EAuthConfig(
            auth_service_url="http://localhost:8083",
            backend_url="http://localhost:8002",
            websocket_url="ws://localhost:8002/ws"
        )
        self.auth_helper = E2EWebSocketAuthHelper(
            config=self.auth_config,
            environment="test"
        )
        
        # Initialize golden path validation
        self.golden_path_validator = WebSocketGoldenPathValidator()
        
        # Wait for complete system readiness
        await self._wait_for_full_system_ready()
        
        # Initialize tracking systems
        self.chat_sessions: List[Dict[str, Any]] = []
        self.race_condition_failures: List[Dict[str, Any]] = []
        self.websocket_event_tracking: Dict[str, List[str]] = {}
        self.business_value_metrics: List[Dict[str, Any]] = []
    
    async def _wait_for_full_system_ready(self, max_wait_time: float = 60.0):
        """
        Wait for complete system readiness including all agent services.
        
        This ensures no race conditions in the test setup itself and validates
        that all components required for golden path are operational.
        """
        start_time = time.time()
        
        while time.time() - start_time < max_wait_time:
            try:
                # Check infrastructure services
                postgres_ready = await self.services.is_postgres_ready()
                redis_ready = await self.services.is_redis_ready()
                backend_ready = await self._check_backend_health()
                
                # Check agent system readiness
                agent_registry_ready = await self._check_agent_registry_ready()
                tool_dispatcher_ready = await self._check_tool_dispatcher_ready()
                
                if all([postgres_ready, redis_ready, backend_ready, agent_registry_ready, tool_dispatcher_ready]):
                    return
                
                await asyncio.sleep(1.0)
                
            except Exception as e:
                await asyncio.sleep(1.0)
        
        pytest.fail(f"Full system not ready after {max_wait_time}s wait")
    
    async def _check_backend_health(self) -> bool:
        """Check backend service health for WebSocket connections."""
        try:
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.get("http://localhost:8002/health") as response:
                    return response.status == 200
        except Exception:
            return False
    
    async def _check_agent_registry_ready(self) -> bool:
        """Check if agent registry is ready for golden path execution."""
        try:
            # This would check if supervisor agent and sub-agents are available
            # For now, simulate readiness check
            await asyncio.sleep(0.1)
            return True
        except Exception:
            return False
    
    async def _check_tool_dispatcher_ready(self) -> bool:
        """Check if tool dispatcher is ready for agent execution."""
        try:
            # This would check if tool execution infrastructure is ready
            await asyncio.sleep(0.1) 
            return True
        except Exception:
            return False
    
    def _record_chat_session(self, session_data: Dict[str, Any]):
        """Record chat session for business value analysis."""
        session_data["recorded_at"] = datetime.now(timezone.utc).isoformat()
        self.chat_sessions.append(session_data)
    
    def _record_race_condition_failure(self, failure_type: str, details: Dict[str, Any]):
        """Record race condition failure for analysis."""
        failure = {
            "failure_type": failure_type,
            "details": details,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        self.race_condition_failures.append(failure)
    
    def _track_websocket_event(self, user_id: str, event_type: str):
        """Track WebSocket events for completeness validation."""
        if user_id not in self.websocket_event_tracking:
            self.websocket_event_tracking[user_id] = []
        self.websocket_event_tracking[user_id].append(event_type)
    
    def _record_business_value_metric(self, metric_type: str, value: float, context: Dict[str, Any]):
        """Record business value metrics for ROI validation."""
        metric = {
            "metric_type": metric_type,
            "value": value,
            "context": context,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        self.business_value_metrics.append(metric)
    
    @pytest.mark.asyncio
    async def test_golden_path_chat_session_no_race_conditions(self):
        """
        Test complete golden path user chat session without race conditions.
        
        BVJ: Validates core chat functionality delivers business value reliably
        This is the PRIMARY test for $500K+ ARR protection through chat reliability
        """
        # Create authenticated user for golden path testing
        user_context = await create_authenticated_user_context(
            environment="test",
            websocket_enabled=True,
            permissions=["read", "write", "agent_execute"]
        )
        
        session_start = time.time()
        chat_session_data = {
            "user_id": user_context.user_id,
            "session_type": "golden_path_complete",
            "race_conditions_detected": [],
            "websocket_events_received": [],
            "business_value_delivered": False
        }
        
        try:
            # Phase 1: WebSocket Connection with Race Condition Prevention
            connection_start = time.time()
            websocket_client = WebSocketTestClient(
                url=self.auth_config.websocket_url,
                headers=self.auth_helper.get_websocket_headers_for_context(user_context)
            )
            
            async with websocket_client as ws:
                # Wait for connection ready message
                connection_ready = await asyncio.wait_for(ws.receive_json(), timeout=10.0)
                connection_duration = time.time() - connection_start
                
                # Validate no race conditions in connection
                if "1011" in str(connection_ready) or "error" in str(connection_ready).lower():
                    self._record_race_condition_failure("connection_establishment", {
                        "ready_message": connection_ready,
                        "duration_ms": connection_duration * 1000
                    })
                    pytest.fail(f"Race condition in connection establishment: {connection_ready}")
                
                chat_session_data["connection_duration_ms"] = connection_duration * 1000
                self.assertIn("connection_ready", str(connection_ready).lower())
                
                # Phase 2: Send Business Value Chat Message  
                chat_message = {
                    "type": "chat_message",
                    "content": "Analyze my AI infrastructure costs and provide optimization recommendations",
                    "user_id": user_context.user_id,
                    "thread_id": str(uuid.uuid4()),
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
                
                message_send_start = time.time()
                await ws.send_json(chat_message)
                
                # Phase 3: Collect All Critical WebSocket Events
                events_received = []
                agent_responses = []
                event_collection_timeout = 30.0  # Allow time for complete agent execution
                
                event_start = time.time()
                while time.time() - event_start < event_collection_timeout:
                    try:
                        event = await asyncio.wait_for(ws.receive_json(), timeout=2.0)
                        events_received.append(event)
                        
                        # Track specific event types
                        event_type = event.get("type", "unknown")
                        if event_type in self.CRITICAL_WEBSOCKET_EVENTS:
                            self._track_websocket_event(user_context.user_id, event_type)
                        
                        # Check for agent completion  
                        if event_type == "agent_completed":
                            agent_responses.append(event)
                            break
                        
                        # Check for race condition indicators
                        if "error" in event_type or "1011" in str(event):
                            self._record_race_condition_failure("event_delivery_race_condition", {
                                "event": event,
                                "events_received_count": len(events_received)
                            })
                            
                    except asyncio.TimeoutError:
                        # Check if we've received enough events to consider success
                        if len(events_received) >= len(self.CRITICAL_WEBSOCKET_EVENTS):
                            break
                        continue
                    except Exception as e:
                        if "1011" in str(e) or "connection closed" in str(e).lower():
                            self._record_race_condition_failure("websocket_connection_race_condition", {
                                "error": str(e),
                                "events_before_failure": len(events_received)
                            })
                            pytest.fail(f"WebSocket race condition during event collection: {e}")
                        raise
                
                # Phase 4: Validate Complete Business Value Delivery
                session_duration = time.time() - session_start
                
                # Check all critical WebSocket events were received
                events_by_type = {}
                for event in events_received:
                    event_type = event.get("type", "unknown")
                    if event_type not in events_by_type:
                        events_by_type[event_type] = []
                    events_by_type[event_type].append(event)
                
                chat_session_data["events_received"] = list(events_by_type.keys())
                chat_session_data["total_events"] = len(events_received)
                chat_session_data["session_duration_ms"] = session_duration * 1000
                
                # Critical validation: All 5 WebSocket events must be present
                missing_events = set(self.CRITICAL_WEBSOCKET_EVENTS) - set(events_by_type.keys())
                if missing_events:
                    chat_session_data["missing_events"] = list(missing_events)
                    self._record_race_condition_failure("missing_critical_events", {
                        "missing_events": list(missing_events),
                        "events_received": list(events_by_type.keys())
                    })
                    pytest.fail(f"Missing critical WebSocket events: {missing_events}")
                
                # Validate agent responses contain business value
                if not agent_responses:
                    pytest.fail("No agent responses received - potential race condition prevented completion")
                
                final_response = agent_responses[-1]
                response_content = final_response.get("content", "")
                
                # Business value validation: Response should contain actionable insights
                business_value_indicators = [
                    "cost", "optimization", "recommendation", "analysis", "insight"
                ]
                business_value_present = any(indicator in response_content.lower() 
                                           for indicator in business_value_indicators)
                
                if not business_value_present:
                    pytest.fail(f"Agent response lacks business value indicators: {response_content[:200]}")
                
                chat_session_data["business_value_delivered"] = True
                chat_session_data["response_content_length"] = len(response_content)
                
                # Record successful golden path completion
                self._record_business_value_metric("golden_path_completion", 1.0, {
                    "session_duration_ms": session_duration * 1000,
                    "events_count": len(events_received),
                    "response_length": len(response_content)
                })
                
        except Exception as e:
            session_duration = time.time() - session_start
            chat_session_data["error"] = str(e)
            chat_session_data["session_duration_ms"] = session_duration * 1000
            
            if "1011" in str(e) or "race" in str(e).lower():
                self._record_race_condition_failure("golden_path_race_condition", {
                    "error": str(e),
                    "session_duration_ms": session_duration * 1000
                })
                pytest.fail(f"Race condition prevented golden path completion: {e}")
            raise
        finally:
            self._record_chat_session(chat_session_data)
        
        # Final validations
        self.assertTrue(chat_session_data["business_value_delivered"])
        self.assertEqual(len(chat_session_data["race_conditions_detected"]), 0)
        self.assertGreaterEqual(len(chat_session_data["events_received"]), len(self.CRITICAL_WEBSOCKET_EVENTS))
    
    @pytest.mark.asyncio
    async def test_multi_user_concurrent_chat_sessions(self):
        """
        Test multiple users having concurrent chat sessions without race conditions.
        
        BVJ: Validates system handles concurrent users without cross-contamination
        Critical for multi-tenant business model scalability
        """
        # Create multiple authenticated users
        user_contexts = []
        for i in range(3):  # Test with 3 concurrent users
            context = await create_authenticated_user_context(
                environment="test",
                websocket_enabled=True,
                permissions=["read", "write", "agent_execute"]
            )
            user_contexts.append(context)
        
        # Track concurrent sessions
        concurrent_sessions = {}
        session_start = time.time()
        
        try:
            # Start concurrent chat sessions
            session_tasks = []
            for i, user_context in enumerate(user_contexts):
                task = asyncio.create_task(
                    self._run_concurrent_chat_session(user_context, session_id=i)
                )
                session_tasks.append(task)
                
                # Stagger session starts slightly to test race conditions
                await asyncio.sleep(0.1)
            
            # Wait for all sessions to complete
            session_results = await asyncio.gather(*session_tasks, return_exceptions=True)
            
            concurrent_duration = time.time() - session_start
            
            # Analyze results for race conditions and cross-contamination
            successful_sessions = sum(1 for r in session_results if isinstance(r, dict) and r.get("success", False))
            race_condition_failures = sum(1 for r in session_results 
                                        if isinstance(r, Exception) and ("1011" in str(r) or "race" in str(r).lower()))
            
            # Record concurrent session metrics
            self._record_business_value_metric("concurrent_sessions_success_rate", 
                                             successful_sessions / len(user_contexts), {
                "total_users": len(user_contexts),
                "successful_sessions": successful_sessions, 
                "concurrent_duration_ms": concurrent_duration * 1000
            })
            
            # Validate concurrent session success
            success_rate = successful_sessions / len(user_contexts)
            self.assertGreaterEqual(success_rate, 1.0, 
                                  f"Concurrent sessions success rate too low: {success_rate:.1%}")
            
            # Zero tolerance for race conditions in concurrent sessions
            self.assertEqual(race_condition_failures, 0,
                           f"Race conditions detected in concurrent sessions: {race_condition_failures}")
            
            # Validate user isolation (no cross-contamination)
            for i, result in enumerate(session_results):
                if isinstance(result, dict):
                    expected_user_id = user_contexts[i].user_id
                    actual_user_id = result.get("user_id")
                    self.assertEqual(actual_user_id, expected_user_id,
                                   f"User isolation violated: expected {expected_user_id}, got {actual_user_id}")
            
        except Exception as e:
            concurrent_duration = time.time() - session_start
            if "1011" in str(e):
                self._record_race_condition_failure("concurrent_sessions_race_condition", {
                    "error": str(e),
                    "user_count": len(user_contexts),
                    "duration_ms": concurrent_duration * 1000
                })
                pytest.fail(f"Race condition in concurrent sessions: {e}")
            raise
    
    async def _run_concurrent_chat_session(self, user_context, session_id: int) -> Dict[str, Any]:
        """Run a single concurrent chat session for multi-user testing."""
        session_data = {
            "session_id": session_id,
            "user_id": user_context.user_id,
            "success": False,
            "events_count": 0,
            "race_conditions": []
        }
        
        try:
            websocket_client = WebSocketTestClient(
                url=self.auth_config.websocket_url,
                headers=self.auth_helper.get_websocket_headers_for_context(user_context),
                timeout=15.0
            )
            
            async with websocket_client as ws:
                # Wait for connection
                ready_msg = await asyncio.wait_for(ws.receive_json(), timeout=5.0)
                
                # Send chat message
                chat_msg = {
                    "type": "chat_message",
                    "content": f"User {session_id} optimization request",
                    "user_id": user_context.user_id
                }
                await ws.send_json(chat_msg)
                
                # Collect events with timeout
                events = []
                timeout_start = time.time()
                while time.time() - timeout_start < 10.0:
                    try:
                        event = await asyncio.wait_for(ws.receive_json(), timeout=1.0)
                        events.append(event)
                        
                        if event.get("type") == "agent_completed":
                            break
                            
                    except asyncio.TimeoutError:
                        continue
                
                session_data["events_count"] = len(events)
                session_data["success"] = len(events) >= 3  # Minimum events for success
                
        except Exception as e:
            session_data["error"] = str(e)
            if "1011" in str(e):
                session_data["race_conditions"].append(str(e))
        
        return session_data
    
    @pytest.mark.asyncio
    async def test_chat_session_recovery_after_timing_issues(self):
        """
        Test chat session recovery after WebSocket timing issues.
        
        BVJ: Validates system resilience ensures business continuity
        Critical for user experience when network conditions cause timing issues
        """
        user_context = await create_authenticated_user_context(
            environment="test",
            websocket_enabled=True
        )
        
        recovery_test_data = {
            "user_id": user_context.user_id,
            "recovery_attempts": [],
            "successful_recovery": False,
            "business_value_preserved": False
        }
        
        # Simulate timing issues and recovery
        for attempt in range(3):  # Allow up to 3 recovery attempts
            attempt_start = time.time()
            attempt_data = {
                "attempt_number": attempt + 1,
                "timing_simulation": f"attempt_{attempt}",
                "success": False,
                "race_conditions": []
            }
            
            try:
                # Introduce artificial timing stress
                if attempt == 0:
                    # First attempt: simulate rapid connection
                    connection_delay = 0.001
                elif attempt == 1:
                    # Second attempt: simulate network latency
                    connection_delay = 0.1  
                else:
                    # Third attempt: use normal timing
                    connection_delay = 0.05
                
                await asyncio.sleep(connection_delay)
                
                websocket_client = WebSocketTestClient(
                    url=self.auth_config.websocket_url,
                    headers=self.auth_helper.get_websocket_headers_for_context(user_context),
                    timeout=8.0
                )
                
                async with websocket_client as ws:
                    # Test connection recovery
                    ready_msg = await asyncio.wait_for(ws.receive_json(), timeout=5.0)
                    
                    # Send recovery test message
                    recovery_msg = {
                        "type": "chat_message",
                        "content": f"Recovery test attempt {attempt + 1}",
                        "user_id": user_context.user_id
                    }
                    await ws.send_json(recovery_msg)
                    
                    # Validate recovery success
                    response = await asyncio.wait_for(ws.receive_json(), timeout=5.0)
                    
                    attempt_duration = time.time() - attempt_start
                    attempt_data["duration_ms"] = attempt_duration * 1000
                    attempt_data["success"] = True
                    
                    recovery_test_data["successful_recovery"] = True
                    recovery_test_data["business_value_preserved"] = "response" in str(response).lower()
                    break
                    
            except Exception as e:
                attempt_duration = time.time() - attempt_start
                attempt_data["duration_ms"] = attempt_duration * 1000
                attempt_data["error"] = str(e)
                
                if "1011" in str(e) or "race" in str(e).lower():
                    attempt_data["race_conditions"].append(str(e))
                    self._record_race_condition_failure("recovery_race_condition", {
                        "attempt": attempt + 1,
                        "error": str(e),
                        "timing_simulation": attempt_data["timing_simulation"]
                    })
                
                # Continue to next attempt unless it's the last one
                if attempt == 2:  # Last attempt failed
                    pytest.fail(f"All recovery attempts failed. Final error: {e}")
                    
            finally:
                recovery_test_data["recovery_attempts"].append(attempt_data)
        
        # Validate recovery success
        self.assertTrue(recovery_test_data["successful_recovery"],
                       "Chat session recovery failed after timing issues")
        self.assertTrue(recovery_test_data["business_value_preserved"],
                       "Business value not preserved during recovery")
        
        # Record recovery metrics
        self._record_business_value_metric("session_recovery_success", 1.0, {
            "recovery_attempts": len(recovery_test_data["recovery_attempts"]),
            "successful_recovery": recovery_test_data["successful_recovery"]
        })
    
    def tearDown(self):
        """Clean up and report golden path race condition analysis."""
        super().tearDown()
        
        # Report race condition failures
        if self.race_condition_failures:
            print(f"\n ALERT:  GOLDEN PATH RACE CONDITION FAILURES: {len(self.race_condition_failures)}")
            for failure in self.race_condition_failures:
                print(f"  - {failure['failure_type']}: {failure['details']}")
            
            # CRITICAL: Golden path should have ZERO race conditions
            self.fail(f"Golden path race conditions detected: {len(self.race_condition_failures)} failures")
        
        # Report business value metrics
        if self.business_value_metrics:
            print(f"\n[U+1F4C8] BUSINESS VALUE METRICS:")
            for metric in self.business_value_metrics:
                print(f"  - {metric['metric_type']}: {metric['value']}")
        
        # Report chat session summary
        if self.chat_sessions:
            successful_sessions = sum(1 for s in self.chat_sessions if s.get("business_value_delivered", False))
            print(f"\n[U+1F4AC] CHAT SESSION SUMMARY:")
            print(f"  - Total sessions: {len(self.chat_sessions)}")
            print(f"  - Successful sessions: {successful_sessions}")
            print(f"  - Success rate: {successful_sessions/len(self.chat_sessions):.1%}")
        
        # Validate golden path success criteria
        self.assertEqual(len(self.race_condition_failures), 0,
                       "Golden path must have zero race condition failures")
        
        if self.chat_sessions:
            business_value_success_rate = sum(1 for s in self.chat_sessions 
                                            if s.get("business_value_delivered", False)) / len(self.chat_sessions)
            self.assertGreaterEqual(business_value_success_rate, 1.0,
                                  f"Golden path business value success rate must be 100%, got {business_value_success_rate:.1%}")


if __name__ == "__main__":
    unittest.main()
