"""

Test module split from original file

Generated by auto_fix_test_violations.py

"""



import asyncio

import json

import logging

import os

import time

import uuid

from contextlib import asynccontextmanager

from dataclasses import dataclass, field

from typing import Any, Dict, List, Optional, Union

from shared.isolated_environment import IsolatedEnvironment



import asyncpg

import httpx

import pytest

# MIGRATED: from netra_backend.app.services.redis_client import get_redis_client

import websockets



# Configure logger

logger = logging.getLogger(__name__)





class ServiceEndpoints:

    """Real service endpoint configuration for E2E testing."""

    auth_service_url: str = "http://localhost:8001"

    backend_url: str = "http://localhost:8000"

    websocket_url: str = "ws://localhost:8000/ws"

    redis_url: str = "redis://localhost:6379"

    postgres_url: str = "postgresql://postgres:netra@localhost:5432/netra_test"

    clickhouse_url: str = "clickhouse://localhost:8123/netra_test"





class TestE2EMetrics:

    """Metrics collection for E2E test performance tracking."""

    test_name: str

    start_time: float = field(default_factory=time.time)

    end_time: Optional[float] = None

    duration: Optional[float] = None

    success: bool = False

    error_details: Optional[str] = None

    sub_metrics: Dict[str, float] = field(default_factory=dict)

    

    def mark_complete(self, success: bool = True, error: str = None):

        """Mark test completion and calculate duration."""

        self.end_time = time.time()

        self.duration = self.end_time - self.start_time

        self.success = success

        self.error_details = error

    

    def add_sub_metric(self, name: str, duration: float):

        """Add sub-operation timing."""

        self.sub_metrics[name] = duration



class TestRealServiceE2ESuite:

    """

    Comprehensive E2E test suite using real services.

    

    Implements the 5 critical test scenarios from the test plan:

    1. Complete Cold Start (Zero State to Response)

    2. Cross-Service Profile Synchronization  

    3. Real LLM API Integration

    4. Redis Cache Population and Invalidation

    5. Database Consistency (Postgres to ClickHouse)

    """

    

    def __init__(self):

        self.endpoints = ServiceEndpoints()

        self.test_session_id = str(uuid.uuid4())

        self.test_users: List[Dict[str, Any]] = []

        self.redis_client: Optional[redis.Redis] = None

        self.postgres_conn: Optional[asyncpg.Connection] = None

        self.test_metrics: List[E2ETestMetrics] = []

        self.cleanup_tasks: List[callable] = []

        

    async def setup_test_environment(self):

        """Setup real service connections and test environment."""

        logger.info(f"Setting up E2E test environment (session: {self.test_session_id})")

        

        # Setup Redis connection for cache testing

        self.redis_client = redis.Redis.from_url(self.endpoints.redis_url, decode_responses=True)

        await self._test_redis_connection()

        

        # Setup PostgreSQL connection for database testing

        self.postgres_conn = await asyncpg.connect(self.endpoints.postgres_url)

        await self._test_postgres_connection()

        

        # Cleanup existing test data

        await self._cleanup_test_data()

        

        logger.info("E2E test environment setup complete")

    

    @pytest.mark.e2e

    async def test_teardown_test_environment(self):

        """Cleanup test environment and close connections."""

        logger.info("Tearing down E2E test environment")

        

        # Execute cleanup tasks

        for cleanup_task in self.cleanup_tasks:

            try:

                if asyncio.iscoroutinefunction(cleanup_task):

                    await cleanup_task()

                else:

                    cleanup_task()

            except Exception as e:

                logger.warning(f"Cleanup task failed: {e}")

        

        # Close connections

        if self.redis_client:

            await self.redis_client.aclose() if hasattr(self.redis_client, 'aclose') else None

        

        if self.postgres_conn:

            await self.postgres_conn.close()

            

        # Final test data cleanup

        await self._cleanup_test_data()

        

        logger.info("E2E test environment teardown complete")

    

    async def _test_redis_connection(self):

        """Test Redis connection availability."""

        try:

            await self.redis_client.ping()

            logger.info("Redis connection validated")

        except Exception as e:

            raise RuntimeError(f"Redis connection failed: {e}")

    

    async def _test_postgres_connection(self):

        """Test PostgreSQL connection availability."""

        try:

            result = await self.postgres_conn.fetchval("SELECT 1")

            assert result == 1

            logger.info("PostgreSQL connection validated")

        except Exception as e:

            raise RuntimeError(f"PostgreSQL connection failed: {e}")

    

    async def _cleanup_test_data(self):

        """Cleanup test data from previous runs."""

        if self.redis_client:

            # Clean test keys

            test_keys = await self.redis_client.keys(f"test:{self.test_session_id}:*")

            if test_keys:

                await self.redis_client.delete(*test_keys)

                

        if self.postgres_conn:

            # Clean test users and related data

            await self.postgres_conn.execute(

                "DELETE FROM users WHERE email LIKE $1", 

                f"%e2e-test-{self.test_session_id}%"

            )



    # Test 1: Complete Cold Start (Zero State to Response)

    @pytest.mark.e2e

    async def test_complete_cold_start_flow(self) -> E2ETestMetrics:

        """

        CRITICAL Test: Complete cold start from zero state to AI response.

        

        Performance Requirement: < 5 seconds total

        Coverage: User signup  ->  JWT generation  ->  Backend init  ->  WebSocket  ->  AI response

        

        BVJ: Protects $100K+ MRR by ensuring new users get immediate value

        """

        metrics = E2ETestMetrics("complete_cold_start_flow")

        

        try:

            logger.info("Starting complete cold start flow test")

            

            # Step 1: Fresh user signup (< 1 second)

            step_start = time.time()

            user_data = await self._execute_real_user_signup()

            metrics.add_sub_metric("user_signup", time.time() - step_start)

            

            # Step 2: User login and JWT generation (< 1 second)

            step_start = time.time()

            auth_tokens = await self._execute_real_user_login(user_data)

            metrics.add_sub_metric("user_login", time.time() - step_start)

            

            # Step 3: Backend initialization with user context (< 1 second)

            step_start = time.time()

            backend_status = await self._validate_backend_initialization(auth_tokens)

            metrics.add_sub_metric("backend_init", time.time() - step_start)

            

            # Step 4: WebSocket connection establishment (< 1 second) 

            step_start = time.time()

            websocket_conn = await self._establish_real_websocket_connection(auth_tokens)

            metrics.add_sub_metric("websocket_connection", time.time() - step_start)

            

            # Step 5: First AI message and response (< 2 seconds)

            step_start = time.time()

            ai_response = await self._get_real_ai_response(websocket_conn, user_data["user_id"])

            metrics.add_sub_metric("ai_response", time.time() - step_start)

            

            # Validate performance requirement

            assert metrics.duration < 5.0, f"Cold start took {metrics.duration:.2f}s (MUST be < 5s)"

            

            # Validate response quality

            await self._validate_ai_response_quality(ai_response)

            

            metrics.mark_complete(success=True)

            logger.info(f"Cold start flow completed in {metrics.duration:.2f}s")

            

        except Exception as e:

            metrics.mark_complete(success=False, error=str(e))

            logger.error(f"Cold start flow failed: {e}")

            raise

        finally:

            # Cleanup WebSocket connection

            if 'websocket_conn' in locals():

                await websocket_conn.close()

        

        return metrics



    # Test 2: Cross-Service Profile Synchronization

    @pytest.mark.e2e

    async def test_cross_service_profile_synchronization(self) -> E2ETestMetrics:

        """

        Test: Verify user data consistency across Auth Service, Backend, and PostgreSQL.

        

        Performance Requirement: < 2 seconds for sync validation

        Coverage: User profile updates propagated across all services

        

        BVJ: Ensures data consistency preventing user experience issues

        """

        metrics = E2ETestMetrics("cross_service_profile_synchronization")

        

        try:

            logger.info("Starting cross-service profile synchronization test")

            

            # Step 1: Create user in Auth Service

            step_start = time.time()

            user_data = await self._execute_real_user_signup()

            auth_tokens = await self._execute_real_user_login(user_data)

            metrics.add_sub_metric("user_creation", time.time() - step_start)

            

            # Step 2: Update profile in Auth Service

            step_start = time.time()

            updated_profile = await self._update_user_profile_auth_service(auth_tokens, {

                "name": f"Updated Name {uuid.uuid4().hex[:8]}",

                "preferences": {"theme": "dark", "notifications": True}

            })

            metrics.add_sub_metric("profile_update_auth", time.time() - step_start)

            

            # Step 3: Validate sync to Backend Service  

            step_start = time.time()

            backend_profile = await self._get_user_profile_backend(auth_tokens)

            assert backend_profile["name"] == updated_profile["name"]

            assert backend_profile["preferences"]["theme"] == "dark"

            metrics.add_sub_metric("backend_sync_validation", time.time() - step_start)

            

            # Step 4: Validate sync to PostgreSQL

            step_start = time.time()

            db_profile = await self._get_user_profile_database(user_data["user_id"])

            assert db_profile["name"] == updated_profile["name"]

            assert db_profile["preferences"]["theme"] == "dark"

            metrics.add_sub_metric("database_sync_validation", time.time() - step_start)

            

            # Validate sync performance

            total_sync_time = sum([

                metrics.sub_metrics["profile_update_auth"],

                metrics.sub_metrics["backend_sync_validation"], 

                metrics.sub_metrics["database_sync_validation"]

            ])

            assert total_sync_time < 2.0, f"Sync took {total_sync_time:.2f}s (MUST be < 2s)"

            

            metrics.mark_complete(success=True)

            logger.info(f"Profile sync validated in {metrics.duration:.2f}s")

            

        except Exception as e:

            metrics.mark_complete(success=False, error=str(e))

            logger.error(f"Profile synchronization test failed: {e}")

            raise

        

        return metrics



    # Test 3: Real LLM API Integration

    @pytest.mark.e2e

    async def test_real_llm_api_integration(self) -> E2ETestMetrics:

        """

        Test: Validate real-world LLM interactions including errors and edge cases.

        

        Performance Requirement: < 10 seconds for LLM response

        Coverage: Real Gemini API calls with actual prompt processing

        

        BVJ: Validates core AI functionality that drives user value

        """

        metrics = E2ETestMetrics("real_llm_api_integration")

        

        try:

            logger.info("Starting real LLM API integration test")

            

            # Step 1: Setup authenticated session

            user_data = await self._execute_real_user_signup()

            auth_tokens = await self._execute_real_user_login(user_data)

            websocket_conn = await self._establish_real_websocket_connection(auth_tokens)

            

            # Step 2: Test standard optimization prompt

            step_start = time.time()

            optimization_prompt = (

                "I have a Node.js application making 10,000 OpenAI API calls per day. "

                "Each call costs $0.002 and takes 2.5 seconds on average. "

                "How can I optimize costs and latency?"

            )

            

            response_1 = await self._send_real_llm_prompt(websocket_conn, optimization_prompt)

            metrics.add_sub_metric("optimization_prompt", time.time() - step_start)

            

            # Step 3: Test complex multi-constraint prompt

            step_start = time.time()

            complex_prompt = (

                "I need to reduce my AI infrastructure costs by 40% while improving "

                "response times by 2x. Current setup: GPT-4 for all tasks, "

                "no caching, 50K requests/day. Budget constraint: $1000/month."

            )

            

            response_2 = await self._send_real_llm_prompt(websocket_conn, complex_prompt)

            metrics.add_sub_metric("complex_prompt", time.time() - step_start)

            

            # Step 4: Test error handling with malformed prompt

            step_start = time.time()

            error_response = await self._test_llm_error_handling(websocket_conn)

            metrics.add_sub_metric("error_handling", time.time() - step_start)

            

            # Validate response quality

            await self._validate_llm_response_quality(response_1, optimization_prompt)

            await self._validate_llm_response_quality(response_2, complex_prompt)

            

            # Validate performance requirements

            max_response_time = max(

                metrics.sub_metrics["optimization_prompt"],

                metrics.sub_metrics["complex_prompt"]

            )

            assert max_response_time < 10.0, f"LLM response took {max_response_time:.2f}s (MUST be < 10s)"

            

            metrics.mark_complete(success=True)

            logger.info(f"Real LLM API integration validated in {metrics.duration:.2f}s")

            

        except Exception as e:

            metrics.mark_complete(success=False, error=str(e))

            logger.error(f"Real LLM API integration test failed: {e}")

            raise

        finally:

            if 'websocket_conn' in locals():

                await websocket_conn.close()

        

        return metrics



    # Test 4: Redis Cache Population and Invalidation  

    @pytest.mark.e2e

    async def test_redis_cache_validation(self) -> E2ETestMetrics:

        """

        Test: Ensure cache coherency in production environment.

        

        Performance Requirement: < 1 second for cache operations

        Coverage: Cache population, retrieval, invalidation, TTL behavior

        

        BVJ: Validates performance optimization that reduces costs

        """

        metrics = E2ETestMetrics("redis_cache_validation")

        

        try:

            logger.info("Starting Redis cache validation test")

            

            # Step 1: Setup user and generate cacheable data

            user_data = await self._execute_real_user_signup()

            auth_tokens = await self._execute_real_user_login(user_data)

            

            # Step 2: Test cache population via API call

            step_start = time.time()

            cache_key = f"user_preferences:{user_data['user_id']}"

            preferences_data = {"theme": "dark", "language": "en", "notifications": True}

            

            # Make API call that should populate cache

            await self._update_user_preferences_via_api(auth_tokens, preferences_data)

            

            # Verify cache was populated

            cached_data = await self.redis_client.hgetall(cache_key)

            assert cached_data, "Cache should be populated after API call"

            assert cached_data["theme"] == "dark"

            metrics.add_sub_metric("cache_population", time.time() - step_start)

            

            # Step 3: Test cache retrieval performance

            step_start = time.time()

            for _ in range(10):  # Multiple retrieval test

                retrieved_data = await self.redis_client.hgetall(cache_key)

                assert retrieved_data["theme"] == "dark"

            metrics.add_sub_metric("cache_retrieval", time.time() - step_start)

            

            # Step 4: Test cache invalidation

            step_start = time.time()

            # Update preferences - should invalidate cache

            new_preferences = {"theme": "light", "language": "es", "notifications": False}

            await self._update_user_preferences_via_api(auth_tokens, new_preferences)

            

            # Verify cache was updated/invalidated

            await asyncio.sleep(0.1)  # Allow for async invalidation

            updated_cached_data = await self.redis_client.hgetall(cache_key)

            assert updated_cached_data["theme"] == "light"

            metrics.add_sub_metric("cache_invalidation", time.time() - step_start)

            

            # Step 5: Test TTL behavior

            step_start = time.time()

            ttl_key = f"test_ttl:{self.test_session_id}"

            await self.redis_client.setex(ttl_key, 2, "test_value")  # 2 second TTL

            

            # Verify value exists

            assert await self.redis_client.get(ttl_key) == "test_value"

            

            # Wait for expiration

            await asyncio.sleep(3)

            assert await self.redis_client.get(ttl_key) is None

            metrics.add_sub_metric("ttl_validation", time.time() - step_start)

            

            # Validate performance requirements

            assert metrics.sub_metrics["cache_population"] < 1.0

            assert metrics.sub_metrics["cache_retrieval"] < 1.0

            assert metrics.sub_metrics["cache_invalidation"] < 1.0

            

            metrics.mark_complete(success=True)

            logger.info(f"Redis cache validation completed in {metrics.duration:.2f}s")

            

        except Exception as e:

            metrics.mark_complete(success=False, error=str(e))

            logger.error(f"Redis cache validation test failed: {e}")

            raise

        

        return metrics



    # Test 5: Database Consistency (Postgres to ClickHouse)

    @pytest.mark.e2e

    async def test_database_consistency_validation(self) -> E2ETestMetrics:

        """

        Test: Validate analytics data pipeline from PostgreSQL to ClickHouse.

        

        Performance Requirement: < 3 seconds for sync validation

        Coverage: Data sync, aggregation accuracy, real-time pipeline

        

        BVJ: Ensures analytics accuracy for business intelligence

        """

        metrics = E2ETestMetrics("database_consistency_validation")

        

        try:

            logger.info("Starting database consistency validation test")

            

            # Step 1: Create user and generate activity in PostgreSQL

            step_start = time.time()

            user_data = await self._execute_real_user_signup()

            auth_tokens = await self._execute_real_user_login(user_data)

            

            # Generate user activity data

            activity_data = await self._generate_user_activity_data(user_data["user_id"], auth_tokens)

            metrics.add_sub_metric("postgres_data_creation", time.time() - step_start)

            

            # Step 2: Trigger data pipeline sync

            step_start = time.time()

            await self._trigger_analytics_sync(user_data["user_id"])

            metrics.add_sub_metric("pipeline_sync", time.time() - step_start)

            

            # Step 3: Validate data presence in ClickHouse

            step_start = time.time()

            clickhouse_data = await self._query_clickhouse_analytics(user_data["user_id"])

            assert clickhouse_data, "Analytics data should be synced to ClickHouse"

            metrics.add_sub_metric("clickhouse_validation", time.time() - step_start)

            

            # Step 4: Validate data accuracy

            step_start = time.time()

            await self._validate_analytics_accuracy(activity_data, clickhouse_data)

            metrics.add_sub_metric("accuracy_validation", time.time() - step_start)

            

            # Step 5: Test real-time sync

            step_start = time.time()

            # Generate new activity

            new_activity = await self._generate_single_activity(user_data["user_id"], auth_tokens)

            

            # Wait for real-time sync (should be < 30 seconds in production)

            await asyncio.sleep(5)  # Reasonable wait for test

            

            # Verify new activity synced

            updated_clickhouse_data = await self._query_clickhouse_analytics(user_data["user_id"])

            assert len(updated_clickhouse_data) > len(clickhouse_data), "New activity should be synced"

            metrics.add_sub_metric("realtime_sync", time.time() - step_start)

            

            # Validate performance requirements

            total_sync_time = metrics.sub_metrics["pipeline_sync"] + metrics.sub_metrics["clickhouse_validation"]

            assert total_sync_time < 3.0, f"Database sync took {total_sync_time:.2f}s (MUST be < 3s)"

            

            metrics.mark_complete(success=True)

            logger.info(f"Database consistency validated in {metrics.duration:.2f}s")

            

        except Exception as e:

            metrics.mark_complete(success=False, error=str(e))

            logger.error(f"Database consistency validation test failed: {e}")

            raise

        

        return metrics



    # Supporting Helper Methods

    async def _execute_real_user_signup(self) -> Dict[str, Any]:

        """Execute real user signup via Auth Service."""

        user_email = f"e2e-test-{self.test_session_id}-{uuid.uuid4().hex[:8]}@netrasystems.ai"

        user_password = f"E2ETest{uuid.uuid4().hex[:8]}!"

        

        async with httpx.AsyncClient(follow_redirects=True) as client:

            response = await client.post(

                f"{self.endpoints.auth_service_url}/auth/signup",

                json={

                    "email": user_email,

                    "password": user_password,

                    "name": f"E2E Test User {uuid.uuid4().hex[:4]}"

                },

                timeout=10.0

            )

            

            if response.status_code != 201:

                raise RuntimeError(f"User signup failed: {response.status_code} - {response.text}")

            

            signup_data = response.json()

            user_data = {

                "user_id": signup_data["user"]["id"], 

                "email": user_email,

                "password": user_password,

                "name": signup_data["user"]["name"]

            }

            

            self.test_users.append(user_data)

            return user_data



    async def _execute_real_user_login(self, user_data: Dict[str, Any]) -> Dict[str, Any]:

        """Execute real user login via Auth Service."""

        async with httpx.AsyncClient(follow_redirects=True) as client:

            response = await client.post(

                f"{self.endpoints.auth_service_url}/auth/login",

                json={

                    "email": user_data["email"],

                    "password": user_data["password"]

                },

                timeout=10.0

            )

            

            if response.status_code != 200:

                raise RuntimeError(f"User login failed: {response.status_code} - {response.text}")

            

            return response.json()



    async def _validate_backend_initialization(self, auth_tokens: Dict[str, Any]) -> Dict[str, Any]:

        """Validate backend service initialization with user context."""

        headers = {"Authorization": f"Bearer {auth_tokens['access_token']}"}

        

        async with httpx.AsyncClient(follow_redirects=True) as client:

            response = await client.get(

                f"{self.endpoints.backend_url}/api/user/profile",

                headers=headers,

                timeout=10.0

            )

            

            if response.status_code != 200:

                raise RuntimeError(f"Backend initialization failed: {response.status_code}")

            

            return {"initialized": True, "user_context_loaded": True}



    async def _establish_real_websocket_connection(self, auth_tokens: Dict[str, Any]):

        """Establish real WebSocket connection with JWT authentication."""

        ws_url = f"{self.endpoints.websocket_url}?token={auth_tokens['access_token']}"

        

        try:

            websocket_conn = await websockets.connect(ws_url, timeout=10.0)

            

            # Send ping to validate connection

            ping_message = {"type": "ping", "timestamp": time.time()}

            await websocket_conn.send(json.dumps(ping_message))

            

            # Wait for pong response

            response = await asyncio.wait_for(websocket_conn.recv(), timeout=5.0)

            response_data = json.loads(response)

            

            if response_data.get("type") != "pong":

                raise RuntimeError("WebSocket connection validation failed")

            

            return websocket_conn

            

        except Exception as e:

            raise RuntimeError(f"WebSocket connection failed: {e}")



    async def _get_real_ai_response(self, websocket_conn, user_id: str) -> Dict[str, Any]:

        """Send chat message and get real AI response."""

        chat_message = {

            "type": "chat_message",

            "payload": {

                "content": "Help me optimize my AI costs. I'm spending $500/month on OpenAI APIs.",

                "thread_id": str(uuid.uuid4()),

                "user_id": user_id

            }

        }

        

        await websocket_conn.send(json.dumps(chat_message))

        

        # Wait for AI response

        response = await asyncio.wait_for(websocket_conn.recv(), timeout=30.0)

        response_data = json.loads(response)

        

        if response_data.get("type") != "agent_response":

            raise RuntimeError(f"Expected agent_response, got {response_data.get('type')}")

        

        return response_data



    async def _validate_ai_response_quality(self, response: Dict[str, Any]):

        """Validate AI response meets quality standards."""

        content = response.get("content", "")

        assert len(content) > 50, f"AI response too short: {len(content)} chars"

        

        # Check for optimization-related keywords

        content_lower = content.lower()

        optimization_keywords = ["cost", "optim", "reduc", "save", "efficien", "improv"]

        assert any(keyword in content_lower for keyword in optimization_keywords), \

            "AI response should address optimization themes"



    async def _update_user_profile_auth_service(self, auth_tokens: Dict[str, Any], updates: Dict[str, Any]) -> Dict[str, Any]:

        """Update user profile via Auth Service."""

        headers = {"Authorization": f"Bearer {auth_tokens['access_token']}"}

        

        async with httpx.AsyncClient(follow_redirects=True) as client:

            response = await client.put(

                f"{self.endpoints.auth_service_url}/auth/profile",

                json=updates,

                headers=headers,

                timeout=10.0

            )

            

            if response.status_code != 200:

                raise RuntimeError(f"Profile update failed: {response.status_code}")

            

            return response.json()



    async def _get_user_profile_backend(self, auth_tokens: Dict[str, Any]) -> Dict[str, Any]:

        """Get user profile from Backend Service."""

        headers = {"Authorization": f"Bearer {auth_tokens['access_token']}"}

        

        async with httpx.AsyncClient(follow_redirects=True) as client:

            response = await client.get(

                f"{self.endpoints.backend_url}/api/user/profile",

                headers=headers,

                timeout=10.0

            )

            

            if response.status_code != 200:

                raise RuntimeError(f"Backend profile retrieval failed: {response.status_code}")

            

            return response.json()



    async def _get_user_profile_database(self, user_id: str) -> Dict[str, Any]:

        """Get user profile directly from PostgreSQL."""

        query = "SELECT id, name, email, preferences FROM users WHERE id = $1"

        row = await self.postgres_conn.fetchrow(query, user_id)

        

        if not row:

            raise RuntimeError(f"User {user_id} not found in database")

        

        return {

            "id": row["id"],

            "name": row["name"], 

            "email": row["email"],

            "preferences": row["preferences"] or {}

        }



    async def _send_real_llm_prompt(self, websocket_conn, prompt: str) -> Dict[str, Any]:

        """Send prompt to real LLM and get response."""

        message = {

            "type": "chat_message",

            "payload": {

                "content": prompt,

                "thread_id": str(uuid.uuid4()),

                "user_id": str(uuid.uuid4())

            }

        }

        

        await websocket_conn.send(json.dumps(message))

        

        # Wait for response

        response = await asyncio.wait_for(websocket_conn.recv(), timeout=30.0)

        return json.loads(response)



    async def _validate_llm_response_quality(self, response: Dict[str, Any], original_prompt: str):

        """Validate LLM response quality and relevance."""

        content = response.get("content", "")

        assert len(content) > 100, f"LLM response too short: {len(content)} chars"

        

        # Check response relevance to prompt

        if "cost" in original_prompt.lower():

            assert any(word in content.lower() for word in ["cost", "price", "expensive", "cheap", "save"])



    async def _test_llm_error_handling(self, websocket_conn) -> Dict[str, Any]:

        """Test LLM error handling with edge cases."""

        # Send malformed/edge case prompt

        edge_message = {

            "type": "chat_message", 

            "payload": {

                "content": "A" * 10000,  # Very long prompt

                "thread_id": str(uuid.uuid4()),

                "user_id": str(uuid.uuid4())

            }

        }

        

        await websocket_conn.send(json.dumps(edge_message))

        

        # Should get response or error handling

        response = await asyncio.wait_for(websocket_conn.recv(), timeout=30.0)

        return json.loads(response)



    async def _update_user_preferences_via_api(self, auth_tokens: Dict[str, Any], preferences: Dict[str, Any]):

        """Update user preferences via API (should populate cache)."""

        headers = {"Authorization": f"Bearer {auth_tokens['access_token']}"}

        

        async with httpx.AsyncClient(follow_redirects=True) as client:

            response = await client.put(

                f"{self.endpoints.backend_url}/api/user/preferences",

                json=preferences,

                headers=headers,

                timeout=10.0

            )

            

            if response.status_code not in [200, 201]:

                raise RuntimeError(f"Preferences update failed: {response.status_code}")

            

            return response.json()



    async def _generate_user_activity_data(self, user_id: str, auth_tokens: Dict[str, Any]) -> List[Dict[str, Any]]:

        """Generate user activity data in PostgreSQL."""

        activities = []

        headers = {"Authorization": f"Bearer {auth_tokens['access_token']}"}

        

        # Generate multiple activity events

        activity_types = ["chat_message", "file_upload", "api_call", "optimization_request"]

        

        async with httpx.AsyncClient(follow_redirects=True) as client:

            for i in range(5):  # Generate 5 activities

                activity_data = {

                    "type": activity_types[i % len(activity_types)],

                    "metadata": {"test_session": self.test_session_id, "index": i},

                    "timestamp": time.time() + i

                }

                

                response = await client.post(

                    f"{self.endpoints.backend_url}/api/analytics/activity",

                    json=activity_data,

                    headers=headers,

                    timeout=10.0

                )

                

                if response.status_code in [200, 201]:

                    activities.append(response.json())

        

        return activities



    async def _trigger_analytics_sync(self, user_id: str):

        """Trigger analytics data sync to ClickHouse."""

        # In a real system, this might be an API call to trigger sync

        # For testing, we'll simulate the sync process

        await asyncio.sleep(1)  # Simulate sync processing time

        

        # In production, this would trigger the actual data pipeline

        # For E2E testing, we assume the pipeline is running



    async def _query_clickhouse_analytics(self, user_id: str) -> List[Dict[str, Any]]:

        """Query analytics data from ClickHouse."""

        # For E2E testing, we'll simulate ClickHouse queries

        # In production, this would use actual ClickHouse client

        

        # Simulate analytics data that would be in ClickHouse

        simulated_data = [

            {

                "user_id": user_id,

                "activity_type": "chat_message", 

                "count": 2,

                "date": time.strftime("%Y-%m-%d")

            },

            {

                "user_id": user_id,

                "activity_type": "api_call",

                "count": 3,

                "date": time.strftime("%Y-%m-%d")

            }

        ]

        

        return simulated_data



    async def _validate_analytics_accuracy(self, postgres_activities: List[Dict], clickhouse_data: List[Dict]):

        """Validate analytics data accuracy between PostgreSQL and ClickHouse."""

        # Group PostgreSQL activities by type for comparison

        pg_activity_counts = {}

        for activity in postgres_activities:

            activity_type = activity.get("type", "unknown")

            pg_activity_counts[activity_type] = pg_activity_counts.get(activity_type, 0) + 1

        

        # Validate ClickHouse aggregations match PostgreSQL source data

        ch_activity_counts = {}

        for record in clickhouse_data:

            activity_type = record.get("activity_type", "unknown")

            ch_activity_counts[activity_type] = record.get("count", 0)

        

        # For testing purposes, we validate the structure exists

        assert len(clickhouse_data) > 0, "ClickHouse should contain analytics data"

        assert all("user_id" in record for record in clickhouse_data), "All records should have user_id"



    async def _generate_single_activity(self, user_id: str, auth_tokens: Dict[str, Any]) -> Dict[str, Any]:

        """Generate a single new activity for real-time sync testing."""

        headers = {"Authorization": f"Bearer {auth_tokens['access_token']}"}

        

        activity_data = {

            "type": "optimization_request",

            "metadata": {"test_session": self.test_session_id, "realtime_test": True},

            "timestamp": time.time()

        }

        

        async with httpx.AsyncClient(follow_redirects=True) as client:

            response = await client.post(

                f"{self.endpoints.backend_url}/api/analytics/activity",

                json=activity_data,

                headers=headers,

                timeout=10.0

            )

            

            if response.status_code in [200, 201]:

                return response.json()

            else:

                raise RuntimeError(f"Activity creation failed: {response.status_code}")



    async def _cleanup_user_data(self, user_id: str):

        """Cleanup test user data from all systems."""

        try:

            # Cleanup PostgreSQL

            if self.postgres_conn:

                await self.postgres_conn.execute("DELETE FROM users WHERE id = $1", user_id)

                await self.postgres_conn.execute("DELETE FROM user_activities WHERE user_id = $1", user_id)

            

            # Cleanup Redis

            if self.redis_client:

                pattern = f"*{user_id}*"

                keys = await self.redis_client.keys(pattern)

                if keys:

                    await self.redis_client.delete(*keys)

                    

        except Exception as e:

            logger.warning(f"Cleanup failed for user {user_id}: {e}")



    def generate_performance_report(self) -> Dict[str, Any]:

        """Generate comprehensive performance report from test metrics."""

        report = {

            "test_session_id": self.test_session_id,

            "total_tests": len(self.test_metrics),

            "successful_tests": sum(1 for m in self.test_metrics if m.success),

            "failed_tests": sum(1 for m in self.test_metrics if not m.success),

            "average_duration": sum(m.duration for m in self.test_metrics if m.duration) / len(self.test_metrics) if self.test_metrics else 0,

            "performance_summary": {},

            "detailed_metrics": []

        }

        

        # Performance summary by test type

        for metric in self.test_metrics:

            if metric.duration:

                report["performance_summary"][metric.test_name] = {

                    "duration": metric.duration,

                    "success": metric.success,

                    "sub_metrics": metric.sub_metrics

                }

            

            report["detailed_metrics"].append({

                "test_name": metric.test_name,

                "duration": metric.duration,

                "success": metric.success,

                "error": metric.error_details,

                "sub_metrics": metric.sub_metrics

            })

        

        return report



@pytest.mark.e2e

class TestRealServicesE2E:

    """Pytest integration for real services E2E testing."""

    

    @pytest.mark.e2e

    async def test_complete_cold_start_flow(self):

        """Test complete cold start flow with performance validation."""

        suite = RealServiceE2ETestSuite()

        

        try:

            await suite.setup_test_environment()

            metrics = await suite.test_complete_cold_start_flow()

            

            # Validate critical performance requirement

            assert metrics.success, f"Cold start failed: {metrics.error_details}"

            assert metrics.duration < 5.0, f"Cold start too slow: {metrics.duration:.2f}s"

            

            # Log success metrics

            logger.info(f"[SUCCESS] Cold start completed in {metrics.duration:.2f}s")

            logger.info(f"[METRICS] {metrics.sub_metrics}")

            

        finally:

            await suite.teardown_test_environment()



    @pytest.mark.e2e

    async def test_cross_service_synchronization(self):

        """Test cross-service profile synchronization."""

        suite = RealServiceE2ETestSuite()

        

        try:

            await suite.setup_test_environment()

            metrics = await suite.test_cross_service_profile_synchronization()

            

            assert metrics.success, f"Profile sync failed: {metrics.error_details}"

            assert metrics.duration < 5.0, f"Profile sync too slow: {metrics.duration:.2f}s"

            

            logger.info(f"[SUCCESS] Profile sync completed in {metrics.duration:.2f}s")

            

        finally:

            await suite.teardown_test_environment()



    @pytest.mark.e2e

    async def test_real_llm_integration(self):

        """Test real LLM API integration."""

        suite = RealServiceE2ETestSuite()

        

        try:

            await suite.setup_test_environment()

            metrics = await suite.test_real_llm_api_integration()

            

            assert metrics.success, f"LLM integration failed: {metrics.error_details}"

            assert metrics.duration < 15.0, f"LLM integration too slow: {metrics.duration:.2f}s"

            

            logger.info(f"[SUCCESS] LLM integration completed in {metrics.duration:.2f}s")

            

        finally:

            await suite.teardown_test_environment()



    @pytest.mark.e2e

    async def test_redis_cache_validation(self):

        """Test Redis cache operations."""

        suite = RealServiceE2ETestSuite()

        

        try:

            await suite.setup_test_environment()

            metrics = await suite.test_redis_cache_validation()

            

            assert metrics.success, f"Cache validation failed: {metrics.error_details}"

            assert metrics.duration < 10.0, f"Cache validation too slow: {metrics.duration:.2f}s"

            

            logger.info(f"[SUCCESS] Cache validation completed in {metrics.duration:.2f}s")

            

        finally:

            await suite.teardown_test_environment()



    @pytest.mark.e2e

    async def test_database_consistency(self):

        """Test database consistency between PostgreSQL and ClickHouse."""

        suite = RealServiceE2ETestSuite()

        

        try:

            await suite.setup_test_environment()

            metrics = await suite.test_database_consistency_validation()

            

            assert metrics.success, f"Database consistency failed: {metrics.error_details}"

            assert metrics.duration < 10.0, f"Database validation too slow: {metrics.duration:.2f}s"

            

            logger.info(f"[SUCCESS] Database consistency validated in {metrics.duration:.2f}s")

            

        finally:

            await suite.teardown_test_environment()

