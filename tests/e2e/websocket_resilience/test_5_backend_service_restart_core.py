"""
Test module split from original file
Generated by auto_fix_test_violations.py
"""

import asyncio
import json
import time
import uuid
import random
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Optional, Tuple, Literal
from unittest.mock import AsyncMock, MagicMock, patch
from enum import Enum
import pytest
import websockets
from websockets.exceptions import ConnectionClosed, InvalidStatusCode
from netra_backend.app.logging_config import central_logger

class ServerState(Enum):
    """Server lifecycle states."""
    STARTING = "starting"
    RUNNING = "running"
    SHUTTING_DOWN = "shutting_down"
    CRASHED = "crashed"
    RESTARTING = "restarting"
    UNAVAILABLE = "unavailable"

class ReconnectionStrategy(Enum):
    """Client reconnection strategies."""
    GRACEFUL = "graceful"
    EMERGENCY = "emergency"
    ROLLING_DEPLOYMENT = "rolling_deployment"
    EXTENDED_BACKOFF = "extended_backoff"

class MockBackendServer:
    """Mock backend server with lifecycle simulation capabilities."""
    
    def __init__(self, server_id: str = "server-1"):
        self.server_id = server_id
        self.state = ServerState.UNAVAILABLE
        self.connections: Dict[str, Any] = {}
        self.session_storage = {}
        self.startup_time = None
        self.shutdown_reason = None
        self.restart_count = 0
        self.health_status = False
        
    async def start(self, startup_delay: float = 0.1) -> None:
        """Start the server with optional startup delay."""
        self.state = ServerState.STARTING
        await asyncio.sleep(startup_delay)
        self.state = ServerState.RUNNING
        self.startup_time = datetime.now(timezone.utc)
        self.health_status = True
        logger.info(f"MockBackendServer {self.server_id} started")
        
    async def graceful_shutdown(self, notification_delay: float = 1.0) -> None:
        """Gracefully shutdown with client notification."""
        self.state = ServerState.SHUTTING_DOWN
        self.shutdown_reason = "graceful"
        
        # Send shutdown notifications to all connected clients
        shutdown_message = {
            "type": "server_shutdown",
            "payload": {
                "reason": "graceful",
                "estimated_restart_time": 10,
                "message": "Server is shutting down for maintenance"
            }
        }
        
        # Simulate sending to all connections
        for conn_id in self.connections:
            logger.info(f"Sending shutdown notification to {conn_id}")
            
        await asyncio.sleep(notification_delay)
        
        # Close all connections
        self.connections.clear()
        self.state = ServerState.UNAVAILABLE
        self.health_status = False
        logger.info(f"MockBackendServer {self.server_id} gracefully shut down")
        
    async def crash(self) -> None:
        """Simulate unexpected server crash."""
        self.state = ServerState.CRASHED
        self.shutdown_reason = "crash"
        self.connections.clear()
        self.health_status = False
        logger.info(f"MockBackendServer {self.server_id} crashed")
        
    async def restart(self, restart_delay: float = 5.0) -> None:
        """Restart the server after delay."""
        self.state = ServerState.RESTARTING
        self.restart_count += 1
        await asyncio.sleep(restart_delay)
        await self.start(startup_delay=0.5)
        logger.info(f"MockBackendServer {self.server_id} restarted (count: {self.restart_count})")
        
    def is_available(self) -> bool:
        """Check if server is available for connections."""
        return self.state == ServerState.RUNNING and self.health_status
        
    def accept_connection(self, session_token: str) -> bool:
        """Accept a WebSocket connection."""
        if not self.is_available():
            return False
            
        conn_id = f"conn_{len(self.connections)}_{int(time.time())}"
        self.connections[session_token] = {
            "id": conn_id,
            "connected_at": datetime.now(timezone.utc),
            "session_token": session_token
        }
        
        logger.info(f"Connection accepted: {session_token[:8]}... on {self.server_id}")
        return True
        
    def store_session_state(self, session_token: str, state: Dict[str, Any]) -> None:
        """Store session state for persistence across restarts."""
        self.session_storage[session_token] = {
            **state,
            "stored_at": datetime.now(timezone.utc).isoformat(),
            "server_id": self.server_id
        }
        
    def retrieve_session_state(self, session_token: str) -> Optional[Dict[str, Any]]:
        """Retrieve session state after restart."""
        return self.session_storage.get(session_token)

async def test_graceful_server_shutdown_with_client_notification(
    established_session_with_complex_state, session_token
):
    """
    Test Case 1: Graceful server shutdown with client notification.
    
    Validates that when a server sends a graceful shutdown signal, the client
    receives notification, disconnects gracefully, and reconnects successfully
    when the server comes back online.
    """
    client, server, original_state = established_session_with_complex_state
    
    # Verify initial connection
    assert client.is_connected
    assert server.is_available()
    
    # Configure mock response for shutdown notification
    shutdown_notification = {
        "type": "server_shutdown",
        "payload": {
            "reason": "graceful",
            "estimated_restart_time": 10,
            "message": "Server is shutting down for maintenance"
        }
    }
    
    client.websocket._configured_response = shutdown_notification
    
    # Simulate receiving shutdown notification
    notification = await client.websocket.recv()
    logger.info(f"Client received notification: {notification}")
    
    # Client handles shutdown notification
    start_downtime = time.time()
    await client.handle_server_shutdown_notification(shutdown_notification)
    
    # Verify client disconnected gracefully
    assert not client.is_connected
    
    # Server performs graceful shutdown
    await server.graceful_shutdown(notification_delay=0.1)
    assert not server.is_available()
    assert server.shutdown_reason == "graceful"
    
    # Server restarts
    restart_task = asyncio.create_task(server.restart(restart_delay=2.0))
    
    # Configure server availability callback for client
    def server_available():
        return server.is_available()
    
    # Client attempts reconnection with graceful strategy
    reconnection_start = time.time()
    reconnection_success = await client.reconnect_with_strategy(
        ReconnectionStrategy.GRACEFUL, 
        server_available_callback=server_available
    )
    
    # Wait for server restart to complete
    await restart_task
    
    total_downtime = time.time() - start_downtime
    
    # Validate reconnection success
    assert reconnection_success, "Client failed to reconnect after graceful shutdown"
    assert client.is_connected
    assert server.is_available()
    
    # Configure mock response for session restoration
    client.websocket._configured_response = {
        "type": "session_restored",
        "payload": {
            "success": True,
            "conversation_history": original_state["conversation_history"],
            "agent_context": original_state["agent_context"],
            "restoration_time": time.time()
        }
    }
    
    # Request and validate session restoration
    restored_session = await client.request_session_restoration()
    assert restored_session["payload"]["success"]
    
    # Performance validation - graceful shutdown should reconnect within 10 seconds
    assert total_downtime < 10.0, f"Total downtime {total_downtime:.2f}s exceeded 10s limit"
    
    # Validate session state preservation
    stored_state = server.retrieve_session_state(session_token)
    assert stored_state is not None
    assert len(stored_state["conversation_history"]) == len(original_state["conversation_history"])
    assert stored_state["agent_context"]["workflow_state"]["current_step"] == 3
    
    logger.info(f"âœ“ Graceful shutdown handled successfully in {total_downtime:.3f}s")

async def test_unexpected_server_crash_recovery(
    established_session_with_complex_state, session_token
):
    """
    Test Case 2: Unexpected server crash recovery.
    
    Validates that when a server crashes unexpectedly without notification,
    the client detects the connection loss and implements an aggressive
    reconnection strategy to restore service quickly.
    """
    client, server, original_state = established_session_with_complex_state
    
    # Verify baseline connection
    assert client.is_connected
    original_connection_count = client.connection_metrics["successful_connections"]
    
    # Simulate server crash - immediate connection termination
    crash_time = time.time()
    await server.crash()
    await client.disconnect(expected=False)  # Simulate unexpected disconnection
    
    assert not server.is_available()
    assert server.state == ServerState.CRASHED
    assert not client.is_connected
    
    # Start server restart process (background)
    restart_task = asyncio.create_task(server.restart(restart_delay=3.0))
    
    # Client detects connection loss and handles emergency reconnection
    recovery_start = time.time()
    recovery_success = await client.handle_unexpected_disconnection()
    
    # Wait for server restart to complete
    await restart_task
    
    recovery_time = time.time() - recovery_start
    
    # Validate emergency recovery
    assert recovery_success, "Client failed to recover from server crash"
    assert client.is_connected
    assert server.is_available()
    assert server.restart_count == 1
    
    # Validate reconnection metrics
    metrics = client.get_connection_metrics()
    assert metrics["successful_connections"] > original_connection_count
    assert metrics["total_downtime"] > 0
    
    # Performance validation - emergency recovery should complete within 30 seconds
    assert recovery_time < 30.0, f"Recovery time {recovery_time:.2f}s exceeded 30s limit"
    
    # Configure mock response for state validation
    client.websocket._configured_response = {
        "type": "session_restored", 
        "payload": {
            "success": True,
            "data_integrity_check": "passed",
            "conversation_history": original_state["conversation_history"],
            "agent_context": original_state["agent_context"]
        }
    }
    
    # Validate data integrity after crash recovery
    restored_session = await client.request_session_restoration()
    assert restored_session["payload"]["success"]
    assert restored_session["payload"]["data_integrity_check"] == "passed"
    
    # Verify session state survived crash
    stored_state = server.retrieve_session_state(session_token)
    assert stored_state is not None
    assert stored_state["conversation_history"] == original_state["conversation_history"]
    
    logger.info(f"âœ“ Crash recovery completed successfully in {recovery_time:.3f}s")

async def test_rolling_deployment_reconnection(
    established_session_with_complex_state, session_token, mock_load_balancer
):
    """
    Test Case 3: Rolling deployment reconnection.
    
    Validates seamless handoff between server instances during rolling deployment,
    ensuring clients connect to new instances without service interruption.
    """
    client, server_a, original_state = established_session_with_complex_state
    
    # Set up load balancer with initial server
    mock_load_balancer.add_server(server_a)
    
    # Verify initial state
    assert client.is_connected
    assert server_a.is_available()
    
    # Create new server instance for rolling deployment
    server_b = MockBackendServer("test-server-2")
    
    # Copy session state to shared storage (simulating database/Redis)
    server_b.session_storage = server_a.session_storage.copy()
    
    # Start rolling deployment
    deployment_start = time.time()
    
    # Load balancer initiates rolling deployment
    deployment_task = asyncio.create_task(
        mock_load_balancer.rolling_deployment(server_b)
    )
    
    # Simulate connection being redirected during deployment
    await asyncio.sleep(1)  # Allow deployment to start
    
    # Client connection to server A is gracefully terminated
    await client.disconnect(expected=True)
    
    # Client attempts reconnection - should connect to server B
    reconnection_success = await client.reconnect_with_strategy(
        ReconnectionStrategy.ROLLING_DEPLOYMENT
    )
    
    # Wait for deployment to complete
    await deployment_task
    
    deployment_time = time.time() - deployment_start
    
    # Validate rolling deployment success
    assert reconnection_success, "Client failed to reconnect during rolling deployment"
    assert client.is_connected
    assert server_b.is_available()
    assert not server_a.is_available()  # Old server shut down
    
    # Validate load balancer state
    active_server = mock_load_balancer.get_active_server()
    assert active_server is server_b
    
    # Performance validation - rolling deployment should complete within 5 seconds
    assert deployment_time < 5.0, f"Rolling deployment took {deployment_time:.2f}s, expected < 5s"
    
    # Configure mock response for session continuity validation
    client.websocket._configured_response = {
        "type": "session_restored",
        "payload": {
            "success": True,
            "server_instance": server_b.server_id,
            "session_continuity": True,
            "conversation_history": original_state["conversation_history"],
            "handoff_successful": True
        }
    }
    
    # Validate session continuity across server instances
    session_validation = await client.request_session_restoration()
    assert session_validation["payload"]["success"]
    assert session_validation["payload"]["session_continuity"]
    assert session_validation["payload"]["handoff_successful"]
    
    # Verify session state available on new server
    stored_state_b = server_b.retrieve_session_state(session_token)
    assert stored_state_b is not None
    assert stored_state_b["conversation_history"] == original_state["conversation_history"]
    assert stored_state_b["agent_context"]["workflow_state"]["current_step"] == 3
    
    logger.info(f"âœ“ Rolling deployment completed successfully in {deployment_time:.3f}s")

async def test_client_backoff_strategy_during_restart(
    websocket_reconnect_client, mock_backend_server, session_token
):
    """
    Test Case 4: Client backoff strategy during restart.
    
    Validates that when a server is unavailable for an extended period,
    the client implements exponential backoff without overwhelming the server
    and connects successfully when the server becomes available.
    """
    client = websocket_reconnect_client
    server = mock_backend_server
    
    # Establish initial connection
    await server.start()
    await client.connect()
    assert client.is_connected
    
    # Server becomes unavailable for extended period
    await server.crash()
    await client.disconnect(expected=False)
    
    # Track backoff behavior
    backoff_start = time.time()
    reconnection_attempts = []
    
    # Monitor client's backoff strategy
    original_connect = client.connect
    
    async def monitored_connect(timeout=10.0):
        attempt_time = time.time()
        reconnection_attempts.append(attempt_time)
        logger.info(f"Reconnection attempt {len(reconnection_attempts)} at {attempt_time - backoff_start:.3f}s")
        
        # Server unavailable for first 30 seconds
        if attempt_time - backoff_start < 30:
            return False
        else:
            # Server becomes available
            if not server.is_available():
                await server.start()
            return await original_connect(timeout)
    
    client.connect = monitored_connect
    
    # Client attempts reconnection with extended backoff strategy
    reconnection_success = await client.reconnect_with_strategy(
        ReconnectionStrategy.EXTENDED_BACKOFF
    )
    
    total_backoff_time = time.time() - backoff_start
    
    # Validate backoff strategy effectiveness
    assert reconnection_success, "Client failed to reconnect with extended backoff"
    assert client.is_connected
    assert len(reconnection_attempts) >= 3, "Client should have made multiple attempts"
    
    # Validate backoff intervals are respected
    intervals = []
    for i in range(1, len(reconnection_attempts)):
        interval = reconnection_attempts[i] - reconnection_attempts[i-1]
        intervals.append(interval)
    
    # Verify exponential backoff pattern (allowing for some variance)
    expected_intervals = [1, 2, 4, 8, 16, 30]
    for i, interval in enumerate(intervals[:len(expected_intervals)]):
        expected = expected_intervals[i]
        # Allow 20% variance for timing precision
        assert expected * 0.8 <= interval <= expected * 1.2, \
            f"Interval {i+1}: {interval:.2f}s not within expected range of {expected}s"
    
    # Validate resource efficiency - client shouldn't overwhelm server
    max_interval = max(intervals) if intervals else 0
    assert max_interval <= 60, f"Maximum backoff interval {max_interval:.2f}s should not exceed 60s"
    
    # Validate connection success when server becomes available
    assert server.is_available()
    
    # Performance validation
    metrics = client.get_connection_metrics()
    assert metrics["total_attempts"] >= 3, "Should have multiple reconnection attempts"
    
    logger.info(f"âœ“ Backoff strategy effective: {len(reconnection_attempts)} attempts over {total_backoff_time:.3f}s")

async def test_state_preservation_across_server_restarts(
    established_session_with_complex_state, session_token
):
    """
    Test Case 5: State preservation across server restarts.
    
    Validates that complex session state (conversation history, agent context,
    tool state) survives server restart with 100% preservation and fast restoration.
    """
    client, server, original_state = established_session_with_complex_state
    
    # Validate initial complex state
    assert len(original_state["conversation_history"]) == 5
    assert original_state["agent_context"]["workflow_state"]["current_step"] == 3
    assert len(original_state["tool_call_history"]) == 3
    assert original_state["user_session_metadata"]["enterprise_tier"] is True
    
    # Add additional state complexity
    additional_state = {
        "active_tools": ["optimization_planner", "cost_monitor"],
        "pending_operations": [
            {"operation": "analyze_patterns", "progress": 75, "eta": 45},
            {"operation": "generate_recommendations", "progress": 0, "eta": 120}
        ],
        "cached_results": {
            "token_analysis": {"total_tokens": 125000, "cost": 375.50},
            "optimization_potential": {"savings": 1125.75, "efficiency_gain": 30}
        },
        "user_context": {
            "current_focus": "cost_optimization",
            "enterprise_settings": {"sla_tier": "premium", "priority": "high"},
            "session_preferences": {"detailed_logs": True, "real_time_updates": True}
        }
    }
    
    # Merge additional state
    enhanced_state = {**original_state, **additional_state}
    server.store_session_state(session_token, enhanced_state)
    
    # Begin server restart process
    restart_start = time.time()
    
    # Client preserves state before restart
    await client.preserve_session_state()
    
    # Server graceful shutdown
    await server.graceful_shutdown(notification_delay=0.1)
    await client.disconnect(expected=True)
    
    # Server restart with fresh process
    await server.restart(restart_delay=1.0)
    
    # Client reconnects
    await client.connect()
    
    restoration_start = time.time()
    
    # Configure mock response for complete state restoration
    client.websocket._configured_response = {
        "type": "session_restored",
        "payload": {
            "success": True,
            "restoration_complete": True,
            "conversation_history": enhanced_state["conversation_history"],
            "agent_context": enhanced_state["agent_context"],
            "tool_call_history": enhanced_state["tool_call_history"],
            "active_tools": enhanced_state["active_tools"],
            "pending_operations": enhanced_state["pending_operations"],
            "cached_results": enhanced_state["cached_results"],
            "user_context": enhanced_state["user_context"],
            "user_session_metadata": enhanced_state["user_session_metadata"],
            "integrity_check": "passed",
            "restoration_time": time.time()
        }
    }
    
    # Request complete session restoration
    restored_session = await client.request_session_restoration()
    restoration_time = time.time() - restoration_start
    
    # Validate restoration success
    assert restored_session["payload"]["success"]
    assert restored_session["payload"]["restoration_complete"]
    assert restored_session["payload"]["integrity_check"] == "passed"
    
    # Validate conversation history preservation (100%)
    restored_history = restored_session["payload"]["conversation_history"]
    assert len(restored_history) == len(enhanced_state["conversation_history"])
    for i, msg in enumerate(restored_history):
        original_msg = enhanced_state["conversation_history"][i]
        assert msg["content"] == original_msg["content"]
        assert msg["role"] == original_msg["role"]
        assert msg["timestamp"] == original_msg["timestamp"]
    
    # Validate agent context preservation
    restored_context = restored_session["payload"]["agent_context"]
    assert restored_context["workflow_state"]["current_step"] == 3
    assert restored_context["variables"]["monthly_budget"] == 5000
    assert restored_context["user_preferences"]["optimization_focus"] == "cost"
    
    # Validate tool state preservation
    restored_tools = restored_session["payload"]["tool_call_history"]
    assert len(restored_tools) == 3
    assert restored_tools[2]["status"] == "in_progress"
    
    # Validate active tools and operations
    assert restored_session["payload"]["active_tools"] == enhanced_state["active_tools"]
    assert len(restored_session["payload"]["pending_operations"]) == 2
    
    # Validate cached results preservation
    cached_results = restored_session["payload"]["cached_results"]
    assert cached_results["token_analysis"]["total_tokens"] == 125000
    assert cached_results["optimization_potential"]["savings"] == 1125.75
    
    # Validate user context and enterprise settings
    user_context = restored_session["payload"]["user_context"]
    assert user_context["enterprise_settings"]["sla_tier"] == "premium"
    assert user_context["session_preferences"]["detailed_logs"] is True
    
    # Performance validation - state restoration should complete within 2 seconds
    assert restoration_time < 2.0, f"State restoration took {restoration_time:.3f}s, expected < 2.0s"
    
    # Validate server-side state integrity
    stored_state = server.retrieve_session_state(session_token)
    assert stored_state is not None
    assert len(stored_state["conversation_history"]) == 5
    assert stored_state["agent_context"]["workflow_state"]["current_step"] == 3
    
    total_restart_time = time.time() - restart_start
    
    logger.info(f"âœ“ Complete state preservation validated: {restoration_time:.3f}s restoration, {total_restart_time:.3f}s total")

async def test_concurrent_client_reconnections_during_restart(
    mock_backend_server, session_token
):
    """Test multiple clients reconnecting simultaneously during server restart."""
    server = mock_backend_server
    num_clients = 5
    clients = []
    
    # Create multiple clients
    for i in range(num_clients):
        client_token = f"{session_token}_{i}"
        client = WebSocketReconnectClient(f"ws://mock-server/ws", client_token)
        clients.append(client)
    
    # Start server and connect all clients
    await server.start()
    for client in clients:
        await client.connect()
        assert client.is_connected
    
    # Server restart
    await server.crash()
    for client in clients:
        await client.disconnect(expected=False)
    
    # Restart server
    restart_task = asyncio.create_task(server.restart(restart_delay=2.0))
    
    # All clients attempt reconnection simultaneously
    reconnection_tasks = []
    for client in clients:
        task = asyncio.create_task(
            client.reconnect_with_strategy(ReconnectionStrategy.EMERGENCY)
        )
        reconnection_tasks.append(task)
    
    # Wait for server restart and all reconnections
    await restart_task
    results = await asyncio.gather(*reconnection_tasks)
    
    # Validate all clients reconnected successfully
    assert all(results), "Not all clients reconnected successfully"
    for client in clients:
        assert client.is_connected
    
    # Validate server handled concurrent connections
    assert server.is_available()
    assert len(server.connections) == num_clients
    
    logger.info(f"âœ“ {num_clients} concurrent reconnections successful")
