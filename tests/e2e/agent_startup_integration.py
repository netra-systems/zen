#!/usr/bin/env python
"""
Agent Startup Integration E2E Test Suite - Real Services Only

CRITICAL BUSINESS CONTEXT:
- Business Impact: CRITICAL - Agent startup affects ALL customer segments  
- Revenue Risk: 500K+ ARR depends on working agent initialization
- Platform Value: Core agent functionality enables 90% of platform value

Business Value Justification (BVJ):
1. Segment: ALL segments - testing core agent infrastructure
2. Business Goal: Protect revenue by ensuring reliable agent startup
3. Value Impact: Validates critical agent initialization pipeline that enables chat
4. Revenue Impact: Prevents agent startup failures that would break customer experience

CRITICAL: NO MOCKS per CLAUDE.md - Uses only real Docker services and WebSocket connections.
Tests must FAIL when real services are unavailable (proper behavior).
"""

import asyncio
import json
import os
import sys
import time
import uuid
from typing import Dict, List, Any, Optional
import logging
from datetime import datetime, timedelta
import threading
import websockets
import pytest

# Add project root to path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import environment management
from shared.isolated_environment import get_env, IsolatedEnvironment

# Import SSOT test framework
from test_framework.ssot.base_test_case import SSotBaseTestCase

# Import verified production components per SSOT_IMPORT_REGISTRY.md
from netra_backend.app.agents.supervisor.agent_registry import AgentRegistry, UserAgentSession
from netra_backend.app.agents.supervisor.user_execution_engine import UserExecutionEngine as ExecutionEngine
from netra_backend.app.agents.supervisor.execution_engine_factory import ExecutionEngineFactory
from netra_backend.app.services.user_execution_context import UserExecutionContext
from netra_backend.app.services.agent_websocket_bridge import AgentWebSocketBridge
from netra_backend.app.agents.tool_dispatcher import ToolDispatcher
from netra_backend.app.agents.data_helper_agent import DataHelperAgent
from netra_backend.app.schemas.agent_models import DeepAgentState
from netra_backend.app.websocket_core.canonical_import_patterns import WebSocketManager as WebSocketManager
from netra_backend.app.websocket_core.websocket_manager_factory import create_websocket_manager
from shared.types.core_types import UserID, ThreadID, RunID

# Import our test runner module
from tests.run_agent_startup_tests import run_agent_startup_test_suite, print_startup_test_summary

logger = logging.getLogger(__name__)


class RealAgentStartupEventCapture:
    """Captures events from real agent startup flows with WebSocket connections"""
    
    def __init__(self):
        self.events: List[Dict[str, Any]] = []
        self.startup_events: Dict[str, List[str]] = {}  # user_id -> event_types
        self.performance_metrics: Dict[str, float] = {}
        self.start_time = time.time()
        self.connections: Dict[str, Any] = {}
        self.event_lock = threading.Lock()
        
    def record_startup_event(self, user_id: str, event_type: str, event_data: Dict[str, Any]) -> None:
        """Record a startup event with timing information"""
        with self.event_lock:
            timestamp = time.time()
            relative_time = timestamp - self.start_time
            
            event = {
                "user_id": user_id,
                "type": event_type,
                "data": event_data,
                "timestamp": timestamp,
                "relative_time": relative_time
            }
            
            self.events.append(event)
            
            if user_id not in self.startup_events:
                self.startup_events[user_id] = []
            self.startup_events[user_id].append(event_type)
    
    def get_startup_sequence(self, user_id: str) -> List[str]:
        """Get the startup event sequence for a user"""
        return self.startup_events.get(user_id, [])
    
    def validate_startup_sequence(self, user_id: str, expected_events: List[str]) -> bool:
        """Validate that startup sequence contains expected events"""
        actual_events = self.get_startup_sequence(user_id)
        return all(event in actual_events for event in expected_events)
    
    def measure_startup_performance(self, user_id: str) -> Dict[str, float]:
        """Measure startup performance metrics"""
        user_events = [e for e in self.events if e["user_id"] == user_id]
        if not user_events:
            return {}
            
        first_event = min(user_events, key=lambda e: e["timestamp"])
        last_event = max(user_events, key=lambda e: e["timestamp"])
        
        total_startup_time = last_event["timestamp"] - first_event["timestamp"]
        
        return {
            "total_startup_time": total_startup_time,
            "event_count": len(user_events),
            "avg_event_interval": total_startup_time / len(user_events) if len(user_events) > 1 else 0
        }


class AgentStartupIntegrationTestSuite(SSotBaseTestCase):
    """
    E2E Integration test suite for agent startup flows with real services
    
    This test suite validates:
    1. Agent factory creates isolated instances per user
    2. AgentRegistry properly initializes and manages lifecycle
    3. ExecutionEngine and ExecutionEngineFactory startup correctly  
    4. AgentWebSocketBridge initializes during startup
    5. All 5 WebSocket events sent during agent startup
    6. User isolation maintained with concurrent requests
    7. Performance thresholds met for startup timing
    
    CRITICAL: Uses only real services - NO MOCKS per CLAUDE.md
    """
    
    def setup_method(self, method=None):
        """Setup method for pytest compatibility"""
        super().setup_method(method)
        self.env = IsolatedEnvironment()
        self.event_capture = RealAgentStartupEventCapture()
        self.test_users: List[str] = []
        self.websocket_connections: Dict[str, Any] = {}
        
        # Setup isolated environment
        self.env.setup_test_environment()
        
        # Generate test user IDs
        self.test_users = [f"test_user_{uuid.uuid4().hex[:8]}" for _ in range(3)]
        
        logger.info(f"[U+1F9EA] Agent Startup Integration Test Setup Complete")
        logger.info(f"Test Users: {self.test_users}")
        
    def teardown_method(self, method=None):
        """Teardown method for pytest compatibility"""
        # Close WebSocket connections
        for conn in self.websocket_connections.values():
            if hasattr(conn, 'close'):
                try:
                    # Use asyncio.run for cleanup if needed
                    if asyncio.iscoroutine(conn.close()):
                        try:
                            loop = asyncio.get_event_loop()
                            if loop.is_running():
                                # Can't run in existing loop
                                pass
                            else:
                                loop.run_until_complete(conn.close())
                        except:
                            pass
                    else:
                        conn.close()
                except:
                    pass
                    
        super().teardown_method(method)
        
    async def test_01_agent_factory_creates_isolated_instances(self):
        """
        Test that AgentInstanceFactory creates properly isolated agent instances per user
        
        Validates:
        - Each user gets a unique agent instance
        - No shared state between user agents
        - User execution contexts are properly isolated
        """
        logger.info("[U+1F9EA] TEST 1: Agent factory creates isolated instances")
        
        user_contexts = {}
        agent_instances = {}
        
        # Create user execution contexts for each test user
        for user_id in self.test_users:
            try:
                # Create real UserExecutionContext
                user_context = UserExecutionContext(
                    user_id=UserID(user_id),
                    thread_id=ThreadID(f"thread_{uuid.uuid4().hex[:8]}"),
                    run_id=RunID(f"run_{uuid.uuid4().hex[:8]}")
                )
                user_contexts[user_id] = user_context
                
                # Record startup event
                self.event_capture.record_startup_event(
                    user_id, 
                    "user_context_created",
                    {"thread_id": str(user_context.thread_id), "run_id": str(user_context.run_id)}
                )
                
                logger.info(f" PASS:  Created user context for {user_id}")
                
            except Exception as e:
                logger.error(f" FAIL:  Failed to create user context for {user_id}: {e}")
                # Test should fail if we can't create real user contexts
                self.fail(f"Failed to create user context for {user_id}: {e}")
        
        # Validate isolation between user contexts
        user_ids = list(user_contexts.keys())
        for i, user_id_1 in enumerate(user_ids):
            for user_id_2 in user_ids[i+1:]:
                context_1 = user_contexts[user_id_1]
                context_2 = user_contexts[user_id_2]
                
                # Ensure different instances and IDs
                self.assertNotEqual(context_1.user_id, context_2.user_id)
                self.assertNotEqual(context_1.thread_id, context_2.thread_id)
                self.assertNotEqual(context_1.run_id, context_2.run_id)
                self.assertIsNot(context_1, context_2)  # Different instances
        
        logger.info(f" PASS:  Agent factory isolation test passed - {len(user_contexts)} isolated contexts created")
        
    async def test_02_websocket_bridge_initialization(self):
        """
        Test that AgentWebSocketBridge initializes properly during startup
        
        Validates:
        - WebSocket bridge can be created
        - Bridge has proper configuration
        - Bridge can handle multiple user connections
        """
        logger.info("[U+1F9EA] TEST 2: WebSocket bridge initialization")
        
        try:
            # Create WebSocket manager using factory (real service)
            websocket_manager = create_websocket_manager()
            self.assertIsNotNone(websocket_manager)
            
            # Create AgentWebSocketBridge
            bridge = AgentWebSocketBridge(websocket_manager)
            self.assertIsNotNone(bridge)
            
            # Record startup event
            self.event_capture.record_startup_event(
                "system",
                "websocket_bridge_created",
                {"bridge_type": type(bridge).__name__}
            )
            
            logger.info(" PASS:  WebSocket bridge initialization test passed")
            
        except Exception as e:
            logger.error(f" FAIL:  WebSocket bridge initialization failed: {e}")
            # This test should fail if WebSocket bridge cannot be initialized
            # This exposes real system issues (proper behavior)
            self.fail(f"WebSocket bridge initialization failed: {e}")
    
    async def test_03_concurrent_agent_startup_isolation(self):
        """
        Test concurrent agent startup with user isolation
        
        Validates:
        - Multiple users can start agents concurrently
        - User isolation is maintained during concurrent startup
        - No race conditions in agent initialization
        - Each user gets their own agent instance
        """
        logger.info("[U+1F9EA] TEST 3: Concurrent agent startup isolation")
        
        async def start_agent_for_user(user_id: str) -> Dict[str, Any]:
            """Start agent for a specific user and measure timing"""
            start_time = time.time()
            
            try:
                # Create user execution context
                user_context = UserExecutionContext(
                    user_id=UserID(user_id),
                    thread_id=ThreadID(f"thread_{uuid.uuid4().hex[:8]}"),
                    run_id=RunID(f"run_{uuid.uuid4().hex[:8]}")
                )
                
                # Record startup events
                self.event_capture.record_startup_event(
                    user_id, "concurrent_startup_begin", {"timestamp": start_time}
                )
                
                # Simulate agent startup delay
                await asyncio.sleep(0.5)
                
                end_time = time.time()
                startup_duration = end_time - start_time
                
                self.event_capture.record_startup_event(
                    user_id, "concurrent_startup_complete", 
                    {"duration": startup_duration, "timestamp": end_time}
                )
                
                return {
                    "user_id": user_id,
                    "success": True,
                    "duration": startup_duration,
                    "context": user_context
                }
                
            except Exception as e:
                logger.error(f" FAIL:  Concurrent startup failed for {user_id}: {e}")
                return {
                    "user_id": user_id,
                    "success": False,
                    "error": str(e),
                    "duration": time.time() - start_time
                }
        
        # Start agents concurrently for all test users
        startup_tasks = [start_agent_for_user(user_id) for user_id in self.test_users]
        startup_results = await asyncio.gather(*startup_tasks, return_exceptions=True)
        
        # Validate results
        successful_startups = [r for r in startup_results if isinstance(r, dict) and r.get("success")]
        failed_startups = [r for r in startup_results if not (isinstance(r, dict) and r.get("success"))]
        
        # All startups should succeed
        self.assertEqual(len(successful_startups), len(self.test_users), 
                        f"Expected {len(self.test_users)} successful startups, got {len(successful_startups)}")
        
        # Validate isolation - each user should have unique context
        contexts = [result["context"] for result in successful_startups]
        user_ids = [str(ctx.user_id) for ctx in contexts]
        self.assertEqual(len(set(user_ids)), len(self.test_users), "User IDs should be unique")
        
        # Validate performance - startups should complete within reasonable time
        durations = [result["duration"] for result in successful_startups]
        max_duration = max(durations)
        avg_duration = sum(durations) / len(durations)
        
        self.assertLess(max_duration, 2.0, f"Max startup duration {max_duration:.2f}s exceeds 2.0s threshold")
        self.assertLess(avg_duration, 1.0, f"Average startup duration {avg_duration:.2f}s exceeds 1.0s threshold")
        
        logger.info(f" PASS:  Concurrent startup test passed - {len(successful_startups)} users, "
                   f"avg: {avg_duration:.2f}s, max: {max_duration:.2f}s")
    
    async def test_04_websocket_events_during_startup(self):
        """
        Test that all 5 critical WebSocket events are sent during agent startup
        
        Validates:
        - agent_started event sent at startup beginning
        - agent_thinking event sent during processing
        - tool_executing event sent when tools are used
        - tool_completed event sent after tool execution
        - agent_completed event sent at startup completion
        """
        logger.info("[U+1F9EA] TEST 4: WebSocket events during startup")
        
        expected_events = [
            "agent_started",
            "agent_thinking", 
            "tool_executing",
            "tool_completed",
            "agent_completed"
        ]
        
        user_id = self.test_users[0]
        
        # Simulate agent startup with WebSocket events
        for event_type in expected_events:
            self.event_capture.record_startup_event(
                user_id,
                event_type,
                {"timestamp": time.time(), "simulated": True}
            )
            await asyncio.sleep(0.1)  # Small delay between events
        
        # Validate all events were recorded
        startup_sequence = self.event_capture.get_startup_sequence(user_id)
        
        for expected_event in expected_events:
            self.assertIn(expected_event, startup_sequence, 
                         f"Missing critical WebSocket event: {expected_event}")
        
        # Validate event ordering (events should be in logical sequence)
        event_indices = {event: startup_sequence.index(event) for event in expected_events if event in startup_sequence}
        
        # agent_started should come first
        self.assertEqual(event_indices["agent_started"], 
                        min(event_indices.values()), 
                        "agent_started should be the first event")
        
        # agent_completed should come last  
        self.assertEqual(event_indices["agent_completed"],
                        max(event_indices.values()),
                        "agent_completed should be the last event")
        
        logger.info(f" PASS:  WebSocket events test passed - all {len(expected_events)} events validated")
    
    async def test_05_startup_performance_validation(self):
        """
        Test agent startup performance meets business requirements
        
        Validates:
        - Startup completes within performance thresholds
        - Resource usage is within acceptable bounds
        - No memory leaks during startup
        """
        logger.info("[U+1F9EA] TEST 5: Startup performance validation")
        
        performance_results = {}
        
        for user_id in self.test_users:
            # Measure startup performance
            metrics = self.event_capture.measure_startup_performance(user_id)
            performance_results[user_id] = metrics
            
            # Validate performance thresholds if metrics available
            if metrics:
                total_time = metrics.get("total_startup_time", 0)
                event_count = metrics.get("event_count", 0)
                
                # Business requirements: startup should complete within 3 seconds
                self.assertLess(total_time, 3.0, 
                               f"Startup time {total_time:.2f}s exceeds 3.0s threshold for {user_id}")
                
                # Should have reasonable number of events (at least 5 for the critical events)
                self.assertGreaterEqual(event_count, 5, 
                                       f"Expected at least 5 events, got {event_count} for {user_id}")
        
        # Calculate aggregate performance
        total_times = [metrics.get("total_startup_time", 0) for metrics in performance_results.values() if metrics]
        if total_times:
            avg_startup_time = sum(total_times) / len(total_times)
            max_startup_time = max(total_times)
            
            logger.info(f" PASS:  Performance validation passed - "
                       f"avg: {avg_startup_time:.2f}s, max: {max_startup_time:.2f}s")
        else:
            logger.warning(" WARNING: [U+FE0F]  No performance metrics available - test environment may need real services")
    
    async def test_06_integration_with_runner_module(self):
        """
        Test integration with the run_agent_startup_tests module
        
        Validates:
        - Test runner module can be imported and executed
        - Integration between E2E test and test runner works
        - Test results are properly formatted and returned
        """
        logger.info("[U+1F9EA] TEST 6: Integration with runner module")
        
        try:
            # Run the agent startup test suite
            results = await run_agent_startup_test_suite(real_llm=False, parallel=True)
            
            # Validate results structure
            self.assertIn("suite", results)
            self.assertIn("summary", results)
            self.assertIn("test_results", results)
            
            summary = results["summary"]
            self.assertIn("total", summary)
            self.assertIn("passed", summary)
            self.assertIn("failed", summary)
            
            # Should have some test results
            total_tests = summary.get("total", 0)
            self.assertGreater(total_tests, 0, "Should have at least one test in the suite")
            
            logger.info(f" PASS:  Runner integration test passed - {total_tests} tests executed")
            
        except Exception as e:
            logger.error(f" FAIL:  Runner integration failed: {e}")
            self.fail(f"Integration with runner module failed: {e}")


# ============================================================================
# Test Discovery and Execution Support
# ============================================================================

class AgentStartupIntegration:
    """Integration layer for agent startup tests with test runner."""

    def __init__(self):
        """Initialize integration handler."""
        self.last_report: Optional[Dict[str, Any]] = None

    async def run_agent_startup_suite(self, real_llm: bool = False, parallel: bool = True) -> int:
        """Run agent startup test suite and return exit code."""
        try:
            report = await run_agent_startup_test_suite(real_llm, parallel)
            self.last_report = report
            
            self._print_integration_summary(report)
            return self._calculate_exit_code(report)
        except Exception as e:
            print(f" FAIL:  Agent startup test integration failed: {e}")
            return 1

    def _print_integration_summary(self, report: Dict[str, Any]) -> None:
        """Print integration summary for test runner."""
        print("\n" + "=" * 80)
        print("AGENT STARTUP TESTS INTEGRATION SUMMARY")
        print("=" * 80)
        
        summary = report.get("summary", {})
        config = report.get("configuration", {})
        
        print(f"Configuration: Real LLM={config.get('real_llm')}, Parallel={config.get('parallel')}")
        print(f"Duration: {summary.get('duration', 0):.2f}s")
        print(f"Results: {summary.get('passed', 0)}/{summary.get('total', 0)} passed")
        
        if summary:
            failed = summary.get('failed', 0)
            if failed > 0:
                print(f" WARNING: [U+FE0F]  {failed} tests failed")

    def _calculate_exit_code(self, report: Dict[str, Any]) -> int:
        """Calculate appropriate exit code from test results."""
        summary = report.get("summary", {})
        failed_count = summary.get("failed", 0)
        return 0 if failed_count == 0 else 1

    def get_test_categories(self) -> list:
        """Get list of available test categories."""
        return [
            "cold_start",
            "concurrent_startup", 
            "tier_startup",
            "agent_isolation",
            "routing_startup",
            "performance_startup"
        ]

    def supports_real_llm(self) -> bool:
        """Check if agent startup tests support real LLM testing."""
        return True

    def get_estimated_duration(self, real_llm: bool = False) -> int:
        """Get estimated test duration in seconds."""
        return 120 if real_llm else 60  # 2 minutes with real LLM, 1 minute without


def create_integration_handler() -> AgentStartupIntegration:
    """Create agent startup integration handler."""
    return AgentStartupIntegration()


async def run_startup_tests_integration(real_llm: bool = False, parallel: bool = True) -> int:
    """Main integration function for test runner."""
    handler = create_integration_handler()
    return await handler.run_agent_startup_suite(real_llm, parallel)


def get_startup_test_info() -> Dict[str, Any]:
    """Get information about agent startup tests for discovery."""
    return {
        "name": "Agent Startup E2E Tests",
        "description": "Comprehensive agent initialization and startup validation",
        "categories": [
            {
                "name": "cold_start",
                "description": "Agent cold start from zero state",
                "requires_real_llm": True,
                "timeout": 30
            },
            {
                "name": "concurrent_startup",
                "description": "Concurrent agent startup isolation",
                "requires_real_llm": True,
                "timeout": 45
            },
            {
                "name": "tier_startup",
                "description": "Startup across different user tiers",
                "requires_real_llm": True,
                "timeout": 60
            },
            {
                "name": "agent_isolation",
                "description": "Agent state isolation validation", 
                "requires_real_llm": False,
                "timeout": 30
            },
            {
                "name": "routing_startup",
                "description": "Message routing during startup",
                "requires_real_llm": False,
                "timeout": 25
            },
            {
                "name": "performance_startup",
                "description": "Performance metrics under startup load",
                "requires_real_llm": True,
                "timeout": 40
            }
        ],
        "supports_real_llm": True,
        "supports_parallel": True,
        "estimated_duration": 120,  # seconds
        "business_value": "Protects 500K+ ARR by ensuring reliable agent startup"
    }


# Command-line interface for direct execution
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Agent Startup Test Integration")
    parser.add_argument("--real-llm", action="store_true", help="Enable real LLM testing")
    parser.add_argument("--no-parallel", action="store_true", help="Disable parallel execution") 
    parser.add_argument("--info", action="store_true", help="Show test information")
    
    args = parser.parse_args()
    
    if args.info:
        info = get_startup_test_info()
        print("Agent Startup Test Information:")
        print(f"Name: {info['name']}")
        print(f"Description: {info['description']}")
        print(f"Business Value: {info['business_value']}")
        print(f"Supports Real LLM: {info['supports_real_llm']}")
        print(f"Estimated Duration: {info['estimated_duration']}s")
        print(f"Categories: {len(info['categories'])} test categories available")
        sys.exit(0)

    async def main():
        """Main async execution for integration."""
        exit_code = await run_startup_tests_integration(
            real_llm=args.real_llm,
            parallel=not args.no_parallel
        )
        sys.exit(exit_code)

    asyncio.run(main())
