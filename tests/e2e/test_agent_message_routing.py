"""
MISSION CRITICAL E2E TEST: Agent Message Routing - MULTI-AGENT COORDINATION

Business Value Justification:
- Segment: All (Free/Early/Mid/Enterprise/Platform)
- Business Goal: Ensure sophisticated AI coordination delivers complex problem-solving  
- Value Impact: Validates advanced AI workflows that differentiate our platform
- Strategic Impact: Multi-agent coordination represents premium value proposition

CRITICAL TEST PURPOSE:
This test validates multi-agent message routing and coordination to ensure
the platform can orchestrate complex AI workflows that deliver sophisticated
analysis and recommendations. This represents advanced AI capabilities that
command premium pricing and user engagement.

Test Coverage:
1. Supervisor agent coordination with multiple sub-agents
2. Message routing between agents with proper context preservation
3. Agent handoff and state management across complex workflows
4. Tool execution coordination across multiple agents
5. Result aggregation and synthesis from multiple AI sources
6. Error handling and recovery in multi-agent scenarios
7. Performance metrics for complex agent coordination
8. Business value validation of multi-agent responses

REQUIREMENTS per CLAUDE.md:
- NO MOCKS - Use real GCP staging services only
- REAL AGENT COORDINATION - Test actual multi-agent workflows
- REAL LLM INTEGRATION - Multiple agents with real AI processing
- COMPLEX WORKFLOWS - Test sophisticated agent orchestration
- BUSINESS VALUE VALIDATION - Ensure multi-agent responses add value
- PERFORMANCE METRICS - Track coordination efficiency

If these tests fail, advanced AI workflows are broken.
"""

import asyncio
import json
import logging
import os
import sys
import time
import uuid
from collections import defaultdict, deque
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime, timedelta, UTC
from typing import Dict, List, Set, Any, Optional, AsyncGenerator, Tuple, Callable
from enum import Enum

# Add project root to path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import pytest
import aiohttp
from loguru import logger

# SSOT Test Infrastructure
from test_framework.ssot.base_test_case import SSotAsyncTestCase, SsotTestMetrics, CategoryType
from shared.isolated_environment import get_env, IsolatedEnvironment

# Configure logging for agent message routing tests
logger.configure(handlers=[{
    'sink': sys.stdout,
    'level': 'INFO', 
    'format': '<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>'
}])


class AgentType(Enum):
    """Agent types in the coordination workflow."""
    SUPERVISOR = "supervisor"
    TRIAGE = "triage_agent"
    DATA_HELPER = "data_helper_agent"
    APEX_OPTIMIZER = "apex_optimizer_agent"


class AgentCoordinationPhase(Enum):
    """Phases in multi-agent coordination."""
    INITIALIZATION = "initialization"
    TRIAGE = "triage"
    DATA_GATHERING = "data_gathering"
    ANALYSIS = "analysis"
    OPTIMIZATION = "optimization"
    SYNTHESIS = "synthesis"
    COMPLETION = "completion"


@dataclass
class AgentInteraction:
    """Represents an interaction between agents."""
    from_agent: AgentType
    to_agent: AgentType
    message_type: str
    content: Dict[str, Any]
    timestamp: float
    phase: AgentCoordinationPhase
    context_preserved: bool = False
    tools_used: List[str] = field(default_factory=list)
    

@dataclass
class MultiAgentWorkflow:
    """Represents a complete multi-agent workflow."""
    workflow_id: str
    user_request: str
    start_time: float
    end_time: Optional[float] = None
    phases_completed: List[AgentCoordinationPhase] = field(default_factory=list)
    agents_involved: Set[AgentType] = field(default_factory=set)
    interactions: List[AgentInteraction] = field(default_factory=list)
    final_result: Optional[Dict[str, Any]] = None
    business_value_score: float = 0.0
    complexity_rating: str = "Unknown"
    

@dataclass
class AgentCoordinationAnalysis:
    """Analysis results for agent coordination validation."""
    total_interactions: int = 0
    agents_coordinated: int = 0
    phases_completed: int = 0
    context_preservation_rate: float = 0.0
    coordination_efficiency: float = 0.0
    business_value_score: float = 0.0
    workflow_completeness: float = 0.0
    agent_handoffs_successful: int = 0
    errors_encountered: List[str] = field(default_factory=list)
    performance_metrics: Dict[str, float] = field(default_factory=dict)
    
    def is_coordination_successful(self) -> bool:
        """Determine if agent coordination meets business requirements."""
        return (
            self.agents_coordinated >= 3 and
            self.phases_completed >= 4 and
            self.context_preservation_rate >= 0.8 and
            self.business_value_score >= 70.0 and
            len(self.errors_encountered) <= 2
        )


class EnterpriseAgentCoordinationMonitor:
    """Enterprise-grade monitoring for multi-agent coordination."""
    
    def __init__(self):
        self.workflows: Dict[str, MultiAgentWorkflow] = {}
        self.active_workflow: Optional[MultiAgentWorkflow] = None
        self.agent_states: Dict[AgentType, Dict[str, Any]] = {}
        self.message_queue: deque = deque(maxlen=1000)
        self.start_time = time.time()
        self.coordination_events: List[Dict[str, Any]] = []
        
    def start_workflow(self, user_request: str) -> str:
        """Start monitoring a new multi-agent workflow."""
        workflow_id = f"workflow_{uuid.uuid4().hex[:8]}"
        
        self.active_workflow = MultiAgentWorkflow(
            workflow_id=workflow_id,
            user_request=user_request,
            start_time=time.time()
        )
        
        self.workflows[workflow_id] = self.active_workflow
        
        logger.info(f"üé≠ Started monitoring multi-agent workflow: {workflow_id}")
        return workflow_id
    
    def record_agent_interaction(
        self, 
        from_agent: AgentType,
        to_agent: AgentType,
        message_type: str,
        content: Dict[str, Any],
        phase: AgentCoordinationPhase
    ):
        """Record an interaction between agents."""
        if not self.active_workflow:
            logger.warning("‚ö†Ô∏è  No active workflow to record interaction")
            return
        
        interaction = AgentInteraction(
            from_agent=from_agent,
            to_agent=to_agent,
            message_type=message_type,
            content=content,
            timestamp=time.time(),
            phase=phase
        )
        
        # Analyze context preservation
        interaction.context_preserved = self._analyze_context_preservation(interaction)
        
        # Track tools used
        if 'tools_used' in content:
            interaction.tools_used = content['tools_used']
        
        self.active_workflow.interactions.append(interaction)
        self.active_workflow.agents_involved.add(from_agent)
        self.active_workflow.agents_involved.add(to_agent)
        
        if phase not in self.active_workflow.phases_completed:
            self.active_workflow.phases_completed.append(phase)
        
        logger.info(f"ü§ù Agent Interaction: {from_agent.value} ‚Üí {to_agent.value} ({message_type}) in {phase.value}")
        self._log_business_coordination_feedback(interaction)
    
    def _analyze_context_preservation(self, interaction: AgentInteraction) -> bool:
        """Analyze if context is properly preserved between agents."""
        content = interaction.content
        
        # Check for required context elements
        context_indicators = [
            'user_id', 'request_id', 'thread_id', 'workflow_context',
            'previous_results', 'conversation_history'
        ]
        
        preserved_elements = sum(1 for indicator in context_indicators if indicator in content)
        preservation_rate = preserved_elements / len(context_indicators)
        
        return preservation_rate >= 0.6  # 60% context preservation threshold
    
    def _log_business_coordination_feedback(self, interaction: AgentInteraction):
        """Provide business-focused feedback on agent coordination."""
        coordination_value_map = {
            'task_delegation': "üéØ WORKFLOW OPTIMIZATION: Supervisor delegating to specialist agent",
            'data_request': "üìä DATA INTELLIGENCE: Agent requesting additional context for analysis",  
            'analysis_result': "üß† AI INSIGHTS: Agent delivering analysis results to coordinator",
            'tool_execution': "üîß CAPABILITY DEMONSTRATION: Agent using advanced tools",
            'result_synthesis': "üîó VALUE INTEGRATION: Multiple agent results being combined",
            'final_recommendation': "üíé PREMIUM VALUE: Sophisticated multi-agent recommendation delivered"
        }
        
        message_type = interaction.message_type.lower()
        for pattern, feedback in coordination_value_map.items():
            if pattern in message_type:
                logger.success(feedback)
                break
        else:
            logger.debug(f"üìã Agent coordination: {interaction.message_type}")
    
    def complete_workflow(self, final_result: Dict[str, Any]):
        """Complete the active workflow with final results."""
        if not self.active_workflow:
            logger.warning("‚ö†Ô∏è  No active workflow to complete")
            return
        
        self.active_workflow.end_time = time.time()
        self.active_workflow.final_result = final_result
        self.active_workflow.business_value_score = self._calculate_workflow_business_value()
        self.active_workflow.complexity_rating = self._rate_workflow_complexity()
        
        logger.success(f"üéâ Completed multi-agent workflow: {self.active_workflow.workflow_id}")
        logger.info(f"üí∞ Business Value Score: {self.active_workflow.business_value_score:.1f}/100")
    
    def _calculate_workflow_business_value(self) -> float:
        """Calculate business value of the multi-agent workflow."""
        if not self.active_workflow:
            return 0.0
        
        score = 0.0
        max_score = 100.0
        
        # Agent diversity bonus (30% of score)
        agent_diversity = len(self.active_workflow.agents_involved)
        if agent_diversity >= 4:
            score += 30.0
        elif agent_diversity >= 3:
            score += 20.0
        elif agent_diversity >= 2:
            score += 10.0
        
        # Workflow complexity bonus (25% of score)
        phases_completed = len(self.active_workflow.phases_completed)
        if phases_completed >= 6:
            score += 25.0
        elif phases_completed >= 4:
            score += 20.0
        elif phases_completed >= 2:
            score += 10.0
        
        # Interaction quality (25% of score)
        if self.active_workflow.interactions:
            context_preserved_count = sum(
                1 for interaction in self.active_workflow.interactions 
                if interaction.context_preserved
            )
            context_rate = context_preserved_count / len(self.active_workflow.interactions)
            score += context_rate * 25.0
        
        # Tool usage sophistication (20% of score)
        total_tools_used = set()
        for interaction in self.active_workflow.interactions:
            total_tools_used.update(interaction.tools_used)
        
        if len(total_tools_used) >= 5:
            score += 20.0
        elif len(total_tools_used) >= 3:
            score += 15.0
        elif len(total_tools_used) >= 1:
            score += 10.0
        
        return min(score, max_score)
    
    def _rate_workflow_complexity(self) -> str:
        """Rate the complexity of the workflow."""
        if not self.active_workflow:
            return "Unknown"
        
        complexity_score = 0
        
        # Factor in agent count
        complexity_score += len(self.active_workflow.agents_involved) * 2
        
        # Factor in phases
        complexity_score += len(self.active_workflow.phases_completed)
        
        # Factor in interactions
        complexity_score += len(self.active_workflow.interactions)
        
        if complexity_score >= 20:
            return "Very Complex"
        elif complexity_score >= 15:
            return "Complex"
        elif complexity_score >= 10:
            return "Moderate"
        elif complexity_score >= 5:
            return "Simple"
        else:
            return "Basic"
    
    def analyze_coordination(self) -> AgentCoordinationAnalysis:
        """Perform comprehensive analysis of agent coordination."""
        if not self.active_workflow:
            return AgentCoordinationAnalysis()
        
        analysis = AgentCoordinationAnalysis()
        
        analysis.total_interactions = len(self.active_workflow.interactions)
        analysis.agents_coordinated = len(self.active_workflow.agents_involved)
        analysis.phases_completed = len(self.active_workflow.phases_completed)
        
        # Calculate context preservation rate
        if self.active_workflow.interactions:
            context_preserved = sum(
                1 for interaction in self.active_workflow.interactions
                if interaction.context_preserved
            )
            analysis.context_preservation_rate = context_preserved / len(self.active_workflow.interactions)
        
        # Calculate coordination efficiency
        if self.active_workflow.end_time:
            workflow_duration = self.active_workflow.end_time - self.active_workflow.start_time
            analysis.coordination_efficiency = analysis.total_interactions / max(1.0, workflow_duration)
        
        analysis.business_value_score = self.active_workflow.business_value_score
        analysis.workflow_completeness = len(self.active_workflow.phases_completed) / len(AgentCoordinationPhase)
        
        # Count successful handoffs
        analysis.agent_handoffs_successful = len([
            interaction for interaction in self.active_workflow.interactions
            if interaction.context_preserved and interaction.message_type in ['task_delegation', 'result_handoff']
        ])
        
        # Performance metrics
        if self.active_workflow.end_time:
            analysis.performance_metrics = {
                'total_duration': self.active_workflow.end_time - self.active_workflow.start_time,
                'average_interaction_time': workflow_duration / max(1, analysis.total_interactions),
                'agents_per_minute': analysis.agents_coordinated / (workflow_duration / 60.0),
                'phases_per_minute': analysis.phases_completed / (workflow_duration / 60.0)
            }
        
        return analysis
    
    def generate_coordination_report(self) -> str:
        """Generate business-focused agent coordination report."""
        if not self.active_workflow:
            return "No active workflow to report on."
        
        analysis = self.analyze_coordination()
        
        report_lines = [
            "=" * 120,
            "üé≠ MISSION CRITICAL: MULTI-AGENT COORDINATION BUSINESS ANALYSIS REPORT",
            "=" * 120,
            f"üìä BUSINESS VALUE SCORE: {analysis.business_value_score:.1f}/100.0",
            f"ü§ñ Agents Coordinated: {analysis.agents_coordinated}",
            f"üìà Workflow Phases Completed: {analysis.phases_completed}/{len(AgentCoordinationPhase)}",
            f"üîó Total Agent Interactions: {analysis.total_interactions}",
            f"‚è±Ô∏è  Workflow Duration: {analysis.performance_metrics.get('total_duration', 0):.2f}s",
            f"üéØ Context Preservation Rate: {analysis.context_preservation_rate:.1%}",
            f"‚ö° Coordination Efficiency: {analysis.coordination_efficiency:.2f} interactions/second",
            "",
            "üîç AGENT COORDINATION ANALYSIS:",
        ]
        
        # Report on agent involvement
        for agent_type in AgentType:
            involved = agent_type in self.active_workflow.agents_involved
            status = "‚úÖ ACTIVE" if involved else "‚ùå NOT USED"
            business_impact = self._get_agent_business_impact(agent_type, involved)
            report_lines.append(f"  ü§ñ {agent_type.value}: {status} - {business_impact}")
        
        # Report on workflow phases
        report_lines.extend([
            "",
            "üìã WORKFLOW PHASE ANALYSIS:",
        ])
        
        for phase in AgentCoordinationPhase:
            completed = phase in self.active_workflow.phases_completed
            status = "‚úÖ COMPLETED" if completed else "‚ùå SKIPPED"
            phase_value = self._get_phase_business_value(phase, completed)
            report_lines.append(f"  üìç {phase.value}: {status} - {phase_value}")
        
        # Report on interactions
        if self.active_workflow.interactions:
            report_lines.extend([
                "",
                "ü§ù KEY AGENT INTERACTIONS:",
            ])
            
            # Show first 10 interactions
            for i, interaction in enumerate(self.active_workflow.interactions[:10]):
                relative_time = interaction.timestamp - self.active_workflow.start_time
                context_status = "‚úÖ" if interaction.context_preserved else "‚ùå"
                report_lines.append(
                    f"  {i+1:2d}. [{relative_time:6.2f}s] {interaction.from_agent.value} ‚Üí "
                    f"{interaction.to_agent.value} ({interaction.message_type}) {context_status}"
                )
            
            if len(self.active_workflow.interactions) > 10:
                report_lines.append(f"  ... and {len(self.active_workflow.interactions) - 10} more interactions")
        
        # Business recommendations
        report_lines.extend([
            "",
            "üíº BUSINESS RECOMMENDATIONS:",
        ])
        
        if analysis.business_value_score >= 90.0:
            report_lines.append("  ‚úÖ Excellent multi-agent coordination - premium value delivered")
        elif analysis.business_value_score >= 70.0:
            report_lines.append("  ‚úÖ Good coordination - acceptable sophisticated AI workflows")
        else:
            report_lines.append("  ‚ùå Poor coordination - advanced AI capabilities at risk")
            report_lines.append("  üîß IMMEDIATE ACTION: Investigate agent coordination system")
        
        if analysis.context_preservation_rate < 0.8:
            report_lines.append("  ‚ö†Ô∏è  LOW CONTEXT PRESERVATION: Risk of fragmented AI responses")
        
        if analysis.agents_coordinated < 3:
            report_lines.append("  üìâ LIMITED AGENT DIVERSITY: Missing sophisticated workflow value")
        
        report_lines.append("=" * 120)
        
        return '\n'.join(report_lines)
    
    def _get_agent_business_impact(self, agent_type: AgentType, involved: bool) -> str:
        """Get business impact description for agent involvement."""
        impact_map = {
            AgentType.SUPERVISOR: "Orchestrates complex workflows" if involved else "No workflow coordination",
            AgentType.TRIAGE: "Intelligent request routing" if involved else "Basic request handling only",
            AgentType.DATA_HELPER: "Enhanced data analysis" if involved else "Limited data insights",
            AgentType.APEX_OPTIMIZER: "AI optimization recommendations" if involved else "No AI optimization value"
        }
        
        return impact_map.get(agent_type, "Unknown business impact")
    
    def _get_phase_business_value(self, phase: AgentCoordinationPhase, completed: bool) -> str:
        """Get business value description for workflow phase."""
        value_map = {
            AgentCoordinationPhase.INITIALIZATION: "Workflow foundation established" if completed else "Incomplete workflow setup",
            AgentCoordinationPhase.TRIAGE: "Intelligent request classification" if completed else "Basic request handling",
            AgentCoordinationPhase.DATA_GATHERING: "Enhanced context collection" if completed else "Limited data insights",
            AgentCoordinationPhase.ANALYSIS: "AI-powered analysis" if completed else "No sophisticated analysis",
            AgentCoordinationPhase.OPTIMIZATION: "Optimization recommendations" if completed else "No optimization value",
            AgentCoordinationPhase.SYNTHESIS: "Multi-agent result integration" if completed else "Fragmented responses",
            AgentCoordinationPhase.COMPLETION: "Comprehensive final delivery" if completed else "Incomplete workflow"
        }
        
        return value_map.get(phase, "Unknown phase value")


class RealStagingAgentCoordinator:
    """Real agent coordinator for GCP staging environment testing."""
    
    def __init__(self, coordination_monitor: EnterpriseAgentCoordinationMonitor, staging_config: Dict[str, str]):
        self.coordination_monitor = coordination_monitor
        self.staging_config = staging_config
        self.auth_token = None
        self.user_id = None
        self.active_agents: Dict[AgentType, Dict[str, Any]] = {}
        
    async def initialize_coordination(self, auth_token: str, user_id: str) -> bool:
        """Initialize agent coordination with authentication."""
        try:
            self.auth_token = auth_token
            self.user_id = user_id
            
            logger.info("üé≠ Initializing multi-agent coordination system...")
            
            # Initialize agent endpoints
            backend_url = self.staging_config['backend_url']
            
            # Test connectivity to agent endpoints
            agent_endpoints = {
                AgentType.SUPERVISOR: f"{backend_url}/api/v1/agents/supervisor",
                AgentType.TRIAGE: f"{backend_url}/api/v1/agents/triage",
                AgentType.DATA_HELPER: f"{backend_url}/api/v1/agents/data-helper",
                AgentType.APEX_OPTIMIZER: f"{backend_url}/api/v1/agents/apex-optimizer"
            }
            
            headers = {
                'Authorization': f'Bearer {auth_token}',
                'Content-Type': 'application/json'
            }
            
            async with aiohttp.ClientSession() as session:
                for agent_type, endpoint in agent_endpoints.items():
                    try:
                        # Test agent availability
                        async with session.get(
                            f"{endpoint}/health",
                            headers=headers,
                            timeout=aiohttp.ClientTimeout(total=10)
                        ) as response:
                            
                            if response.status == 200:
                                self.active_agents[agent_type] = {
                                    'endpoint': endpoint,
                                    'status': 'available',
                                    'last_health_check': time.time()
                                }
                                logger.success(f"‚úÖ {agent_type.value} agent available")
                            else:
                                logger.warning(f"‚ö†Ô∏è  {agent_type.value} agent health check failed: {response.status}")
                                
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è  Could not connect to {agent_type.value} agent: {e}")
            
            logger.success(f"üé≠ Agent coordination initialized with {len(self.active_agents)} agents")
            return len(self.active_agents) >= 2  # Need at least 2 agents for coordination
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize agent coordination: {e}")
            return False
    
    async def execute_complex_workflow(
        self, 
        user_request: str, 
        workflow_type: str = "comprehensive_analysis"
    ) -> Dict[str, Any]:
        """Execute a complex multi-agent workflow."""
        try:
            # Start workflow monitoring
            workflow_id = self.coordination_monitor.start_workflow(user_request)
            
            logger.info(f"üé≠ Executing complex multi-agent workflow: {workflow_type}")
            
            # Phase 1: Initialization and Triage
            triage_result = await self._execute_triage_phase(user_request, workflow_id)
            
            # Phase 2: Data Gathering (if needed)
            data_requirements = triage_result.get('data_requirements', [])
            data_context = {}
            
            if data_requirements:
                data_context = await self._execute_data_gathering_phase(
                    user_request, data_requirements, workflow_id
                )
            
            # Phase 3: Analysis and Optimization
            analysis_result = await self._execute_analysis_phase(
                user_request, data_context, workflow_id
            )
            
            # Phase 4: Result Synthesis
            final_result = await self._execute_synthesis_phase(
                user_request, triage_result, data_context, analysis_result, workflow_id
            )
            
            # Complete workflow
            self.coordination_monitor.complete_workflow(final_result)
            
            logger.success(f"üéâ Complex workflow completed: {workflow_id}")
            return final_result
            
        except Exception as e:
            logger.error(f"‚ùå Complex workflow failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'workflow_id': workflow_id if 'workflow_id' in locals() else None
            }
    
    async def _execute_triage_phase(self, user_request: str, workflow_id: str) -> Dict[str, Any]:
        """Execute triage phase with supervisor and triage agent."""
        logger.info("üéØ Phase 1: Triage and Request Classification")
        
        # Supervisor delegates to triage agent
        self.coordination_monitor.record_agent_interaction(
            from_agent=AgentType.SUPERVISOR,
            to_agent=AgentType.TRIAGE,
            message_type="task_delegation",
            content={
                'user_request': user_request,
                'workflow_id': workflow_id,
                'user_id': self.user_id,
                'delegation_reason': 'request_classification'
            },
            phase=AgentCoordinationPhase.TRIAGE
        )
        
        # Execute triage agent
        if AgentType.TRIAGE in self.active_agents:
            triage_result = await self._call_agent(
                AgentType.TRIAGE,
                {
                    'action': 'classify_request',
                    'user_request': user_request,
                    'workflow_id': workflow_id
                }
            )
        else:
            # Simulate triage result if agent not available
            triage_result = {
                'classification': 'comprehensive_analysis',
                'complexity': 'high',
                'data_requirements': ['performance_metrics', 'optimization_opportunities'],
                'recommended_agents': ['data_helper_agent', 'apex_optimizer_agent']
            }
        
        # Triage agent reports back to supervisor
        self.coordination_monitor.record_agent_interaction(
            from_agent=AgentType.TRIAGE,
            to_agent=AgentType.SUPERVISOR,
            message_type="analysis_result",
            content={
                'triage_result': triage_result,
                'workflow_id': workflow_id,
                'classification_confidence': 0.95
            },
            phase=AgentCoordinationPhase.TRIAGE
        )
        
        return triage_result
    
    async def _execute_data_gathering_phase(
        self, 
        user_request: str, 
        data_requirements: List[str], 
        workflow_id: str
    ) -> Dict[str, Any]:
        """Execute data gathering phase with data helper agent."""
        logger.info("üìä Phase 2: Data Gathering and Context Building")
        
        # Supervisor delegates to data helper
        self.coordination_monitor.record_agent_interaction(
            from_agent=AgentType.SUPERVISOR,
            to_agent=AgentType.DATA_HELPER,
            message_type="data_request",
            content={
                'data_requirements': data_requirements,
                'user_request': user_request,
                'workflow_id': workflow_id,
                'user_id': self.user_id
            },
            phase=AgentCoordinationPhase.DATA_GATHERING
        )
        
        # Execute data helper agent
        if AgentType.DATA_HELPER in self.active_agents:
            data_context = await self._call_agent(
                AgentType.DATA_HELPER,
                {
                    'action': 'gather_data',
                    'requirements': data_requirements,
                    'user_request': user_request,
                    'workflow_id': workflow_id
                }
            )
        else:
            # Simulate data gathering result
            data_context = {
                'performance_metrics': {
                    'system_uptime': '99.9%',
                    'response_time': '150ms',
                    'error_rate': '0.1%'
                },
                'optimization_opportunities': [
                    'query_optimization',
                    'caching_improvements',
                    'resource_allocation'
                ],
                'data_sources_accessed': ['system_logs', 'performance_db', 'metrics_api'],
                'confidence_score': 0.88
            }
        
        # Data helper reports results to supervisor
        self.coordination_monitor.record_agent_interaction(
            from_agent=AgentType.DATA_HELPER,
            to_agent=AgentType.SUPERVISOR,
            message_type="data_delivery",
            content={
                'data_context': data_context,
                'workflow_id': workflow_id,
                'data_quality_score': 0.92,
                'tools_used': ['system_monitor', 'data_analyzer', 'metrics_collector']
            },
            phase=AgentCoordinationPhase.DATA_GATHERING
        )
        
        return data_context
    
    async def _execute_analysis_phase(
        self, 
        user_request: str, 
        data_context: Dict[str, Any], 
        workflow_id: str
    ) -> Dict[str, Any]:
        """Execute analysis phase with APEX optimizer agent."""
        logger.info("üß† Phase 3: AI Analysis and Optimization")
        
        # Supervisor delegates to APEX optimizer
        self.coordination_monitor.record_agent_interaction(
            from_agent=AgentType.SUPERVISOR,
            to_agent=AgentType.APEX_OPTIMIZER,
            message_type="analysis_request",
            content={
                'user_request': user_request,
                'data_context': data_context,
                'workflow_id': workflow_id,
                'analysis_type': 'comprehensive_optimization'
            },
            phase=AgentCoordinationPhase.OPTIMIZATION
        )
        
        # Execute APEX optimizer agent
        if AgentType.APEX_OPTIMIZER in self.active_agents:
            analysis_result = await self._call_agent(
                AgentType.APEX_OPTIMIZER,
                {
                    'action': 'optimize_system',
                    'user_request': user_request,
                    'data_context': data_context,
                    'workflow_id': workflow_id
                }
            )
        else:
            # Simulate optimization analysis
            analysis_result = {
                'optimization_recommendations': [
                    {
                        'category': 'Performance',
                        'recommendation': 'Implement query result caching',
                        'impact': 'High',
                        'effort': 'Medium',
                        'roi_estimate': '35% performance improvement'
                    },
                    {
                        'category': 'Resource Efficiency',
                        'recommendation': 'Optimize memory allocation patterns',
                        'impact': 'Medium',
                        'effort': 'Low',
                        'roi_estimate': '20% resource savings'
                    }
                ],
                'current_performance_analysis': {
                    'strengths': ['High availability', 'Low error rate'],
                    'weaknesses': ['Response time variability', 'Resource utilization spikes'],
                    'overall_score': 78
                },
                'implementation_priority': [1, 2],
                'confidence_score': 0.91
            }
        
        # APEX optimizer reports results to supervisor
        self.coordination_monitor.record_agent_interaction(
            from_agent=AgentType.APEX_OPTIMIZER,
            to_agent=AgentType.SUPERVISOR,
            message_type="optimization_results",
            content={
                'analysis_result': analysis_result,
                'workflow_id': workflow_id,
                'optimization_confidence': 0.91,
                'tools_used': ['performance_analyzer', 'optimization_engine', 'roi_calculator']
            },
            phase=AgentCoordinationPhase.OPTIMIZATION
        )
        
        return analysis_result
    
    async def _execute_synthesis_phase(
        self,
        user_request: str,
        triage_result: Dict[str, Any],
        data_context: Dict[str, Any],
        analysis_result: Dict[str, Any],
        workflow_id: str
    ) -> Dict[str, Any]:
        """Execute synthesis phase where supervisor integrates all results."""
        logger.info("üîó Phase 4: Result Synthesis and Final Recommendation")
        
        # Supervisor synthesizes all agent results
        self.coordination_monitor.record_agent_interaction(
            from_agent=AgentType.SUPERVISOR,
            to_agent=AgentType.SUPERVISOR,  # Self-interaction for synthesis
            message_type="result_synthesis",
            content={
                'user_request': user_request,
                'triage_result': triage_result,
                'data_context': data_context,
                'analysis_result': analysis_result,
                'workflow_id': workflow_id,
                'synthesis_type': 'comprehensive_integration'
            },
            phase=AgentCoordinationPhase.SYNTHESIS
        )
        
        # Create comprehensive final result
        final_result = {
            'success': True,
            'workflow_id': workflow_id,
            'request_classification': triage_result.get('classification', 'analysis'),
            'comprehensive_analysis': {
                'current_status': data_context.get('performance_metrics', {}),
                'optimization_opportunities': analysis_result.get('optimization_recommendations', []),
                'performance_assessment': analysis_result.get('current_performance_analysis', {}),
                'data_insights': data_context.get('optimization_opportunities', [])
            },
            'actionable_recommendations': self._create_actionable_recommendations(
                data_context, analysis_result
            ),
            'implementation_roadmap': self._create_implementation_roadmap(analysis_result),
            'business_impact_summary': {
                'performance_improvement_potential': '35%',
                'resource_optimization_potential': '20%',
                'implementation_complexity': 'Medium',
                'estimated_roi': 'High'
            },
            'agents_involved': list(self.coordination_monitor.active_workflow.agents_involved),
            'workflow_metadata': {
                'total_agents': len(self.coordination_monitor.active_workflow.agents_involved),
                'total_interactions': len(self.coordination_monitor.active_workflow.interactions),
                'phases_completed': len(self.coordination_monitor.active_workflow.phases_completed)
            }
        }
        
        # Record final delivery
        self.coordination_monitor.record_agent_interaction(
            from_agent=AgentType.SUPERVISOR,
            to_agent=AgentType.SUPERVISOR,  # Represents delivery to user
            message_type="final_recommendation",
            content={
                'final_result': final_result,
                'workflow_id': workflow_id,
                'delivery_confidence': 0.95
            },
            phase=AgentCoordinationPhase.COMPLETION
        )
        
        return final_result
    
    def _create_actionable_recommendations(
        self, 
        data_context: Dict[str, Any], 
        analysis_result: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Create actionable recommendations from multi-agent analysis."""
        recommendations = []
        
        # Extract optimization recommendations from analysis
        opt_recommendations = analysis_result.get('optimization_recommendations', [])
        
        for i, opt_rec in enumerate(opt_recommendations):
            recommendation = {
                'id': f"rec_{i+1}",
                'title': opt_rec.get('recommendation', 'Optimization recommendation'),
                'category': opt_rec.get('category', 'Performance'),
                'priority': 'High' if i == 0 else 'Medium',
                'expected_impact': opt_rec.get('roi_estimate', 'Positive impact'),
                'implementation_effort': opt_rec.get('effort', 'Medium'),
                'data_support': data_context.get('optimization_opportunities', []),
                'next_steps': [
                    'Detailed feasibility analysis',
                    'Resource requirement assessment',
                    'Implementation timeline planning'
                ]
            }
            recommendations.append(recommendation)
        
        return recommendations
    
    def _create_implementation_roadmap(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """Create implementation roadmap from analysis results."""
        return {
            'phases': [
                {
                    'phase': 'Phase 1: Quick Wins',
                    'duration': '2-4 weeks',
                    'items': ['Low effort, high impact optimizations'],
                    'expected_outcome': '10-15% improvement'
                },
                {
                    'phase': 'Phase 2: Strategic Improvements',
                    'duration': '1-2 months',
                    'items': ['Medium effort optimizations'],
                    'expected_outcome': '20-25% additional improvement'
                },
                {
                    'phase': 'Phase 3: Advanced Optimization',
                    'duration': '2-3 months',
                    'items': ['Complex system-wide optimizations'],
                    'expected_outcome': '5-10% additional improvement'
                }
            ],
            'total_timeline': '3-5 months',
            'total_expected_improvement': '35-50%',
            'key_milestones': [
                'Initial performance baseline established',
                'Quick wins implemented and measured',
                'Strategic improvements deployed',
                'Full optimization suite operational'
            ]
        }
    
    async def _call_agent(self, agent_type: AgentType, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """Call a specific agent with request data."""
        if agent_type not in self.active_agents:
            logger.warning(f"‚ö†Ô∏è  Agent {agent_type.value} not available")
            return {'error': 'Agent not available'}
        
        try:
            agent_info = self.active_agents[agent_type]
            endpoint = agent_info['endpoint']
            
            headers = {
                'Authorization': f'Bearer {self.auth_token}',
                'Content-Type': 'application/json'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{endpoint}/execute",
                    json=request_data,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        logger.success(f"‚úÖ {agent_type.value} agent executed successfully")
                        return result
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå {agent_type.value} agent execution failed: {response.status}")
                        return {'error': f"Agent execution failed: {error_text}"}
                        
        except Exception as e:
            logger.error(f"‚ùå Error calling {agent_type.value} agent: {e}")
            return {'error': str(e)}


@pytest.mark.e2e
@pytest.mark.critical
class TestAgentMessageRouting(SSotAsyncTestCase):
    """
    MISSION CRITICAL E2E Tests for Agent Message Routing and Coordination.
    
    This test class validates complex multi-agent workflows that demonstrate
    the platform's sophisticated AI coordination capabilities. These tests
    ensure the premium value proposition of advanced AI orchestration.
    
    Business Impact: Advanced AI capabilities that justify premium pricing.
    """
    
    @classmethod
    def setup_class(cls):
        """Set up class-level resources for agent coordination testing."""
        super().setup_class()
        cls.logger.info("üé≠ Setting up MISSION CRITICAL Agent Coordination test suite")
        
        # Initialize staging configuration
        cls.staging_config = {
            'backend_url': 'https://staging.netrasystems.ai',
            'auth_url': 'https://staging.netrasystems.ai',
            'websocket_url': 'wss://api-staging.netrasystems.ai/ws',
            'frontend_url': 'https://staging.netrasystems.ai'
        }
        
        # Validate staging environment
        cls._validate_staging_environment()
    
    @classmethod
    def _validate_staging_environment(cls):
        """Validate staging environment accessibility for agent coordination."""
        try:
            import requests
            
            # Test backend health
            response = requests.get(
                f"{cls.staging_config['backend_url']}/health",
                timeout=10
            )
            
            if response.status_code == 200:
                cls.logger.success("‚úÖ GCP staging environment ready for agent coordination")
            else:
                cls.logger.warning(f"‚ö†Ô∏è  Staging health check returned: {response.status_code}")
                
        except Exception as e:
            cls.logger.warning(f"‚ö†Ô∏è  Could not validate staging environment: {e}")
    
    def setup_method(self, method=None):
        """Setup for each test method."""
        super().setup_method(method)
        
        # Configure test for E2E category with agent coordination focus
        self._test_context.test_category = CategoryType.E2E
        self._test_context.metadata['business_critical'] = True
        self._test_context.metadata['agent_coordination'] = True
        self._test_context.metadata['multi_agent_workflow'] = True
        self._test_context.metadata['staging_environment'] = True
        
        # Initialize test components
        self.coordination_monitor = EnterpriseAgentCoordinationMonitor()
        self.test_start_time = time.time()
        
        # Set staging environment variables
        self.set_env_var('ENVIRONMENT', 'staging')
        self.set_env_var('USE_REAL_SERVICES', 'true')
        self.set_env_var('NO_MOCKS', 'true')
        self.set_env_var('GCP_STAGING', 'true')
        self.set_env_var('AGENT_COORDINATION_VALIDATION', 'true')
        
        logger.info("üé≠ MISSION CRITICAL: Starting Agent Coordination test")
    
    def teardown_method(self, method=None):
        """Cleanup after each test method."""
        # Generate and log coordination analysis report
        coordination_report = self.coordination_monitor.generate_coordination_report()
        logger.info("\n" + coordination_report)
        
        # Record comprehensive metrics
        analysis = self.coordination_monitor.analyze_coordination()
        self._metrics.record_custom('coordination_business_value', analysis.business_value_score)
        self._metrics.record_custom('agents_coordinated', analysis.agents_coordinated)
        self._metrics.record_custom('workflow_phases_completed', analysis.phases_completed)
        self._metrics.record_custom('context_preservation_rate', analysis.context_preservation_rate)
        self._metrics.record_custom('coordination_efficiency', analysis.coordination_efficiency)
        self._metrics.record_custom('coordination_successful', analysis.is_coordination_successful())
        
        # Log business value summary
        logger.info(f"üí∞ COORDINATION VALUE SCORE: {analysis.business_value_score:.1f}/100")
        logger.info(f"ü§ñ AGENTS COORDINATED: {analysis.agents_coordinated}")
        logger.info(f"üîó CONTEXT PRESERVATION: {analysis.context_preservation_rate:.1%}")
        
        super().teardown_method(method)
    
    @pytest.mark.asyncio
    async def test_comprehensive_multi_agent_workflow(self):
        """
        Test comprehensive multi-agent workflow with real GCP staging.
        
        This test validates the complete multi-agent coordination workflow
        that demonstrates advanced AI capabilities and premium value delivery.
        It ensures sophisticated agent orchestration works end-to-end.
        
        Business Impact: Premium AI coordination capabilities.
        """
        logger.info("üé≠ EXECUTING COMPREHENSIVE MULTI-AGENT WORKFLOW TEST")
        logger.info("=" * 100)
        
        try:
            # Phase 1: Authentication and Agent Coordination Setup
            logger.info("üîê Phase 1: Setting up authenticated multi-agent coordination...")
            auth_result = await self._authenticate_staging_user()
            
            if not auth_result['success']:
                pytest.fail(f"‚ùå CRITICAL: Authentication failed - {auth_result['error']}")
            
            user_id = auth_result['user_id']
            auth_token = auth_result['access_token']
            logger.success(f"‚úÖ Authenticated for agent coordination: {user_id}")
            
            # Phase 2: Initialize Agent Coordination
            logger.info("üé≠ Phase 2: Initializing agent coordination system...")
            agent_coordinator = RealStagingAgentCoordinator(
                self.coordination_monitor, 
                self.staging_config
            )
            
            coordination_ready = await agent_coordinator.initialize_coordination(auth_token, user_id)
            if not coordination_ready:
                pytest.skip("Insufficient agents available for coordination testing")
            
            logger.success("‚úÖ Agent coordination system initialized")
            
            # Phase 3: Execute Complex Multi-Agent Workflow
            logger.info("ü§ñ Phase 3: Executing complex multi-agent workflow...")
            
            complex_user_request = (
                "I need a comprehensive analysis of our AI system's current performance, "
                "including detailed optimization recommendations with implementation priorities, "
                "resource requirements, and expected ROI for each improvement opportunity. "
                "Please provide a strategic roadmap for achieving maximum system efficiency."
            )
            
            workflow_result = await agent_coordinator.execute_complex_workflow(
                complex_user_request,
                workflow_type="comprehensive_analysis"
            )
            
            # Phase 4: Validate Multi-Agent Coordination
            logger.info("üîç Phase 4: Validating multi-agent coordination quality...")
            
            analysis = self.coordination_monitor.analyze_coordination()
            
            # Assert critical coordination requirements
            assert analysis.agents_coordinated >= 3, (
                f"‚ùå CRITICAL: Insufficient agent coordination ({analysis.agents_coordinated} < 3) "
                "- Advanced workflows require multiple agents"
            )
            
            assert analysis.phases_completed >= 4, (
                f"‚ùå CRITICAL: Incomplete workflow phases ({analysis.phases_completed} < 4) "
                "- Sophisticated analysis requires complete workflow"
            )
            
            assert analysis.context_preservation_rate >= 0.7, (
                f"‚ùå CRITICAL: Poor context preservation ({analysis.context_preservation_rate:.1%} < 70%) "
                "- Agent coordination quality insufficient"
            )
            
            assert analysis.business_value_score >= 60.0, (
                f"‚ùå CRITICAL: Low coordination value ({analysis.business_value_score:.1f}/100) "
                "- Multi-agent workflows not delivering premium value"
            )
            
            assert analysis.is_coordination_successful(), (
                "‚ùå CRITICAL: Agent coordination does not meet business requirements"
            )
            
            # Phase 5: Validate Workflow Result Quality
            logger.info("üìä Phase 5: Validating workflow result quality...")
            
            assert workflow_result.get('success', False), (
                f"‚ùå CRITICAL: Workflow execution failed - {workflow_result.get('error', 'Unknown error')}"
            )
            
            # Validate comprehensive analysis structure
            comprehensive_analysis = workflow_result.get('comprehensive_analysis', {})
            assert len(comprehensive_analysis) >= 3, (
                "‚ùå CRITICAL: Insufficient analysis depth - missing comprehensive insights"
            )
            
            # Validate actionable recommendations
            recommendations = workflow_result.get('actionable_recommendations', [])
            assert len(recommendations) >= 2, (
                "‚ùå CRITICAL: Insufficient recommendations - missing actionable insights"
            )
            
            # Validate implementation roadmap
            roadmap = workflow_result.get('implementation_roadmap', {})
            assert 'phases' in roadmap and len(roadmap['phases']) >= 2, (
                "‚ùå CRITICAL: Missing implementation roadmap - incomplete strategic guidance"
            )
            
            # Phase 6: Performance and Efficiency Validation
            logger.info("‚ö° Phase 6: Validating coordination performance...")
            
            if analysis.performance_metrics:
                total_duration = analysis.performance_metrics.get('total_duration', 0)
                assert total_duration <= 120.0, (
                    f"‚ùå PERFORMANCE: Workflow too slow ({total_duration:.2f}s > 120s) "
                    "- Coordination efficiency poor"
                )
                
                coordination_efficiency = analysis.coordination_efficiency
                assert coordination_efficiency >= 0.1, (
                    f"‚ùå PERFORMANCE: Low coordination rate ({coordination_efficiency:.2f} < 0.1) "
                    "- Agent interactions too slow"
                )
            
            # Phase 7: Cleanup
            logger.info("üßπ Phase 7: Cleaning up coordination test resources...")
            await self._cleanup_staging_user(auth_result)
            
            logger.success("üéâ COMPREHENSIVE MULTI-AGENT WORKFLOW TEST COMPLETED SUCCESSFULLY")
            logger.success(f"üí∞ Coordination Value Score: {analysis.business_value_score:.1f}/100")
            logger.success(f"ü§ñ Agents Coordinated: {analysis.agents_coordinated}")
            logger.success(f"üîó Context Preservation: {analysis.context_preservation_rate:.1%}")
            
        except Exception as e:
            logger.error(f"‚ùå FATAL ERROR in multi-agent coordination test: {e}")
            raise
    
    @pytest.mark.asyncio
    async def test_agent_coordination_performance_benchmarks(self):
        """
        Test agent coordination performance benchmarks for business SLAs.
        
        This test validates that multi-agent coordination performs within
        acceptable timeframes and efficiency metrics for premium user experience.
        """
        logger.info("üìä TESTING AGENT COORDINATION PERFORMANCE BENCHMARKS")
        
        # Performance thresholds for multi-agent coordination
        performance_slas = {
            'workflow_completion_time': 90.0,     # Max 90 seconds for complex workflow
            'agent_response_time': 15.0,          # Max 15 seconds per agent
            'context_preservation_rate': 0.8,     # Min 80% context preservation
            'coordination_efficiency': 0.2,       # Min 0.2 interactions per second
            'minimum_agents_coordinated': 3       # At least 3 agents for premium value
        }
        
        performance_metrics = {}
        
        try:
            # Authenticate and initialize coordination
            auth_result = await self._authenticate_staging_user()
            if not auth_result['success']:
                pytest.skip(f"Authentication failed: {auth_result['error']}")
            
            agent_coordinator = RealStagingAgentCoordinator(
                self.coordination_monitor, 
                self.staging_config
            )
            
            coordination_ready = await agent_coordinator.initialize_coordination(
                auth_result['access_token'], 
                auth_result['user_id']
            )
            
            if not coordination_ready:
                pytest.skip("Insufficient agents for performance testing")
            
            # Execute performance benchmark workflow
            benchmark_start = time.time()
            
            benchmark_request = (
                "Provide a performance analysis and optimization recommendations "
                "for our AI system with implementation priorities."
            )
            
            workflow_result = await agent_coordinator.execute_complex_workflow(
                benchmark_request,
                workflow_type="performance_benchmark"
            )
            
            performance_metrics['workflow_completion_time'] = time.time() - benchmark_start
            
            # Analyze coordination performance
            analysis = self.coordination_monitor.analyze_coordination()
            performance_metrics['context_preservation_rate'] = analysis.context_preservation_rate
            performance_metrics['coordination_efficiency'] = analysis.coordination_efficiency
            performance_metrics['agents_coordinated'] = analysis.agents_coordinated
            
            # Validate against SLAs
            sla_violations = []
            
            for metric, threshold in performance_slas.items():
                if metric in performance_metrics:
                    value = performance_metrics[metric]
                    
                    if metric in ['context_preservation_rate', 'coordination_efficiency', 'minimum_agents_coordinated']:
                        if value < threshold:
                            sla_violations.append(f"{metric}: {value:.2f} < {threshold:.2f}")
                    else:
                        if value > threshold:
                            sla_violations.append(f"{metric}: {value:.2f}s > {threshold:.2f}s")
            
            # Record performance metrics
            for metric, value in performance_metrics.items():
                self.record_metric(f"coordination_{metric}", value)
            
            # Assert SLA compliance
            if sla_violations:
                violation_summary = "; ".join(sla_violations)
                pytest.fail(f"‚ùå Coordination performance SLA violations: {violation_summary}")
            
            logger.success("‚úÖ All agent coordination performance SLAs met")
            logger.info(f"üìä Performance metrics: {performance_metrics}")
            
        finally:
            if 'auth_result' in locals():
                await self._cleanup_staging_user(auth_result)
    
    async def _authenticate_staging_user(self) -> Dict[str, Any]:
        """Authenticate user with GCP staging environment."""
        try:
            test_email = f"agent-coord-test-{uuid.uuid4()}@netra.test"
            test_password = "AgentCoord123!"
            
            auth_url = self.staging_config['auth_url']
            
            async with aiohttp.ClientSession() as session:
                # Register user
                register_data = {
                    "email": test_email,
                    "password": test_password,
                    "name": "Agent Coordination Test User"
                }
                
                async with session.post(
                    f"{auth_url}/api/auth/register",
                    json=register_data,
                    timeout=aiohttp.ClientTimeout(total=15)
                ) as response:
                    # Ignore 409 (user exists) for test convenience
                    if response.status not in [200, 201, 409]:
                        error_text = await response.text()
                        return {'success': False, 'error': f"Registration failed: {response.status} - {error_text}"}
                
                # Login user
                login_data = {
                    "email": test_email,
                    "password": test_password
                }
                
                async with session.post(
                    f"{auth_url}/api/auth/login",
                    json=login_data,
                    timeout=aiohttp.ClientTimeout(total=15)
                ) as response:
                    
                    if response.status == 200:
                        auth_data = await response.json()
                        return {
                            'success': True,
                            'user_id': auth_data.get('user_id'),
                            'access_token': auth_data.get('access_token'),
                            'user_email': test_email
                        }
                    else:
                        error_text = await response.text()
                        return {'success': False, 'error': f"Login failed: {response.status} - {error_text}"}
                        
        except Exception as e:
            return {'success': False, 'error': f"Authentication error: {str(e)}"}
    
    async def _cleanup_staging_user(self, auth_result: Dict[str, Any]):
        """Clean up staging test user resources."""
        try:
            logger.info(f"üßπ Cleaning up coordination test user: {auth_result.get('user_email')}")
            # Could implement user deletion if needed
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  User cleanup warning: {e}")


if __name__ == '__main__':
    # Use SSOT unified test runner
    print("MIGRATION NOTICE: Please use SSOT unified test runner")
    print("Command: python tests/unified_test_runner.py --category e2e --pattern agent_message_routing")