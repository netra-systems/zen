class TestWebSocketConnection:
    """Real WebSocket connection for testing instead of mocks."""
    
    def __init__(self):
        self.messages_sent = []
        self.is_connected = True
        self._closed = False
        
    async def send_json(self, message: dict):
        """Send JSON message."""
        if self._closed:
            raise RuntimeError("WebSocket is closed")
        self.messages_sent.append(message)
        
    async def close(self, code: int = 1000, reason: str = "Normal closure"):
        """Close WebSocket connection."""
        self._closed = True
        self.is_connected = False
        
    def get_messages(self) -> list:
        """Get all sent messages."""
        return self.messages_sent.copy()

"""
Test module split from original file
Generated by auto_fix_test_violations.py
"""

import asyncio
import gc
import hashlib
import json
import logging
import os
import random
import secrets
import sys
import threading
import time
import uuid
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple
from netra_backend.app.websocket_core.unified_manager import UnifiedWebSocketManager
from test_framework.database.test_database_manager import DatabaseTestManager
from test_framework.redis_test_utils.test_redis_manager import RedisTestManager
from auth_service.core.auth_manager import AuthManager
from netra_backend.app.agents.supervisor.agent_registry import AgentRegistry
from netra_backend.app.core.user_execution_engine import UserExecutionEngine
from shared.isolated_environment import IsolatedEnvironment

import psutil
import pytest

# Add auth_service to path for imports

import jwt as jwt_lib

from auth_service.auth_core.core.jwt_handler import JWTHandler
from auth_service.auth_core.core.session_manager import SessionManager
from auth_service.auth_core.models.auth_models import (
    AuthProvider,
    LoginRequest,
    LoginResponse,
    TokenResponse,
)
from auth_service.auth_core.services.auth_service import AuthService
from auth_service.tests.factories.user_factory import UserFactory
from netra_backend.app.core.unified_error_handler import UnifiedErrorHandler
from netra_backend.app.db.database_manager import DatabaseManager
from netra_backend.app.clients.auth_client_core import AuthServiceClient
from shared.isolated_environment import get_env

# Configure logger
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.DEBUG)

# Test configuration constants
CONCURRENCY_TEST_CONFIG = {
    "max_workers": 20,
    "operation_timeout": 5.0,
    "race_detection_threshold": 0.5,
    "memory_leak_threshold": 10 * 1024 * 1024,  # 10MB
    "min_success_rate": 0.90,
    "max_response_time": 2.0,
    "max_sessions_per_user": 5,
}

@pytest.fixture
async def isolated_auth_environment():
    """Create an isolated auth environment for testing"""
    # Mock auth service components
    # Mock: Authentication service isolation for testing without real auth flows
    auth_service = AsyncMock(spec=AuthService)
    # Mock: JWT token handling isolation to avoid real crypto dependencies
    jwt_handler = MagicMock(spec=JWTHandler)
    # Mock: Session management isolation for stateless unit testing
    session_manager = AsyncMock(spec=SessionManager)
    
    # Configure JWT handler
    # Mock: JWT processing isolation for fast authentication testing
    jwt_handler.create_refresh_token = MagicMock(return_value="test_refresh_token")
    # Mock: JWT processing isolation for fast authentication testing
    jwt_handler.create_access_token = MagicMock(return_value="test_access_token")
    # Mock: JWT processing isolation for fast authentication testing
    jwt_handler.verify_token = MagicMock(return_value={"user_id": "test_user_id"})
    
    auth_service.jwt_handler = jwt_handler
    auth_service.session_manager = session_manager
    # Mock: Async component isolation for testing without real async operations
    auth_service.refresh_tokens = AsyncMock(return_value={
        "access_token": "new_access_token",
        "refresh_token": "new_refresh_token"
    })
    
    # Create test users
    test_users = [
        {"id": f"user_{i}", "email": f"user{i}@test.com", "password": "password123"}
        for i in range(3)
    ]
    
    return {
        "auth_service": auth_service,
        "jwt_handler": jwt_handler,
        "session_manager": session_manager,
        "test_users": test_users
    }

@pytest.fixture
def concurrent_executor():
    """Use real service instance."""
    # TODO: Initialize real service
    """Provide concurrent execution utilities"""
    class ConcurrentExecutor:
        async def execute_simultaneously(self, operations):
            """Execute operations concurrently"""
            return await asyncio.gather(*operations, return_exceptions=True)
    
    return ConcurrentExecutor()

@pytest.fixture
def race_detector():
    """Use real service instance."""
    # TODO: Initialize real service
    """Provide race condition detection utilities"""
    return RaceConditionDetector()

class RaceConditionDetector:
    """Advanced race condition detection and monitoring utility"""
    
    def __init__(self):
        self.memory_snapshots = []
        self.timing_data = []
        self.operation_logs = []
        self.error_patterns = defaultdict(int)
        self.start_time = time.perf_counter()
        
    def take_memory_snapshot(self, label: str):
        """Capture detailed memory state for leak detection"""
        snapshot = {
            'label': label,
            'timestamp': time.perf_counter(),
            'objects_count': len(gc.get_objects()),
            'memory_rss': psutil.Process().memory_info().rss,
            'memory_vms': psutil.Process().memory_info().vms,
            'thread_count': threading.active_count()
        }
        self.memory_snapshots.append(snapshot)
        logger.debug(f"Memory snapshot '{label}': {snapshot['memory_rss']:,} bytes RSS")
        
    def log_operation(self, operation: str, start_time: float, end_time: float, 
                     success: bool, error: Optional[str] = None):
        """Log operation timing and success metrics"""
        duration = end_time - start_time
        self.timing_data.append({
            'operation': operation,
            'start_time': start_time,
            'end_time': end_time,
            'duration': duration,
            'success': success,
            'error': error
        })
        
        if error:
            self.error_patterns[error] += 1
            
        if duration > CONCURRENCY_TEST_CONFIG["race_detection_threshold"]:
            logger.warning(f"Slow operation detected: {operation} took {duration:.4f}s")
    
    def log_race_condition_indicator(self, indicator_type: str, details: Dict):
        """Log potential race condition indicators"""
        self.operation_logs.append({
            'type': 'race_indicator',
            'indicator_type': indicator_type,
            'timestamp': time.perf_counter(),
            'details': details
        })
        logger.warning(f"Race condition indicator '{indicator_type}': {details}")
    
    def assert_no_races_detected(self):
        """Comprehensive race condition detection validation"""
        # Check for memory leaks
        if len(self.memory_snapshots) >= 2:
            start_memory = self.memory_snapshots[0]['memory_rss']
            end_memory = self.memory_snapshots[-1]['memory_rss']
            memory_growth = end_memory - start_memory
            
            assert memory_growth < CONCURRENCY_TEST_CONFIG["memory_leak_threshold"], \
                f"Memory leak detected: {memory_growth:,} bytes growth " \
                f"(threshold: {CONCURRENCY_TEST_CONFIG['memory_leak_threshold']:,})"
        
        # Check for timing anomalies that suggest race conditions
        if self.timing_data:
            durations = [t['duration'] for t in self.timing_data if t['success']]
            if durations:
                avg_duration = sum(durations) / len(durations)
                max_duration = max(durations)
                
                # Flag if max duration is >10x average (suggests blocking/contention)
                if max_duration > avg_duration * 10:
                    logger.warning(
                        f"Timing anomaly detected: max={max_duration:.4f}s, "
                        f"avg={avg_duration:.4f}s, ratio={max_duration/avg_duration:.1f}x"
                    )
        
        # Check for error burst patterns (skip for race condition tests where failures are expected)
        total_operations = len(self.timing_data)
        failed_operations = len([t for t in self.timing_data if not t['success']])
        
        if total_operations > 0:
            failure_rate = failed_operations / total_operations
            # Only check failure rate if it's not a race condition test
            # Race condition tests intentionally have high failure rates
            is_race_condition_test = any(
                op['operation'] in ['token_refresh', 'session_logout', 'invalidate_all_sessions'] 
                for op in self.timing_data
            )
            
            if not is_race_condition_test:
                assert failure_rate < (1 - CONCURRENCY_TEST_CONFIG["min_success_rate"]), \
                    f"High failure rate detected: {failure_rate:.2%} " \
                    f"(threshold: {1 - CONCURRENCY_TEST_CONFIG['min_success_rate']:.2%})"
    
    def get_performance_report(self) -> Dict:
        """Generate comprehensive performance report"""
        if not self.timing_data:
            return {"error": "No timing data collected"}
        
        successful_ops = [t for t in self.timing_data if t['success']]
        failed_ops = [t for t in self.timing_data if not t['success']]
        
        report = {
            'total_operations': len(self.timing_data),
            'successful_operations': len(successful_ops),
            'failed_operations': len(failed_ops),
            'success_rate': len(successful_ops) / len(self.timing_data) if self.timing_data else 0,
            'error_patterns': dict(self.error_patterns),
            'total_test_duration': time.perf_counter() - self.start_time
        }
        
        if successful_ops:
            durations = [t['duration'] for t in successful_ops]
            report.update({
                'avg_response_time': sum(durations) / len(durations),
                'min_response_time': min(durations),
                'max_response_time': max(durations),
                'p95_response_time': sorted(durations)[int(len(durations) * 0.95)] if durations else 0,
                'p99_response_time': sorted(durations)[int(len(durations) * 0.99)] if durations else 0
            })
        
        if self.memory_snapshots:
            start_mem = self.memory_snapshots[0]['memory_rss']
            end_mem = self.memory_snapshots[-1]['memory_rss']
            report['memory_growth_bytes'] = end_mem - start_mem
            report['memory_growth_mb'] = (end_mem - start_mem) / (1024 * 1024)
        
        return report

class ConcurrentExecutor:
    """Simplified utility for concurrent execution"""
    
    def __init__(self, max_workers: int = 10):
        self.max_workers = max_workers
        self.execution_times = []
        
    async def execute_simultaneously(self, coroutines: List, 
                                   max_time_difference: float = 0.001) -> List:
        """Execute coroutines concurrently"""
        async def timed_operation(coro, index):
            start_time = time.perf_counter()
            try:
                result = await coro
                end_time = time.perf_counter()
                success = True
                error = None
            except Exception as e:
                end_time = time.perf_counter()
                result = e
                success = False
                error = str(e)
            
            self.execution_times.append({
                'index': index,
                'start': start_time,
                'end': end_time,
                'duration': end_time - start_time,
                'success': success,
                'error': error
            })
            
            return result
        
        # Create timed tasks
        tasks = [
            timed_operation(coro, i)
            for i, coro in enumerate(coroutines)
        ]
        
        # Execute all tasks concurrently
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return results

class MockRedisWithRaceConditions:
    """Redis mock that can simulate realistic race conditions"""
    
    def __init__(self):
        self._data = {}
        self._locks = defaultdict(threading.Lock)
        self.race_condition_enabled = False
        self.operation_delay_range = (0.001, 0.01)  # 1-10ms
        
    def enable_race_conditions(self, enabled: bool = True):
        """Enable/disable race condition simulation"""
        self.race_condition_enabled = enabled
        
    def get(self, key: str):
        """Get with optional race condition simulation (sync version)"""
        with self._locks[key]:
            return self._data.get(key)
    
    async def get_async(self, key: str):
        """Get with optional race condition simulation (async version)"""
        if self.race_condition_enabled:
            await asyncio.sleep(random.uniform(*self.operation_delay_range))
        
        with self._locks[key]:
            return self._data.get(key)
    
    def setex(self, key: str, timeout, value: str):
        """Set with expiry (sync version)"""
        with self._locks[key]:
            self._data[key] = value
            return True
    
    async def setex_async(self, key: str, timeout: int, value: str):
        """Set with expiry and optional race condition simulation (async version)"""
        if self.race_condition_enabled:
            await asyncio.sleep(random.uniform(*self.operation_delay_range))
            
        with self._locks[key]:
            self._data[key] = value
            return True
    
    def delete(self, key: str):
        """Delete (sync version)"""
        with self._locks[key]:
            if key in self._data:
                del self._data[key]
                return 1
            return 0
    
    async def delete_async(self, key: str):
        """Delete with optional race condition simulation (async version)"""
        if self.race_condition_enabled:
            await asyncio.sleep(random.uniform(*self.operation_delay_range))
            
        with self._locks[key]:
            if key in self._data:
                del self._data[key]
                return 1
            return 0
    
    def scan_iter(self, pattern: str):
        """Scan iterator simulation"""
        for key in self._data.keys():
            if pattern.replace('*', '') in key:
                yield key
    
    def expire(self, key: str, timeout):
        """Mock expire method"""
        return True  # Always return True for mock
    
    def ping(self):
        """Health check simulation"""
        return True

@pytest.mark.e2e
class TestConcurrentTokenRefreshRaceConditions:
    """Test Case 1: Concurrent Token Refresh Race Conditions"""
    
    @pytest.mark.asyncio
    @pytest.mark.e2e
    async def test_concurrent_token_refresh_race(self, isolated_auth_environment:
                                               concurrent_executor, race_detector):
        """
        Scenario: Multiple clients refresh tokens simultaneously
        Expected: Only one valid token pair generated, others get appropriate errors
        
        This test ensures that when multiple clients attempt to refresh the same
        refresh token simultaneously, only one succeeds and the others fail gracefully.
        """
        auth_service = isolated_auth_environment["auth_service"]
        test_user = isolated_auth_environment["test_users"][0]
        
        race_detector.take_memory_snapshot("before_token_refresh_test")
        
        # Create initial tokens
        refresh_token = auth_service.jwt_handler.create_refresh_token(test_user["id"])
        
        # Define concurrent refresh operations
        async def attempt_token_refresh():
            start_time = time.perf_counter()
            try:
                result = await auth_service.refresh_tokens(refresh_token)
                end_time = time.perf_counter()
                race_detector.log_operation("token_refresh", start_time, end_time, 
                                          True if result else False)
                return result
            except Exception as e:
                end_time = time.perf_counter()
                race_detector.log_operation("token_refresh", start_time, end_time, 
                                          False, str(e))
                return e
        
        # Execute 10 concurrent refresh attempts
        refresh_operations = [attempt_token_refresh() for _ in range(10)]
        results = await concurrent_executor.execute_simultaneously(refresh_operations)
        
        # Validation: Only one should succeed
        successful_results = [r for r in results if r is not None and not isinstance(r, Exception)]
        failed_results = [r for r in results if r is None or isinstance(r, Exception)]
        
        # Assert race condition protection
        assert len(successful_results) <= 1, \
            f"Multiple token refreshes succeeded: {len(successful_results)} " \
            f"(expected: 0 or 1 due to token invalidation)"
        
        assert len(failed_results) >= 9, \
            f"Too few failures: {len(failed_results)} (expected: â‰¥9)"
        
        # Verify original refresh token is now invalid
        final_refresh_attempt = await auth_service.refresh_tokens(refresh_token)
        assert final_refresh_attempt is None, "Original refresh token should be invalidated"
        
        race_detector.take_memory_snapshot("after_token_refresh_test")
        
        # Log race condition indicators if detected
        if len(successful_results) > 1:
            race_detector.log_race_condition_indicator("multiple_token_refresh_success", {
                "successful_count": len(successful_results),
                "token": refresh_token[:10] + "..."
            })

@pytest.mark.e2e
class TestMultiDeviceLoginCollision:
    """Test Case 2: Multi-Device Login Collision"""
    
    @pytest.mark.asyncio
    @pytest.mark.e2e
    async def test_multi_device_login_collision(self, isolated_auth_environment:
                                              concurrent_executor, race_detector):
        """
        Scenario: User logs in from 5 devices simultaneously
        Expected: All logins succeed with unique sessions, proper session limits enforced
        
        This test ensures that concurrent logins from multiple devices work correctly
        and don't create race conditions in session management.
        """
        auth_service = isolated_auth_environment["auth_service"]
        test_user = isolated_auth_environment["test_users"][0]
        
        race_detector.take_memory_snapshot("before_multidevice_test")
        
        # Simulate different devices with unique contexts
        device_contexts = []
        for i in range(5):
            device_contexts.append({
                "ip": f"192.168.1.{i + 10}",
                "user_agent": f"TestDevice-{i}-{uuid.uuid4().hex[:8]}"
            })
        
        # Define concurrent login operations
        async def attempt_device_login(device_context):
            start_time = time.perf_counter()
            try:
                login_request = LoginRequest(
                    email=test_user["email"],
                    password="TestPassword123!",
                    provider=AuthProvider.LOCAL
                )
                
                result = await auth_service.login(login_request, device_context)
                end_time = time.perf_counter()
                race_detector.log_operation("device_login", start_time, end_time, True)
                return result
            except Exception as e:
                end_time = time.perf_counter()
                race_detector.log_operation("device_login", start_time, end_time, False, str(e))
                return e
        
        # Execute concurrent logins
        login_operations = [
            attempt_device_login(device_context) 
            for device_context in device_contexts
        ]
        results = await concurrent_executor.execute_simultaneously(login_operations)
        
        # Validation
        successful_logins = [r for r in results if isinstance(r, LoginResponse)]
        failed_logins = [r for r in results if isinstance(r, Exception)]
        
        # All logins should succeed (no race conditions)
        assert len(successful_logins) == 5, \
            f"Expected 5 successful logins, got {len(successful_logins)}"
        
        # Verify unique session IDs
        session_ids = [login.user["session_id"] for login in successful_logins]
        unique_sessions = set(session_ids)
        assert len(unique_sessions) == 5, \
            f"Duplicate session IDs detected: {len(unique_sessions)} unique out of 5"
        
        # Verify session isolation in Redis
        redis_client = isolated_auth_environment["redis_client"]
        stored_sessions = []
        for key in redis_client.scan_iter("session:*"):
            session_data = await redis_client.get(key)
            if session_data:
                stored_sessions.append(json.loads(session_data))
        
        # Check session count and data integrity
        user_sessions = [s for s in stored_sessions if s.get("user_id") == test_user["id"]]
        assert len(user_sessions) <= CONCURRENCY_TEST_CONFIG["max_sessions_per_user"], \
            f"Too many sessions: {len(user_sessions)} " \
            f"(max: {CONCURRENCY_TEST_CONFIG['max_sessions_per_user']})"
        
        # Verify each session has unique device context
        device_fingerprints = set()
        for session in user_sessions:
            fingerprint = f"{session.get('ip_address')}:{session.get('user_agent')}"
            assert fingerprint not in device_fingerprints, \
                f"Duplicate device fingerprint: {fingerprint}"
            device_fingerprints.add(fingerprint)
        
        race_detector.take_memory_snapshot("after_multidevice_test")

@pytest.mark.e2e
class TestConcurrentSessionInvalidation:
    """Test Case 3: Session Invalidation Race Conditions"""
    
    @pytest.mark.asyncio
    @pytest.mark.e2e
    async def test_concurrent_session_invalidation(self, isolated_auth_environment:
                                                 concurrent_executor, race_detector):
        """
        Scenario: Multiple logout requests and session invalidations happen simultaneously
        Expected: Clean session cleanup without orphaned data
        
        This test ensures that concurrent session invalidation operations don't
        leave orphaned session data or cause race conditions.
        """
        auth_service = isolated_auth_environment["auth_service"]
        test_user = isolated_auth_environment["test_users"][0]
        session_manager = auth_service.session_manager
        
        race_detector.take_memory_snapshot("before_invalidation_test")
        
        # Create multiple active sessions
        session_ids = []
        access_tokens = []
        
        for i in range(5):
            # Create session
            session_id = session_manager.create_session(
                test_user["id"], 
                {"device": f"device-{i}", "ip_address": f"192.168.1.{i}"}
            )
            session_ids.append(session_id)
            
            # Create corresponding access token
            access_token = auth_service.jwt_handler.create_access_token(
                test_user["id"], test_user["email"], ["read", "write"]
            )
            access_tokens.append(access_token)
        
        # Verify sessions were created
        initial_sessions = await session_manager.get_user_sessions(test_user["id"])
        assert len(initial_sessions) == 5, f"Expected 5 sessions, got {len(initial_sessions)}"
        
        # Define concurrent invalidation operations
        invalidation_operations = []
        
        # Mix of specific session deletions and user-wide invalidations
        for i in range(3):
            async def logout_specific_session(token, session_id):
                start_time = time.perf_counter()
                try:
                    result = await auth_service.logout(token, session_id)
                    end_time = time.perf_counter()
                    race_detector.log_operation("session_logout", start_time, end_time, result)
                    return result
                except Exception as e:
                    end_time = time.perf_counter()
                    race_detector.log_operation("session_logout", start_time, end_time, 
                                              False, str(e))
                    return e
            
            invalidation_operations.append(
                logout_specific_session(access_tokens[i], session_ids[i])
            )
        
        # Add user-wide invalidation operations
        async def invalidate_all_user_sessions():
            start_time = time.perf_counter()
            try:
                count = await session_manager.invalidate_user_sessions(test_user["id"])
                end_time = time.perf_counter()
                race_detector.log_operation("invalidate_all_sessions", start_time, end_time, True)
                return count
            except Exception as e:
                end_time = time.perf_counter()
                race_detector.log_operation("invalidate_all_sessions", start_time, end_time, 
                                          False, str(e))
                return e
        
        invalidation_operations.append(invalidate_all_user_sessions())
        invalidation_operations.append(invalidate_all_user_sessions())  # Duplicate to test race
        
        # Execute concurrent invalidations
        results = await concurrent_executor.execute_simultaneously(invalidation_operations)
        
        # Validation: All sessions should be cleaned up
        await asyncio.sleep(0.1)  # Brief delay for async cleanup
        
        final_sessions = await session_manager.get_user_sessions(test_user["id"])
        assert len(final_sessions) == 0, \
            f"Sessions not properly cleaned up: {len(final_sessions)} remaining"
        
        # Verify Redis cleanup
        redis_client = isolated_auth_environment["redis_client"]
        remaining_keys = list(redis_client.scan_iter("session:*"))
        user_session_keys = []
        
        for key in remaining_keys:
            session_data = await redis_client.get(key)
            if session_data:
                data = json.loads(session_data)
                if data.get("user_id") == test_user["id"]:
                    user_session_keys.append(key)
        
        assert len(user_session_keys) == 0, \
            f"Orphaned session keys in Redis: {user_session_keys}"
        
        race_detector.take_memory_snapshot("after_invalidation_test")

@pytest.mark.e2e
class TestJWTTokenCollisionDetection:
    """Test Case 4: JWT Token Collision Detection"""
    
    @pytest.mark.asyncio
    @pytest.mark.e2e
    async def test_jwt_token_collision_detection(self, isolated_auth_environment:
                                                race_detector):
        """
        Scenario: Generate thousands of tokens simultaneously
        Expected: All tokens are unique, no collisions in token IDs
        
        This test ensures JWT tokens are unique even under high concurrency
        and that the token generation process is thread-safe.
        """
        auth_service = isolated_auth_environment["auth_service"]
        jwt_handler = auth_service.jwt_handler
        
        race_detector.take_memory_snapshot("before_token_collision_test")
        
        # Generate test user IDs
        user_ids = [f"user-{i}" for i in range(100)]
        
        # Define token generation operations
        def generate_token_batch(user_batch):
            """Generate multiple token types for a batch of users"""
            tokens = []
            for user_id in user_batch:
                try:
                    # Generate different token types
                    access_token = jwt_handler.create_access_token(
                        user_id, f"{user_id}@example.com", ["read", "write"]
                    )
                    refresh_token = jwt_handler.create_refresh_token(user_id)
                    service_token = jwt_handler.create_service_token(
                        f"service-{user_id}", f"test-service-{user_id}"
                    )
                    
                    tokens.extend([access_token, refresh_token, service_token])
                except Exception as e:
                    logger.error(f"Token generation failed for {user_id}: {e}")
                    
            return tokens
        
        # Split users into batches for concurrent processing
        batch_size = 10
        user_batches = [user_ids[i:i + batch_size] for i in range(0, len(user_ids), batch_size)]
        
        # Execute concurrent token generation
        start_time = time.perf_counter()
        with ThreadPoolExecutor(max_workers=CONCURRENCY_TEST_CONFIG["max_workers"]) as executor:
            future_to_batch = {
                executor.submit(generate_token_batch, batch): batch 
                for batch in user_batches
            }
            
            all_tokens = []
            for future in as_completed(future_to_batch):
                batch = future_to_batch[future]
                try:
                    tokens = future.result(timeout=CONCURRENCY_TEST_CONFIG["operation_timeout"])
                    all_tokens.extend(tokens)
                except Exception as e:
                    logger.error(f"Batch processing failed for {batch}: {e}")
        
        end_time = time.perf_counter()
        generation_duration = end_time - start_time
        
        race_detector.log_operation("token_generation_batch", start_time, end_time, True)
        
        # Validation: All tokens must be unique
        assert len(all_tokens) > 0, "No tokens were generated"
        
        unique_tokens = set(all_tokens)
        assert len(all_tokens) == len(unique_tokens), \
            f"Token collisions detected: {len(all_tokens)} generated, " \
            f"{len(unique_tokens)} unique"
        
        # Verify token structure and validity
        valid_tokens = 0
        invalid_tokens = 0
        
        for token in all_tokens:
            try:
                # Decode without verification to check structure
                payload = jwt_lib.decode(token, options={"verify_signature": False})
                
                # Verify required fields
                required_fields = ["iat", "exp", "sub", "token_type", "iss"]
                if all(field in payload for field in required_fields):
                    # Verify time ordering
                    if payload["iat"] < payload["exp"]:
                        valid_tokens += 1
                    else:
                        invalid_tokens += 1
                        race_detector.log_race_condition_indicator("invalid_token_timing", {
                            "iat": payload["iat"],
                            "exp": payload["exp"]
                        })
                else:
                    invalid_tokens += 1
                    race_detector.log_race_condition_indicator("missing_token_fields", {
                        "missing": [f for f in required_fields if f not in payload],
                        "present": list(payload.keys())
                    })
                    
            except Exception as e:
                invalid_tokens += 1
                race_detector.log_race_condition_indicator("token_decode_error", {
                    "error": str(e),
                    "token_prefix": token[:20] + "..."
                })
        
        assert invalid_tokens == 0, \
            f"Invalid tokens detected: {invalid_tokens} out of {len(all_tokens)}"
        
        # Performance validation
        tokens_per_second = len(all_tokens) / generation_duration
        logger.info(f"Token generation rate: {tokens_per_second:.1f} tokens/second")
        
        assert generation_duration < CONCURRENCY_TEST_CONFIG["max_response_time"], \
            f"Token generation too slow: {generation_duration:.2f}s " \
            f"(threshold: {CONCURRENCY_TEST_CONFIG['max_response_time']}s)"
        
        race_detector.take_memory_snapshot("after_token_collision_test")

@pytest.mark.e2e
class TestRaceConditionLoadStress:
    """Comprehensive load stress test for race condition detection"""
    
    @pytest.mark.asyncio
    @pytest.mark.slow  # Mark as slow test for optional execution
    @pytest.mark.e2e
    async def test_comprehensive_race_condition_stress(self, isolated_auth_environment:
                                                     race_detector):
        """
        Comprehensive stress test combining all race condition scenarios
        under high load to detect subtle timing issues.
        """
        auth_service = isolated_auth_environment["auth_service"]
        
        race_detector.take_memory_snapshot("before_stress_test")
        
        # Enable race condition simulation in mock Redis
        redis_client = isolated_auth_environment["redis_client"]
        redis_client.enable_race_conditions(True)
        
        stress_results = {
            "operations_completed": 0,
            "operations_failed": 0,
            "race_conditions_detected": 0,
            "performance_issues": 0
        }
        
        async def stress_operation_batch():
            """Execute a batch of mixed auth operations"""
            batch_results = []
            
            for i in range(10):  # 10 operations per batch
                try:
                    # Random operation selection
                    operation_type = random.choice([
                        "login", "token_refresh", "session_check", "logout"
                    ])
                    
                    if operation_type == "login":
                        # Simulate login
                        user_data = UserFactory.create_local_user_data()
                        login_request = LoginRequest(
                            email=user_data["email"],
                            password="TestPassword123!",
                            provider=AuthProvider.LOCAL
                        )
                        # Mock login success
                        result = {"success": True, "operation": "login"}
                        
                    elif operation_type == "token_refresh":
                        # Simulate token refresh
                        refresh_token = auth_service.jwt_handler.create_refresh_token("test-user")
                        result = await auth_service.refresh_tokens(refresh_token)
                        result = {"success": result is not None, "operation": "token_refresh"}
                        
                    elif operation_type == "session_check":
                        # Simulate session validation
                        session_id = str(uuid.uuid4())
                        result = await auth_service.session_manager.validate_session(session_id)
                        result = {"success": True, "operation": "session_check"}
                        
                    else:  # logout
                        # Simulate logout
                        access_token = auth_service.jwt_handler.create_access_token(
                            "test-user", "test@example.com"
                        )
                        result = await auth_service.logout(access_token)
                        result = {"success": result, "operation": "logout"}
                    
                    batch_results.append(result)
                    stress_results["operations_completed"] += 1
                    
                except Exception as e:
                    batch_results.append({"success": False, "operation": "unknown", "error": str(e)})
                    stress_results["operations_failed"] += 1
                    
                # Random delay to simulate real usage patterns
                await asyncio.sleep(random.uniform(0.001, 0.005))
            
            return batch_results
        
        # Execute stress test with multiple concurrent batches
        num_batches = 20
        batch_tasks = [stress_operation_batch() for _ in range(num_batches)]
        
        start_time = time.perf_counter()
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
        end_time = time.perf_counter()
        
        total_duration = end_time - start_time
        
        # Analyze results
        all_operations = []
        for batch_result in batch_results:
            if isinstance(batch_result, list):
                all_operations.extend(batch_result)
        
        successful_ops = [op for op in all_operations if op.get("success", False)]
        failed_ops = [op for op in all_operations if not op.get("success", False)]
        
        # Performance validation
        operations_per_second = len(all_operations) / total_duration
        success_rate = len(successful_ops) / len(all_operations) if all_operations else 0
        
        logger.info(f"Stress test results:")
        logger.info(f"  Total operations: {len(all_operations)}")
        logger.info(f"  Success rate: {success_rate:.2%}")
        logger.info(f"  Operations/sec: {operations_per_second:.1f}")
        logger.info(f"  Duration: {total_duration:.2f}s")
        
        # Validate performance thresholds
        assert success_rate >= CONCURRENCY_TEST_CONFIG["min_success_rate"], \
            f"Success rate too low: {success_rate:.2%} " \
            f"(threshold: {CONCURRENCY_TEST_CONFIG['min_success_rate']:.2%})"
        
        assert operations_per_second > 50, \
            f"Performance too slow: {operations_per_second:.1f} ops/sec (threshold: 50)"
        
        # Disable race condition simulation
        redis_client.enable_race_conditions(False)
        
        race_detector.take_memory_snapshot("after_stress_test")
        
        # Generate final performance report
        performance_report = race_detector.get_performance_report()
        logger.info(f"Final performance report: {json.dumps(performance_report, indent=2)}")
        
        return performance_report
