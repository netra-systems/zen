"""
Unit tests for environment timeout configuration validation (Issue #395).

REPRODUCTION TARGET: Environment-specific timeout configuration issues.
These tests SHOULD FAIL initially to demonstrate timeout configuration problems.

Key Configuration Issues to Reproduce:
1. Hardcoded 0.5s staging timeout causing failures
2. Environment detection affecting timeout behavior  
3. Timeout configuration not matching environment needs
4. Missing timeout validation and bounds checking
"""

import pytest
import time
from unittest.mock import patch, MagicMock
from typing import Dict, Any

from netra_backend.app.clients.auth_client_core import AuthServiceClient
from test_framework.ssot.base_test_case import SSotAsyncTestCase
import httpx


class TestEnvironmentTimeoutConfiguration(SSotAsyncTestCase):
    """
    Unit tests for environment-specific timeout configuration issues.
    
    These tests validate timeout configuration across different environments
    and identify configuration issues causing auth service failures.
    """
    
    def test_staging_timeout_configuration_too_aggressive(self):
        """
        REPRODUCTION TEST: Staging timeout configuration is too aggressive.
        
        This test validates that staging environment has inappropriately
        aggressive timeout settings compared to other environments.
        
        EXPECTED RESULT: Should FAIL, showing staging timeouts are too low.
        """
        
        timeout_analysis = {}
        environments = ["development", "staging", "production", "test"]
        auth_clients = {}
        
        # Create auth clients for each environment
        for env in environments:
            with patch('shared.isolated_environment.get_env') as mock_env:
                mock_env_dict = MagicMock()
                mock_env_dict.get.side_effect = lambda key, default=None, current_env=env: {
                    "ENVIRONMENT": current_env,
                    "AUTH_CLIENT_TIMEOUT": "30"
                }.get(key, default)
                mock_env.return_value = mock_env_dict
                auth_clients[env] = AuthServiceClient()
        
        # Analyze timeout configuration for each environment
        for env in environments:
            client = auth_clients[env]
            timeouts = client._get_environment_specific_timeouts()
            
            timeout_analysis[env] = {
                "connect": timeouts.connect,
                "read": timeouts.read,
                "write": timeouts.write,
                "pool": timeouts.pool,
                "total": timeouts.connect + timeouts.read + timeouts.write + timeouts.pool
            }
            
            # Add hardcoded health check timeout (from code analysis)
            if env == "staging":
                timeout_analysis[env]["health_check"] = 0.5  # Hardcoded in _check_auth_service_connectivity
            else:
                timeout_analysis[env]["health_check"] = 1.0  # Other environments
        
        print(f"\n CHART:  Environment Timeout Analysis:")
        for env, config in timeout_analysis.items():
            print(f"  {env.upper()}:")
            print(f"    Connect: {config['connect']}s")
            print(f"    Read: {config['read']}s") 
            print(f"    Write: {config['write']}s")
            print(f"    Pool: {config['pool']}s")
            print(f"    Health Check: {config['health_check']}s")
            print(f"    Total HTTP: {config['total']}s")
        
        # REPRODUCTION ASSERTIONS: Staging timeouts too aggressive
        
        # Staging should have the most aggressive timeouts
        staging_total = timeout_analysis["staging"]["total"]
        staging_health = timeout_analysis["staging"]["health_check"]
        
        # Compare with other environments
        for env in ["development", "production"]:
            if env in timeout_analysis:
                other_total = timeout_analysis[env]["total"]
                other_health = timeout_analysis[env]["health_check"]
                
                # Staging should be more aggressive (lower timeouts)
                self.assertLess(staging_total, other_total,
                               f"Staging total timeout ({staging_total}s) should be less than "
                               f"{env} ({other_total}s) - indicating overly aggressive configuration")
                
                self.assertLess(staging_health, other_health, 
                               f"Staging health check timeout ({staging_health}s) should be less than "
                               f"{env} ({other_health}s) - indicating overly aggressive configuration")
        
        # Staging health check timeout is dangerously low
        self.assertLessEqual(staging_health, 0.5,
                            f"Staging health check timeout ({staging_health}s) is at or below 0.5s - "
                            f"this is too aggressive for GCP Cloud Run networking")
        
        # Total staging timeout budget is too low for reliable operation
        self.assertLess(staging_total, 10.0,
                       f"Staging total timeout ({staging_total}s) is less than 10s - "
                       f"this may be too aggressive for reliable Cloud Run operation")

    def test_hardcoded_timeout_values_issue(self):
        """
        REPRODUCTION TEST: Hardcoded timeout values causing configuration inflexibility.
        
        This test identifies hardcoded timeout values that should be configurable
        but are causing issues across environments.
        
        EXPECTED RESULT: Should reveal hardcoded values that need to be configurable.
        """
        
        hardcoded_analysis = {
            "staging_health_check": 0.5,  # Hardcoded in _check_auth_service_connectivity
            "other_health_check": 1.0,    # Hardcoded for non-staging
            "circuit_breaker_call_timeout": 3.0,  # Hardcoded in circuit breaker config
            "circuit_breaker_recovery_timeout": 10.0,  # Hardcoded in circuit breaker config
            "httpx_timeouts_by_env": {},
            "configuration_issues": []
        }
        
        # Analyze httpx timeout configuration
        for env in self.environments:
            client = self.auth_clients[env]
            timeouts = client._get_environment_specific_timeouts()
            
            hardcoded_analysis["httpx_timeouts_by_env"][env] = {
                "connect": timeouts.connect,
                "read": timeouts.read,
                "write": timeouts.write,
                "pool": timeouts.pool
            }
        
        print(f"\n[U+1F527] Hardcoded Timeout Analysis:")
        print(f"Health Check Timeouts:")
        print(f"  Staging: {hardcoded_analysis['staging_health_check']}s (hardcoded)")
        print(f"  Other envs: {hardcoded_analysis['other_health_check']}s (hardcoded)")
        
        print(f"\nCircuit Breaker Timeouts:")
        print(f"  Call timeout: {hardcoded_analysis['circuit_breaker_call_timeout']}s (hardcoded)")
        print(f"  Recovery timeout: {hardcoded_analysis['circuit_breaker_recovery_timeout']}s (hardcoded)")
        
        print(f"\nHTTP Timeouts by Environment:")
        for env, config in hardcoded_analysis["httpx_timeouts_by_env"].items():
            print(f"  {env.upper()}: connect={config['connect']}, read={config['read']}, "
                  f"write={config['write']}, pool={config['pool']}")
        
        # Identify hardcoding issues
        
        # Issue 1: Health check timeout should be environment-configurable
        if hardcoded_analysis["staging_health_check"] == 0.5:
            hardcoded_analysis["configuration_issues"].append({
                "issue": "staging_health_check_hardcoded",
                "description": "Staging health check timeout hardcoded to 0.5s",
                "impact": "Cannot adjust for different Cloud Run networking conditions",
                "recommendation": "Make health check timeout environment-configurable"
            })
        
        # Issue 2: Circuit breaker timeouts don't align with environment-specific HTTP timeouts
        staging_http_total = sum(hardcoded_analysis["httpx_timeouts_by_env"]["staging"].values())
        
        if hardcoded_analysis["circuit_breaker_call_timeout"] < staging_http_total:
            hardcoded_analysis["configuration_issues"].append({
                "issue": "circuit_breaker_http_timeout_mismatch",
                "description": f"Circuit breaker call timeout ({hardcoded_analysis['circuit_breaker_call_timeout']}s) "
                              f"less than staging HTTP total ({staging_http_total}s)",
                "impact": "Circuit breaker may timeout before HTTP layer",
                "recommendation": "Align circuit breaker timeout with HTTP timeout configuration"
            })
        
        # Issue 3: Timeout values not validated for reasonableness
        if hardcoded_analysis["staging_health_check"] < 0.3:
            hardcoded_analysis["configuration_issues"].append({
                "issue": "health_check_timeout_too_low",
                "description": f"Health check timeout ({hardcoded_analysis['staging_health_check']}s) below minimum safe threshold",
                "impact": "May cause false timeout failures in normal networking conditions",
                "recommendation": "Set minimum health check timeout to 1.0s for all environments"
            })
        
        print(f"\n WARNING: [U+FE0F]  Configuration Issues Found: {len(hardcoded_analysis['configuration_issues'])}")
        for issue in hardcoded_analysis["configuration_issues"]:
            print(f"\n  Issue: {issue['issue']}")
            print(f"  Description: {issue['description']}")
            print(f"  Impact: {issue['impact']}")
            print(f"  Recommendation: {issue['recommendation']}")
        
        # REPRODUCTION ASSERTIONS: Hardcoded values causing issues
        self.assertGreater(len(hardcoded_analysis["configuration_issues"]), 0,
                          f"Expected to find hardcoded timeout configuration issues, "
                          f"but no issues were detected")
        
        # Specific assertion for staging health check hardcoding
        staging_hardcode_issues = [i for i in hardcoded_analysis["configuration_issues"] 
                                  if "staging_health_check_hardcoded" in i["issue"]]
        self.assertGreater(len(staging_hardcode_issues), 0,
                          f"Expected staging health check hardcoding issue, "
                          f"but hardcoded 0.5s timeout not detected")

    def test_timeout_bounds_validation_missing(self):
        """
        REPRODUCTION TEST: Missing timeout bounds validation.
        
        This test demonstrates that timeout values are not validated
        for minimum/maximum reasonable bounds, allowing dangerous configurations.
        
        EXPECTED RESULT: Should show lack of timeout validation.
        """
        
        bounds_test = {
            "minimum_safe_timeouts": {
                "connect": 0.5,     # Minimum for reliable connection
                "read": 1.0,        # Minimum for reliable read
                "write": 0.5,       # Minimum for reliable write  
                "pool": 1.0,        # Minimum for pool management
                "health_check": 1.0  # Minimum for reliable health check
            },
            "maximum_reasonable_timeouts": {
                "connect": 10.0,    # Maximum before too slow
                "read": 30.0,       # Maximum before too slow
                "write": 10.0,      # Maximum before too slow
                "pool": 10.0,       # Maximum before resource waste
                "health_check": 5.0  # Maximum before too slow
            },
            "violations": [],
            "validation_missing": []
        }
        
        print(f"\n SEARCH:  Timeout Bounds Validation Analysis:")
        
        # Test each environment's timeouts against bounds
        for env in self.environments:
            client = self.auth_clients[env]
            timeouts = client._get_environment_specific_timeouts()
            
            env_config = {
                "connect": timeouts.connect,
                "read": timeouts.read,
                "write": timeouts.write,
                "pool": timeouts.pool,
                "health_check": 0.5 if env == "staging" else 1.0  # Known hardcoded values
            }
            
            print(f"\n  {env.upper()} Environment:")
            
            # Check against minimum bounds
            for timeout_type, value in env_config.items():
                min_safe = bounds_test["minimum_safe_timeouts"][timeout_type]
                max_reasonable = bounds_test["maximum_reasonable_timeouts"][timeout_type]
                
                status = " PASS: "
                if value < min_safe:
                    status = " FAIL:  TOO LOW"
                    bounds_test["violations"].append({
                        "env": env,
                        "timeout_type": timeout_type,
                        "value": value,
                        "violation": "below_minimum",
                        "min_safe": min_safe
                    })
                elif value > max_reasonable:
                    status = " WARNING: [U+FE0F]  TOO HIGH" 
                    bounds_test["violations"].append({
                        "env": env,
                        "timeout_type": timeout_type,
                        "value": value,
                        "violation": "above_maximum",
                        "max_reasonable": max_reasonable
                    })
                
                print(f"    {timeout_type}: {value}s {status} (safe: {min_safe}-{max_reasonable}s)")
        
        # Check if validation logic exists (it doesn't)
        client = self.auth_clients["staging"]
        
        # These methods don't exist - indicating missing validation
        validation_methods = [
            "_validate_timeout_bounds",
            "_check_timeout_safety",
            "_validate_environment_timeouts",
            "_ensure_minimum_timeouts"
        ]
        
        for method in validation_methods:
            if not hasattr(client, method):
                bounds_test["validation_missing"].append(method)
        
        print(f"\n CHART:  Bounds Validation Results:")
        print(f"  Total violations: {len(bounds_test['violations'])}")
        print(f"  Missing validation methods: {len(bounds_test['validation_missing'])}")
        
        if bounds_test["violations"]:
            print(f"\n  Violations by type:")
            below_min = [v for v in bounds_test["violations"] if v["violation"] == "below_minimum"]
            above_max = [v for v in bounds_test["violations"] if v["violation"] == "above_maximum"]
            print(f"    Below minimum safe: {len(below_min)}")
            print(f"    Above maximum reasonable: {len(above_max)}")
            
            for violation in bounds_test["violations"]:
                print(f"    {violation['env']}.{violation['timeout_type']}: "
                      f"{violation['value']}s violates {violation['violation']}")
        
        print(f"\n  Missing validation methods:")
        for method in bounds_test["validation_missing"]:
            print(f"    {method}")
        
        # REPRODUCTION ASSERTIONS: Missing bounds validation
        
        # Should have timeout violations (particularly staging being too low)
        below_minimum_violations = [v for v in bounds_test["violations"] if v["violation"] == "below_minimum"]
        self.assertGreater(len(below_minimum_violations), 0,
                          f"Expected timeout values below minimum safe bounds, "
                          f"but all timeouts appear within safe limits")
        
        # Should be missing validation methods
        self.assertEqual(len(bounds_test["validation_missing"]), len(validation_methods),
                        f"Expected all {len(validation_methods)} validation methods to be missing, "
                        f"but {len(validation_methods) - len(bounds_test['validation_missing'])} exist")
        
        # Specific staging violations
        staging_violations = [v for v in bounds_test["violations"] if v["env"] == "staging"]
        self.assertGreater(len(staging_violations), 0,
                          f"Expected staging environment to have timeout violations, "
                          f"but no violations found")

    def test_environment_detection_timeout_impact(self):
        """
        REPRODUCTION TEST: Environment detection affecting timeout behavior.
        
        This test demonstrates how environment detection logic impacts
        timeout configuration and can cause issues if detection fails.
        
        EXPECTED RESULT: Should show environment detection dependency issues.
        """
        
        detection_test = {
            "environment_sources": [],
            "timeout_variations": {},
            "detection_failures": [],
            "consistency_issues": []
        }
        
        # Test different environment detection scenarios
        environment_test_cases = [
            ("staging", "staging"),           # Correct detection
            ("STAGING", "staging"),           # Case sensitivity
            ("production", "production"),     # Correct detection  
            ("PRODUCTION", "production"),     # Case sensitivity
            ("development", "development"),   # Correct detection
            ("dev", "development"),          # Alias handling
            ("test", "development"),         # Test mapping
            ("unknown", "development"),      # Fallback
            ("", "development"),             # Empty fallback
            (None, "development")            # None fallback
        ]
        
        print(f"\n[U+1F30D] Environment Detection Impact Analysis:")
        
        for env_value, expected_behavior in environment_test_cases:
            with patch('shared.isolated_environment.get_env') as mock_env:
                # Mock environment detection
                if env_value is None:
                    mock_env.return_value.get.return_value = None
                else:
                    mock_env.return_value.get.return_value = env_value
                
                try:
                    # Create client with this environment detection
                    test_client = AuthServiceClient()
                    timeouts = test_client._get_environment_specific_timeouts()
                    
                    detection_result = {
                        "env_value": env_value,
                        "expected": expected_behavior,
                        "timeouts": {
                            "connect": timeouts.connect,
                            "read": timeouts.read,
                            "write": timeouts.write,
                            "pool": timeouts.pool,
                            "total": timeouts.connect + timeouts.read + timeouts.write + timeouts.pool
                        },
                        "detection_success": True
                    }
                    
                    detection_test["timeout_variations"][str(env_value)] = detection_result
                    
                    print(f"  ENV='{env_value}' -> Total: {detection_result['timeouts']['total']}s")
                    
                    # Clean up - note: client cleanup will happen in test tearDown
                        
                except Exception as e:
                    detection_test["detection_failures"].append({
                        "env_value": env_value,
                        "error": str(e),
                        "error_type": type(e).__name__
                    })
                    
                    print(f"  ENV='{env_value}' -> ERROR: {e}")
        
        # Analyze consistency issues
        
        # Group by expected behavior
        timeout_groups = {}
        for env_str, result in detection_test["timeout_variations"].items():
            expected = result["expected"]
            if expected not in timeout_groups:
                timeout_groups[expected] = []
            timeout_groups[expected].append(result)
        
        print(f"\n CHART:  Timeout Consistency Analysis:")
        
        for expected_env, results in timeout_groups.items():
            if len(results) > 1:
                # Check if all results for same expected environment have same timeouts
                first_total = results[0]["timeouts"]["total"]
                inconsistent = [r for r in results if r["timeouts"]["total"] != first_total]
                
                if inconsistent:
                    detection_test["consistency_issues"].append({
                        "expected_env": expected_env,
                        "inconsistent_count": len(inconsistent),
                        "timeout_variations": [r["timeouts"]["total"] for r in results]
                    })
                    
                    print(f"  {expected_env}: INCONSISTENT - {len(inconsistent)} variations")
                    for result in results:
                        print(f"    '{result['env_value']}' -> {result['timeouts']['total']}s")
                else:
                    print(f"  {expected_env}: CONSISTENT - {first_total}s ({len(results)} variants)")
        
        # Check for dangerous fallback behavior
        unknown_envs = [r for r in detection_test["timeout_variations"].values() 
                       if r["env_value"] in ["unknown", "", None]]
        
        for unknown_result in unknown_envs:
            if unknown_result["timeouts"]["total"] < 10.0:  # Too aggressive for unknown env
                detection_test["consistency_issues"].append({
                    "issue": "dangerous_unknown_env_fallback",
                    "env_value": unknown_result["env_value"],
                    "timeout": unknown_result["timeouts"]["total"],
                    "description": "Unknown environment gets aggressive timeout settings"
                })
        
        print(f"\n CHART:  Detection Impact Results:")
        print(f"  Environment variations tested: {len(environment_test_cases)}")
        print(f"  Successful detections: {len(detection_test['timeout_variations'])}")
        print(f"  Detection failures: {len(detection_test['detection_failures'])}")
        print(f"  Consistency issues: {len(detection_test['consistency_issues'])}")
        
        # REPRODUCTION ASSERTIONS: Environment detection issues
        
        # Should have detection failures or consistency issues  
        total_issues = len(detection_test["detection_failures"]) + len(detection_test["consistency_issues"])
        self.assertGreater(total_issues, 0,
                          f"Expected environment detection to cause timeout configuration issues, "
                          f"but no issues found in {len(environment_test_cases)} test cases")
        
        # Unknown/fallback environments should not get aggressive timeouts
        dangerous_fallbacks = [i for i in detection_test["consistency_issues"] 
                              if i.get("issue") == "dangerous_unknown_env_fallback"]
        
        if len(dangerous_fallbacks) > 0:
            self.fail(f"Environment Detection ISSUE: Unknown environments receive aggressive timeout "
                     f"configuration. {len(dangerous_fallbacks)} cases found with timeouts < 10s. "
                     f"Unknown environments should use conservative timeout settings.")

    @pytest.mark.asyncio
    async def test_timeout_configuration_runtime_modification(self):
        """
        REPRODUCTION TEST: Timeout configuration cannot be modified at runtime.
        
        This test demonstrates that timeout configuration is fixed at initialization
        and cannot be adjusted based on runtime conditions or performance feedback.
        
        EXPECTED RESULT: Should show inflexibility in timeout configuration.
        """
        
        runtime_test = {
            "modification_attempts": [],
            "configuration_locked": True,
            "runtime_adjustment_methods": [],
            "flexibility_issues": []
        }
        
        print(f"\n[U+1F527] Runtime Timeout Modification Analysis:")
        
        # Test client with staging configuration
        staging_client = self.auth_clients["staging"]
        original_timeouts = staging_client._get_environment_specific_timeouts()
        
        print(f"Original staging timeouts:")
        print(f"  Connect: {original_timeouts.connect}s")
        print(f"  Read: {original_timeouts.read}s")
        print(f"  Total: {original_timeouts.connect + original_timeouts.read + original_timeouts.write + original_timeouts.pool}s")
        
        # Attempt 1: Try to modify timeout configuration directly
        modification_methods = [
            "_set_environment_timeouts",
            "_update_timeout_configuration", 
            "_adjust_timeouts_for_performance",
            "_increase_staging_timeouts",
            "_modify_health_check_timeout"
        ]
        
        for method in modification_methods:
            try:
                if hasattr(staging_client, method):
                    runtime_test["runtime_adjustment_methods"].append(method)
                    print(f"   PASS:  Found method: {method}")
                else:
                    runtime_test["modification_attempts"].append({
                        "method": method,
                        "exists": False,
                        "result": "method_not_found"
                    })
                    print(f"   FAIL:  Missing method: {method}")
                    
            except Exception as e:
                runtime_test["modification_attempts"].append({
                    "method": method,
                    "exists": True,
                    "result": f"error_{type(e).__name__}",
                    "error": str(e)
                })
                print(f"   WARNING: [U+FE0F]  Method {method} error: {e}")
        
        # Attempt 2: Try to create new client with different timeout settings
        print(f"\n  Testing runtime client reconfiguration...")
        
        try:
            # Try to override timeout settings via environment
            with patch('shared.isolated_environment.get_env') as mock_env:
                # Mock a different environment to get different timeouts
                mock_env.return_value.get.return_value = "production"  # Different from staging
                
                runtime_client = AuthServiceClient()
                runtime_timeouts = runtime_client._get_environment_specific_timeouts()
                
                # Check if we can get different timeouts
                timeout_changed = (runtime_timeouts.connect != original_timeouts.connect or 
                                 runtime_timeouts.read != original_timeouts.read)
                
                if timeout_changed:
                    runtime_test["modification_attempts"].append({
                        "method": "environment_override", 
                        "exists": True,
                        "result": "success",
                        "old_total": original_timeouts.connect + original_timeouts.read + original_timeouts.write + original_timeouts.pool,
                        "new_total": runtime_timeouts.connect + runtime_timeouts.read + runtime_timeouts.write + runtime_timeouts.pool
                    })
                    print(f"     PASS:  Environment override: {original_timeouts.connect + original_timeouts.read + original_timeouts.write + original_timeouts.pool}s -> {runtime_timeouts.connect + runtime_timeouts.read + runtime_timeouts.write + runtime_timeouts.pool}s")
                else:
                    runtime_test["flexibility_issues"].append("environment_override_failed")
                    print(f"     FAIL:  Environment override failed - timeouts unchanged")
                
                await runtime_client._client.aclose()
                
        except Exception as e:
            runtime_test["modification_attempts"].append({
                "method": "environment_override",
                "exists": True, 
                "result": f"error_{type(e).__name__}",
                "error": str(e)
            })
            print(f"     FAIL:  Environment override error: {e}")
        
        # Attempt 3: Check for performance-based adjustment capabilities
        print(f"\n  Testing performance-based timeout adjustment...")
        
        performance_methods = [
            "_adjust_timeouts_based_on_latency",
            "_increase_timeouts_after_failures",
            "_adaptive_timeout_configuration", 
            "_learn_optimal_timeouts"
        ]
        
        for method in performance_methods:
            if hasattr(staging_client, method):
                runtime_test["runtime_adjustment_methods"].append(method)
                print(f"     PASS:  Found performance method: {method}")
            else:
                runtime_test["flexibility_issues"].append(f"missing_{method}")
                print(f"     FAIL:  Missing performance method: {method}")
        
        # Analyze flexibility issues
        missing_methods = len([a for a in runtime_test["modification_attempts"] 
                              if a["result"] == "method_not_found"])
        
        runtime_test["configuration_locked"] = (
            len(runtime_test["runtime_adjustment_methods"]) == 0 and
            missing_methods >= len(modification_methods) - 1
        )
        
        print(f"\n CHART:  Runtime Configuration Analysis:")
        print(f"  Runtime adjustment methods found: {len(runtime_test['runtime_adjustment_methods'])}")
        print(f"  Missing configuration methods: {missing_methods}")
        print(f"  Flexibility issues: {len(runtime_test['flexibility_issues'])}")
        print(f"  Configuration appears locked: {runtime_test['configuration_locked']}")
        
        # REPRODUCTION ASSERTIONS: Configuration inflexibility
        
        # Should have few or no runtime adjustment methods
        self.assertLessEqual(len(runtime_test["runtime_adjustment_methods"]), 1,
                            f"Expected few runtime timeout adjustment methods, "
                            f"but found {len(runtime_test['runtime_adjustment_methods'])}: "
                            f"{runtime_test['runtime_adjustment_methods']}")
        
        # Should be missing most configuration modification methods
        self.assertGreaterEqual(missing_methods, len(modification_methods) - 2,
                               f"Expected most timeout modification methods to be missing, "
                               f"but only {missing_methods}/{len(modification_methods)} are missing")
        
        # Configuration should appear locked
        if runtime_test["configuration_locked"]:
            self.fail(f"Runtime Configuration INFLEXIBILITY: Timeout configuration is locked at "
                     f"initialization and cannot be adjusted based on runtime conditions. "
                     f"Missing {missing_methods} modification methods and "
                     f"{len(runtime_test['flexibility_issues'])} flexibility features. "
                     f"This prevents adapting to varying GCP Cloud Run network conditions.")
