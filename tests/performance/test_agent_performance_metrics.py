"""
Agent Performance Metrics Aggregation Tests

Comprehensive performance testing for agent orchestration system with real
performance characteristics, metrics tracking, and business-critical benchmarks.

Business Value Justification (BVJ):
- Segment: Enterprise (80% of revenue relies on agent performance SLAs)
- Business Goal: Ensure consistent performance under production loads
- Value Impact: Prevents service degradation affecting 10K+ concurrent workflows 
- Strategic Impact: Enables Enterprise SLA guarantees (99.9% uptime, <2s response) 
                   protecting $500K ARR from performance-related churn

Critical Performance Requirements:
- Agent execution time: <2s for 95th percentile
- Memory usage: <1GB peak during concurrent operations
- Token consumption tracking: Real-time cost monitoring
- Response time benchmarks: Sub-5s for complex workflows
- Resource utilization: <80% CPU under normal load
- Load testing: 25+ concurrent workflows with <10% failure rate

Test Coverage:
- Individual agent performance profiling
- Multi-agent pipeline aggregation
- Resource consumption patterns
- Performance degradation detection
- Scalability threshold validation
- Cost optimization metrics
"""

import asyncio
import gc
import json
import logging
import os
import psutil
import statistics
import time
import uuid
from dataclasses import asdict, dataclass, field
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Set, Tuple
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from netra_backend.app.agents.base_agent import BaseSubAgent
from netra_backend.app.agents.state import DeepAgentState
from netra_backend.app.agents.supervisor.agent_registry import AgentRegistry
from netra_backend.app.agents.supervisor_consolidated import SupervisorAgent
from netra_backend.app.agents.tool_dispatcher import ToolDispatcher
from netra_backend.app.llm.llm_manager import LLMManager
from netra_backend.app.monitoring.metrics_collector import PerformanceMetric
from netra_backend.app.redis_manager import RedisManager
from netra_backend.app.services.state.state_manager import StateManager, StateStorage
from netra_backend.app.websocket_core.manager import WebSocketManager
from netra_backend.tests.performance.performance_baseline_config import (
    get_benchmark_runner,
    PerformanceCategory,
    PerformanceMetric as BasePerformanceMetric,
    SeverityLevel
)
from test_framework.environment_isolation import IsolatedEnvironment

logger = logging.getLogger(__name__)


@dataclass
class AgentPerformanceMetrics:
    """Comprehensive metrics for agent performance tracking."""
    
    agent_id: str
    agent_type: str
    workflow_id: str
    
    # Execution metrics
    execution_time_ms: float
    queue_wait_time_ms: float = 0.0
    processing_time_ms: float = 0.0
    
    # Resource metrics  
    memory_usage_mb: float = 0.0
    cpu_usage_percent: float = 0.0
    peak_memory_mb: float = 0.0
    
    # Token and cost metrics
    input_tokens: int = 0
    output_tokens: int = 0
    total_tokens: int = 0
    estimated_cost_usd: float = 0.0
    
    # Quality metrics
    success: bool = True
    error_type: Optional[str] = None
    retry_count: int = 0
    
    # Performance metadata
    timestamp: float = field(default_factory=time.time)
    llm_provider: Optional[str] = None
    model_name: Optional[str] = None
    additional_metadata: Dict[str, Any] = field(default_factory=dict)

    def calculate_total_time(self) -> float:
        """Calculate total execution time including queue wait."""
        return self.execution_time_ms + self.queue_wait_time_ms

    def calculate_tokens_per_second(self) -> float:
        """Calculate token processing rate."""
        if self.processing_time_ms <= 0:
            return 0.0
        return self.total_tokens / (self.processing_time_ms / 1000.0)

    def calculate_cost_per_token(self) -> float:
        """Calculate cost efficiency per token."""
        if self.total_tokens == 0:
            return 0.0
        return self.estimated_cost_usd / self.total_tokens


@dataclass
class PipelinePerformanceMetrics:
    """Performance metrics for multi-agent pipeline aggregation."""
    
    pipeline_id: str
    workflow_type: str
    start_time: float
    end_time: Optional[float] = None
    
    # Agent metrics aggregation
    agent_metrics: List[AgentPerformanceMetrics] = field(default_factory=list)
    
    # Pipeline-level metrics
    total_execution_time_ms: float = 0.0
    agents_executed: int = 0
    agents_succeeded: int = 0
    agents_failed: int = 0
    
    # Resource aggregation
    peak_memory_usage_mb: float = 0.0
    total_cpu_time_ms: float = 0.0
    avg_cpu_usage_percent: float = 0.0
    
    # Cost aggregation  
    total_tokens_consumed: int = 0
    total_cost_usd: float = 0.0
    cost_per_request: float = 0.0
    
    # Performance benchmarks
    p50_response_time_ms: float = 0.0
    p95_response_time_ms: float = 0.0  
    p99_response_time_ms: float = 0.0
    throughput_requests_per_second: float = 0.0
    
    def add_agent_metrics(self, metrics: AgentPerformanceMetrics):
        """Add agent metrics to pipeline aggregation."""
        self.agent_metrics.append(metrics)
        
        # Update aggregated metrics
        self.agents_executed += 1
        if metrics.success:
            self.agents_succeeded += 1
        else:
            self.agents_failed += 1
            
        # Update resource metrics
        self.peak_memory_usage_mb = max(self.peak_memory_usage_mb, metrics.peak_memory_mb)
        self.total_cpu_time_ms += metrics.execution_time_ms * (metrics.cpu_usage_percent / 100)
        
        # Update cost metrics
        self.total_tokens_consumed += metrics.total_tokens
        self.total_cost_usd += metrics.estimated_cost_usd

    def finalize_metrics(self):
        """Calculate final pipeline metrics."""
        if self.end_time:
            self.total_execution_time_ms = (self.end_time - self.start_time) * 1000
            
        if self.agents_executed > 0:
            self.avg_cpu_usage_percent = sum(m.cpu_usage_percent for m in self.agent_metrics) / self.agents_executed
            
        # Calculate percentiles
        if self.agent_metrics:
            execution_times = [m.execution_time_ms for m in self.agent_metrics]
            execution_times.sort()
            
            length = len(execution_times)
            self.p50_response_time_ms = execution_times[int(0.5 * length)] if length > 0 else 0
            self.p95_response_time_ms = execution_times[int(0.95 * length)] if length > 0 else 0
            self.p99_response_time_ms = execution_times[int(0.99 * length)] if length > 0 else 0
            
        # Calculate throughput
        if self.total_execution_time_ms > 0:
            self.throughput_requests_per_second = (self.agents_succeeded / (self.total_execution_time_ms / 1000))
            self.cost_per_request = self.total_cost_usd / max(1, self.agents_succeeded)

    def get_success_rate(self) -> float:
        """Calculate pipeline success rate."""
        if self.agents_executed == 0:
            return 0.0
        return self.agents_succeeded / self.agents_executed

    def get_performance_summary(self) -> Dict[str, Any]:
        """Get comprehensive performance summary."""
        return {
            "pipeline_id": self.pipeline_id,
            "workflow_type": self.workflow_type,
            "execution_time_ms": self.total_execution_time_ms,
            "success_rate": self.get_success_rate(),
            "agents": {
                "executed": self.agents_executed,
                "succeeded": self.agents_succeeded,
                "failed": self.agents_failed
            },
            "resources": {
                "peak_memory_mb": self.peak_memory_usage_mb,
                "avg_cpu_percent": self.avg_cpu_usage_percent,
                "total_cpu_time_ms": self.total_cpu_time_ms
            },
            "costs": {
                "total_tokens": self.total_tokens_consumed,
                "total_cost_usd": self.total_cost_usd,
                "cost_per_request": self.cost_per_request
            },
            "performance": {
                "p50_response_time_ms": self.p50_response_time_ms,
                "p95_response_time_ms": self.p95_response_time_ms,
                "p99_response_time_ms": self.p99_response_time_ms,
                "throughput_rps": self.throughput_requests_per_second
            }
        }


class PerformanceProfiledAgent(BaseSubAgent):
    """Agent wrapper that captures detailed performance metrics."""
    
    def __init__(self, agent_type: str, workflow_id: str):
        """Initialize profiled agent."""
        super().__init__(
            llm_manager=AsyncMock(),
            name=agent_type,
            description=f"Performance profiled {agent_type}"
        )
        self.agent_type = agent_type
        self.agent_id = f"{agent_type}_{uuid.uuid4().hex[:8]}"
        self.workflow_id = workflow_id
        self.metrics: Optional[AgentPerformanceMetrics] = None
        
        # Mock LLM responses with realistic token usage
        self._setup_realistic_llm_mock()
        
    def _setup_realistic_llm_mock(self):
        """Setup realistic LLM response patterns."""
        async def mock_llm_call(*args, **kwargs):
            # Simulate realistic processing time based on agent type
            processing_times = {
                "triage_agent": (0.5, 1.2),
                "supervisor_agent": (0.8, 2.0),
                "data_agent": (1.0, 3.0),
                "optimization_agent": (2.0, 5.0),
                "analysis_agent": (1.5, 4.0),
                "reporting_agent": (0.8, 1.8)
            }
            
            min_time, max_time = processing_times.get(self.agent_type, (0.5, 2.0))
            processing_delay = min_time + (max_time - min_time) * (hash(self.agent_id) % 100) / 100
            await asyncio.sleep(processing_delay)
            
            # Return realistic token usage
            base_tokens = {"input": 150, "output": 200}
            variance = (hash(self.workflow_id) % 50) - 25
            
            return {
                "content": f"Processed by {self.agent_type}",
                "tokens": {
                    "input": base_tokens["input"] + variance,
                    "output": base_tokens["output"] + variance,
                    "total": base_tokens["input"] + base_tokens["output"] + (variance * 2)
                },
                "model": "gpt-4",
                "provider": "openai"
            }
            
        if hasattr(self, 'llm_manager') and self.llm_manager:
            self.llm_manager.chat_completion = AsyncMock(side_effect=mock_llm_call)
    
    async def execute(self, state: DeepAgentState, run_id: str, stream_updates: bool) -> None:
        """Execute agent with performance profiling."""
        start_time = time.perf_counter()
        start_memory = self._get_memory_usage()
        process = psutil.Process()
        
        # Initialize metrics
        self.metrics = AgentPerformanceMetrics(
            agent_id=self.agent_id,
            agent_type=self.agent_type,
            workflow_id=self.workflow_id,
            execution_time_ms=0.0
        )
        
        try:
            # Simulate queue wait time (realistic for production)
            queue_wait = 0.05 + (hash(run_id) % 50) / 1000  # 50-100ms variance
            await asyncio.sleep(queue_wait)
            self.metrics.queue_wait_time_ms = queue_wait * 1000
            
            # Start processing timer
            processing_start = time.perf_counter()
            
            # Simulate realistic agent processing
            if hasattr(self, 'llm_manager') and self.llm_manager:
                llm_response = await self.llm_manager.chat_completion(
                    messages=[{"role": "user", "content": f"Process {self.agent_type} task"}],
                    model="gpt-4"
                )
                
                # Extract token metrics
                if llm_response and "tokens" in llm_response:
                    tokens = llm_response["tokens"]
                    self.metrics.input_tokens = tokens.get("input", 0)
                    self.metrics.output_tokens = tokens.get("output", 0)
                    self.metrics.total_tokens = tokens.get("total", 0)
                    self.metrics.llm_provider = llm_response.get("provider", "mock")
                    self.metrics.model_name = llm_response.get("model", "mock-model")
                    
                    # Estimate realistic costs (OpenAI GPT-4 pricing)
                    input_cost = self.metrics.input_tokens * 0.00003  # $0.03 per 1K input tokens
                    output_cost = self.metrics.output_tokens * 0.00006  # $0.06 per 1K output tokens
                    self.metrics.estimated_cost_usd = input_cost + output_cost
            
            # Simulate memory and CPU usage
            end_memory = self._get_memory_usage()
            self.metrics.memory_usage_mb = end_memory
            self.metrics.peak_memory_mb = max(start_memory, end_memory)
            self.metrics.cpu_usage_percent = process.cpu_percent()
            
            # Mark successful execution
            self.metrics.success = True
            
        except Exception as e:
            self.metrics.success = False
            self.metrics.error_type = type(e).__name__
            logger.error(f"Agent {self.agent_type} execution failed: {e}")
            raise
            
        finally:
            # Finalize execution metrics
            end_time = time.perf_counter()
            self.metrics.execution_time_ms = (end_time - start_time) * 1000
            if 'processing_start' in locals():
                self.metrics.processing_time_ms = (end_time - processing_start) * 1000

    def _get_memory_usage(self) -> float:
        """Get current memory usage in MB."""
        try:
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except:
            return 0.0

    def get_performance_metrics(self) -> Optional[AgentPerformanceMetrics]:
        """Get captured performance metrics."""
        return self.metrics


class AgentPerformanceTestSuite:
    """Comprehensive agent performance test suite with real metrics aggregation."""
    
    def __init__(self):
        """Initialize performance test suite."""
        self.isolated_env = IsolatedEnvironment()
        self.redis_manager: Optional[RedisManager] = None
        self.state_manager: Optional[StateManager] = None
        self.benchmark_runner = get_benchmark_runner()
        
        # Performance test scenarios
        self.test_scenarios = {
            "lightweight": {
                "agents": ["triage_agent", "supervisor_agent"],
                "expected_time_ms": 2000,
                "max_memory_mb": 256
            },
            "standard": {
                "agents": ["triage_agent", "supervisor_agent", "data_agent", "analysis_agent"],
                "expected_time_ms": 5000,
                "max_memory_mb": 512
            },
            "complex": {
                "agents": ["triage_agent", "supervisor_agent", "data_agent", "optimization_agent", "analysis_agent", "reporting_agent"],
                "expected_time_ms": 10000,
                "max_memory_mb": 1024
            }
        }
        
    async def setup_test_environment(self):
        """Setup isolated test environment with real services."""
        await self.isolated_env.setup()
        
        # Initialize Redis for metrics storage
        try:
            self.redis_manager = RedisManager(test_mode=True)
            await self.redis_manager.connect()
            
            if self.redis_manager.enabled:
                self.state_manager = StateManager(storage=StateStorage.HYBRID)
                self.state_manager._redis = self.redis_manager
                logger.info("Performance tests using Redis metrics storage")
            else:
                self.state_manager = StateManager(storage=StateStorage.MEMORY)
                logger.info("Performance tests using memory metrics storage")
                
        except Exception as e:
            logger.warning(f"Redis unavailable for performance tests: {e}")
            self.state_manager = StateManager(storage=StateStorage.MEMORY)

    async def cleanup_test_environment(self):
        """Clean up test environment."""
        if self.redis_manager:
            try:
                await self.redis_manager.disconnect()
            except Exception as e:
                logger.warning(f"Redis cleanup error: {e}")
        
        await self.isolated_env.cleanup()

    async def execute_agent_performance_test(
        self, 
        agent_type: str, 
        workflow_id: str, 
        iterations: int = 1
    ) -> List[AgentPerformanceMetrics]:
        """Execute performance test for individual agent."""
        metrics_results = []
        
        for i in range(iterations):
            agent = PerformanceProfiledAgent(agent_type, f"{workflow_id}_{i}")
            
            # Create mock state
            state = DeepAgentState()
            state.run_id = f"perf_test_{uuid.uuid4().hex[:8]}"
            
            try:
                await agent.execute(state, state.run_id, stream_updates=False)
                
                metrics = agent.get_performance_metrics()
                if metrics:
                    metrics_results.append(metrics)
                    
            except Exception as e:
                logger.error(f"Agent performance test failed: {e}")
                # Create failed metrics entry
                failed_metrics = AgentPerformanceMetrics(
                    agent_id=agent.agent_id,
                    agent_type=agent_type,
                    workflow_id=workflow_id,
                    execution_time_ms=0.0,
                    success=False,
                    error_type=type(e).__name__
                )
                metrics_results.append(failed_metrics)
        
        return metrics_results

    async def execute_pipeline_performance_test(
        self,
        scenario_name: str,
        concurrent_pipelines: int = 1
    ) -> List[PipelinePerformanceMetrics]:
        """Execute performance test for multi-agent pipeline."""
        if scenario_name not in self.test_scenarios:
            raise ValueError(f"Unknown test scenario: {scenario_name}")
        
        scenario = self.test_scenarios[scenario_name]
        pipeline_results = []
        
        # Execute concurrent pipelines
        pipeline_tasks = []
        for i in range(concurrent_pipelines):
            pipeline_id = f"pipeline_{scenario_name}_{i}_{uuid.uuid4().hex[:8]}"
            task = self._execute_single_pipeline(pipeline_id, scenario_name, scenario["agents"])
            pipeline_tasks.append(task)
        
        results = await asyncio.gather(*pipeline_tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, PipelinePerformanceMetrics):
                pipeline_results.append(result)
            else:
                logger.error(f"Pipeline execution failed: {result}")
        
        return pipeline_results

    async def _execute_single_pipeline(
        self,
        pipeline_id: str,
        workflow_type: str,
        agent_types: List[str]
    ) -> PipelinePerformanceMetrics:
        """Execute single pipeline with performance tracking."""
        pipeline_metrics = PipelinePerformanceMetrics(
            pipeline_id=pipeline_id,
            workflow_type=workflow_type,
            start_time=time.perf_counter()
        )
        
        # Execute agents sequentially (realistic pipeline behavior)
        for agent_type in agent_types:
            agent_metrics_list = await self.execute_agent_performance_test(
                agent_type, pipeline_id, iterations=1
            )
            
            if agent_metrics_list:
                pipeline_metrics.add_agent_metrics(agent_metrics_list[0])
        
        # Finalize pipeline metrics
        pipeline_metrics.end_time = time.perf_counter()
        pipeline_metrics.finalize_metrics()
        
        return pipeline_metrics

    async def execute_load_test(
        self,
        scenario_name: str,
        concurrent_pipelines: int,
        duration_seconds: int = 30
    ) -> Dict[str, Any]:
        """Execute load test with multiple concurrent pipelines."""
        logger.info(f"Starting load test: {scenario_name} with {concurrent_pipelines} concurrent pipelines")
        
        start_time = time.perf_counter()
        end_time = start_time + duration_seconds
        
        load_test_results = {
            "scenario": scenario_name,
            "concurrent_pipelines": concurrent_pipelines,
            "duration_seconds": duration_seconds,
            "pipelines_completed": 0,
            "pipelines_failed": 0,
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "avg_response_time_ms": 0.0,
            "throughput_rps": 0.0,
            "pipeline_metrics": []
        }
        
        completed_pipelines = []
        
        # Execute pipelines in batches during the test duration
        batch_size = min(concurrent_pipelines, 10)  # Limit batch size for stability
        
        while time.perf_counter() < end_time:
            batch_pipelines = await self.execute_pipeline_performance_test(
                scenario_name, min(batch_size, concurrent_pipelines)
            )
            
            completed_pipelines.extend(batch_pipelines)
            load_test_results["pipelines_completed"] += len(batch_pipelines)
            
            # Small delay between batches to prevent resource exhaustion
            await asyncio.sleep(0.1)
        
        # Calculate load test metrics
        if completed_pipelines:
            load_test_results["pipeline_metrics"] = [p.get_performance_summary() for p in completed_pipelines]
            
            # Aggregate metrics
            total_agents = sum(p.agents_executed for p in completed_pipelines)
            successful_agents = sum(p.agents_succeeded for p in completed_pipelines)
            failed_agents = sum(p.agents_failed for p in completed_pipelines)
            
            load_test_results.update({
                "total_requests": total_agents,
                "successful_requests": successful_agents,
                "failed_requests": failed_agents,
                "avg_response_time_ms": statistics.mean([p.p50_response_time_ms for p in completed_pipelines]),
                "throughput_rps": sum(p.throughput_requests_per_second for p in completed_pipelines)
            })
        
        actual_duration = time.perf_counter() - start_time
        load_test_results["actual_duration_seconds"] = actual_duration
        
        logger.info(f"Load test completed: {load_test_results['pipelines_completed']} pipelines in {actual_duration:.2f}s")
        return load_test_results

    def save_performance_report(self, test_name: str, results: Any):
        """Save detailed performance report."""
        os.makedirs("test_reports/performance", exist_ok=True)
        timestamp = int(time.time())
        filename = f"test_reports/performance/agent_performance_{test_name}_{timestamp}.json"
        
        report_data = {
            "test_name": test_name,
            "timestamp": timestamp,
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "results": results if isinstance(results, dict) else asdict(results)
        }
        
        with open(filename, 'w') as f:
            json.dump(report_data, f, indent=2, default=str)
        
        logger.info(f"Performance report saved: {filename}")


# Test Fixtures

@pytest.fixture
async def performance_test_suite():
    """Performance test suite fixture."""
    suite = AgentPerformanceTestSuite()
    await suite.setup_test_environment()
    yield suite
    await suite.cleanup_test_environment()


@pytest.fixture
def benchmark_runner():
    """Benchmark runner fixture."""
    return get_benchmark_runner()


# Performance Tests

@pytest.mark.performance
@pytest.mark.asyncio
async def test_single_agent_execution_time_benchmarks(performance_test_suite, benchmark_runner):
    """Test individual agent execution time benchmarks."""
    suite = performance_test_suite
    
    agent_types = ["triage_agent", "supervisor_agent", "data_agent", "optimization_agent"]
    
    for agent_type in agent_types:
        workflow_id = f"benchmark_{agent_type}_{uuid.uuid4().hex[:8]}"
        
        # Execute performance test with multiple iterations for statistical accuracy
        metrics_results = await suite.execute_agent_performance_test(
            agent_type, workflow_id, iterations=10
        )
        
        # Calculate performance statistics
        successful_metrics = [m for m in metrics_results if m.success]
        
        assert len(successful_metrics) >= 8, f"At least 80% of {agent_type} executions should succeed"
        
        execution_times = [m.execution_time_ms for m in successful_metrics]
        avg_execution_time = statistics.mean(execution_times)
        p95_execution_time = sorted(execution_times)[int(0.95 * len(execution_times))]
        
        # Record benchmark results
        benchmark_runner.record_result(
            'agent_processing_time', avg_execution_time / 1000, time.time(),
            {
                'agent_type': agent_type,
                'iterations': len(successful_metrics),
                'p95_time_ms': p95_execution_time,
                'success_rate': len(successful_metrics) / len(metrics_results)
            }
        )
        
        # Performance assertions
        assert avg_execution_time < 3000, f"{agent_type} average execution time should be <3s"
        assert p95_execution_time < 5000, f"{agent_type} 95th percentile should be <5s"
        
        logger.info(f"{agent_type} performance: avg={avg_execution_time:.0f}ms, p95={p95_execution_time:.0f}ms")

    suite.save_performance_report("single_agent_benchmarks", {
        "agent_types": agent_types,
        "total_executions": len(metrics_results) * len(agent_types),
        "benchmark_results": benchmark_runner.results
    })


@pytest.mark.performance 
@pytest.mark.asyncio
async def test_memory_usage_monitoring_and_tracking(performance_test_suite, benchmark_runner):
    """Test memory usage monitoring during agent execution."""
    suite = performance_test_suite
    
    # Test memory-intensive scenario
    workflow_id = f"memory_test_{uuid.uuid4().hex[:8]}"
    
    # Execute complex pipeline to observe memory patterns
    pipeline_results = await suite.execute_pipeline_performance_test(
        "complex", concurrent_pipelines=3
    )
    
    assert len(pipeline_results) >= 2, "At least 2 pipelines should complete successfully"
    
    # Analyze memory usage patterns
    memory_metrics = []
    peak_memories = []
    
    for pipeline in pipeline_results:
        memory_metrics.extend([
            agent.memory_usage_mb for agent in pipeline.agent_metrics 
            if agent.memory_usage_mb > 0
        ])
        peak_memories.append(pipeline.peak_memory_usage_mb)
    
    if memory_metrics:
        avg_memory = statistics.mean(memory_metrics)
        max_memory = max(peak_memories) if peak_memories else 0
        
        # Record memory benchmarks
        benchmark_runner.record_result(
            'memory_peak_usage', max_memory, time.time(),
            {
                'avg_memory_mb': avg_memory,
                'test_scenario': 'complex_pipeline',
                'concurrent_pipelines': len(pipeline_results)
            }
        )
        
        # Memory usage assertions
        assert max_memory < 1024, "Peak memory usage should stay under 1GB"
        assert avg_memory < 512, "Average memory usage should be reasonable"
        
        logger.info(f"Memory usage: avg={avg_memory:.1f}MB, peak={max_memory:.1f}MB")

    suite.save_performance_report("memory_usage_monitoring", {
        "peak_memory_mb": max(peak_memories) if peak_memories else 0,
        "avg_memory_mb": statistics.mean(memory_metrics) if memory_metrics else 0,
        "pipeline_count": len(pipeline_results),
        "memory_distribution": sorted(memory_metrics) if memory_metrics else []
    })


@pytest.mark.performance
@pytest.mark.asyncio
async def test_token_consumption_tracking_and_cost_analysis(performance_test_suite, benchmark_runner):
    """Test token consumption tracking for cost optimization."""
    suite = performance_test_suite
    
    # Test different workflow complexities
    cost_analysis = {}
    
    for scenario_name in ["lightweight", "standard", "complex"]:
        pipeline_results = await suite.execute_pipeline_performance_test(
            scenario_name, concurrent_pipelines=2
        )
        
        assert len(pipeline_results) >= 1, f"At least 1 {scenario_name} pipeline should complete"
        
        # Aggregate token and cost metrics
        total_tokens = sum(p.total_tokens_consumed for p in pipeline_results)
        total_cost = sum(p.total_cost_usd for p in pipeline_results)
        avg_cost_per_request = statistics.mean([p.cost_per_request for p in pipeline_results])
        
        cost_analysis[scenario_name] = {
            "total_tokens": total_tokens,
            "total_cost_usd": total_cost,
            "avg_cost_per_request": avg_cost_per_request,
            "pipelines_analyzed": len(pipeline_results)
        }
        
        # Record token consumption benchmarks
        benchmark_runner.record_result(
            f'token_consumption_{scenario_name}', total_tokens, time.time(),
            {
                'total_cost_usd': total_cost,
                'cost_per_request': avg_cost_per_request,
                'scenario': scenario_name
            }
        )
        
        # Cost efficiency assertions
        assert total_cost < 1.0, f"{scenario_name} total cost should be reasonable for testing"
        assert avg_cost_per_request < 0.10, f"{scenario_name} per-request cost should be under $0.10"
        
        logger.info(f"{scenario_name} costs: {total_tokens} tokens, ${total_cost:.4f}, ${avg_cost_per_request:.4f}/req")

    # Validate cost scaling is reasonable
    lightweight_cost = cost_analysis["lightweight"]["avg_cost_per_request"]
    complex_cost = cost_analysis["complex"]["avg_cost_per_request"]
    
    assert complex_cost <= lightweight_cost * 5, "Complex workflows should not be more than 5x expensive"

    suite.save_performance_report("token_consumption_analysis", cost_analysis)


@pytest.mark.performance
@pytest.mark.asyncio
async def test_pipeline_aggregation_and_response_time_benchmarks(performance_test_suite, benchmark_runner):
    """Test multi-agent pipeline performance and response time benchmarks."""
    suite = performance_test_suite
    
    # Test all scenarios for comprehensive benchmarking
    pipeline_benchmarks = {}
    
    for scenario_name, scenario_config in suite.test_scenarios.items():
        pipeline_results = await suite.execute_pipeline_performance_test(
            scenario_name, concurrent_pipelines=5
        )
        
        assert len(pipeline_results) >= 3, f"At least 3 {scenario_name} pipelines should complete"
        
        # Calculate response time percentiles
        response_times_ms = [p.p95_response_time_ms for p in pipeline_results]
        p50_response = statistics.median(response_times_ms)
        p95_response = sorted(response_times_ms)[int(0.95 * len(response_times_ms))]
        
        # Calculate success rates
        success_rates = [p.get_success_rate() for p in pipeline_results]
        avg_success_rate = statistics.mean(success_rates)
        
        # Calculate throughput
        throughputs = [p.throughput_requests_per_second for p in pipeline_results]
        avg_throughput = statistics.mean(throughputs)
        
        pipeline_benchmarks[scenario_name] = {
            "p50_response_time_ms": p50_response,
            "p95_response_time_ms": p95_response,
            "avg_success_rate": avg_success_rate,
            "avg_throughput_rps": avg_throughput,
            "expected_time_ms": scenario_config["expected_time_ms"]
        }
        
        # Record pipeline benchmarks
        benchmark_runner.record_result(
            'concurrent_agent_throughput', avg_throughput, time.time(),
            {
                'scenario': scenario_name,
                'p95_response_ms': p95_response,
                'success_rate': avg_success_rate,
                'concurrent_pipelines': len(pipeline_results)
            }
        )
        
        # Response time assertions
        assert p95_response < scenario_config["expected_time_ms"], f"{scenario_name} p95 response time too high"
        assert avg_success_rate >= 0.8, f"{scenario_name} success rate should be at least 80%"
        
        logger.info(f"{scenario_name}: p50={p50_response:.0f}ms, p95={p95_response:.0f}ms, "
                   f"success={avg_success_rate:.2f}, throughput={avg_throughput:.1f}rps")

    suite.save_performance_report("pipeline_response_benchmarks", pipeline_benchmarks)


@pytest.mark.performance
@pytest.mark.asyncio  
async def test_resource_utilization_patterns_under_load(performance_test_suite, benchmark_runner):
    """Test resource utilization patterns during concurrent load."""
    suite = performance_test_suite
    
    # Test resource utilization under different load levels
    load_levels = [
        {"concurrent": 5, "name": "light_load"},
        {"concurrent": 10, "name": "medium_load"},
        {"concurrent": 15, "name": "heavy_load"}
    ]
    
    utilization_results = {}
    
    for load_config in load_levels:
        load_name = load_config["name"]
        concurrent_count = load_config["concurrent"]
        
        # Force garbage collection before load test
        gc.collect()
        
        start_time = time.perf_counter()
        pipeline_results = await suite.execute_pipeline_performance_test(
            "standard", concurrent_pipelines=concurrent_count
        )
        end_time = time.perf_counter()
        
        assert len(pipeline_results) >= concurrent_count * 0.6, f"At least 60% of {load_name} pipelines should complete"
        
        # Analyze resource utilization
        cpu_utilizations = []
        memory_utilizations = []
        
        for pipeline in pipeline_results:
            cpu_utilizations.append(pipeline.avg_cpu_usage_percent)
            memory_utilizations.append(pipeline.peak_memory_usage_mb)
        
        if cpu_utilizations and memory_utilizations:
            avg_cpu = statistics.mean(cpu_utilizations)
            max_cpu = max(cpu_utilizations)
            avg_memory = statistics.mean(memory_utilizations)
            max_memory = max(memory_utilizations)
            
            utilization_results[load_name] = {
                "concurrent_pipelines": concurrent_count,
                "avg_cpu_percent": avg_cpu,
                "max_cpu_percent": max_cpu,
                "avg_memory_mb": avg_memory,
                "max_memory_mb": max_memory,
                "total_duration_seconds": end_time - start_time,
                "pipelines_completed": len(pipeline_results)
            }
            
            # Record resource utilization benchmarks
            benchmark_runner.record_result(
                'concurrent_user_response_time', (end_time - start_time), time.time(),
                {
                    'load_level': load_name,
                    'concurrent_count': concurrent_count,
                    'avg_cpu_percent': avg_cpu,
                    'max_memory_mb': max_memory
                }
            )
            
            # Resource utilization assertions
            assert max_cpu < 90, f"{load_name} CPU usage should stay under 90%"
            assert max_memory < 2048, f"{load_name} memory usage should stay under 2GB"
            
            logger.info(f"{load_name}: CPU avg={avg_cpu:.1f}% max={max_cpu:.1f}%, "
                       f"Memory avg={avg_memory:.0f}MB max={max_memory:.0f}MB")

    suite.save_performance_report("resource_utilization_patterns", utilization_results)


@pytest.mark.performance
@pytest.mark.asyncio
async def test_concurrent_workflow_load_testing_25_plus(performance_test_suite, benchmark_runner):
    """Test concurrent workflow handling with 25+ simultaneous pipelines."""
    suite = performance_test_suite
    
    # High-concurrency load test
    concurrent_count = 25
    test_duration = 30  # seconds
    
    logger.info(f"Starting high-concurrency load test: {concurrent_count} concurrent pipelines")
    
    # Execute sustained load test
    load_results = await suite.execute_load_test(
        "lightweight",  # Use lightweight scenario for high concurrency
        concurrent_count,
        test_duration
    )
    
    # Load test assertions
    assert load_results["pipelines_completed"] >= 20, "At least 20 pipelines should complete under high load"
    assert load_results["successful_requests"] > 0, "Some requests should succeed under high load"
    
    success_rate = load_results["successful_requests"] / max(1, load_results["total_requests"])
    assert success_rate >= 0.7, "Success rate should be at least 70% under high load"
    
    # Record load test benchmarks
    benchmark_runner.record_result(
        'max_concurrent_users', load_results["pipelines_completed"], load_results["actual_duration_seconds"],
        {
            'concurrent_target': concurrent_count,
            'success_rate': success_rate,
            'avg_response_time_ms': load_results["avg_response_time_ms"],
            'throughput_rps': load_results["throughput_rps"]
        }
    )
    
    # Performance degradation checks
    if load_results["avg_response_time_ms"] > 0:
        assert load_results["avg_response_time_ms"] < 8000, "Average response time should be reasonable under load"
    
    logger.info(f"High-concurrency load test results: {load_results['pipelines_completed']} pipelines, "
               f"{success_rate:.2f} success rate, {load_results['throughput_rps']:.1f} RPS")

    suite.save_performance_report("concurrent_load_test_25plus", load_results)


@pytest.mark.performance
@pytest.mark.asyncio
async def test_performance_degradation_detection_and_alerting(performance_test_suite, benchmark_runner):
    """Test detection of performance degradation patterns."""
    suite = performance_test_suite
    
    # Baseline performance measurement
    baseline_results = await suite.execute_pipeline_performance_test("standard", concurrent_pipelines=3)
    assert len(baseline_results) >= 2, "Need baseline measurements"
    
    baseline_response_times = [p.p95_response_time_ms for p in baseline_results]
    baseline_p95 = statistics.mean(baseline_response_times)
    baseline_success_rates = [p.get_success_rate() for p in baseline_results]
    baseline_success_rate = statistics.mean(baseline_success_rates)
    
    # Simulated load for degradation testing
    degraded_results = await suite.execute_pipeline_performance_test("complex", concurrent_pipelines=8)
    assert len(degraded_results) >= 4, "Need degraded performance measurements"
    
    degraded_response_times = [p.p95_response_time_ms for p in degraded_results]
    degraded_p95 = statistics.mean(degraded_response_times)
    degraded_success_rates = [p.get_success_rate() for p in degraded_results]
    degraded_success_rate = statistics.mean(degraded_success_rates)
    
    # Detect performance degradation
    response_time_degradation = (degraded_p95 - baseline_p95) / baseline_p95
    success_rate_degradation = (baseline_success_rate - degraded_success_rate) / baseline_success_rate
    
    degradation_analysis = {
        "baseline": {
            "p95_response_time_ms": baseline_p95,
            "success_rate": baseline_success_rate,
            "scenario": "standard"
        },
        "degraded": {
            "p95_response_time_ms": degraded_p95,
            "success_rate": degraded_success_rate,
            "scenario": "complex_high_load"
        },
        "degradation_metrics": {
            "response_time_increase_percent": response_time_degradation * 100,
            "success_rate_decrease_percent": success_rate_degradation * 100,
            "performance_degraded": response_time_degradation > 0.5 or success_rate_degradation > 0.2
        }
    }
    
    # Record degradation metrics
    benchmark_runner.record_result(
        'performance_degradation_factor', response_time_degradation, time.time(),
        {
            'baseline_p95_ms': baseline_p95,
            'degraded_p95_ms': degraded_p95,
            'success_rate_impact': success_rate_degradation
        }
    )
    
    # Degradation detection assertions
    assert response_time_degradation < 3.0, "Response time degradation should be within acceptable limits"
    assert success_rate_degradation < 0.5, "Success rate should not degrade more than 50%"
    
    logger.info(f"Performance degradation analysis: response time +{response_time_degradation*100:.1f}%, "
               f"success rate -{success_rate_degradation*100:.1f}%")

    suite.save_performance_report("performance_degradation_detection", degradation_analysis)


if __name__ == "__main__":
    # Run performance tests
    pytest.main([__file__, "-v", "--asyncio-mode=auto", "-m", "performance"])