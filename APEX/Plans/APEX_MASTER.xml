<?xml version="1.0" encoding="UTF-8"?>
<specification>
    <metadata>
        <name>APEX-MASTER</name>
        <type>APEX BUSINESS PRODUCT SPEC</type>
        <version>1.0</version>
    </metadata>
    
    <primary-spec>
        <description>AIOps Closed-Loop Platform that captures 20% of AI cost savings</description>
        <goals>
            <goal priority="critical">LLM working as a complete end to end system with structured outputs</goal>
            <goal priority="critical">The Coherence of the system working in harmony with the application</goal>
            <goal priority="high">Type-safe structured generation for reliable agent interactions</goal>
        </goals>
    </primary-spec>
    
    <sections>
        <section id="PLANS" order="1">
            <title>MUST READ PLANS</title>
            <requirements>
                <requirement>Read APEX/Plans/APEX_AGENT_SPAWNING_GUIDE.md</requirement>
                <requirement>Read APEX/Plans/APEX_COMPLETE_IMPLEMENTATION_PLAN.md</requirement>
                <requirement>Read APEX/Plans/APEX_MIGRATION_PLAN.md</requirement>
                <requirement>Read APEX/Plans/APEX_QUICK_START_GUIDE.md</requirement>
                <requirement>Read APEX/Plans/APEX_TECHNICAL_ARCHITECTURE.md</requirement>
            </requirements>
        </section>
        
        <section id="Optional Background Context" order="2">
            <title>If extra context is needed</title>
            <background>
                <requirement>Prompts are templated and reusable</requirement>
                <requirement>Prompts are versioned and managed</requirement>
                <requirement>Prompt engineering is a key focus</requirement>
                <requirement>The_Wedge_Engine_Strategy.txt</requirement>
            </requirements>
        </section>
        
        <section id="responses" order="3">
            <title>Responses</title>
            <backgrounds>
                <background>LLM responses are parsed and validated</background>
                <background>Responses are cached to reduce latency and cost</background>
                <background>Fallbacks are in place for failed LLM requests</background>
                <background>Support for streaming responses via astream</background>
            </requirements>
        </section>
        
        <section id="configuration" order="4">
            <title>Configuration and Usage</title>
            <locations>
                <location>Central LLM Manager: app/llm/llm_manager.py</location>
                <location>Configuration: app/schemas/Config.py:167-197</location>
                <location>LLM configs in AppConfig.llm_configs dictionary</location>
            </locations>
            <agent-contexts>
                <context>triage: Message classification and routing</context>
                <context>data: Data analysis and processing</context>
                <context>optimizations_core: AI workload optimization</context>
                <context>actions_to_meet_goals: Goal-driven task execution</context>
                <context>reporting: Report generation</context>
                <context>analysis: General analysis tasks</context>
            </agent-contexts>
            <adding-provider>
                <step>Add to Config in app/schemas/Config.py with provider, model_name, generation_config</step>
                <step>Add SecretReference for API key management</step>
                <step>Set environment variable for API key</step>
                <step>Use via: response = await llm_manager.ask_llm(prompt, "config_name")</step>
            </adding-provider>
            <features>
                <feature>Supports Google (Gemini) and OpenAI providers out of the box</feature>
                <feature>Handles caching, mocking for dev mode, and streaming</feature>
                <feature>Secrets managed via SecretReference system</feature>
                <feature>Dev mode can disable LLMs with DEV_MODE_DISABLE_LLM=true</feature>
            </features>
        </section>
        
        <section id="structured-generation" order="5">
            <title>Structured Generation</title>
            <description>Support for type-safe structured outputs from LLMs using Pydantic models</description>
            <requirements>
                <requirement priority="critical">LLM manager must support structured output via with_structured_output method</requirement>
                <requirement priority="critical">All agent responses should use Pydantic models for type safety</requirement>
                <requirement priority="high">Support JSON schema validation for outputs</requirement>
                <requirement priority="high">Graceful fallback to text parsing when structured output fails</requirement>
            </requirements>
            <implementation>
                <step>Add with_structured_output support to LLM manager</step>
                <step>Define Pydantic response models for each agent type</step>
                <step>Update agents to use structured generation</step>
                <step>Add validation and error handling</step>
            </implementation>
            <usage-examples>
                <example name="triage-agent">
                    <description>Triage agent using structured output for classification</description>
                    <code><![CDATA[
from pydantic import BaseModel, Field
from typing import Literal, Optional, List

class TriageResponse(BaseModel):
    category: Literal["data", "optimization", "reporting", "general"]
    confidence: float = Field(ge=0.0, le=1.0)
    reasoning: str
    suggested_actions: List[str]
    requires_human: bool = False

# In triage_sub_agent.py
llm = self.llm_manager.get_llm("triage")
structured_llm = llm.with_structured_output(TriageResponse)
response = await structured_llm.ainvoke(prompt)
# response is now a TriageResponse instance with validated fields
                    ]]></code>
                </example>
                <example name="data-agent">
                    <description>Data agent using structured output for analysis results</description>
                    <code><![CDATA[
class DataAnalysisResponse(BaseModel):
    query: str
    results: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    error: Optional[str] = None
    execution_time_ms: float
    
# In data_sub_agent.py
structured_llm = llm.with_structured_output(DataAnalysisResponse)
response = await structured_llm.ainvoke(analysis_prompt)
                    ]]></code>
                </example>
            </usage-examples>
        </section>
        
        <section id="configuration" order="5">
            <title>Configuration</title>
            <requirements>
                <requirement>LLM configurations stored in AppConfig.llm_configs</requirement>
                <requirement>Support for environment-specific settings</requirement>
                <requirement>Mock LLM support for development mode</requirement>
                <requirement>Configurable generation parameters (temperature, max_tokens, etc.)</requirement>
            </requirements>
        </section>
        
        <section id="caching" order="6">
            <title>Caching</title>
            <requirements>
                <requirement>Response caching via llm_cache_service</requirement>
                <requirement>Cache key generation based on prompt and model</requirement>
                <requirement>Configurable cache TTL and size limits</requirement>
                <requirement>Cache invalidation on model updates</requirement>
            </requirements>
        </section>
        
        <section id="observability" order="7">
            <title>Observability and Logging</title>
            <description>Comprehensive logging for LLM calls and agent interactions to enable debugging and monitoring</description>
            <requirements>
                <requirement priority="critical">INFO level: Heartbeat logging every 2-3 seconds for long-running LLM calls</requirement>
                <requirement priority="critical">DEBUG level: Log all LLM input data (prompts, parameters)</requirement>
                <requirement priority="critical">DEBUG level: Log all LLM output data (responses, tokens used)</requirement>
                <requirement priority="critical">INFO level: Log all input/output data between subagents</requirement>
                <requirement priority="high">Include correlation IDs for tracing requests across agents</requirement>
                <requirement priority="high">Track execution time for all LLM calls</requirement>
                <requirement priority="high">Log retry attempts and fallback operations</requirement>
            </requirements>
            <implementation>
                <heartbeat-logging>
                    <description>Heartbeat mechanism for long-running operations</description>
                    <features>
                        <feature>Async task that logs heartbeat every 2-3 seconds while LLM call is running</feature>
                        <feature>Include correlation ID, agent name, and elapsed time in heartbeat</feature>
                        <feature>Stop heartbeat when LLM call completes or fails</feature>
                        <feature>Thread-safe implementation using asyncio tasks</feature>
                    </features>
                    <log-format>
                        <info-level>[INFO] LLM heartbeat: {agent_name} - {correlation_id} - elapsed: {elapsed_time}s - status: processing</info-level>
                    </log-format>
                </heartbeat-logging>
                <data-logging>
                    <description>Comprehensive data logging for debugging</description>
                    <input-logging>
                        <debug-level>[DEBUG] LLM input: {agent_name} - {correlation_id} - prompt_size: {size} - params: {params}</debug-level>
                        <debug-level>[DEBUG] LLM input prompt: {truncated_prompt}</debug-level>
                    </input-logging>
                    <output-logging>
                        <debug-level>[DEBUG] LLM output: {agent_name} - {correlation_id} - response_size: {size} - tokens: {tokens}</debug-level>
                        <debug-level>[DEBUG] LLM response: {truncated_response}</debug-level>
                    </output-logging>
                </data-logging>
                <subagent-communication>
                    <description>Log all communication between agents</description>
                    <info-level>[INFO] Agent communication: {from_agent} -> {to_agent} - {correlation_id} - type: {message_type}</info-level>
                    <info-level>[INFO] Agent input: {from_agent} -> {to_agent} - data_size: {size}</info-level>
                    <info-level>[INFO] Agent output: {to_agent} -> {from_agent} - data_size: {size} - status: {status}</info-level>
                </subagent-communication>
            </implementation>

        </section>
    </sections>
    
    <best-practices>
        <practice>Always use structured generation for agent responses to ensure type safety</practice>
        <practice>Define clear Pydantic models for each response type</practice>
        <practice>Include validation constraints in field definitions</practice>
        <practice>Provide fallback behavior when structured output fails</practice>
        <practice>Use appropriate generation config for each use case</practice>
        <practice>Cache structured responses to reduce costs</practice>
    </best-practices>
</specification>