name: Unified Test Runner

on:
  workflow_call:
    inputs:
      test_level:
        description: 'Test level: smoke, unit, integration, e2e, comprehensive'
        required: false
        type: string
        default: 'unit'
      changed_areas:
        description: 'JSON string of changed areas from determine-strategy'
        required: false
        type: string
        default: '{}'
      act_mode:
        description: 'Whether running in ACT mode'
        required: false
        type: string
        default: 'false'
      runner_type:
        description: 'Runner type to use'
        required: false
        type: string
        default: 'warp-custom-default'
      strategy:
        description: 'Test strategy: series, parallel, matrix'
        required: false
        type: string
        default: 'parallel'
      timeout_minutes:
        description: 'Test timeout in minutes'
        required: false
        type: number
        default: 30
      retry_failed:
        description: 'Retry failed tests'
        required: false
        type: boolean
        default: true
      max_retries:
        description: 'Maximum retry attempts'
        required: false
        type: number
        default: 2
      python_version:
        description: 'Python version to use'
        required: false
        type: string
        default: '3.11'
      node_version:
        description: 'Node.js version to use'
        required: false
        type: string
        default: '20'
      enable_real_services:
        description: 'Enable real service dependencies (Docker)'
        required: false
        type: boolean
        default: false
      shard_index:
        description: 'Test shard index for parallel execution'
        required: false
        type: number
        default: 0
      total_shards:
        description: 'Total number of test shards'
        required: false
        type: number
        default: 1
    outputs:
      test_status:
        description: 'Overall test status'
        value: ${{ jobs.determine-test-strategy.outputs.final_status }}
      coverage_percentage:
        description: 'Test coverage percentage'
        value: ${{ jobs.collect-coverage.outputs.coverage_percentage }}
      test_duration:
        description: 'Test execution duration'
        value: ${{ jobs.finalize-tests.outputs.total_duration }}
      failed_tests:
        description: 'List of failed tests'
        value: ${{ jobs.collect-results.outputs.failed_tests }}
      artifacts_url:
        description: 'URL to test artifacts'
        value: ${{ jobs.upload-artifacts.outputs.artifacts_url }}

permissions:
  contents: read
  pull-requests: write
  issues: write
  statuses: write
  checks: write

env:
  ACT: 'false'
  IS_ACT: 'false'
  TEST_DATABASE_URL: postgresql://test_user:test_password@localhost:5433/netra_test
  TEST_REDIS_URL: redis://:test_password@localhost:6380/0
  TEST_CLICKHOUSE_URL: http://test_user:test_password@localhost:8124/netra_analytics_test

jobs:
  # ==========================================
  # PHASE 1: DETERMINE TEST STRATEGY
  # ==========================================
  
  determine-test-strategy:
    runs-on: ${{ inputs.runner_type }}
    timeout-minutes: 5
    outputs:
      matrix_strategy: ${{ steps.build-matrix.outputs.strategy }}
      enable_services: ${{ steps.services.outputs.enable_services }}
      test_stages: ${{ steps.stages.outputs.stages }}
      final_status: ${{ steps.status.outputs.status }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Analyze test requirements
        id: analyze
        run: |
          echo "=== Test Strategy Analysis ==="
          
          # Parse changed areas
          CHANGED_AREAS='${{ inputs.changed_areas }}'
          TEST_LEVEL='${{ inputs.test_level }}'
          
          # Determine if we need real services
          ENABLE_SERVICES=false
          if [[ "$TEST_LEVEL" =~ ^(integration|e2e|comprehensive)$ ]]; then
            ENABLE_SERVICES=true
          fi
          
          # Override with input if specified
          if [[ "${{ inputs.enable_real_services }}" == "true" ]]; then
            ENABLE_SERVICES=true
          fi
          
          echo "enable_services=$ENABLE_SERVICES" >> $GITHUB_OUTPUT
          echo "test_level=$TEST_LEVEL" >> $GITHUB_OUTPUT
          
          echo "Test level: $TEST_LEVEL"
          echo "Enable services: $ENABLE_SERVICES"
          echo "Strategy: ${{ inputs.strategy }}"
          
      - name: Build test matrix
        id: build-matrix
        run: |
          echo "=== Building Test Matrix ==="
          
          STRATEGY='${{ inputs.strategy }}'
          TOTAL_SHARDS=${{ inputs.total_shards }}
          TEST_LEVEL='${{ inputs.test_level }}'
          
          if [[ "$STRATEGY" == "matrix" && $TOTAL_SHARDS -gt 1 ]]; then
            # Build parallel matrix
            MATRIX='{"include":['
            for i in $(seq 0 $((TOTAL_SHARDS-1))); do
              if [ $i -gt 0 ]; then MATRIX+=','; fi
              MATRIX+="{\"shard_index\":$i,\"shard_name\":\"shard-$i\"}"
            done
            MATRIX+=']}'
            echo "strategy=$MATRIX" >> $GITHUB_OUTPUT
            echo "Matrix strategy: $MATRIX"
          elif [[ "$STRATEGY" == "parallel" ]]; then
            # Parallel execution with default sharding
            MATRIX='{"include":[{"shard_index":0,"shard_name":"backend"},{"shard_index":1,"shard_name":"frontend"},{"shard_index":2,"shard_name":"integration"}]}'
            echo "strategy=$MATRIX" >> $GITHUB_OUTPUT
            echo "Parallel strategy: $MATRIX"
          else
            # Series execution - single job
            MATRIX='{"include":[{"shard_index":0,"shard_name":"unified"}]}'
            echo "strategy=$MATRIX" >> $GITHUB_OUTPUT
            echo "Series strategy: $MATRIX"
          fi
          
      - name: Determine service requirements
        id: services
        run: |
          ENABLE_SERVICES=${{ steps.analyze.outputs.enable_services }}
          echo "enable_services=$ENABLE_SERVICES" >> $GITHUB_OUTPUT
          
          if [[ "$ENABLE_SERVICES" == "true" ]]; then
            echo "Services will be started: PostgreSQL, Redis, ClickHouse"
          else
            echo "Services will use mocks/in-memory alternatives"
          fi
          
      - name: Define test stages
        id: stages
        run: |
          TEST_LEVEL='${{ inputs.test_level }}'
          
          case "$TEST_LEVEL" in
            "smoke")
              STAGES='["smoke"]'
              ;;
            "unit")
              STAGES='["unit"]'
              ;;
            "integration")
              STAGES='["unit","integration"]'
              ;;
            "e2e")
              STAGES='["unit","integration","e2e"]'
              ;;
            "comprehensive")
              STAGES='["smoke","unit","integration","e2e"]'
              ;;
            *)
              STAGES='["unit"]'
              ;;
          esac
          
          echo "stages=$STAGES" >> $GITHUB_OUTPUT
          echo "Test stages: $STAGES"
          
      - name: Set initial status
        id: status
        run: |
          echo "status=pending" >> $GITHUB_OUTPUT

  # ==========================================
  # PHASE 2: SERVICE DEPENDENCIES SETUP
  # ==========================================
  
  setup-services:
    runs-on: ${{ inputs.runner_type }}
    needs: determine-test-strategy
    if: needs.determine-test-strategy.outputs.enable_services == 'true'
    timeout-minutes: 10
    outputs:
      services_ready: ${{ steps.health-check.outputs.ready }}
      
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: netra_test
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
        ports:
          - 5433:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          
      redis:
        image: redis:7-alpine
        env:
          REDIS_PASSWORD: test_password
        ports:
          - 6380:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          
      clickhouse:
        image: clickhouse/clickhouse-server:24.1-alpine
        env:
          CLICKHOUSE_DB: netra_analytics_test
          CLICKHOUSE_USER: test_user
          CLICKHOUSE_PASSWORD: test_password
        ports:
          - 8124:8123
          - 9001:9000
        options: >-
          --health-cmd "wget --no-verbose --tries=1 --spider http://localhost:8123/ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Verify service connectivity
        id: health-check
        run: |
          echo "=== Service Health Check ==="
          
          # Check PostgreSQL
          until pg_isready -h localhost -p 5433 -U test_user; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done
          echo "✅ PostgreSQL is ready"
          
          # Check Redis
          until redis-cli -h localhost -p 6380 -a test_password ping; do
            echo "Waiting for Redis..."
            sleep 2
          done
          echo "✅ Redis is ready"
          
          # Check ClickHouse
          until curl -f http://localhost:8124/ping; do
            echo "Waiting for ClickHouse..."
            sleep 2
          done
          echo "✅ ClickHouse is ready"
          
          echo "ready=true" >> $GITHUB_OUTPUT
          echo "All services are healthy and ready"

  # ==========================================
  # PHASE 3: PARALLEL TEST EXECUTION
  # ==========================================
  
  run-tests:
    runs-on: ${{ inputs.runner_type }}
    needs: [determine-test-strategy, setup-services]
    if: always() && needs.determine-test-strategy.result == 'success'
    timeout-minutes: ${{ inputs.timeout_minutes }}
    
    strategy:
      matrix: ${{ fromJSON(needs.determine-test-strategy.outputs.matrix_strategy) }}
      fail-fast: false
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ inputs.python_version }}
          cache: 'pip'
          
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ inputs.node_version }}
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'
          
      - name: Install dependencies
        run: |
          echo "=== Installing Dependencies ==="
          
          # Install Python dependencies
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
          # Install frontend dependencies
          cd frontend && npm ci
          
      - name: Configure test environment
        run: |
          echo "=== Configuring Test Environment ==="
          
          # Set test environment variables
          echo "TEST_SHARD_INDEX=${{ matrix.shard_index }}" >> $GITHUB_ENV
          echo "TEST_SHARD_NAME=${{ matrix.shard_name }}" >> $GITHUB_ENV
          echo "TEST_LEVEL=${{ inputs.test_level }}" >> $GITHUB_ENV
          echo "CI_MODE=true" >> $GITHUB_ENV
          
          # Configure service URLs based on availability
          if [[ "${{ needs.setup-services.outputs.services_ready }}" == "true" ]]; then
            echo "USING_REAL_SERVICES=true" >> $GITHUB_ENV
            echo "DATABASE_URL=${{ env.TEST_DATABASE_URL }}" >> $GITHUB_ENV
            echo "REDIS_URL=${{ env.TEST_REDIS_URL }}" >> $GITHUB_ENV
            echo "CLICKHOUSE_URL=${{ env.TEST_CLICKHOUSE_URL }}" >> $GITHUB_ENV
          else
            echo "USING_REAL_SERVICES=false" >> $GITHUB_ENV
            echo "USE_MOCK_SERVICES=true" >> $GITHUB_ENV
          fi
          
          echo "Test configuration complete for shard: ${{ matrix.shard_name }}"
          
      - name: Run database migrations
        if: needs.setup-services.outputs.services_ready == 'true' && matrix.shard_index == 0
        run: |
          echo "=== Running Database Migrations ==="
          alembic upgrade head
          echo "Migrations completed successfully"
          
      - name: Execute test suite
        id: test-execution
        uses: nick-fields/retry@v2
        with:
          timeout_minutes: ${{ inputs.timeout_minutes }}
          max_attempts: ${{ inputs.retry_failed && inputs.max_retries || 1 }}
          retry_wait_seconds: 30
          command: |
            echo "=== Executing Tests for ${{ matrix.shard_name }} ==="
            
            # Build test command based on shard and level
            if [[ "${{ inputs.act_mode }}" == "true" ]]; then
              echo "🧪 ACT Mode: Running mock tests"
              echo "✅ Tests passed (mock)" > test_results_${{ matrix.shard_index }}.txt
              exit 0
            fi
            
            # Determine shard-specific arguments
            case "${{ matrix.shard_name }}" in
              "backend")
                TEST_ARGS="--backend-only --shard core"
                ;;
              "frontend")
                TEST_ARGS="--frontend-only"
                ;;
              "integration")
                TEST_ARGS="--level integration --shard api"
                ;;
              "unified"|*)
                TEST_ARGS="--level ${{ inputs.test_level }}"
                ;;
            esac
            
            # Add CI optimizations
            TEST_ARGS="$TEST_ARGS --ci --no-warnings --fast-fail"
            
            # Add coverage collection for non-smoke tests
            if [[ "${{ inputs.test_level }}" != "smoke" ]]; then
              TEST_ARGS="$TEST_ARGS --coverage-output coverage_${{ matrix.shard_index }}.xml"
            else
              TEST_ARGS="$TEST_ARGS --no-coverage"
            fi
            
            # Execute the test runner
            echo "Running: python test_runner.py $TEST_ARGS"
            python test_runner.py $TEST_ARGS
            
      - name: Collect test results
        if: always()
        run: |
          echo "=== Collecting Test Results ==="
          
          # Create results directory
          mkdir -p test_results
          
          # Collect test reports
          if [ -f "test-results/test_results.json" ]; then
            cp test-results/test_results.json test_results/results_${{ matrix.shard_index }}.json
          fi
          
          # Collect coverage reports
          if [ -f "coverage_${{ matrix.shard_index }}.xml" ]; then
            cp coverage_${{ matrix.shard_index }}.xml test_results/
          fi
          
          # Collect pytest XML reports
          if [ -f "pytest.xml" ]; then
            cp pytest.xml test_results/pytest_${{ matrix.shard_index }}.xml
          fi
          
          # Create shard summary
          SHARD_STATUS="${{ steps.test-execution.outcome }}"
          echo "{
            \"shard_index\": ${{ matrix.shard_index }},
            \"shard_name\": \"${{ matrix.shard_name }}\",
            \"status\": \"$SHARD_STATUS\",
            \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"
          }" > test_results/shard_${{ matrix.shard_index }}_summary.json
          
          echo "Results collection completed for shard ${{ matrix.shard_index }}"
          
      - name: Upload shard artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-shard-${{ matrix.shard_index }}-${{ github.run_id }}
          path: |
            test_results/
            *.log
          retention-days: 7
          
      - name: Generate failure report
        if: failure()
        run: |
          echo "=== Generating Failure Report ==="
          
          # Create failure report directory
          mkdir -p failure_reports
          
          # Collect logs
          echo "Collecting system logs..."
          if command -v journalctl &> /dev/null; then
            journalctl --no-pager --since "1 hour ago" > failure_reports/system_logs.txt 2>/dev/null || true
          fi
          
          # Collect service logs if services are running
          if [[ "${{ needs.setup-services.outputs.services_ready }}" == "true" ]]; then
            echo "Collecting service logs..."
            docker logs postgres > failure_reports/postgres_logs.txt 2>&1 || true
            docker logs redis > failure_reports/redis_logs.txt 2>&1 || true
            docker logs clickhouse > failure_reports/clickhouse_logs.txt 2>&1 || true
          fi
          
          # Collect Python process information
          ps aux | grep python > failure_reports/python_processes.txt || true
          
          # Create failure summary
          echo "{
            \"shard_index\": ${{ matrix.shard_index }},
            \"shard_name\": \"${{ matrix.shard_name }}\",
            \"failure_time\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",
            \"test_level\": \"${{ inputs.test_level }}\",
            \"runner_type\": \"${{ inputs.runner_type }}\",
            \"services_enabled\": \"${{ needs.setup-services.outputs.services_ready }}\"
          }" > failure_reports/failure_summary.json
          
      - name: Upload failure artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: failure-report-shard-${{ matrix.shard_index }}-${{ github.run_id }}
          path: |
            failure_reports/
            *.log
            core.*
          retention-days: 14

  # ==========================================
  # PHASE 4: COLLECT AND AGGREGATE RESULTS
  # ==========================================
  
  collect-results:
    runs-on: ${{ inputs.runner_type }}
    needs: [determine-test-strategy, run-tests]
    if: always() && needs.determine-test-strategy.result == 'success'
    timeout-minutes: 10
    outputs:
      overall_status: ${{ steps.aggregate.outputs.status }}
      failed_tests: ${{ steps.aggregate.outputs.failed_tests }}
      total_tests: ${{ steps.aggregate.outputs.total_tests }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: test-results-shard-*
          path: collected_results/
          
      - name: Aggregate test results
        id: aggregate
        run: |
          echo "=== Aggregating Test Results ==="
          
          # Initialize counters
          TOTAL_TESTS=0
          FAILED_TESTS=0
          PASSED_TESTS=0
          FAILED_TEST_LIST=""
          OVERALL_STATUS="success"
          
          # Process all shard results
          for shard_dir in collected_results/test-results-shard-*; do
            if [ -d "$shard_dir" ]; then
              echo "Processing $shard_dir"
              
              # Check shard summary
              if [ -f "$shard_dir/shard_*_summary.json" ]; then
                SHARD_STATUS=$(jq -r '.status' "$shard_dir"/shard_*_summary.json 2>/dev/null || echo "unknown")
                if [[ "$SHARD_STATUS" == "failure" ]]; then
                  OVERALL_STATUS="failure"
                fi
              fi
              
              # Process test results if available
              if [ -f "$shard_dir/results_*.json" ]; then
                # Parse test counts (implement based on your test result format)
                SHARD_TOTAL=$(jq -r '.stats.total // 0' "$shard_dir"/results_*.json 2>/dev/null || echo "0")
                SHARD_FAILED=$(jq -r '.stats.failed // 0' "$shard_dir"/results_*.json 2>/dev/null || echo "0")
                
                TOTAL_TESTS=$((TOTAL_TESTS + SHARD_TOTAL))
                FAILED_TESTS=$((FAILED_TESTS + SHARD_FAILED))
                
                # Collect failed test names
                if [ "$SHARD_FAILED" -gt 0 ]; then
                  SHARD_FAILURES=$(jq -r '.failures[].test_name // empty' "$shard_dir"/results_*.json 2>/dev/null || echo "")
                  if [ -n "$SHARD_FAILURES" ]; then
                    FAILED_TEST_LIST="$FAILED_TEST_LIST $SHARD_FAILURES"
                  fi
                fi
              fi
            fi
          done
          
          PASSED_TESTS=$((TOTAL_TESTS - FAILED_TESTS))
          
          # Output results
          echo "status=$OVERALL_STATUS" >> $GITHUB_OUTPUT
          echo "total_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
          echo "failed_tests=$FAILED_TESTS" >> $GITHUB_OUTPUT
          echo "passed_tests=$PASSED_TESTS" >> $GITHUB_OUTPUT
          echo "failed_test_list=$FAILED_TEST_LIST" >> $GITHUB_OUTPUT
          
          echo "=== Test Results Summary ==="
          echo "Overall Status: $OVERALL_STATUS"
          echo "Total Tests: $TOTAL_TESTS"
          echo "Passed Tests: $PASSED_TESTS"
          echo "Failed Tests: $FAILED_TESTS"
          if [ -n "$FAILED_TEST_LIST" ]; then
            echo "Failed Test Names: $FAILED_TEST_LIST"
          fi
          
      - name: Create unified test report
        run: |
          echo "=== Creating Unified Test Report ==="
          
          # Create comprehensive test report
          cat > unified_test_report.json << EOF
          {
            "workflow_run_id": "${{ github.run_id }}",
            "test_level": "${{ inputs.test_level }}",
            "strategy": "${{ inputs.strategy }}",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "results": {
              "overall_status": "${{ steps.aggregate.outputs.status }}",
              "total_tests": ${{ steps.aggregate.outputs.total_tests }},
              "passed_tests": ${{ steps.aggregate.outputs.passed_tests }},
              "failed_tests": ${{ steps.aggregate.outputs.failed_tests }}
            },
            "configuration": {
              "runner_type": "${{ inputs.runner_type }}",
              "python_version": "${{ inputs.python_version }}",
              "node_version": "${{ inputs.node_version }}",
              "timeout_minutes": ${{ inputs.timeout_minutes }},
              "retry_failed": ${{ inputs.retry_failed }},
              "services_enabled": "${{ needs.setup-services.outputs.services_ready || 'false' }}"
            },
            "shards": []
          }
          EOF
          
          # Add shard information to report
          python3 -c "
          import json
          import glob
          import os
          
          # Load base report
          with open('unified_test_report.json', 'r') as f:
              report = json.load(f)
          
          # Add shard data
          for shard_file in glob.glob('collected_results/*/shard_*_summary.json'):
              try:
                  with open(shard_file, 'r') as f:
                      shard_data = json.load(f)
                      report['shards'].append(shard_data)
              except Exception as e:
                  print(f'Warning: Could not process {shard_file}: {e}')
          
          # Save updated report
          with open('unified_test_report.json', 'w') as f:
              json.dump(report, f, indent=2)
          "
          
          echo "Unified test report created"
          
  # ==========================================
  # PHASE 5: COVERAGE COLLECTION
  # ==========================================
  
  collect-coverage:
    runs-on: ${{ inputs.runner_type }}
    needs: [determine-test-strategy, run-tests]
    if: always() && needs.determine-test-strategy.result == 'success' && inputs.test_level != 'smoke'
    timeout-minutes: 10
    outputs:
      coverage_percentage: ${{ steps.coverage.outputs.percentage }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ inputs.python_version }}
          cache: 'pip'
          
      - name: Install coverage tools
        run: |
          pip install coverage[toml] coverage-combine
          
      - name: Download coverage artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: test-results-shard-*
          path: coverage_results/
          
      - name: Combine coverage reports
        id: coverage
        run: |
          echo "=== Combining Coverage Reports ==="
          
          # Find all coverage files
          find coverage_results -name "coverage_*.xml" -exec cp {} . \;
          
          # Combine coverage if multiple files exist
          if ls coverage_*.xml 1> /dev/null 2>&1; then
            # Use Python to combine XML coverage reports
            python3 -c "
            import xml.etree.ElementTree as ET
            import glob
            import os
            
            coverage_files = glob.glob('coverage_*.xml')
            if not coverage_files:
                print('No coverage files found')
                exit(1)
            
            if len(coverage_files) == 1:
                # Single file, just rename it
                os.rename(coverage_files[0], 'combined_coverage.xml')
                print('Single coverage file found, renamed to combined_coverage.xml')
            else:
                # Multiple files, combine them
                print(f'Combining {len(coverage_files)} coverage files')
                
                # Simple combination - take the first file as base
                # In practice, you'd want more sophisticated merging
                import shutil
                shutil.copy(coverage_files[0], 'combined_coverage.xml')
                print('Coverage files combined (simplified merge)')
            
            # Extract coverage percentage
            tree = ET.parse('combined_coverage.xml')
            root = tree.getroot()
            line_rate = float(root.attrib.get('line-rate', 0))
            coverage_pct = int(line_rate * 100)
            print(f'Coverage percentage: {coverage_pct}%')
            
            with open('coverage_percentage.txt', 'w') as f:
                f.write(str(coverage_pct))
            "
            
            COVERAGE_PCT=$(cat coverage_percentage.txt 2>/dev/null || echo "0")
          else
            echo "No coverage files found"
            COVERAGE_PCT=0
          fi
          
          echo "percentage=$COVERAGE_PCT" >> $GITHUB_OUTPUT
          echo "Coverage percentage: $COVERAGE_PCT%"
          
      - name: Generate coverage badge
        run: |
          COVERAGE=${{ steps.coverage.outputs.percentage }}
          
          # Determine badge color based on coverage
          if [ $COVERAGE -ge 90 ]; then
            COLOR="brightgreen"
          elif [ $COVERAGE -ge 80 ]; then
            COLOR="green"
          elif [ $COVERAGE -ge 70 ]; then
            COLOR="yellow"
          elif [ $COVERAGE -ge 60 ]; then
            COLOR="orange"
          else
            COLOR="red"
          fi
          
          echo "Coverage: $COVERAGE% - Color: $COLOR"
          
          # Create simple badge data
          echo "{\"schemaVersion\":1,\"label\":\"coverage\",\"message\":\"${COVERAGE}%\",\"color\":\"$COLOR\"}" > coverage_badge.json

  # ==========================================
  # PHASE 6: UPLOAD ARTIFACTS
  # ==========================================
  
  upload-artifacts:
    runs-on: ${{ inputs.runner_type }}
    needs: [collect-results, collect-coverage]
    if: always()
    timeout-minutes: 10
    outputs:
      artifacts_url: ${{ steps.upload.outputs.artifact-url }}
      
    steps:
      - name: Create final artifacts structure
        run: |
          echo "=== Creating Final Artifacts ==="
          
          mkdir -p final_artifacts/{reports,coverage,logs,debug}
          
          # Copy reports
          if [ -f "unified_test_report.json" ]; then
            cp unified_test_report.json final_artifacts/reports/
          fi
          
          # Copy coverage
          if [ -f "combined_coverage.xml" ]; then
            cp combined_coverage.xml final_artifacts/coverage/
          fi
          
          if [ -f "coverage_badge.json" ]; then
            cp coverage_badge.json final_artifacts/coverage/
          fi
          
          # Create summary file
          cat > final_artifacts/test_summary.json << EOF
          {
            "run_id": "${{ github.run_id }}",
            "workflow": "unified-test-runner",
            "test_level": "${{ inputs.test_level }}",
            "status": "${{ needs.collect-results.outputs.overall_status }}",
            "total_tests": ${{ needs.collect-results.outputs.total_tests }},
            "failed_tests": ${{ needs.collect-results.outputs.failed_tests }},
            "coverage_percentage": "${{ needs.collect-coverage.outputs.coverage_percentage || '0' }}",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF
          
      - name: Upload final artifacts
        id: upload
        uses: actions/upload-artifact@v4
        with:
          name: unified-test-results-${{ github.run_id }}
          path: final_artifacts/
          retention-days: 30

  # ==========================================
  # PHASE 7: FINALIZE AND REPORT
  # ==========================================
  
  finalize-tests:
    runs-on: ${{ inputs.runner_type }}
    needs: [determine-test-strategy, collect-results, collect-coverage, upload-artifacts]
    if: always()
    timeout-minutes: 10
    outputs:
      total_duration: ${{ steps.duration.outputs.duration }}
      
    steps:
      - name: Calculate total duration
        id: duration
        run: |
          START_TIME="${{ github.event.head_commit.timestamp }}"
          END_TIME=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          
          # Simple duration calculation (in practice, you'd want more precise timing)
          echo "duration=estimated" >> $GITHUB_OUTPUT
          echo "Total test duration: estimated based on workflow execution"
          
      - name: Update commit status
        if: github.event_name == 'pull_request' && inputs.act_mode != 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const status = '${{ needs.collect-results.outputs.overall_status }}' === 'success' ? 'success' : 'failure';
            const coverage = '${{ needs.collect-coverage.outputs.coverage_percentage || '0' }}';
            const totalTests = '${{ needs.collect-results.outputs.total_tests }}';
            const failedTests = '${{ needs.collect-results.outputs.failed_tests }}';
            
            const description = status === 'success' ? 
              `✅ All tests passed (${totalTests} tests, ${coverage}% coverage)` :
              `❌ ${failedTests}/${totalTests} tests failed (${coverage}% coverage)`;
            
            await github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: status,
              description: description,
              context: 'tests/unified'
            });
            
      - name: Comment on PR with results
        if: github.event_name == 'pull_request' && inputs.act_mode != 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const status = '${{ needs.collect-results.outputs.overall_status }}';
            const totalTests = '${{ needs.collect-results.outputs.total_tests }}';
            const failedTests = '${{ needs.collect-results.outputs.failed_tests }}';
            const passedTests = '${{ needs.collect-results.outputs.passed_tests }}';
            const coverage = '${{ needs.collect-coverage.outputs.coverage_percentage || '0' }}';
            const testLevel = '${{ inputs.test_level }}';
            
            const statusIcon = status === 'success' ? '✅' : '❌';
            const coverageColor = coverage >= 80 ? '🟢' : coverage >= 60 ? '🟡' : '🔴';
            
            const body = `## ${statusIcon} Test Results - ${testLevel.toUpperCase()}
            
            **Overall Status:** ${status.toUpperCase()}
            
            ### 📊 Test Summary
            - **Total Tests:** ${totalTests}
            - **Passed:** ${passedTests}
            - **Failed:** ${failedTests}
            - **Coverage:** ${coverageColor} ${coverage}%
            
            ### 🔧 Configuration
            - **Test Level:** ${testLevel}
            - **Strategy:** ${{ inputs.strategy }}
            - **Runner:** ${{ inputs.runner_type }}
            - **Services:** ${{ needs.setup-services.outputs.services_ready || 'mocked' }}
            
            ${failedTests > 0 ? `### ❌ Failed Tests
            ${failedTests} test(s) failed. Check the [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for details.
            ` : ''}
            
            ### 📁 Artifacts
            - [Test Results](${{ needs.upload-artifacts.outputs.artifacts_url }})
            - [Coverage Report](${{ needs.upload-artifacts.outputs.artifacts_url }})
            
            ---
            *Generated by Unified Test Runner v2.0*`;
            
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
            
      - name: Generate step summary
        if: always()
        run: |
          echo "## 🧪 Unified Test Runner Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Test Level:** ${{ inputs.test_level }}" >> $GITHUB_STEP_SUMMARY
          echo "**Strategy:** ${{ inputs.strategy }}" >> $GITHUB_STEP_SUMMARY
          echo "**Overall Status:** ${{ needs.collect-results.outputs.overall_status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Tests:** ${{ needs.collect-results.outputs.total_tests }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Passed:** ${{ needs.collect-results.outputs.passed_tests }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed:** ${{ needs.collect-results.outputs.failed_tests }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Coverage:** ${{ needs.collect-coverage.outputs.coverage_percentage || '0' }}%" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Services" >> $GITHUB_STEP_SUMMARY
          echo "- **Database Services:** ${{ needs.setup-services.outputs.services_ready || 'disabled' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Service Dependencies:** ${{ inputs.enable_real_services && 'enabled' || 'mocked' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Add failure details if any
          if [[ "${{ needs.collect-results.outputs.overall_status }}" == "failure" ]]; then
            echo "### ❌ Failure Details" >> $GITHUB_STEP_SUMMARY
            echo "- **Failed Tests:** ${{ needs.collect-results.outputs.failed_tests }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Artifacts:** Available in workflow artifacts" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Unified Test Runner completed at $(date -u +%Y-%m-%dT%H:%M:%SZ)*" >> $GITHUB_STEP_SUMMARY