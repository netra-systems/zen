name: Agent Startup E2E Tests

description: |
  Comprehensive agent startup E2E tests with real service dependencies.
  Validates agent initialization, cold start performance, and user journey scenarios.
  
  Business Value Justification (BVJ):
  1. Segment: ALL customer segments (Free, Early, Mid, Enterprise) 
  2. Business Goal: Ensure agent startup reliability prevents customer churn
  3. Value Impact: Prevents 20-30% user abandonment due to slow startup
  4. Revenue Impact: Protects $75K+ ARR by ensuring startup performance SLAs

on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'app/agents/**'
      - 'app/core/**'
      - 'app/websocket/**'
      - 'app/main.py'
      - 'tests/unified/**agent_startup**'
      - 'tests/unified/**startup**'
      - 'run_agent_startup_tests.py'
      - '.github/workflows/agent-startup-e2e-tests.yml'
  push:
    branches: [main, develop]
    paths:
      - 'app/agents/**'
      - 'app/core/**'
      - 'app/websocket/**'
      - 'app/main.py'
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test execution mode'
        required: false
        type: choice
        options:
          - 'mock'
          - 'real-llm'
          - 'full-validation'
        default: 'mock'
      performance_baseline_check:
        description: 'Validate against performance baselines'
        required: false
        type: boolean
        default: true
      parallel_execution:
        description: 'Run tests in parallel (faster but higher resource usage)'
        required: false
        type: boolean
        default: true
      timeout_minutes:
        description: 'Test timeout in minutes'
        required: false
        type: number
        default: 45

# MANDATORY: Use warp-custom-default runner for ALL jobs (per github_actions.xml)
permissions:
  contents: read
  deployments: write
  pull-requests: write
  issues: write
  statuses: write
  checks: write

# Prevent redundant runs for PRs
concurrency:
  group: agent-startup-e2e-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  # Static defaults - ACT compatible (no self-referencing)
  ACT: 'false'
  IS_ACT: 'false'
  RUNNER_TYPE: 'warp-custom-default'
  # Test environment configuration
  TEST_DATABASE_URL: postgresql://test_user:test_password@localhost:5433/netra_test
  TEST_REDIS_URL: redis://:test_password@localhost:6380/0
  TEST_CLICKHOUSE_URL: http://test_user:test_password@localhost:8124/netra_analytics_test
  # Performance monitoring
  ENABLE_PERFORMANCE_BASELINES: 'true'
  AGENT_STARTUP_TIMEOUT: 300
  # Test configuration
  CI_AGENT_STARTUP_MODE: 'true'
  AGENT_TEST_PARALLELIZATION: 'true'

jobs:
  # ==========================================
  # PHASE 1: CONFIGURATION AND SETUP
  # ==========================================
  
  determine-test-strategy:
    name: Determine Test Strategy
    runs-on: warp-custom-default
    timeout-minutes: 5
    outputs:
      test_mode: ${{ steps.config.outputs.test_mode }}
      enable_real_llm: ${{ steps.config.outputs.enable_real_llm }}
      enable_performance_checks: ${{ steps.config.outputs.enable_performance_checks }}
      parallel_execution: ${{ steps.config.outputs.parallel_execution }}
      test_timeout: ${{ steps.config.outputs.test_timeout }}
      should_run_tests: ${{ steps.config.outputs.should_run_tests }}
      test_categories: ${{ steps.config.outputs.test_categories }}
      
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          
      - name: Determine Test Configuration
        id: config
        run: |
          echo "=== Agent Startup E2E Test Configuration ==="
          
          # Determine test mode based on inputs and event
          if [[ -n "${{ inputs.test_mode }}" ]]; then
            TEST_MODE="${{ inputs.test_mode }}"
          elif [[ "${{ github.event_name }}" == "push" && "${{ github.ref }}" == "refs/heads/main" ]]; then
            TEST_MODE="real-llm"  # Main branch gets full validation
          elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
            TEST_MODE="mock"  # PRs use mock mode for speed
          else
            TEST_MODE="mock"  # Default to mock
          fi
          
          # Configure LLM usage
          ENABLE_REAL_LLM="false"
          if [[ "$TEST_MODE" =~ ^(real-llm|full-validation)$ ]]; then
            ENABLE_REAL_LLM="true"
          fi
          
          # Performance baseline checks
          ENABLE_PERFORMANCE="${{ inputs.performance_baseline_check }}"
          if [[ -z "$ENABLE_PERFORMANCE" ]]; then
            ENABLE_PERFORMANCE="true"  # Default enabled
          fi
          
          # Parallel execution
          PARALLEL_EXEC="${{ inputs.parallel_execution }}"
          if [[ -z "$PARALLEL_EXEC" ]]; then
            PARALLEL_EXEC="true"  # Default parallel
          fi
          
          # Timeout configuration
          TIMEOUT_MIN="${{ inputs.timeout_minutes }}"
          if [[ -z "$TIMEOUT_MIN" || "$TIMEOUT_MIN" -eq 0 ]]; then
            if [[ "$ENABLE_REAL_LLM" == "true" ]]; then
              TIMEOUT_MIN=45  # Real LLM needs more time
            else
              TIMEOUT_MIN=30  # Mock mode is faster
            fi
          fi
          
          # Test categories based on mode
          if [[ "$TEST_MODE" == "full-validation" ]]; then
            CATEGORIES='["cold_start", "resilience", "reconnection", "load", "context", "performance"]'
          elif [[ "$TEST_MODE" == "real-llm" ]]; then
            CATEGORIES='["cold_start", "resilience", "reconnection", "context"]'
          else
            CATEGORIES='["cold_start", "resilience", "reconnection"]'  # Mock mode
          fi
          
          # Should run tests check
          SHOULD_RUN="true"
          
          # Check for skip conditions
          COMMIT_MSG="${{ github.event.head_commit.message || github.event.pull_request.title }}"
          if [[ "$COMMIT_MSG" == *"[skip agent-startup]"* || "$COMMIT_MSG" == *"[skip e2e]"* ]]; then
            echo "::notice::Skipping agent startup tests due to commit message"
            SHOULD_RUN="false"
          fi
          
          # Output configuration
          echo "test_mode=$TEST_MODE" >> $GITHUB_OUTPUT
          echo "enable_real_llm=$ENABLE_REAL_LLM" >> $GITHUB_OUTPUT
          echo "enable_performance_checks=$ENABLE_PERFORMANCE" >> $GITHUB_OUTPUT
          echo "parallel_execution=$PARALLEL_EXEC" >> $GITHUB_OUTPUT
          echo "test_timeout=$TIMEOUT_MIN" >> $GITHUB_OUTPUT
          echo "should_run_tests=$SHOULD_RUN" >> $GITHUB_OUTPUT
          echo "test_categories=$CATEGORIES" >> $GITHUB_OUTPUT
          
          echo "=== Configuration Summary ==="
          echo "Test Mode: $TEST_MODE"
          echo "Enable Real LLM: $ENABLE_REAL_LLM"
          echo "Performance Checks: $ENABLE_PERFORMANCE"
          echo "Parallel Execution: $PARALLEL_EXEC"
          echo "Timeout: $TIMEOUT_MIN minutes"
          echo "Should Run: $SHOULD_RUN"
          echo "Categories: $CATEGORIES"
          
      - name: Generate Test Strategy Summary
        run: |
          echo "## 🚀 Agent Startup E2E Test Strategy" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Test Mode:** ${{ steps.config.outputs.test_mode }}" >> $GITHUB_STEP_SUMMARY
          echo "**Real LLM:** ${{ steps.config.outputs.enable_real_llm == 'true' && 'Enabled' || 'Disabled (Mock)' }}" >> $GITHUB_STEP_SUMMARY
          echo "**Performance Checks:** ${{ steps.config.outputs.enable_performance_checks }}" >> $GITHUB_STEP_SUMMARY
          echo "**Execution:** ${{ steps.config.outputs.parallel_execution == 'true' && 'Parallel' || 'Sequential' }}" >> $GITHUB_STEP_SUMMARY
          echo "**Timeout:** ${{ steps.config.outputs.test_timeout }} minutes" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Categories" >> $GITHUB_STEP_SUMMARY
          echo "${{ steps.config.outputs.test_categories }}" | jq -r '.[] | "- " + .' >> $GITHUB_STEP_SUMMARY || echo "- Categories: ${{ steps.config.outputs.test_categories }}"

  # ==========================================
  # PHASE 2: SERVICE DEPENDENCIES SETUP
  # ==========================================
  
  setup-test-services:
    name: Setup Test Services
    runs-on: warp-custom-default
    needs: determine-test-strategy
    if: needs.determine-test-strategy.outputs.should_run_tests == 'true'
    timeout-minutes: 15
    outputs:
      services_ready: ${{ steps.health-check.outputs.ready }}
      postgres_status: ${{ steps.health-check.outputs.postgres_status }}
      redis_status: ${{ steps.health-check.outputs.redis_status }}
      clickhouse_status: ${{ steps.health-check.outputs.clickhouse_status }}
      
    services:
      postgres:
        image: postgres:17.6-alpine
        env:
          POSTGRES_DB: netra_test
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
        ports:
          - 5433:5432
        options: >-
          --health-cmd "pg_isready -U test_user -d netra_test"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
          --health-start-period 30s
          
      redis:
        image: redis:7-alpine
        env:
          REDIS_PASSWORD: test_password
        ports:
          - 6380:6379
        options: >-
          --health-cmd "redis-cli --no-auth-warning -a test_password ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
          
      clickhouse:
        image: clickhouse/clickhouse-server:24.1-alpine
        env:
          CLICKHOUSE_DB: netra_analytics_test
          CLICKHOUSE_USER: test_user
          CLICKHOUSE_PASSWORD: test_password
          CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
        ports:
          - 8124:8123
          - 9001:9000
        options: >-
          --health-cmd "wget --no-verbose --tries=1 --spider http://localhost:8123/ping"
          --health-interval 15s
          --health-timeout 10s
          --health-retries 8
          --health-start-period 60s
          
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Install Service Dependencies
        run: |
          echo "=== Installing Service Dependencies ==="
          
          # Update package lists
          sudo apt-get update -q
          
          # Install PostgreSQL client
          sudo apt-get install -y postgresql-client-common postgresql-client-17
          
          # Install Redis client
          sudo apt-get install -y redis-tools
          
          # Install curl for ClickHouse health checks
          sudo apt-get install -y curl wget jq
          
          echo "Service dependencies installed"
          
      - name: Comprehensive Service Health Check
        id: health-check
        run: |
          echo "=== Comprehensive Service Health Check ==="
          
          POSTGRES_STATUS="unknown"
          REDIS_STATUS="unknown"
          CLICKHOUSE_STATUS="unknown"
          OVERALL_READY="false"
          
          # PostgreSQL health check with retries
          echo "Checking PostgreSQL..."
          for i in {1..20}; do
            if pg_isready -h localhost -p 5433 -U test_user -d netra_test; then
              echo "✅ PostgreSQL is ready"
              POSTGRES_STATUS="ready"
              break
            fi
            echo "PostgreSQL attempt $i/20 - waiting..."
            sleep 3
          done
          
          # Redis health check with authentication
          echo "Checking Redis..."
          for i in {1..15}; do
            if redis-cli -h localhost -p 6380 --no-auth-warning -a test_password ping | grep -q PONG; then
              echo "✅ Redis is ready"
              REDIS_STATUS="ready"
              break
            fi
            echo "Redis attempt $i/15 - waiting..."
            sleep 2
          done
          
          # ClickHouse health check with retries
          echo "Checking ClickHouse..."
          for i in {1..15}; do
            if curl -s --max-time 5 http://localhost:8124/ping | grep -q "Ok"; then
              echo "✅ ClickHouse is ready"
              CLICKHOUSE_STATUS="ready"
              break
            fi
            echo "ClickHouse attempt $i/15 - waiting..."
            sleep 4
          done
          
          # Verify database creation
          if [[ "$POSTGRES_STATUS" == "ready" ]]; then
            echo "Verifying PostgreSQL database..."
            if PGPASSWORD=test_password psql -h localhost -p 5433 -U test_user -d netra_test -c "SELECT 1;" > /dev/null 2>&1; then
              echo "✅ PostgreSQL database accessible"
            else
              echo "❌ PostgreSQL database not accessible"
              POSTGRES_STATUS="error"
            fi
          fi
          
          # Overall readiness check
          if [[ "$POSTGRES_STATUS" == "ready" && "$REDIS_STATUS" == "ready" && "$CLICKHOUSE_STATUS" == "ready" ]]; then
            OVERALL_READY="true"
            echo "🎉 All services are ready!"
          else
            echo "⚠️ Some services are not ready:"
            echo "  PostgreSQL: $POSTGRES_STATUS"
            echo "  Redis: $REDIS_STATUS"
            echo "  ClickHouse: $CLICKHOUSE_STATUS"
          fi
          
          # Output results
          echo "ready=$OVERALL_READY" >> $GITHUB_OUTPUT
          echo "postgres_status=$POSTGRES_STATUS" >> $GITHUB_OUTPUT
          echo "redis_status=$REDIS_STATUS" >> $GITHUB_OUTPUT
          echo "clickhouse_status=$CLICKHOUSE_STATUS" >> $GITHUB_OUTPUT
          
      - name: Service Connectivity Test
        if: steps.health-check.outputs.ready == 'true'
        run: |
          echo "=== Service Connectivity Test ==="
          
          # Test PostgreSQL connection with query
          echo "Testing PostgreSQL query execution..."
          PGPASSWORD=test_password psql -h localhost -p 5433 -U test_user -d netra_test -c "
            CREATE TABLE IF NOT EXISTS health_check (id SERIAL PRIMARY KEY, created_at TIMESTAMP DEFAULT NOW());
            INSERT INTO health_check DEFAULT VALUES;
            SELECT COUNT(*) FROM health_check;
            DROP TABLE health_check;
          "
          
          # Test Redis operations
          echo "Testing Redis operations..."
          redis-cli -h localhost -p 6380 --no-auth-warning -a test_password SET agent_startup_test "connection_ok"
          REDIS_VALUE=$(redis-cli -h localhost -p 6380 --no-auth-warning -a test_password GET agent_startup_test)
          if [[ "$REDIS_VALUE" == "connection_ok" ]]; then
            echo "✅ Redis read/write test passed"
          else
            echo "❌ Redis read/write test failed"
            exit 1
          fi
          redis-cli -h localhost -p 6380 --no-auth-warning -a test_password DEL agent_startup_test
          
          # Test ClickHouse operations
          echo "Testing ClickHouse operations..."
          curl -X POST "http://localhost:8124/" \
            -H "Content-Type: text/plain" \
            -d "CREATE DATABASE IF NOT EXISTS agent_startup_test"
          
          curl -X POST "http://localhost:8124/" \
            -H "Content-Type: text/plain" \
            -d "SELECT 1 as health_check FORMAT JSON" | jq .
          
          echo "🎉 All service connectivity tests passed!"

  # ==========================================
  # PHASE 3: AGENT STARTUP E2E TESTS
  # ==========================================
  
  run-agent-startup-tests:
    name: Agent Startup E2E Tests
    runs-on: warp-custom-default
    needs: [determine-test-strategy, setup-test-services]
    if: always() && needs.determine-test-strategy.outputs.should_run_tests == 'true'
    timeout-minutes: ${{ fromJSON(needs.determine-test-strategy.outputs.test_timeout) }}
    
    strategy:
      matrix:
        test_category: ${{ fromJSON(needs.determine-test-strategy.outputs.test_categories) }}
      fail-fast: false
      
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            requirements-dev.txt
          
      - name: Install Dependencies
        run: |
          echo "=== Installing Python Dependencies ==="
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          
          echo "=== Verifying Agent Startup Test Dependencies ==="
          python -c "
          import sys
          required_modules = [
              'asyncio', 'pytest', 'websockets', 'aiohttp',
              'pydantic', 'sqlalchemy', 'redis', 'clickhouse_connect'
          ]
          missing = []
          for module in required_modules:
              try:
                  __import__(module)
                  print(f'✅ {module}')
              except ImportError:
                  missing.append(module)
                  print(f'❌ {module}')
          
          if missing:
              print(f'Missing modules: {missing}')
              sys.exit(1)
          else:
              print('All required modules available')
          "
          
      - name: Configure Test Environment
        run: |
          echo "=== Configuring Agent Startup Test Environment ==="
          
          # Base test configuration
          echo "TEST_CATEGORY=${{ matrix.test_category }}" >> $GITHUB_ENV
          echo "AGENT_STARTUP_E2E_MODE=true" >> $GITHUB_ENV
          echo "CI_MODE=true" >> $GITHUB_ENV
          echo "PYTEST_CURRENT_TEST=${{ matrix.test_category }}" >> $GITHUB_ENV
          
          # Service configuration based on setup results
          if [[ "${{ needs.setup-test-services.outputs.services_ready }}" == "true" ]]; then
            echo "USING_REAL_SERVICES=true" >> $GITHUB_ENV
            echo "DATABASE_URL=${{ env.TEST_DATABASE_URL }}" >> $GITHUB_ENV
            echo "REDIS_URL=${{ env.TEST_REDIS_URL }}" >> $GITHUB_ENV
            echo "CLICKHOUSE_URL=${{ env.TEST_CLICKHOUSE_URL }}" >> $GITHUB_ENV
            echo "✅ Using real database services"
          else
            echo "USING_REAL_SERVICES=false" >> $GITHUB_ENV
            echo "USE_MOCK_SERVICES=true" >> $GITHUB_ENV
            echo "⚠️ Using mock services (real services not available)"
          fi
          
          # LLM configuration
          if [[ "${{ needs.determine-test-strategy.outputs.enable_real_llm }}" == "true" ]]; then
            echo "ENABLE_REAL_LLM=true" >> $GITHUB_ENV
            echo "✅ Real LLM enabled for comprehensive testing"
          else
            echo "ENABLE_REAL_LLM=false" >> $GITHUB_ENV
            echo "USE_MOCK_LLM=true" >> $GITHUB_ENV
            echo "📝 Using LLM mocks for faster testing"
          fi
          
          # Performance configuration
          echo "ENABLE_PERFORMANCE_MONITORING=true" >> $GITHUB_ENV
          echo "PERFORMANCE_BASELINE_CHECK=${{ needs.determine-test-strategy.outputs.enable_performance_checks }}" >> $GITHUB_ENV
          echo "AGENT_STARTUP_TIMEOUT=300" >> $GITHUB_ENV
          
          # Execution configuration
          echo "PARALLEL_EXECUTION=${{ needs.determine-test-strategy.outputs.parallel_execution }}" >> $GITHUB_ENV
          echo "TEST_TIMEOUT_MINUTES=${{ needs.determine-test-strategy.outputs.test_timeout }}" >> $GITHUB_ENV
          
          echo "Test environment configured for category: ${{ matrix.test_category }}"
          
      - name: Database Migration and Setup
        if: needs.setup-test-services.outputs.services_ready == 'true'
        run: |
          echo "=== Database Migration and Setup ==="
          
          # Run Alembic migrations
          echo "Running database migrations..."
          alembic upgrade head
          
          # Verify database schema
          echo "Verifying database schema..."
          PGPASSWORD=test_password psql -h localhost -p 5433 -U test_user -d netra_test -c "
            SELECT table_name FROM information_schema.tables 
            WHERE table_schema = 'public' 
            ORDER BY table_name;
          "
          
          echo "Database setup completed"
          
      - name: Execute Agent Startup Tests
        id: test-execution
        run: |
          echo "=== Executing Agent Startup E2E Tests ==="
          echo "Category: ${{ matrix.test_category }}"
          echo "Real LLM: ${{ needs.determine-test-strategy.outputs.enable_real_llm }}"
          echo "Parallel: ${{ needs.determine-test-strategy.outputs.parallel_execution }}"
          
          # Build test command based on category and configuration
          TEST_ARGS="--category ${{ matrix.test_category }}"
          
          # Add LLM configuration
          if [[ "${{ needs.determine-test-strategy.outputs.enable_real_llm }}" == "true" ]]; then
            TEST_ARGS="$TEST_ARGS --real-llm"
          else
            TEST_ARGS="$TEST_ARGS --mock-llm"
          fi
          
          # Add execution mode
          if [[ "${{ needs.determine-test-strategy.outputs.parallel_execution }}" == "true" ]]; then
            TEST_ARGS="$TEST_ARGS --parallel"
          else
            TEST_ARGS="$TEST_ARGS --sequential"
          fi
          
          # Add performance checks
          if [[ "${{ needs.determine-test-strategy.outputs.enable_performance_checks }}" == "true" ]]; then
            TEST_ARGS="$TEST_ARGS --performance-baselines"
          fi
          
          # Add CI optimizations
          TEST_ARGS="$TEST_ARGS --ci --no-warnings --verbose"
          
          # Add timeout
          TEST_ARGS="$TEST_ARGS --timeout ${{ needs.determine-test-strategy.outputs.test_timeout }}"
          
          # Create results directory
          mkdir -p test_results
          
          # Execute the agent startup test runner
          echo "Running: python run_agent_startup_tests.py $TEST_ARGS"
          set +e  # Don't exit immediately on test failure
          python run_agent_startup_tests.py $TEST_ARGS
          EXIT_CODE=$?
          set -e
          
          # Capture exit code for analysis
          echo "TEST_EXIT_CODE=$EXIT_CODE" >> $GITHUB_ENV
          echo "test_exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT
          
          # Generate test summary
          if [[ $EXIT_CODE -eq 0 ]]; then
            echo "✅ Agent startup tests passed for category: ${{ matrix.test_category }}"
            echo "status=success" >> $GITHUB_OUTPUT
          else
            echo "❌ Agent startup tests failed for category: ${{ matrix.test_category }} (exit code: $EXIT_CODE)"
            echo "status=failure" >> $GITHUB_OUTPUT
          fi
          
          # Always continue to collect results
          exit 0
          
      - name: Collect Test Results and Artifacts
        if: always()
        run: |
          echo "=== Collecting Test Results and Artifacts ==="
          
          # Create structured results directory
          mkdir -p test_results/${{ matrix.test_category }}
          
          # Collect test reports
          if [ -f "test-reports/agent_startup_test_results.json" ]; then
            cp test-reports/agent_startup_test_results.json test_results/${{ matrix.test_category }}/
          fi
          
          # Collect performance data
          if [ -f "test-reports/performance_measurements.json" ]; then
            cp test-reports/performance_measurements.json test_results/${{ matrix.test_category }}/
          fi
          
          # Collect baseline comparisons
          if [ -f "test-reports/baseline_comparison.json" ]; then
            cp test-reports/baseline_comparison.json test_results/${{ matrix.test_category }}/
          fi
          
          # Collect pytest XML reports
          if [ -f "pytest.xml" ]; then
            cp pytest.xml test_results/${{ matrix.test_category }}/pytest_results.xml
          fi
          
          # Collect agent logs
          if [ -d "logs" ]; then
            cp -r logs/ test_results/${{ matrix.test_category }}/
          fi
          
          # Create category summary
          EXIT_CODE="${{ env.TEST_EXIT_CODE }}"
          TEST_STATUS="${{ steps.test-execution.outputs.status }}"
          
          cat > test_results/${{ matrix.test_category }}/category_summary.json << EOF
          {
            "category": "${{ matrix.test_category }}",
            "status": "$TEST_STATUS",
            "exit_code": $EXIT_CODE,
            "configuration": {
              "real_llm": "${{ needs.determine-test-strategy.outputs.enable_real_llm }}",
              "parallel_execution": "${{ needs.determine-test-strategy.outputs.parallel_execution }}",
              "performance_checks": "${{ needs.determine-test-strategy.outputs.enable_performance_checks }}",
              "services_ready": "${{ needs.setup-test-services.outputs.services_ready }}"
            },
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "runner": "warp-custom-default"
          }
          EOF
          
          echo "Results collected for category: ${{ matrix.test_category }}"
          
      - name: Upload Test Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: agent-startup-results-${{ matrix.test_category }}-${{ github.run_id }}
          path: |
            test_results/
            *.log
            logs/
            test-reports/
          retention-days: 14
          
      - name: Generate Failure Report
        if: failure() || steps.test-execution.outputs.status == 'failure'
        run: |
          echo "=== Generating Failure Report for ${{ matrix.test_category }} ==="
          
          mkdir -p failure_reports/${{ matrix.test_category }}
          
          # Collect system information
          echo "System Information:" > failure_reports/${{ matrix.test_category }}/system_info.txt
          uname -a >> failure_reports/${{ matrix.test_category }}/system_info.txt
          echo "Python version:" >> failure_reports/${{ matrix.test_category }}/system_info.txt
          python --version >> failure_reports/${{ matrix.test_category }}/system_info.txt
          echo "Disk space:" >> failure_reports/${{ matrix.test_category }}/system_info.txt
          df -h >> failure_reports/${{ matrix.test_category }}/system_info.txt
          
          # Collect service logs if services are running
          if [[ "${{ needs.setup-test-services.outputs.services_ready }}" == "true" ]]; then
            echo "Collecting service logs..."
            docker logs $(docker ps -q --filter "ancestor=postgres:17.6-alpine") > failure_reports/${{ matrix.test_category }}/postgres_logs.txt 2>&1 || true
            docker logs $(docker ps -q --filter "ancestor=redis:7-alpine") > failure_reports/${{ matrix.test_category }}/redis_logs.txt 2>&1 || true
            docker logs $(docker ps -q --filter "ancestor=clickhouse/clickhouse-server:24.1-alpine") > failure_reports/${{ matrix.test_category }}/clickhouse_logs.txt 2>&1 || true
          fi
          
          # Collect Python process information
          ps aux | grep python > failure_reports/${{ matrix.test_category }}/python_processes.txt || true
          
          # Create failure summary
          cat > failure_reports/${{ matrix.test_category }}/failure_summary.json << EOF
          {
            "category": "${{ matrix.test_category }}",
            "failure_time": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "exit_code": "${{ env.TEST_EXIT_CODE }}",
            "configuration": {
              "test_mode": "${{ needs.determine-test-strategy.outputs.test_mode }}",
              "real_llm": "${{ needs.determine-test-strategy.outputs.enable_real_llm }}",
              "services_ready": "${{ needs.setup-test-services.outputs.services_ready }}",
              "timeout_minutes": "${{ needs.determine-test-strategy.outputs.test_timeout }}"
            },
            "service_status": {
              "postgres": "${{ needs.setup-test-services.outputs.postgres_status }}",
              "redis": "${{ needs.setup-test-services.outputs.redis_status }}",
              "clickhouse": "${{ needs.setup-test-services.outputs.clickhouse_status }}"
            }
          }
          EOF
          
      - name: Upload Failure Artifacts
        if: failure() || steps.test-execution.outputs.status == 'failure'
        uses: actions/upload-artifact@v4
        with:
          name: agent-startup-failure-${{ matrix.test_category }}-${{ github.run_id }}
          path: |
            failure_reports/
            core.*
            *.log
            logs/
          retention-days: 30
          
      - name: Performance Regression Check
        if: steps.test-execution.outputs.status == 'success' && needs.determine-test-strategy.outputs.enable_performance_checks == 'true'
        run: |
          echo "=== Performance Regression Check for ${{ matrix.test_category }} ==="
          
          # Check if performance measurements exist
          if [ ! -f "test_results/${{ matrix.test_category }}/performance_measurements.json" ]; then
            echo "⚠️ No performance measurements found, skipping regression check"
            exit 0
          fi
          
          # Run performance baseline comparison
          python3 -c "
          import json
          import sys
          from pathlib import Path
          
          # Load current measurements
          measurements_file = Path('test_results/${{ matrix.test_category }}/performance_measurements.json')
          if not measurements_file.exists():
              print('No performance measurements found')
              sys.exit(0)
          
          with open(measurements_file) as f:
              measurements = json.load(f)
          
          # Load baselines
          baseline_file = Path('tests/unified/agent_startup_performance_baselines.json')
          if not baseline_file.exists():
              print('No performance baselines found')
              sys.exit(0)
          
          with open(baseline_file) as f:
              baselines = json.load(f)
          
          # Check for regressions
          regressions = []
          warnings = []
          
          for metric_name, measurement in measurements.get('metrics', {}).items():
              if metric_name not in baselines['metrics']:
                  continue
              
              baseline = baselines['metrics'][metric_name]
              current_value = measurement.get('value', 0)
              baseline_value = baseline['baseline_value']
              warning_threshold = baseline['warning_threshold']
              critical_threshold = baseline['critical_threshold']
              higher_is_better = baseline.get('higher_is_better', False)
              
              if higher_is_better:
                  if current_value < critical_threshold:
                      regressions.append(f'{metric_name}: {current_value} < {critical_threshold} (critical)')
                  elif current_value < warning_threshold:
                      warnings.append(f'{metric_name}: {current_value} < {warning_threshold} (warning)')
              else:
                  if current_value > critical_threshold:
                      regressions.append(f'{metric_name}: {current_value} > {critical_threshold} (critical)')
                  elif current_value > warning_threshold:
                      warnings.append(f'{metric_name}: {current_value} > {warning_threshold} (warning)')
          
          # Report results
          if regressions:
              print('🚨 PERFORMANCE REGRESSIONS DETECTED:')
              for regression in regressions:
                  print(f'  ❌ {regression}')
              sys.exit(1)
          elif warnings:
              print('⚠️ PERFORMANCE WARNINGS:')
              for warning in warnings:
                  print(f'  🟡 {warning}')
              print('Performance is within acceptable limits but showing degradation')
          else:
              print('✅ No performance regressions detected')
          "
          
      - name: Set Test Status
        if: always()
        run: |
          # Propagate test failure to job level
          if [[ "${{ steps.test-execution.outputs.status }}" == "failure" ]]; then
            echo "::error::Agent startup tests failed for category ${{ matrix.test_category }}"
            exit 1
          else
            echo "::notice::Agent startup tests passed for category ${{ matrix.test_category }}"
          fi

  # ==========================================
  # PHASE 4: RESULTS AGGREGATION AND REPORTING
  # ==========================================
  
  collect-and-analyze-results:
    name: Collect and Analyze Results
    runs-on: warp-custom-default
    needs: [determine-test-strategy, setup-test-services, run-agent-startup-tests]
    if: always() && needs.determine-test-strategy.outputs.should_run_tests == 'true'
    timeout-minutes: 15
    outputs:
      overall_status: ${{ steps.analyze.outputs.overall_status }}
      total_categories: ${{ steps.analyze.outputs.total_categories }}
      passed_categories: ${{ steps.analyze.outputs.passed_categories }}
      failed_categories: ${{ steps.analyze.outputs.failed_categories }}
      performance_status: ${{ steps.analyze.outputs.performance_status }}
      
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download All Test Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: agent-startup-results-*
          path: collected_results/
          
      - name: Analyze Results
        id: analyze
        run: |
          echo "=== Analyzing Agent Startup Test Results ==="
          
          # Initialize counters
          TOTAL_CATEGORIES=0
          PASSED_CATEGORIES=0
          FAILED_CATEGORIES=0
          OVERALL_STATUS="success"
          FAILED_CATEGORY_LIST=""
          PERFORMANCE_ISSUES=""
          PERFORMANCE_STATUS="ok"
          
          # Process all category results
          for result_dir in collected_results/agent-startup-results-*; do
            if [ -d "$result_dir" ]; then
              echo "Processing $result_dir"
              TOTAL_CATEGORIES=$((TOTAL_CATEGORIES + 1))
              
              # Find category summary
              SUMMARY_FILE=$(find "$result_dir" -name "category_summary.json" | head -1)
              if [ -f "$SUMMARY_FILE" ]; then
                CATEGORY=$(jq -r '.category' "$SUMMARY_FILE" 2>/dev/null || echo "unknown")
                STATUS=$(jq -r '.status' "$SUMMARY_FILE" 2>/dev/null || echo "unknown")
                EXIT_CODE=$(jq -r '.exit_code' "$SUMMARY_FILE" 2>/dev/null || echo "1")
                
                echo "Category: $CATEGORY, Status: $STATUS, Exit Code: $EXIT_CODE"
                
                if [[ "$STATUS" == "success" && "$EXIT_CODE" == "0" ]]; then
                  PASSED_CATEGORIES=$((PASSED_CATEGORIES + 1))
                  echo "✅ $CATEGORY passed"
                else
                  FAILED_CATEGORIES=$((FAILED_CATEGORIES + 1))
                  OVERALL_STATUS="failure"
                  FAILED_CATEGORY_LIST="$FAILED_CATEGORY_LIST $CATEGORY"
                  echo "❌ $CATEGORY failed"
                fi
                
                # Check for performance issues
                PERFORMANCE_FILE=$(find "$result_dir" -name "performance_measurements.json" | head -1)
                if [ -f "$PERFORMANCE_FILE" ]; then
                  # Simple check for performance warnings/errors in the file
                  if grep -q -i "regression\|critical\|error" "$PERFORMANCE_FILE" 2>/dev/null; then
                    PERFORMANCE_ISSUES="$PERFORMANCE_ISSUES $CATEGORY"
                    PERFORMANCE_STATUS="warning"
                  fi
                fi
              else
                echo "⚠️ No summary found for $result_dir"
                FAILED_CATEGORIES=$((FAILED_CATEGORIES + 1))
                OVERALL_STATUS="failure"
              fi
            fi
          done
          
          # Output results
          echo "overall_status=$OVERALL_STATUS" >> $GITHUB_OUTPUT
          echo "total_categories=$TOTAL_CATEGORIES" >> $GITHUB_OUTPUT
          echo "passed_categories=$PASSED_CATEGORIES" >> $GITHUB_OUTPUT
          echo "failed_categories=$FAILED_CATEGORIES" >> $GITHUB_OUTPUT
          echo "failed_category_list=$FAILED_CATEGORY_LIST" >> $GITHUB_OUTPUT
          echo "performance_status=$PERFORMANCE_STATUS" >> $GITHUB_OUTPUT
          echo "performance_issues=$PERFORMANCE_ISSUES" >> $GITHUB_OUTPUT
          
          echo "=== Analysis Summary ==="
          echo "Total Categories: $TOTAL_CATEGORIES"
          echo "Passed: $PASSED_CATEGORIES"
          echo "Failed: $FAILED_CATEGORIES"
          echo "Overall Status: $OVERALL_STATUS"
          echo "Performance Status: $PERFORMANCE_STATUS"
          if [ -n "$FAILED_CATEGORY_LIST" ]; then
            echo "Failed Categories:$FAILED_CATEGORY_LIST"
          fi
          if [ -n "$PERFORMANCE_ISSUES" ]; then
            echo "Performance Issues:$PERFORMANCE_ISSUES"
          fi
          
      - name: Generate Unified Test Report
        run: |
          echo "=== Generating Unified Test Report ==="
          
          # Create comprehensive test report
          cat > unified_agent_startup_report.json << EOF
          {
            "workflow_run_id": "${{ github.run_id }}",
            "test_suite": "agent-startup-e2e",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "configuration": {
              "test_mode": "${{ needs.determine-test-strategy.outputs.test_mode }}",
              "real_llm": "${{ needs.determine-test-strategy.outputs.enable_real_llm }}",
              "parallel_execution": "${{ needs.determine-test-strategy.outputs.parallel_execution }}",
              "performance_checks": "${{ needs.determine-test-strategy.outputs.enable_performance_checks }}",
              "timeout_minutes": "${{ needs.determine-test-strategy.outputs.test_timeout }}"
            },
            "results": {
              "overall_status": "${{ steps.analyze.outputs.overall_status }}",
              "total_categories": ${{ steps.analyze.outputs.total_categories }},
              "passed_categories": ${{ steps.analyze.outputs.passed_categories }},
              "failed_categories": ${{ steps.analyze.outputs.failed_categories }},
              "performance_status": "${{ steps.analyze.outputs.performance_status }}"
            },
            "service_status": {
              "postgres": "${{ needs.setup-test-services.outputs.postgres_status }}",
              "redis": "${{ needs.setup-test-services.outputs.redis_status }}",
              "clickhouse": "${{ needs.setup-test-services.outputs.clickhouse_status }}"
            },
            "categories": []
          }
          EOF
          
          # Add category details from collected results
          python3 -c "
          import json
          import glob
          import os
          from pathlib import Path
          
          # Load base report
          with open('unified_agent_startup_report.json', 'r') as f:
              report = json.load(f)
          
          # Add category data
          for summary_file in glob.glob('collected_results/*/category_summary.json'):
              try:
                  with open(summary_file, 'r') as f:
                      category_data = json.load(f)
                      report['categories'].append(category_data)
              except Exception as e:
                  print(f'Warning: Could not process {summary_file}: {e}')
          
          # Save updated report
          with open('unified_agent_startup_report.json', 'w') as f:
              json.dump(report, f, indent=2)
          
          print(f'Unified report generated with {len(report[\"categories\"])} categories')
          "
          
      - name: Upload Unified Report
        uses: actions/upload-artifact@v4
        with:
          name: agent-startup-unified-report-${{ github.run_id }}
          path: |
            unified_agent_startup_report.json
          retention-days: 30

  # ==========================================
  # PHASE 5: PR INTEGRATION AND NOTIFICATIONS
  # ==========================================
  
  update-pr-status:
    name: Update PR Status
    runs-on: warp-custom-default
    needs: [determine-test-strategy, setup-test-services, collect-and-analyze-results]
    if: always() && github.event_name == 'pull_request' && needs.determine-test-strategy.outputs.should_run_tests == 'true'
    timeout-minutes: 10
    
    steps:
      - name: Update Commit Status
        uses: actions/github-script@v7
        with:
          script: |
            const overallStatus = '${{ needs.collect-and-analyze-results.outputs.overall_status }}';
            const totalCategories = '${{ needs.collect-and-analyze-results.outputs.total_categories }}';
            const passedCategories = '${{ needs.collect-and-analyze-results.outputs.passed_categories }}';
            const failedCategories = '${{ needs.collect-and-analyze-results.outputs.failed_categories }}';
            const performanceStatus = '${{ needs.collect-and-analyze-results.outputs.performance_status }}';
            const testMode = '${{ needs.determine-test-strategy.outputs.test_mode }}';
            
            const state = overallStatus === 'success' ? 'success' : 'failure';
            const statusIcon = overallStatus === 'success' ? '✅' : '❌';
            const performanceIcon = performanceStatus === 'ok' ? '🚀' : performanceStatus === 'warning' ? '⚠️' : '🚨';
            
            const description = overallStatus === 'success' 
              ? `${statusIcon} All agent startup tests passed (${passedCategories}/${totalCategories}) ${performanceIcon}`
              : `${statusIcon} ${failedCategories}/${totalCategories} agent startup tests failed ${performanceIcon}`;
            
            await github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: state,
              context: 'agent-startup-e2e',
              description: description,
              target_url: `${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`
            });
            
      - name: Comment on PR with Detailed Results
        uses: actions/github-script@v7
        with:
          script: |
            const overallStatus = '${{ needs.collect-and-analyze-results.outputs.overall_status }}';
            const totalCategories = '${{ needs.collect-and-analyze-results.outputs.total_categories }}';
            const passedCategories = '${{ needs.collect-and-analyze-results.outputs.passed_categories }}';
            const failedCategories = '${{ needs.collect-and-analyze-results.outputs.failed_categories }}';
            const performanceStatus = '${{ needs.collect-and-analyze-results.outputs.performance_status }}';
            const testMode = '${{ needs.determine-test-strategy.outputs.test_mode }}';
            const realLLM = '${{ needs.determine-test-strategy.outputs.enable_real_llm }}';
            const servicesReady = '${{ needs.setup-test-services.outputs.services_ready }}';
            
            const statusIcon = overallStatus === 'success' ? '✅' : '❌';
            const performanceIcon = performanceStatus === 'ok' ? '🚀' : performanceStatus === 'warning' ? '⚠️' : '🚨';
            const llmIcon = realLLM === 'true' ? '🧠' : '🤖';
            const servicesIcon = servicesReady === 'true' ? '🐳' : '💾';
            
            let body = `## ${statusIcon} Agent Startup E2E Test Results
            
            **Overall Status:** ${overallStatus.toUpperCase()}
            
            ### 📊 Test Summary
            - **Total Categories:** ${totalCategories}
            - **Passed:** ✅ ${passedCategories}
            - **Failed:** ❌ ${failedCategories}
            - **Performance:** ${performanceIcon} ${performanceStatus.toUpperCase()}
            
            ### 🔧 Test Configuration
            - **Test Mode:** ${testMode}
            - **LLM:** ${llmIcon} ${realLLM === 'true' ? 'Real LLM API calls' : 'Mock/Simulated'}
            - **Services:** ${servicesIcon} ${servicesReady === 'true' ? 'Real database services' : 'Mock services'}
            - **Timeout:** ${{ needs.determine-test-strategy.outputs.test_timeout }} minutes
            
            ### 🏗️ Service Status
            - **PostgreSQL:** ${{ needs.setup-test-services.outputs.postgres_status }}
            - **Redis:** ${{ needs.setup-test-services.outputs.redis_status }}
            - **ClickHouse:** ${{ needs.setup-test-services.outputs.clickhouse_status }}
            `;
            
            if (failedCategories > 0) {
              body += `
            ### ❌ Failed Categories
            Agent startup tests failed for ${failedCategories} categories. Check the [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for detailed logs and failure reports.
            
            **Business Impact:** Agent startup failures can lead to 20-30% user abandonment and impact customer satisfaction.
              `;
            }
            
            if (performanceStatus !== 'ok') {
              body += `
            ### ⚠️ Performance Issues
            Performance regression detected in agent startup tests. This may impact user experience and SLA compliance.
              `;
            }
            
            body += `
            ### 📁 Test Artifacts
            - [Unified Test Report](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - [Performance Measurements](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - [Test Category Results](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            ### 🎯 Business Value
            **Segment Impact:** ALL customer segments (Free, Early, Mid, Enterprise)
            **Revenue Protection:** $75K+ ARR protected by ensuring startup performance SLAs
            **SLA Compliance:** Enterprise customers require <2.0s cold start, Mid-tier <2.5s
            
            ---
            *Generated by Agent Startup E2E Test Runner v1.0*
            `;
            
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
            
      - name: Generate Workflow Summary
        run: |
          echo "## 🚀 Agent Startup E2E Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Overall Status:** ${{ needs.collect-and-analyze-results.outputs.overall_status }}" >> $GITHUB_STEP_SUMMARY
          echo "**Test Mode:** ${{ needs.determine-test-strategy.outputs.test_mode }}" >> $GITHUB_STEP_SUMMARY
          echo "**Real LLM:** ${{ needs.determine-test-strategy.outputs.enable_real_llm }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Categories:** ${{ needs.collect-and-analyze-results.outputs.total_categories }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Passed:** ${{ needs.collect-and-analyze-results.outputs.passed_categories }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed:** ${{ needs.collect-and-analyze-results.outputs.failed_categories }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Performance:** ${{ needs.collect-and-analyze-results.outputs.performance_status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Service Dependencies" >> $GITHUB_STEP_SUMMARY
          echo "- **PostgreSQL:** ${{ needs.setup-test-services.outputs.postgres_status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Redis:** ${{ needs.setup-test-services.outputs.redis_status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **ClickHouse:** ${{ needs.setup-test-services.outputs.clickhouse_status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Business Value: Protects $75K+ ARR by ensuring agent startup reliability*" >> $GITHUB_STEP_SUMMARY

  # ==========================================
  # PHASE 6: FINAL STATUS AND PROPAGATION
  # ==========================================
  
  final-status:
    name: Final Test Status
    runs-on: warp-custom-default
    needs: [determine-test-strategy, collect-and-analyze-results]
    if: always() && needs.determine-test-strategy.outputs.should_run_tests == 'true'
    timeout-minutes: 5
    
    steps:
      - name: Propagate Test Results
        run: |
          echo "=== Final Agent Startup E2E Test Status ==="
          
          OVERALL_STATUS="${{ needs.collect-and-analyze-results.outputs.overall_status }}"
          TOTAL_CATEGORIES="${{ needs.collect-and-analyze-results.outputs.total_categories }}"
          FAILED_CATEGORIES="${{ needs.collect-and-analyze-results.outputs.failed_categories }}"
          
          echo "Overall Status: $OVERALL_STATUS"
          echo "Total Categories: $TOTAL_CATEGORIES"
          echo "Failed Categories: $FAILED_CATEGORIES"
          
          if [[ "$OVERALL_STATUS" == "failure" ]]; then
            echo "::error::Agent startup E2E tests failed ($FAILED_CATEGORIES/$TOTAL_CATEGORIES categories failed)"
            echo "::error::This may impact customer experience and startup SLAs"
            echo "Business Impact: Failed agent startup tests can lead to 20-30% user abandonment"
            exit 1
          elif [[ "${{ needs.collect-and-analyze-results.outputs.performance_status }}" != "ok" ]]; then
            echo "::warning::Agent startup tests passed but performance issues detected"
            echo "::warning::Performance regressions may impact customer SLA compliance"
            # Don't fail for performance warnings, but alert
          else
            echo "::notice::All agent startup E2E tests passed successfully"
            echo "::notice::Agent startup performance is within acceptable limits"
          fi
          
          echo "Agent startup E2E test evaluation completed"