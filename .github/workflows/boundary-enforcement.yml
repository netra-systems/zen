name: Boundary Enforcement
# CRITICAL: Ultra Deep Thinking Enforcement System
# Prevents architectural violations at CI/CD level

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: read
  pull-requests: write  # For PR comments
  checks: write        # For check status

env:
  # ACT compatibility flags
  ACT_DETECTED: ${{ env.ACT || 'false' }}
  ACT_LOCAL_MODE: ${{ env.ACT_LOCAL_MODE || 'true' }}
  ACT_VERBOSE: ${{ env.ACT_VERBOSE || 'true' }}

jobs:
  boundary-enforcement:
    name: Architecture Boundary Enforcement
    runs-on: warp-custom-default
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for growth analysis
      
      - name: ACT Detection and Environment Setup
        run: |
          echo "=== Boundary Enforcement ACT Compatibility ==="
          if [[ "${{ env.ACT_DETECTED }}" == "true" ]]; then
            echo "ðŸ§ª Running in ACT (local) mode"
            echo "- Local file system mode: ${{ env.ACT_LOCAL_MODE }}"
            echo "- Verbose logging: ${{ env.ACT_VERBOSE }}"
            echo "ACT_MODE=true" >> $GITHUB_ENV
            
            # Set local paths for file operations
            echo "WORKSPACE_PATH=${GITHUB_WORKSPACE}" >> $GITHUB_ENV
            echo "SCRIPTS_PATH=${GITHUB_WORKSPACE}/scripts" >> $GITHUB_ENV
            
            if [[ "${{ env.ACT_VERBOSE }}" == "true" ]]; then
              echo "ðŸ“‚ Workspace directory: ${GITHUB_WORKSPACE}"
              echo "ðŸ“‚ Scripts directory: ${GITHUB_WORKSPACE}/scripts"
              ls -la ${GITHUB_WORKSPACE}/scripts/ || echo "Scripts directory not found"
            fi
          else
            echo "â˜ï¸ Running in GitHub Actions (cloud) mode"
            echo "ACT_MODE=false" >> $GITHUB_ENV
          fi
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'
      
      - name: Install enforcement dependencies
        run: |
          if [[ "${{ env.ACT_MODE }}" == "true" ]]; then
            echo "ðŸ§ª Installing dependencies for ACT mode"
          fi
          pip install radon ast-grep
          # Minimal dependencies for boundary enforcement
          
          # Verify installations
          python --version
          pip list | grep -E "(radon|ast-grep)" || echo "Dependencies may not be installed correctly"
      
      - name: Run Critical Boundary Checks
        id: boundary-check
        run: |
          echo "ðŸ”´ CRITICAL: Running Boundary Enforcement..."
          
          # ACT-specific verbose logging
          if [[ "${{ env.ACT_MODE }}" == "true" ]]; then
            echo "ðŸ§ª ACT MODE: Boundary checks with local file system"
            echo "Working directory: $(pwd)"
            echo "Python path: $(which python)"
            
            if [[ "${{ env.ACT_VERBOSE }}" == "true" ]]; then
              echo "ðŸ“‚ Current directory contents:"
              ls -la .
              echo "ðŸ“‚ Scripts directory check:"
              ls -la scripts/ || echo "No scripts directory found"
              
              # Check if boundary enforcer exists
              if [ -f "scripts/boundary_enforcer.py" ]; then
                echo "âœ… boundary_enforcer.py found"
              else
                echo "âŒ boundary_enforcer.py not found, creating mock version"
                mkdir -p scripts
                cat > scripts/boundary_enforcer.py << 'EOF'
#!/usr/bin/env python3
import sys
import json
import argparse

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--enforce', action='store_true')
    parser.add_argument('--json-output')
    parser.add_argument('--pr-comment', action='store_true')
    parser.add_argument('--fail-on-emergency', action='store_true')
    args = parser.parse_args()
    
    if args.json_output:
        # Mock boundary report
        report = {
            "total_violations": 2,
            "file_violations": {"large_files": ["mock_file.py"]},
            "function_violations": {"large_functions": ["mock_function"]},
            "emergency_violations": [],
            "act_mode": True
        }
        with open(args.json_output, 'w') as f:
            json.dump(report, f, indent=2)
    
    if args.pr_comment:
        print("ðŸ§ª ACT MODE: Mock boundary enforcement summary")
        print("- Found 2 minor violations (mocked)")
        print("- No emergency violations")
        
    if args.fail_on_emergency:
        print("ðŸ§ª ACT MODE: No emergency violations (mocked)")
        sys.exit(0)
        
    if args.enforce:
        print("ðŸ§ª ACT MODE: Mock enforcement completed")
        sys.exit(0)

if __name__ == "__main__":
    main()
EOF
                chmod +x scripts/boundary_enforcer.py
              fi
            fi
          fi
          
          # Run boundary enforcement
          if [ -f "scripts/boundary_enforcer.py" ]; then
            python scripts/boundary_enforcer.py --enforce --json-output boundary-report.json
          else
            echo "âŒ Boundary enforcer script not found"
            exit 1
          fi
          
          # Capture exit code for later use
          BOUNDARY_EXIT_CODE=$?
          echo "boundary-exit-code=$BOUNDARY_EXIT_CODE" >> $GITHUB_OUTPUT
          
          # Generate summary for GitHub
          if [ -f "scripts/boundary_enforcer.py" ]; then
            python scripts/boundary_enforcer.py --pr-comment > boundary-summary.txt
          else
            echo "ðŸ§ª ACT MODE: Mock boundary summary" > boundary-summary.txt
          fi
          
          exit $BOUNDARY_EXIT_CODE
        continue-on-error: true
      
      - name: Check for Emergency Violations
        id: emergency-check
        run: |
          echo "ðŸš¨ Checking for EMERGENCY-level violations..."
          python scripts/boundary_enforcer.py --fail-on-emergency
          EMERGENCY_EXIT_CODE=$?
          echo "emergency-exit-code=$EMERGENCY_EXIT_CODE" >> $GITHUB_OUTPUT
          
          if [ $EMERGENCY_EXIT_CODE -ne 0 ]; then
            echo "ðŸš¨ EMERGENCY VIOLATIONS DETECTED - Build MUST fail"
            echo "emergency-detected=true" >> $GITHUB_OUTPUT
          else
            echo "emergency-detected=false" >> $GITHUB_OUTPUT
          fi
          
          exit $EMERGENCY_EXIT_CODE
        continue-on-error: true
      
      - name: Analyze Growth Patterns
        id: growth-analysis
        run: |
          echo "ðŸ“ˆ Analyzing system growth patterns..."
          
          # ACT mode handling
          if [[ "${{ env.ACT_MODE }}" == "true" ]]; then
            echo "ðŸ§ª ACT MODE: Growth pattern analysis"
            
            # Check if compliance checker exists, create mock if not
            if [ ! -f "scripts/check_architecture_compliance.py" ]; then
              echo "Creating mock compliance checker for ACT"
              cat > scripts/check_architecture_compliance.py << 'EOF'
#!/usr/bin/env python3
import sys
import json
import argparse

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--json-output')
    args = parser.parse_args()
    
    if args.json_output:
        # Mock compliance report
        report = {
            "total_violations": 5,
            "compliance_score": 85,
            "file_count": 150,
            "average_file_size": 180,
            "largest_files": ["mock_large_file.py"],
            "act_mode": True
        }
        with open(args.json_output, 'w') as f:
            json.dump(report, f, indent=2)
    
    print("ðŸ§ª ACT MODE: Mock compliance check completed")

if __name__ == "__main__":
    main()
EOF
              chmod +x scripts/check_architecture_compliance.py
            fi
          fi
          
          # Run compliance check
          python scripts/check_architecture_compliance.py --json-output compliance-report.json
          
          # Extract key metrics with error handling
          TOTAL_VIOLATIONS=$(python -c "
          import json
          try:
              with open('compliance-report.json') as f:
                  data = json.load(f)
                  print(data.get('total_violations', 0))
          except:
              print(0)
          ")
          
          COMPLIANCE_SCORE=$(python -c "
          import json
          try:
              with open('compliance-report.json') as f:
                  data = json.load(f)
                  print(data.get('compliance_score', 100))
          except:
              print(100)
          ")
          
          echo "total-violations=$TOTAL_VIOLATIONS" >> $GITHUB_OUTPUT
          echo "compliance-score=$COMPLIANCE_SCORE" >> $GITHUB_OUTPUT
          
          echo "ðŸ“Š Total violations: $TOTAL_VIOLATIONS"
          echo "ðŸ“Š Compliance score: $COMPLIANCE_SCORE%"
          
          if [[ "${{ env.ACT_MODE }}" == "true" ]]; then
            echo "ðŸ§ª ACT MODE: Growth analysis completed with mocked data"
          fi
      
      - name: Generate Enforcement Report
        if: always()
        run: |
          echo "ðŸ“‹ Generating comprehensive enforcement report..."
          
          # Create detailed report
          cat > enforcement-report.md << 'EOF'
          # ðŸ”´ Boundary Enforcement Report
          
          ## System Health Status
          - **Boundary Check**: ${{ steps.boundary-check.outputs.boundary-exit-code == '0' && 'âœ… PASS' || 'âŒ FAIL' }}
          - **Emergency Check**: ${{ steps.emergency-check.outputs.emergency-detected == 'false' && 'âœ… SAFE' || 'ðŸš¨ EMERGENCY' }}
          - **Total Violations**: ${{ steps.growth-analysis.outputs.total-violations }}
          - **Compliance Score**: ${{ steps.growth-analysis.outputs.compliance-score }}%
          
          ## Critical Boundaries (MANDATORY)
          - **File Size**: â‰¤300 lines (HARD LIMIT)
          - **Function Size**: â‰¤8 lines (HARD LIMIT) 
          - **Module Count**: System-wide monitoring
          - **Complexity**: Automated tracking
          
          ## Actions Required
          EOF
          
          if [ "${{ steps.emergency-check.outputs.emergency-detected }}" = "true" ]; then
            cat >> enforcement-report.md << 'EOF'
          
          ### ðŸš¨ EMERGENCY ACTIONS REQUIRED
          1. **STOP** all new feature development
          2. **IMMEDIATE** refactoring sprint required
          3. **REVIEW** system architecture 
          4. **NO MERGES** until violations resolved
          EOF
          fi
          
          if [ "${{ steps.boundary-check.outputs.boundary-exit-code }}" != "0" ]; then
            cat >> enforcement-report.md << 'EOF'
          
          ### âš ï¸ Boundary Violations Detected
          - Review boundary-report.json for details
          - Use automated fix suggestions
          - Consider file/function splitting
          EOF
          fi
          
          cat >> enforcement-report.md << 'EOF'
          
          ## Remediation Tools
          - `python scripts/boundary_enforcer.py --check-file-boundaries`
          - `python scripts/boundary_enforcer.py --check-function-boundaries` 
          - `python scripts/split_large_files.py` (automated splitting)
          - `python scripts/decompose_functions.py` (function refactoring)
          
          ---
          Generated by Boundary Enforcement System v2.0
          EOF
      
      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            let comment = '';
            
            // Read enforcement report
            if (fs.existsSync('enforcement-report.md')) {
              comment = fs.readFileSync('enforcement-report.md', 'utf8');
            } else {
              comment = `# ðŸ”´ Boundary Enforcement Report\n\nReport generation failed - check workflow logs.`;
            }
            
            // Add emergency warning if needed
            if ('${{ steps.emergency-check.outputs.emergency-detected }}' === 'true') {
              comment = `# ðŸš¨ EMERGENCY: CRITICAL VIOLATIONS DETECTED\n\n**THIS PR CANNOT BE MERGED** until violations are resolved.\n\n` + comment;
            }
            
            // Find existing enforcement comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('Boundary Enforcement Report')
            );
            
            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
      
      - name: Upload Enforcement Artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: boundary-enforcement-report
          path: |
            boundary-report.json
            compliance-report.json
            enforcement-report.md
            boundary-summary.txt
          retention-days: 30
      
      - name: Set Check Status
        if: always()
        uses: actions/github-script@v6
        with:
          script: |
            const emergencyDetected = '${{ steps.emergency-check.outputs.emergency-detected }}' === 'true';
            const boundaryFailed = '${{ steps.boundary-check.outputs.boundary-exit-code }}' !== '0';
            const totalViolations = parseInt('${{ steps.growth-analysis.outputs.total-violations }}');
            const complianceScore = parseFloat('${{ steps.growth-analysis.outputs.compliance-score }}');
            
            let conclusion, title, summary;
            
            if (emergencyDetected) {
              conclusion = 'failure';
              title = 'ðŸš¨ EMERGENCY: Critical boundary violations';
              summary = 'System has critical violations requiring immediate attention. No merges allowed.';
            } else if (boundaryFailed || totalViolations > 50) {
              conclusion = 'failure'; 
              title = 'âŒ Boundary violations detected';
              summary = `Found ${totalViolations} violations (${complianceScore}% compliance). Review required.`;
            } else if (totalViolations > 0) {
              conclusion = 'neutral';
              title = 'âš ï¸ Minor boundary issues';
              summary = `Found ${totalViolations} minor violations (${complianceScore}% compliance). Consider fixing.`;
            } else {
              conclusion = 'success';
              title = 'âœ… All boundaries respected';
              summary = 'Perfect compliance! All architectural boundaries respected.';
            }
            
            await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Boundary Enforcement',
              head_sha: context.sha,
              status: 'completed',
              conclusion: conclusion,
              output: {
                title: title,
                summary: summary
              }
            });
      
      - name: Fail Build on Emergency
        if: steps.emergency-check.outputs.emergency-detected == 'true'
        run: |
          echo "ðŸš¨ EMERGENCY VIOLATIONS DETECTED"
          echo "This build MUST fail to prevent system degradation"
          echo "Review emergency actions in the enforcement report"
          exit 1
      
      - name: Fail Build on Critical Violations  
        if: steps.boundary-check.outputs.boundary-exit-code != '0' && github.ref == 'refs/heads/main'
        run: |
          echo "âŒ CRITICAL VIOLATIONS on main branch"
          echo "Main branch must maintain perfect boundary compliance"
          exit 1

  pre-commit-validation:
    name: Pre-commit Hook Validation
    runs-on: warp-custom-default
    
    steps:
      - uses: actions/checkout@v4
      
      - name: ACT Detection for Pre-commit
        run: |
          if [[ "${{ env.ACT_DETECTED }}" == "true" ]]; then
            echo "ðŸ§ª Pre-commit validation in ACT mode"
            echo "ACT_MODE=true" >> $GITHUB_ENV
          else
            echo "â˜ï¸ Pre-commit validation in GitHub Actions"
            echo "ACT_MODE=false" >> $GITHUB_ENV
          fi
      
      - uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      - name: Install pre-commit
        run: |
          pip install pre-commit
          if [[ "${{ env.ACT_MODE }}" == "true" ]]; then
            echo "ðŸ§ª Pre-commit installed in ACT mode"
          fi
      
      - name: Validate pre-commit configuration
        run: |
          echo "ðŸ” Validating pre-commit configuration..."
          
          # Create minimal config if missing in ACT mode
          if [[ "${{ env.ACT_MODE }}" == "true" ]] && [[ ! -f ".pre-commit-config.yaml" ]]; then
            echo "ðŸ§ª Creating mock pre-commit config for ACT"
            cat > .pre-commit-config.yaml << 'EOF'
repos:
- repo: local
  hooks:
  - id: boundary-enforcer
    name: Boundary Enforcer
    entry: python scripts/boundary_enforcer.py
    language: system
    files: '\.py$'
EOF
          fi
          
          # Validate configuration
          if [ -f ".pre-commit-config.yaml" ]; then
            pre-commit validate-config || echo "Config validation failed (acceptable in ACT mode)"
          else
            echo "âŒ No pre-commit config found"
            if [[ "${{ env.ACT_MODE }}" != "true" ]]; then
              exit 1
            fi
          fi
          
          # Check if boundary enforcer hooks are present
          if [ -f ".pre-commit-config.yaml" ]; then
            if grep -q "boundary-enforcer" .pre-commit-config.yaml; then
              echo "âœ… Boundary enforcement hooks found"
            else
              echo "âŒ Boundary enforcement hooks missing"
              if [[ "${{ env.ACT_MODE }}" != "true" ]]; then
                exit 1
              fi
            fi
          fi
      
      - name: Test pre-commit hooks (dry run)
        run: |
          echo "ðŸ§ª Testing pre-commit hooks..."
          if [[ "${{ env.ACT_MODE }}" == "true" ]]; then
            echo "ðŸ§ª ACT MODE: Skipping actual pre-commit run (would require full repo setup)"
            echo "âœ… Pre-commit hooks validation completed in mock mode"
          else
            # Run hooks but don't fail if files need fixing
            pre-commit run --all-files || echo "Pre-commit found issues (expected in dry run)"
          fi
