name: Enhanced CI/CD Pipeline
description: Comprehensive CI/CD with reliability features, security scanning, and automatic rollback

# Disabled - only smoke tests and staging workflows are active
# on:
#   pull_request:
#     types: [opened, synchronize, reopened, ready_for_review]
#   push:
#     branches: [main, master, develop]
#   workflow_dispatch:
#     inputs:
#       force_full_pipeline:
#         description: 'Force full pipeline execution'
#         required: false
#         type: boolean
#         default: false
#       skip_security_scan:
#         description: 'Skip security scanning (emergency only)'
#         required: false
#         type: boolean
#         default: false
#       deploy_to_staging:
#         description: 'Deploy to staging after tests'
#         required: false
#         type: boolean
#         default: true

env:
  CONFIG_FILE: .github/workflow-config.yml
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'
  SECURITY_SCAN_TIMEOUT: 15
  PERFORMANCE_TEST_TIMEOUT: 20
  ROLLBACK_TIMEOUT: 5
  # ACT local testing environment variables
  ACT_RUNNER_NAME: ${{ env.ACT && 'act-runner' || 'github-runner' }}
  ACT_TEST_MODE: ${{ env.ACT || 'false' }}
  GITHUB_API_AVAILABLE: ${{ !env.ACT || 'false' }}

permissions:
  contents: read
  security-events: write
  deployments: write
  pull-requests: write
  issues: write
  statuses: write
  checks: write
  actions: read

concurrency:
  group: enhanced-ci-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  # ==========================================
  # PHASE 1: Pre-flight Checks and Architecture Compliance
  # ==========================================
  
  preflight-checks:
    name: ðŸ” Pre-flight Checks
    runs-on: warp-custom-default
    timeout-minutes: 10
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
      test_level: ${{ steps.check.outputs.test_level }}
      security_required: ${{ steps.check.outputs.security_required }}
      performance_required: ${{ steps.check.outputs.performance_required }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: Check execution conditions
        id: check
        run: |
          SHOULD_RUN=true
          TEST_LEVEL="unit"
          SECURITY_REQUIRED=true
          PERFORMANCE_REQUIRED=false
          
          # Check skip conditions
          if [[ "${{ github.event.head_commit.message }}" == *"[skip ci]"* ]] || \
             [[ "${{ github.event.head_commit.message }}" == *"[ci skip]"* ]]; then
            SHOULD_RUN=false
            echo "::notice::Skipping due to commit message"
          fi
          
          # Analyze changed files (skip in ACT environment)
          if [[ "${{ github.event_name }}" == "pull_request" ]] && [[ "${{ env.ACT_TEST_MODE }}" != "true" ]]; then
            CHANGED_FILES=$(gh pr diff ${{ github.event.pull_request.number }} --name-only || echo "")
            
            # Check for security-sensitive changes
            if echo "$CHANGED_FILES" | grep -E "(auth|security|secrets|api)" > /dev/null; then
              SECURITY_REQUIRED=true
              TEST_LEVEL="integration"
            fi
            
            # Check for performance-critical changes
            if echo "$CHANGED_FILES" | grep -E "(db|database|query|index|cache)" > /dev/null; then
              PERFORMANCE_REQUIRED=true
              TEST_LEVEL="integration"
            fi
            
            # Check for critical system changes
            if echo "$CHANGED_FILES" | grep -E "(app/main.py|app/core|terraform)" > /dev/null; then
              TEST_LEVEL="comprehensive"
              PERFORMANCE_REQUIRED=true
            fi
          fi
          
          # Override with manual input
          if [[ "${{ github.event.inputs.force_full_pipeline }}" == "true" ]]; then
            TEST_LEVEL="comprehensive"
            SECURITY_REQUIRED=true
            PERFORMANCE_REQUIRED=true
          fi
          
          if [[ "${{ github.event.inputs.skip_security_scan }}" == "true" ]]; then
            SECURITY_REQUIRED=false
          fi
          
          echo "should_run=$SHOULD_RUN" >> $GITHUB_OUTPUT
          echo "test_level=$TEST_LEVEL" >> $GITHUB_OUTPUT
          echo "security_required=$SECURITY_REQUIRED" >> $GITHUB_OUTPUT
          echo "performance_required=$PERFORMANCE_REQUIRED" >> $GITHUB_OUTPUT
          
          echo "## ðŸ” Pre-flight Analysis" >> $GITHUB_STEP_SUMMARY
          echo "- Test Level: $TEST_LEVEL" >> $GITHUB_STEP_SUMMARY
          echo "- Security Scan: $SECURITY_REQUIRED" >> $GITHUB_STEP_SUMMARY
          echo "- Performance Test: $PERFORMANCE_REQUIRED" >> $GITHUB_STEP_SUMMARY
        env:
          GH_TOKEN: ${{ github.token }}
          # ACT compatibility: Handle missing GitHub API access gracefully

  architecture-compliance:
    name: ðŸ—ï¸ Architecture Compliance
    needs: preflight-checks
    if: needs.preflight-checks.outputs.should_run == 'true'
    runs-on: warp-custom-default
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Run architecture compliance check
        id: compliance
        run: |
          echo "Running architecture compliance checks..."
          python scripts/check_architecture_compliance.py --strict --output-format json > compliance_report.json
          
          # Parse results
          VIOLATIONS=$(python -c "
          import json
          with open('compliance_report.json', 'r') as f:
              data = json.load(f)
          print(data.get('violations', 0))
          ")
          
          CRITICAL_VIOLATIONS=$(python -c "
          import json
          with open('compliance_report.json', 'r') as f:
              data = json.load(f)
          print(len([v for v in data.get('issues', []) if v.get('severity') == 'critical']))
          ")
          
          echo "violations=$VIOLATIONS" >> $GITHUB_OUTPUT
          echo "critical_violations=$CRITICAL_VIOLATIONS" >> $GITHUB_OUTPUT
          
          # Fail if critical violations
          if [[ "$CRITICAL_VIOLATIONS" -gt 0 ]]; then
            echo "::error::Critical architecture violations found: $CRITICAL_VIOLATIONS"
            exit 1
          fi
          
          # Warn on any violations
          if [[ "$VIOLATIONS" -gt 0 ]]; then
            echo "::warning::Architecture violations found: $VIOLATIONS"
          fi
          
      - name: Upload compliance report
        # ACT compatibility: Skip artifact upload in local testing
        if: always() && env.ACT != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: architecture-compliance-report
          path: compliance_report.json
          retention-days: 30
          
      - name: Store compliance report locally (ACT)
        # ACT compatibility: Store report locally when running with ACT
        if: always() && env.ACT == 'true'
        run: |
          echo "::notice::ACT Mode - Storing compliance report locally"
          mkdir -p /tmp/act-artifacts/architecture-compliance-report
          cp compliance_report.json /tmp/act-artifacts/architecture-compliance-report/ || true

  # ==========================================
  # PHASE 2: Security Scanning
  # ==========================================
  
  security-scan:
    name: ðŸ”’ Security Scanning
    needs: [preflight-checks, architecture-compliance]
    if: needs.preflight-checks.outputs.security_required == 'true'
    runs-on: warp-custom-default
    timeout-minutes: ${{ fromJson(env.SECURITY_SCAN_TIMEOUT) }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install security tools
        run: |
          pip install bandit safety semgrep
          
      - name: Run Bandit security scan
        id: bandit
        run: |
          echo "Running Bandit security scan..."
          bandit -r app/ -f json -o bandit_report.json || true
          
          # Parse results
          ISSUES=$(python -c "
          import json
          try:
              with open('bandit_report.json', 'r') as f:
                  data = json.load(f)
              high_issues = len([issue for issue in data.get('results', []) if issue.get('issue_severity') == 'HIGH'])
              medium_issues = len([issue for issue in data.get('results', []) if issue.get('issue_severity') == 'MEDIUM'])
              print(f'high={high_issues},medium={medium_issues}')
          except:
              print('high=0,medium=0')
          ")
          
          echo "$ISSUES" >> $GITHUB_OUTPUT
          
      - name: Run Safety dependency check
        id: safety
        run: |
          echo "Running Safety dependency scan..."
          safety check --json --output safety_report.json || true
          
      - name: Run Semgrep static analysis
        id: semgrep
        run: |
          echo "Running Semgrep static analysis..."
          semgrep --config=auto --json --output=semgrep_report.json app/ || true
          
      - name: Analyze security results
        id: analyze
        run: |
          CRITICAL_ISSUES=0
          TOTAL_ISSUES=0
          
          # Parse Bandit results
          if [[ -f bandit_report.json ]]; then
            BANDIT_HIGH=$(python -c "
            import json
            try:
                with open('bandit_report.json', 'r') as f:
                    data = json.load(f)
                print(len([issue for issue in data.get('results', []) if issue.get('issue_severity') == 'HIGH']))
            except:
                print(0)
            ")
            CRITICAL_ISSUES=$((CRITICAL_ISSUES + BANDIT_HIGH))
          fi
          
          # Parse Safety results
          if [[ -f safety_report.json ]]; then
            SAFETY_VULNS=$(python -c "
            import json
            try:
                with open('safety_report.json', 'r') as f:
                    data = json.load(f)
                print(len(data.get('vulnerabilities', [])))
            except:
                print(0)
            ")
            CRITICAL_ISSUES=$((CRITICAL_ISSUES + SAFETY_VULNS))
          fi
          
          echo "critical_issues=$CRITICAL_ISSUES" >> $GITHUB_OUTPUT
          echo "total_issues=$TOTAL_ISSUES" >> $GITHUB_OUTPUT
          
          # Create security summary
          cat > security_summary.md << EOF
          # ðŸ”’ Security Scan Results
          
          ## Summary
          - **Critical Issues:** $CRITICAL_ISSUES
          - **Total Issues:** $TOTAL_ISSUES
          
          ## Tool Results
          - **Bandit:** $([ -f bandit_report.json ] && echo "âœ“ Completed" || echo "âŒ Failed")
          - **Safety:** $([ -f safety_report.json ] && echo "âœ“ Completed" || echo "âŒ Failed")
          - **Semgrep:** $([ -f semgrep_report.json ] && echo "âœ“ Completed" || echo "âŒ Failed")
          
          EOF
          
          cat security_summary.md >> $GITHUB_STEP_SUMMARY
          
          # Fail on critical security issues
          if [[ "$CRITICAL_ISSUES" -gt 0 ]]; then
            echo "::error::Critical security issues found: $CRITICAL_ISSUES"
            exit 1
          fi
          
      - name: Upload security reports
        # ACT compatibility: Skip artifact upload in local testing
        if: always() && env.ACT != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: security-scan-reports
          path: |
            bandit_report.json
            safety_report.json
            semgrep_report.json
            security_summary.md
          retention-days: 90
          
      - name: Store security reports locally (ACT)
        # ACT compatibility: Store reports locally when running with ACT
        if: always() && env.ACT == 'true'
        run: |
          echo "::notice::ACT Mode - Storing security reports locally"
          mkdir -p /tmp/act-artifacts/security-scan-reports
          cp bandit_report.json safety_report.json semgrep_report.json security_summary.md /tmp/act-artifacts/security-scan-reports/ || true

  # ==========================================
  # PHASE 3: Core Testing with Coverage
  # ==========================================
  
  run-tests:
    name: ðŸ§ª Core Tests (${{ needs.preflight-checks.outputs.test_level }})
    needs: [preflight-checks, architecture-compliance]
    if: needs.preflight-checks.outputs.should_run == 'true'
    uses: ./.github/workflows/test-unit.yml
    with:
      test_level: ${{ needs.preflight-checks.outputs.test_level }}
      coverage_threshold: 85
      generate_reports: true
    secrets: inherit

  coverage-analysis:
    name: ðŸ“Š Coverage Analysis
    needs: [preflight-checks, run-tests]
    runs-on: warp-custom-default
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Download test artifacts
        # ACT compatibility: Skip artifact download in local testing
        if: env.ACT != 'true'
        uses: actions/download-artifact@v4
        with:
          pattern: test-results-*
          merge-multiple: true
          
      - name: Setup test artifacts (ACT)
        # ACT compatibility: Create dummy test results for local testing
        if: env.ACT == 'true'
        run: |
          echo "::notice::ACT Mode - Creating dummy test artifacts"
          mkdir -p test-results
          echo '{"totals": {"percent_covered": 85.0}}' > coverage_report.json || true
          
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install coverage tools
        run: |
          pip install coverage[toml] coverage-badge
          
      - name: Combine coverage reports
        id: coverage
        run: |
          # Combine coverage data
          coverage combine || true
          coverage report --format=json > coverage_report.json
          coverage html -d htmlcov/
          coverage xml -o coverage.xml
          
          # Parse coverage percentage
          COVERAGE=$(python -c "
          import json
          try:
              with open('coverage_report.json', 'r') as f:
                  data = json.load(f)
              print(f\"{data['totals']['percent_covered']:.1f}\")
          except:
              print('0.0')
          ")
          
          echo "coverage_percent=$COVERAGE" >> $GITHUB_OUTPUT
          
          # Generate coverage badge
          coverage-badge -o coverage_badge.svg
          
          # Check coverage threshold
          THRESHOLD=85
          if (( $(echo "$COVERAGE < $THRESHOLD" | bc -l) )); then
            echo "::warning::Coverage $COVERAGE% is below threshold $THRESHOLD%"
          else
            echo "::notice::Coverage $COVERAGE% meets threshold $THRESHOLD%"
          fi
          
      - name: Comment coverage on PR
        # ACT compatibility: Skip GitHub API calls in local testing
        if: github.event_name == 'pull_request' && env.ACT != 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const coverage = '${{ steps.coverage.outputs.coverage_percent }}';
            const threshold = 85;
            const status = parseFloat(coverage) >= threshold ? 'âœ…' : 'âš ï¸';
            
            const comment = `## ${status} Coverage Report
            
            **Current Coverage:** ${coverage}%
            **Threshold:** ${threshold}%
            
            [View detailed coverage report](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
            
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });
            
      - name: Upload coverage reports
        # ACT compatibility: Skip artifact upload in local testing
        if: env.ACT != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: |
            coverage_report.json
            coverage.xml
            htmlcov/
            coverage_badge.svg
          retention-days: 30
          
      - name: Store coverage reports locally (ACT)
        # ACT compatibility: Store reports locally when running with ACT
        if: env.ACT == 'true'
        run: |
          echo "::notice::ACT Mode - Storing coverage reports locally"
          mkdir -p /tmp/act-artifacts/coverage-reports
          cp coverage_report.json coverage.xml coverage_badge.svg /tmp/act-artifacts/coverage-reports/ || true
          cp -r htmlcov /tmp/act-artifacts/coverage-reports/ || true

  # ==========================================
  # PHASE 4: Performance Testing
  # ==========================================
  
  performance-tests:
    name: âš¡ Performance Tests
    needs: [preflight-checks, run-tests]
    if: needs.preflight-checks.outputs.performance_required == 'true'
    runs-on: warp-custom-default
    timeout-minutes: ${{ fromJson(env.PERFORMANCE_TEST_TIMEOUT) }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install performance testing tools
        run: |
          pip install -r requirements.txt
          pip install pytest-benchmark locust
          
      - name: Run performance benchmarks
        id: benchmark
        run: |
          echo "Running performance benchmarks..."
          
          # Run pytest benchmarks
          pytest app/tests/performance/ --benchmark-json=benchmark_results.json || true
          
          # Parse benchmark results
          if [[ -f benchmark_results.json ]]; then
            SLOWEST_TEST=$(python -c "
            import json
            try:
                with open('benchmark_results.json', 'r') as f:
                    data = json.load(f)
                benchmarks = data.get('benchmarks', [])
                if benchmarks:
                    slowest = max(benchmarks, key=lambda x: x['stats']['mean'])
                    print(f\"{slowest['name']}: {slowest['stats']['mean']:.4f}s\")
                else:
                    print('No benchmarks found')
            except:
                print('Error parsing benchmarks')
            ")
            
            echo "slowest_test=$SLOWEST_TEST" >> $GITHUB_OUTPUT
          fi
          
      - name: Run load tests
        id: load-test
        run: |
          echo "Running load tests..."
          
          # Start test server in background
          python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 &
          SERVER_PID=$!
          
          # Wait for server to start
          sleep 10
          
          # Run load test
          locust -f app/tests/load/locustfile.py --headless -u 10 -r 2 -t 30s --host http://localhost:8000 --json > load_test_results.json || true
          
          # Stop server
          kill $SERVER_PID || true
          
          # Parse load test results
          if [[ -f load_test_results.json ]]; then
            AVG_RESPONSE_TIME=$(python -c "
            import json
            try:
                with open('load_test_results.json', 'r') as f:
                    for line in f:
                        data = json.loads(line)
                        if data.get('type') == 'stats' and data.get('name') == 'Aggregated':
                            print(f\"{data['avg_response_time']:.2f}ms\")
                            break
                    else:
                        print('No aggregated stats found')
            except:
                print('Error parsing load test results')
            ")
            
            echo "avg_response_time=$AVG_RESPONSE_TIME" >> $GITHUB_OUTPUT
          fi
          
      - name: Analyze performance results
        run: |
          cat > performance_summary.md << EOF
          # âš¡ Performance Test Results
          
          ## Benchmark Results
          - **Slowest Test:** ${{ steps.benchmark.outputs.slowest_test }}
          
          ## Load Test Results
          - **Average Response Time:** ${{ steps.load-test.outputs.avg_response_time }}
          
          ## Performance Status
          $(if [[ -n "${{ steps.benchmark.outputs.slowest_test }}" ]]; then
            echo "âœ… Benchmarks completed successfully"
          else
            echo "âš ï¸ No benchmark results available"
          fi)
          
          $(if [[ -n "${{ steps.load-test.outputs.avg_response_time }}" ]]; then
            echo "âœ… Load tests completed successfully"
          else
            echo "âš ï¸ No load test results available"
          fi)
          EOF
          
          cat performance_summary.md >> $GITHUB_STEP_SUMMARY
          
      - name: Upload performance reports
        # ACT compatibility: Skip artifact upload in local testing
        if: always() && env.ACT != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: performance-test-reports
          path: |
            benchmark_results.json
            load_test_results.json
            performance_summary.md
          retention-days: 30
          
      - name: Store performance reports locally (ACT)
        # ACT compatibility: Store reports locally when running with ACT
        if: always() && env.ACT == 'true'
        run: |
          echo "::notice::ACT Mode - Storing performance reports locally"
          mkdir -p /tmp/act-artifacts/performance-test-reports
          cp benchmark_results.json load_test_results.json performance_summary.md /tmp/act-artifacts/performance-test-reports/ || true

  # ==========================================
  # PHASE 5: Staging Deployment and Validation
  # ==========================================
  
  deploy-staging:
    name: ðŸš€ Deploy to Staging
    needs: [preflight-checks, run-tests, security-scan, performance-tests]
    if: |
      always() &&
      needs.preflight-checks.outputs.should_run == 'true' &&
      needs.run-tests.result == 'success' &&
      (needs.security-scan.result == 'success' || needs.security-scan.result == 'skipped') &&
      (needs.performance-tests.result == 'success' || needs.performance-tests.result == 'skipped') &&
      (github.event.inputs.deploy_to_staging != 'false')
    uses: ./.github/workflows/staging-environment.yml
    with:
      environment: staging-ci
      auto_cleanup: true
      health_check_timeout: 300
    secrets: inherit

  staging-validation:
    name: âœ… Staging Validation
    needs: [deploy-staging]
    if: needs.deploy-staging.result == 'success'
    runs-on: warp-custom-default
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Get staging environment details
        id: staging-info
        run: |
          # Get staging URL from deployment
          STAGING_URL=$(echo '${{ needs.deploy-staging.outputs.staging_url }}' || echo "")
          if [[ -z "$STAGING_URL" ]]; then
            STAGING_URL="https://staging-ci-${{ github.run_id }}.example.com"
          fi
          
          echo "staging_url=$STAGING_URL" >> $GITHUB_OUTPUT
          
      - name: Run staging smoke tests
        id: smoke-tests
        run: |
          STAGING_URL="${{ steps.staging-info.outputs.staging_url }}"
          echo "Running smoke tests against $STAGING_URL"
          
          # Health check
          HEALTH_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$STAGING_URL/health" || echo "000")
          echo "health_status=$HEALTH_STATUS" >> $GITHUB_OUTPUT
          
          # API responsiveness
          API_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$STAGING_URL/api/v1/status" || echo "000")
          echo "api_status=$API_STATUS" >> $GITHUB_OUTPUT
          
          # Database connectivity
          DB_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$STAGING_URL/api/v1/health/database" || echo "000")
          echo "db_status=$DB_STATUS" >> $GITHUB_OUTPUT
          
          # Validate all critical endpoints are responding
          if [[ "$HEALTH_STATUS" != "200" ]] || [[ "$API_STATUS" != "200" ]] || [[ "$DB_STATUS" != "200" ]]; then
            echo "::error::Staging validation failed - Health: $HEALTH_STATUS, API: $API_STATUS, DB: $DB_STATUS"
            exit 1
          fi
          
          echo "::notice::Staging validation passed - All endpoints responding"
          
      - name: Run integration tests against staging
        run: |
          echo "Running integration tests against staging environment..."
          STAGING_URL="${{ steps.staging-info.outputs.staging_url }}"
          
          # Run critical path tests
          python -m pytest app/tests/integration/critical/ \
            --base-url="$STAGING_URL" \
            --timeout=30 \
            -v \
            --tb=short \
            || echo "Some integration tests failed"

  # ==========================================
  # PHASE 6: Failure Handling and Rollback
  # ==========================================
  
  handle-failures:
    name: ðŸ”„ Handle Failures & Rollback
    needs: [
      preflight-checks, 
      architecture-compliance, 
      security-scan, 
      run-tests, 
      performance-tests, 
      deploy-staging, 
      staging-validation
    ]
    if: |
      always() && 
      contains(needs.*.result, 'failure') &&
      needs.preflight-checks.outputs.should_run == 'true'
    runs-on: warp-custom-default
    timeout-minutes: ${{ fromJson(env.ROLLBACK_TIMEOUT) }}
    
    steps:
      - name: Analyze failure point
        id: analyze
        run: |
          FAILED_STAGE=""
          ROLLBACK_REQUIRED=false
          CRITICAL_FAILURE=false
          
          # Determine where the failure occurred
          if [[ "${{ needs.architecture-compliance.result }}" == "failure" ]]; then
            FAILED_STAGE="architecture-compliance"
            CRITICAL_FAILURE=true
          elif [[ "${{ needs.security-scan.result }}" == "failure" ]]; then
            FAILED_STAGE="security-scan"
            CRITICAL_FAILURE=true
          elif [[ "${{ needs.run-tests.result }}" == "failure" ]]; then
            FAILED_STAGE="tests"
            CRITICAL_FAILURE=true
          elif [[ "${{ needs.performance-tests.result }}" == "failure" ]]; then
            FAILED_STAGE="performance-tests"
            CRITICAL_FAILURE=false
          elif [[ "${{ needs.deploy-staging.result }}" == "failure" ]]; then
            FAILED_STAGE="staging-deployment"
            ROLLBACK_REQUIRED=true
            CRITICAL_FAILURE=true
          elif [[ "${{ needs.staging-validation.result }}" == "failure" ]]; then
            FAILED_STAGE="staging-validation"
            ROLLBACK_REQUIRED=true
            CRITICAL_FAILURE=true
          fi
          
          echo "failed_stage=$FAILED_STAGE" >> $GITHUB_OUTPUT
          echo "rollback_required=$ROLLBACK_REQUIRED" >> $GITHUB_OUTPUT
          echo "critical_failure=$CRITICAL_FAILURE" >> $GITHUB_OUTPUT
          
      - name: Trigger automatic rollback
        # ACT compatibility: Skip GitHub API workflow dispatch in local testing
        if: steps.analyze.outputs.rollback_required == 'true' && env.ACT != 'true'
        uses: actions/github-script@v7
        with:
          script: |
            console.log('Triggering automatic rollback...');
            
            // Trigger staging cleanup workflow
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'staging-cleanup.yml',
              ref: context.ref,
              inputs: {
                environment: 'staging-ci',
                reason: 'Automatic rollback due to CI failure',
                force: 'true'
              }
            });
            
      - name: Create failure incident
        # ACT compatibility: Skip GitHub API issue creation in local testing
        if: steps.analyze.outputs.critical_failure == 'true' && env.ACT != 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const failed_stage = '${{ steps.analyze.outputs.failed_stage }}';
            const critical = '${{ steps.analyze.outputs.critical_failure }}';
            
            const title = `ðŸš¨ CI/CD Pipeline Failure - ${failed_stage}`;
            const body = `## Pipeline Failure Report
            
            **Failed Stage:** ${failed_stage}
            **Critical Failure:** ${critical}
            **Rollback Triggered:** ${{ steps.analyze.outputs.rollback_required }}
            **Run ID:** ${{ github.run_id }}
            **Commit:** ${{ github.sha }}
            **Branch:** ${{ github.ref }}
            
            ### Failure Details
            - **Actor:** ${{ github.actor }}
            - **Event:** ${{ github.event_name }}
            - **Timestamp:** ${new Date().toISOString()}
            
            ### Recommended Actions
            1. Review the failed stage logs
            2. Fix the identified issues
            3. Re-run the pipeline
            
            ### Quick Links
            - [Workflow Run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - [Commit Details](${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }})
            
            ---
            *This incident was automatically created by the CI/CD failure handler*`;
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['ci-failure', 'automated', 'incident']
            });

  # ==========================================
  # PHASE 7: Production Smoke Tests (for main branch)
  # ==========================================
  
  production-smoke-tests:
    name: ðŸ­ Production Smoke Tests
    needs: [preflight-checks, run-tests, staging-validation]
    if: |
      github.ref == 'refs/heads/main' &&
      needs.run-tests.result == 'success' &&
      needs.staging-validation.result == 'success'
    runs-on: warp-custom-default
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Run production smoke tests
        run: |
          echo "Running production smoke tests..."
          
          # Test production endpoints (read-only)
          # ACT compatibility: Use localhost for local testing
          if [[ "${{ env.ACT_TEST_MODE }}" == "true" ]]; then
            PROD_URL="http://localhost:8000"
          else
            PROD_URL="https://api.netra.ai"  # Replace with actual production URL
          fi
          
          # Health check
          curl -f "$PROD_URL/health" || echo "Production health check failed"
          
          # API status check
          curl -f "$PROD_URL/api/v1/status" || echo "Production API check failed"
          
          echo "Production smoke tests completed"

  # ==========================================
  # PHASE 8: Notifications and Reporting
  # ==========================================
  
  final-report:
    name: ðŸ“ Final Report & Notifications
    needs: [
      preflight-checks,
      architecture-compliance,
      security-scan,
      run-tests,
      coverage-analysis,
      performance-tests,
      deploy-staging,
      staging-validation,
      production-smoke-tests
    ]
    if: always() && needs.preflight-checks.outputs.should_run == 'true'
    runs-on: warp-custom-default
    
    steps:
      - name: Calculate pipeline success rate
        id: metrics
        run: |
          TOTAL_JOBS=0
          SUCCESS_COUNT=0
          
          # Count required jobs
          JOBS=(
            "${{ needs.architecture-compliance.result }}"
            "${{ needs.run-tests.result }}"
            "${{ needs.coverage-analysis.result }}"
          )
          
          # Add optional jobs if they ran
          if [[ "${{ needs.security-scan.result }}" != "skipped" ]]; then
            JOBS+=("${{ needs.security-scan.result }}")
          fi
          
          if [[ "${{ needs.performance-tests.result }}" != "skipped" ]]; then
            JOBS+=("${{ needs.performance-tests.result }}")
          fi
          
          if [[ "${{ needs.deploy-staging.result }}" != "skipped" ]]; then
            JOBS+=("${{ needs.deploy-staging.result }}")
            JOBS+=("${{ needs.staging-validation.result }}")
          fi
          
          if [[ "${{ needs.production-smoke-tests.result }}" != "skipped" ]]; then
            JOBS+=("${{ needs.production-smoke-tests.result }}")
          fi
          
          # Calculate success rate
          for job in "${JOBS[@]}"; do
            TOTAL_JOBS=$((TOTAL_JOBS + 1))
            if [[ "$job" == "success" ]]; then
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
            fi
          done
          
          SUCCESS_RATE=$((SUCCESS_COUNT * 100 / TOTAL_JOBS))
          
          echo "success_rate=$SUCCESS_RATE" >> $GITHUB_OUTPUT
          echo "total_jobs=$TOTAL_JOBS" >> $GITHUB_OUTPUT
          echo "successful_jobs=$SUCCESS_COUNT" >> $GITHUB_OUTPUT
          
      - name: Generate final report
        run: |
          cat > pipeline_report.md << EOF
          # ðŸŽ¯ Enhanced CI/CD Pipeline Report
          
          **Success Rate:** ${{ steps.metrics.outputs.success_rate }}%
          **Jobs:** ${{ steps.metrics.outputs.successful_jobs }}/${{ steps.metrics.outputs.total_jobs }} successful
          **Test Level:** ${{ needs.preflight-checks.outputs.test_level }}
          **Commit:** ${{ github.sha }}
          **Actor:** ${{ github.actor }}
          **Duration:** $(( ($(date +%s) - ${{ github.event.head_commit.timestamp && 'date -d "' }}${{ github.event.head_commit.timestamp }}${{ github.event.head_commit.timestamp && '" +%s' }} || echo ${{ github.run_id }})) / 60 )) minutes
          
          ## Stage Results
          
          | Stage | Status | Notes |
          |-------|--------|-------|
          | Architecture Compliance | ${{ needs.architecture-compliance.result }} | ${{ needs.architecture-compliance.result == 'success' && 'âœ… All checks passed' || 'âŒ Issues found' }} |
          | Security Scanning | ${{ needs.security-scan.result }} | ${{ needs.security-scan.result == 'success' && 'âœ… No critical issues' || needs.security-scan.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Security issues found' }} |
          | Core Tests | ${{ needs.run-tests.result }} | ${{ needs.run-tests.result == 'success' && 'âœ… All tests passed' || 'âŒ Test failures' }} |
          | Coverage Analysis | ${{ needs.coverage-analysis.result }} | ${{ needs.coverage-analysis.result == 'success' && 'âœ… Coverage analyzed' || 'âŒ Coverage analysis failed' }} |
          | Performance Tests | ${{ needs.performance-tests.result }} | ${{ needs.performance-tests.result == 'success' && 'âœ… Performance acceptable' || needs.performance-tests.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Performance issues' }} |
          | Staging Deployment | ${{ needs.deploy-staging.result }} | ${{ needs.deploy-staging.result == 'success' && 'âœ… Deployed successfully' || needs.deploy-staging.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Deployment failed' }} |
          | Staging Validation | ${{ needs.staging-validation.result }} | ${{ needs.staging-validation.result == 'success' && 'âœ… Validation passed' || needs.staging-validation.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Validation failed' }} |
          | Production Smoke Tests | ${{ needs.production-smoke-tests.result }} | ${{ needs.production-smoke-tests.result == 'success' && 'âœ… Production healthy' || needs.production-smoke-tests.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Production issues' }} |
          
          ## Artifacts
          - [Test Reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          - [Coverage Reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          - [Security Scan Results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          
          EOF
          
          cat pipeline_report.md >> $GITHUB_STEP_SUMMARY
          
      - name: Post PR comment with results
        # ACT compatibility: Skip GitHub API PR comments in local testing
        if: github.event_name == 'pull_request' && env.ACT != 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const successRate = ${{ steps.metrics.outputs.success_rate }};
            const status = successRate === 100 ? 'âœ… All checks passed' :
                          successRate >= 80 ? 'âš ï¸ Some issues found' :
                          'âŒ Multiple failures';
            
            const comment = `## ${status}
            
            **Pipeline Success Rate:** ${successRate}%
            
            [View full pipeline report](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
            
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });
            
      - name: Set commit status
        # ACT compatibility: Skip GitHub API commit status in local testing
        if: always() && env.ACT != 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const successRate = ${{ steps.metrics.outputs.success_rate }};
            const state = successRate === 100 ? 'success' :
                         successRate >= 80 ? 'pending' : 'failure';
            
            await github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: state,
              context: 'Enhanced CI/CD Pipeline',
              description: `${successRate}% success rate`,
              target_url: `${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`
            });