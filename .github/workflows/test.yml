name: Unified Test Pipeline

on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main, develop, staging, critical-remediation-*]
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: false
        type: choice
        options:
          - smoke
          - unit
          - integration
          - e2e
          - comprehensive
        default: unit
      real_services:
        description: 'Use real services (Docker)'
        required: false
        type: boolean
        default: false
      real_llm:
        description: 'Use real LLM for tests'
        required: false
        type: boolean
        default: false
      fail_fast:
        description: 'Stop on first failure'
        required: false
        type: boolean
        default: false
      coverage:
        description: 'Generate coverage report'
        required: false
        type: boolean
        default: true

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'
  # Test environment configuration
  CI: true
  CI_MODE: true
  PYTHONPATH: ${{ github.workspace }}
  # Docker service ports for tests
  POSTGRES_TEST_PORT: 5434
  REDIS_TEST_PORT: 6381
  BACKEND_TEST_PORT: 8001
  AUTH_TEST_PORT: 8082

permissions:
  contents: read
  pull-requests: write
  checks: write
  statuses: write

jobs:
  # ==========================================
  # PHASE 1: DETERMINE TEST STRATEGY
  # ==========================================
  
  determine-strategy:
    name: Determine Test Strategy
    runs-on: warp-custom-test
    timeout-minutes: 5
    outputs:
      test_level: ${{ steps.strategy.outputs.test_level }}
      categories: ${{ steps.strategy.outputs.categories }}
      real_services: ${{ steps.strategy.outputs.real_services }}
      real_llm: ${{ steps.strategy.outputs.real_llm }}
      should_run_e2e: ${{ steps.strategy.outputs.should_run_e2e }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history for change detection
        
      - name: Determine test strategy
        id: strategy
        run: |
          echo "=== Determining Test Strategy ==="
          
          # Set defaults based on event type
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # PR: Run unit and integration tests
            TEST_LEVEL="${{ inputs.test_level || 'integration' }}"
            REAL_SERVICES="${{ inputs.real_services || 'false' }}"
            REAL_LLM="${{ inputs.real_llm || 'false' }}"
            SHOULD_RUN_E2E="false"
          elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            # Main branch: Run comprehensive tests
            TEST_LEVEL="${{ inputs.test_level || 'comprehensive' }}"
            REAL_SERVICES="${{ inputs.real_services || 'true' }}"
            REAL_LLM="${{ inputs.real_llm || 'false' }}"
            SHOULD_RUN_E2E="true"
          else
            # Other branches: Run integration tests
            TEST_LEVEL="${{ inputs.test_level || 'integration' }}"
            REAL_SERVICES="${{ inputs.real_services || 'false' }}"
            REAL_LLM="${{ inputs.real_llm || 'false' }}"
            SHOULD_RUN_E2E="false"
          fi
          
          # Determine categories based on test level
          case "$TEST_LEVEL" in
            "smoke")
              CATEGORIES="smoke"
              ;;
            "unit")
              CATEGORIES="unit"
              ;;
            "integration")
              CATEGORIES="unit integration api"
              ;;
            "e2e")
              CATEGORIES="unit integration api e2e"
              SHOULD_RUN_E2E="true"
              ;;
            "comprehensive")
              CATEGORIES="smoke unit integration api websocket agent e2e"
              SHOULD_RUN_E2E="true"
              ;;
            *)
              CATEGORIES="unit"
              ;;
          esac
          
          echo "test_level=$TEST_LEVEL" >> $GITHUB_OUTPUT
          echo "categories=$CATEGORIES" >> $GITHUB_OUTPUT
          echo "real_services=$REAL_SERVICES" >> $GITHUB_OUTPUT
          echo "real_llm=$REAL_LLM" >> $GITHUB_OUTPUT
          echo "should_run_e2e=$SHOULD_RUN_E2E" >> $GITHUB_OUTPUT
          
          echo "Test Level: $TEST_LEVEL"
          echo "Categories: $CATEGORIES"
          echo "Real Services: $REAL_SERVICES"
          echo "Real LLM: $REAL_LLM"
          echo "Run E2E: $SHOULD_RUN_E2E"
      
      - name: Detect changed areas
        id: changes
        run: |
          echo "=== Detecting Changed Areas ==="
          
          # Detect what changed to optimize test runs
          git diff --name-only origin/main...HEAD > changed_files.txt || true
          
          BACKEND_CHANGED=false
          FRONTEND_CHANGED=false
          AUTH_CHANGED=false
          TESTS_CHANGED=false
          
          if grep -q "netra_backend/" changed_files.txt; then
            BACKEND_CHANGED=true
          fi
          
          if grep -q "frontend/" changed_files.txt; then
            FRONTEND_CHANGED=true
          fi
          
          if grep -q "auth_service/" changed_files.txt; then
            AUTH_CHANGED=true
          fi
          
          if grep -q "tests/" changed_files.txt || grep -q "test_framework/" changed_files.txt; then
            TESTS_CHANGED=true
          fi
          
          echo "backend_changed=$BACKEND_CHANGED" >> $GITHUB_OUTPUT
          echo "frontend_changed=$FRONTEND_CHANGED" >> $GITHUB_OUTPUT
          echo "auth_changed=$AUTH_CHANGED" >> $GITHUB_OUTPUT
          echo "tests_changed=$TESTS_CHANGED" >> $GITHUB_OUTPUT
          
          echo "Backend Changed: $BACKEND_CHANGED"
          echo "Frontend Changed: $FRONTEND_CHANGED"
          echo "Auth Changed: $AUTH_CHANGED"
          echo "Tests Changed: $TESTS_CHANGED"

  # ==========================================
  # PHASE 2: SETUP TEST SERVICES (if needed)
  # ==========================================
  
  setup-services:
    name: Setup Test Services
    runs-on: warp-custom-test
    needs: determine-strategy
    if: needs.determine-strategy.outputs.real_services == 'true'
    timeout-minutes: 10
    
    services:
      postgres:
        image: postgres:17-alpine
        env:
          POSTGRES_DB: netra_test
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
        ports:
          - 5434:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          
      redis:
        image: redis:7-alpine
        env:
          REDIS_PASSWORD: test_password
        ports:
          - 6381:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - name: Verify services
        run: |
          echo "=== Verifying Services ==="
          
          # Test PostgreSQL
          until pg_isready -h localhost -p ${{ env.POSTGRES_TEST_PORT }} -U test_user; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done
          echo "‚úÖ PostgreSQL is ready"
          
          # Test Redis
          until redis-cli -h localhost -p ${{ env.REDIS_TEST_PORT }} -a test_password ping; do
            echo "Waiting for Redis..."
            sleep 2
          done
          echo "‚úÖ Redis is ready"

  # ==========================================
  # PHASE 3: RUN PYTHON TESTS
  # ==========================================
  
  python-tests:
    name: Python Tests
    runs-on: warp-custom-test
    needs: [determine-strategy, setup-services]
    if: always() && needs.determine-strategy.result == 'success'
    timeout-minutes: 30
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          echo "=== Installing Python Dependencies ==="
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio pytest-timeout
      
      - name: Configure test environment
        run: |
          echo "=== Configuring Test Environment ==="
          
          # Set test environment variables
          {
            echo "TEST_MODE=ci"
            echo "CI_MODE=true"
            echo "PYTHONPATH=${{ github.workspace }}"
            echo "TEST_CATEGORIES=${{ needs.determine-strategy.outputs.categories }}"
            echo "FAIL_FAST=${{ inputs.fail_fast || 'false' }}"
            echo "GENERATE_COVERAGE=${{ inputs.coverage || 'true' }}"
          } >> $GITHUB_ENV
          
          # Configure service connections if using real services
          if [[ "${{ needs.determine-strategy.outputs.real_services }}" == "true" ]]; then
            {
              echo "USE_REAL_SERVICES=true"
              echo "DATABASE_URL=postgresql://test_user:test_password@localhost:${{ env.POSTGRES_TEST_PORT }}/netra_test"
              echo "REDIS_URL=redis://:test_password@localhost:${{ env.REDIS_TEST_PORT }}/0"
            } >> $GITHUB_ENV
          else
            echo "USE_REAL_SERVICES=false" >> $GITHUB_ENV
          fi
          
          # Configure LLM mode
          if [[ "${{ needs.determine-strategy.outputs.real_llm }}" == "true" ]]; then
            echo "USE_REAL_LLM=true" >> $GITHUB_ENV
          else
            echo "USE_MOCK_LLM=true" >> $GITHUB_ENV
          fi
      
      - name: Run unified test runner
        id: test
        run: |
          echo "=== Running Unified Test Runner ==="
          
          # Build test command
          TEST_CMD="python tests/unified_test_runner.py"
          
          # Add categories
          TEST_CMD="$TEST_CMD --categories ${{ needs.determine-strategy.outputs.categories }}"
          
          # Add options based on configuration
          if [[ "${{ inputs.fail_fast }}" == "true" ]]; then
            TEST_CMD="$TEST_CMD --fast-fail"
          fi
          
          if [[ "${{ inputs.coverage }}" != "false" ]]; then
            TEST_CMD="$TEST_CMD --coverage"
          else
            TEST_CMD="$TEST_CMD --no-coverage"
          fi
          
          if [[ "${{ needs.determine-strategy.outputs.real_services }}" == "true" ]]; then
            TEST_CMD="$TEST_CMD --real-services"
          fi
          
          if [[ "${{ needs.determine-strategy.outputs.real_llm }}" == "true" ]]; then
            TEST_CMD="$TEST_CMD --real-llm"
          else
            TEST_CMD="$TEST_CMD --mock-llm"
          fi
          
          # Add CI optimizations
          TEST_CMD="$TEST_CMD --ci --no-warnings --timeout 120"
          
          echo "Executing: $TEST_CMD"
          $TEST_CMD
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: python-test-results
          path: |
            test-results/
            coverage.xml
            .coverage
            *.log
          retention-days: 7
      
      - name: Upload coverage to Codecov
        if: inputs.coverage != false
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

  # ==========================================
  # PHASE 4: RUN FRONTEND TESTS
  # ==========================================
  
  frontend-tests:
    name: Frontend Tests
    runs-on: warp-custom-test
    needs: determine-strategy
    if: |
      always() && 
      needs.determine-strategy.result == 'success' &&
      (contains(needs.determine-strategy.outputs.categories, 'frontend') || 
       needs.determine-strategy.outputs.test_level == 'comprehensive')
    timeout-minutes: 20
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'
      
      - name: Install dependencies
        working-directory: frontend
        run: |
          echo "=== Installing Frontend Dependencies ==="
          npm ci
      
      - name: Run linting
        working-directory: frontend
        run: |
          echo "=== Running Frontend Linting ==="
          npm run lint || true  # Don't fail on lint errors for now
      
      - name: Run type checking
        working-directory: frontend
        run: |
          echo "=== Running TypeScript Type Checking ==="
          npm run type-check || true  # Don't fail on type errors for now
      
      - name: Run unit tests
        working-directory: frontend
        run: |
          echo "=== Running Frontend Unit Tests ==="
          npm test -- --coverage --watchAll=false
      
      - name: Build frontend
        working-directory: frontend
        run: |
          echo "=== Building Frontend ==="
          npm run build
      
      - name: Upload frontend test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: frontend-test-results
          path: |
            frontend/coverage/
            frontend/test-results/
          retention-days: 7

  # ==========================================
  # PHASE 5: RUN E2E TESTS (if needed)
  # ==========================================
  
  e2e-tests:
    name: E2E Tests
    runs-on: warp-custom-test
    needs: [determine-strategy, python-tests, frontend-tests]
    if: |
      always() && 
      needs.determine-strategy.result == 'success' &&
      needs.python-tests.result == 'success' &&
      needs.determine-strategy.outputs.should_run_e2e == 'true'
    timeout-minutes: 45
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'
      
      - name: Install dependencies
        run: |
          echo "=== Installing Dependencies ==="
          # Python dependencies
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
          # Frontend dependencies
          cd frontend && npm ci && cd ..
      
      - name: Start services with Docker Compose
        run: |
          echo "=== Starting Services with Docker Compose for E2E Tests ==="
          
          # Use Alpine test compose for faster startup in CI
          docker compose -f docker-compose.alpine-test.yml up -d
          
          # Wait for PostgreSQL to be healthy
          echo "Waiting for PostgreSQL..."
          for i in {1..30}; do
            if docker compose -f docker-compose.alpine-test.yml exec -T postgres pg_isready -U test_user; then
              echo "PostgreSQL is ready!"
              break
            fi
            echo "PostgreSQL not ready yet..."
            sleep 2
          done
          
          # Wait for Redis to be healthy
          echo "Waiting for Redis..."
          for i in {1..30}; do
            if docker compose -f docker-compose.alpine-test.yml exec -T redis redis-cli ping; then
              echo "Redis is ready!"
              break
            fi
            echo "Redis not ready yet..."
            sleep 2
          done
          
          # Wait for Backend to be healthy
          echo "Waiting for Backend..."
          for i in {1..30}; do
            if curl -f http://localhost:8000/health 2>/dev/null; then
              echo "Backend is ready!"
              break
            fi
            echo "Backend not ready yet..."
            sleep 2
          done
          
          # Wait for Auth service to be healthy
          echo "Waiting for Auth service..."
          for i in {1..30}; do
            if curl -f http://localhost:8081/health 2>/dev/null; then
              echo "Auth service is ready!"
              break
            fi
            echo "Auth service not ready yet..."
            sleep 2
          done
          
          # Show service status
          docker compose -f docker-compose.alpine-test.yml ps
      
      - name: Run E2E tests
        run: |
          echo "=== Running E2E Tests ==="
          
          # Set test environment variables for Docker Compose services
          export USE_REAL_SERVICES=true
          export DATABASE_URL="postgresql://test_user:test_password@localhost:5434/netra_test"
          export REDIS_URL="redis://:test_password@localhost:6381/0"
          export BACKEND_URL="http://localhost:8000"
          export AUTH_URL="http://localhost:8081"
          export TEST_MODE=ci
          export CI_MODE=true
          
          # Run E2E test suite with proper configuration
          python tests/unified_test_runner.py \
            --category e2e \
            --real-services \
            --timeout 180 \
            --ci
      
      - name: Run Cypress tests
        if: needs.determine-strategy.outputs.test_level == 'comprehensive'
        working-directory: frontend
        run: |
          echo "=== Running Cypress E2E Tests ==="
          npx cypress run --config baseUrl=http://localhost:3001
      
      - name: Cleanup services
        if: always()
        run: |
          echo "=== Cleaning up Docker services ==="
          
          # Collect logs before cleanup
          docker compose -f docker-compose.alpine-test.yml logs --tail=50 > docker-logs.txt || true
          
          # Stop and remove containers
          docker compose -f docker-compose.alpine-test.yml down -v || true
      
      - name: Upload E2E test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-results
          path: |
            test-results/
            frontend/cypress/screenshots/
            frontend/cypress/videos/
            *.log
          retention-days: 7

  # ==========================================
  # PHASE 6: MISSION CRITICAL TESTS
  # ==========================================
  
  mission-critical-tests:
    name: Mission Critical Tests
    runs-on: warp-custom-test
    needs: [determine-strategy, python-tests]
    if: |
      always() && 
      needs.determine-strategy.result == 'success' &&
      (github.ref == 'refs/heads/main' || 
       needs.determine-strategy.outputs.test_level == 'comprehensive')
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run WebSocket agent event tests
        run: |
          echo "=== Running Mission Critical WebSocket Tests ==="
          python tests/mission_critical/test_websocket_agent_events_suite.py
      
      - name: Run configuration validation
        run: |
          echo "=== Running Configuration Validation ==="
          python scripts/check_architecture_compliance.py
      
      - name: Upload mission critical results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mission-critical-results
          path: |
            test-results/mission-critical/
            compliance-report.json
          retention-days: 7

  # ==========================================
  # PHASE 7: FINAL REPORT
  # ==========================================
  
  report:
    name: Test Report
    runs-on: warp-custom-test
    needs: [determine-strategy, python-tests, frontend-tests, e2e-tests, mission-critical-tests]
    if: always()
    timeout-minutes: 5
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: test-artifacts/
      
      - name: Generate test summary
        run: |
          echo "## üß™ Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### Test Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **Test Level:** ${{ needs.determine-strategy.outputs.test_level }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Categories:** ${{ needs.determine-strategy.outputs.categories }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Real Services:** ${{ needs.determine-strategy.outputs.real_services }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Real LLM:** ${{ needs.determine-strategy.outputs.real_llm }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### Test Results" >> $GITHUB_STEP_SUMMARY
          echo "| Component | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|" >> $GITHUB_STEP_SUMMARY
          
          # Python tests
          if [[ "${{ needs.python-tests.result }}" == "success" ]]; then
            echo "| Python Tests | ‚úÖ Passed |" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ needs.python-tests.result }}" == "failure" ]]; then
            echo "| Python Tests | ‚ùå Failed |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Python Tests | ‚è≠Ô∏è Skipped |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Frontend tests
          if [[ "${{ needs.frontend-tests.result }}" == "success" ]]; then
            echo "| Frontend Tests | ‚úÖ Passed |" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ needs.frontend-tests.result }}" == "failure" ]]; then
            echo "| Frontend Tests | ‚ùå Failed |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Frontend Tests | ‚è≠Ô∏è Skipped |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # E2E tests
          if [[ "${{ needs.e2e-tests.result }}" == "success" ]]; then
            echo "| E2E Tests | ‚úÖ Passed |" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ needs.e2e-tests.result }}" == "failure" ]]; then
            echo "| E2E Tests | ‚ùå Failed |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| E2E Tests | ‚è≠Ô∏è Skipped |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Mission Critical tests
          if [[ "${{ needs.mission-critical-tests.result }}" == "success" ]]; then
            echo "| Mission Critical | ‚úÖ Passed |" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ needs.mission-critical-tests.result }}" == "failure" ]]; then
            echo "| Mission Critical | ‚ùå Failed |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Mission Critical | ‚è≠Ô∏è Skipped |" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Overall status
          OVERALL_STATUS="success"
          if [[ "${{ needs.python-tests.result }}" == "failure" ]] || \
             [[ "${{ needs.frontend-tests.result }}" == "failure" ]] || \
             [[ "${{ needs.e2e-tests.result }}" == "failure" ]] || \
             [[ "${{ needs.mission-critical-tests.result }}" == "failure" ]]; then
            OVERALL_STATUS="failure"
          fi
          
          if [[ "$OVERALL_STATUS" == "success" ]]; then
            echo "### ‚úÖ All Tests Passed!" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚ùå Some Tests Failed" >> $GITHUB_STEP_SUMMARY
            echo "Please check the individual test results above for details." >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Generated by Unified Test Pipeline at $(date -u +%Y-%m-%dT%H:%M:%SZ)*" >> $GITHUB_STEP_SUMMARY
      
      - name: Update commit status
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const pythonStatus = '${{ needs.python-tests.result }}';
            const frontendStatus = '${{ needs.frontend-tests.result }}';
            const e2eStatus = '${{ needs.e2e-tests.result }}';
            const missionStatus = '${{ needs.mission-critical-tests.result }}';
            
            let overallStatus = 'success';
            let failedComponents = [];
            
            if (pythonStatus === 'failure') {
              overallStatus = 'failure';
              failedComponents.push('Python');
            }
            if (frontendStatus === 'failure') {
              overallStatus = 'failure';
              failedComponents.push('Frontend');
            }
            if (e2eStatus === 'failure') {
              overallStatus = 'failure';
              failedComponents.push('E2E');
            }
            if (missionStatus === 'failure') {
              overallStatus = 'failure';
              failedComponents.push('Mission Critical');
            }
            
            const description = overallStatus === 'success' ? 
              '‚úÖ All tests passed' :
              `‚ùå Failed: ${failedComponents.join(', ')}`;
            
            await github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: overallStatus,
              description: description,
              context: 'Unified Test Pipeline'
            });