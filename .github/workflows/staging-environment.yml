name: Staging Environment Management

on:
  pull_request:
    types: [opened, synchronize, closed, labeled, unlabeled]
  workflow_dispatch:
    inputs:
      action:
        description: 'Manual action'
        required: true
        type: choice
        options:
          - deploy
          - destroy
          - redeploy
      pr_number:
        description: 'PR number'
        required: true
        type: string

# Cancel any in-progress runs when a new commit is pushed for the same PR
# This ensures only the latest commit's workflow runs to completion
concurrency:
  group: staging-pr-${{ github.event.pull_request.number || github.event.inputs.pr_number }}
  cancel-in-progress: true

env:
  GCP_PROJECT_ID: netra-staging
  GCP_REGION: us-central1
  TERRAFORM_VERSION: 1.5.0
  STAGING_DOMAIN: staging.netrasystems.ai

jobs:
  check-eligibility:
    name: Check Staging Eligibility
    runs-on: ubuntu-latest
    outputs:
      should_deploy: ${{ steps.check.outputs.should_deploy }}
      environment_name: ${{ steps.check.outputs.environment_name }}
    steps:
      - name: Check labels and configuration
        id: check
        run: |
          PR_NUMBER=${{ github.event.pull_request.number || github.event.inputs.pr_number }}
          
          # Check for exclusion labels
          if [[ "${{ contains(github.event.pull_request.labels.*.name, 'no-staging') }}" == "true" ]]; then
            echo "should_deploy=false" >> $GITHUB_OUTPUT
            echo "❌ Staging skipped: 'no-staging' label present"
            exit 0
          fi
          
          if [[ "${{ contains(github.event.pull_request.labels.*.name, 'WIP') }}" == "true" ]]; then
            echo "should_deploy=false" >> $GITHUB_OUTPUT
            echo "❌ Staging skipped: 'WIP' label present"
            exit 0
          fi
          
          # Check for manual trigger
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            if [[ "${{ github.event.inputs.action }}" == "destroy" ]]; then
              echo "should_deploy=false" >> $GITHUB_OUTPUT
            else
              echo "should_deploy=true" >> $GITHUB_OUTPUT
            fi
          # Check for PR close
          elif [[ "${{ github.event.action }}" == "closed" ]]; then
            echo "should_deploy=false" >> $GITHUB_OUTPUT
          else
            echo "should_deploy=true" >> $GITHUB_OUTPUT
          fi
          
          echo "environment_name=pr-${PR_NUMBER}" >> $GITHUB_OUTPUT

  # Parallel build jobs for better performance
  build-backend:
    name: Build Backend Container
    needs: check-eligibility
    if: needs.check-eligibility.outputs.should_deploy == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      backend_image: ${{ steps.backend-build.outputs.backend_image }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
          fetch-depth: 0

      - name: Cache gcloud SDK
        uses: actions/cache@v4
        with:
          path: |
            ~/.config/gcloud
            ~/google-cloud-sdk
          key: gcloud-sdk-${{ runner.os }}-${{ hashFiles('.github/workflows/staging-environment.yml') }}
          restore-keys: |
            gcloud-sdk-${{ runner.os }}-

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_STAGING_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}
          skip_install: false

      - name: Configure Docker for GCP
        run: |
          gcloud auth configure-docker ${{ env.GCP_REGION }}-docker.pkg.dev

      - name: Check for backend changes
        id: backend-changes
        run: |
          PR_NUMBER=${{ github.event.pull_request.number || github.event.inputs.pr_number }}
          CURRENT_SHA=${{ github.sha }}
          
          # Try to get the last successful build SHA from cache
          CACHE_KEY="backend-build-cache-pr-${PR_NUMBER}"
          LAST_BUILD_SHA=""
          LAST_BUILD_IMAGE=""
          
          # Check if we have a cached build record in GCS
          if gsutil -q stat "gs://${{ env.GCP_PROJECT_ID }}-terraform-state/build-cache/${CACHE_KEY}.json" 2>/dev/null; then
            CACHE_DATA=$(gsutil cat "gs://${{ env.GCP_PROJECT_ID }}-terraform-state/build-cache/${CACHE_KEY}.json" 2>/dev/null || echo "{}")
            LAST_BUILD_SHA=$(echo "$CACHE_DATA" | jq -r '.sha // ""')
            LAST_BUILD_IMAGE=$(echo "$CACHE_DATA" | jq -r '.image // ""')
            echo "Found cached build: SHA=$LAST_BUILD_SHA"
          fi
          
          # Initialize as changed by default (safe for first commits or when unsure)
          CHANGED="true"
          CHANGED_FILES=""
          
          # If we have a last build SHA, check if files changed since then
          if [[ -n "$LAST_BUILD_SHA" ]] && [[ "$LAST_BUILD_SHA" != "null" ]]; then
            echo "Checking for changes since last successful build at $LAST_BUILD_SHA"
            
            # Check if the commit exists in our history
            if git rev-parse "$LAST_BUILD_SHA" >/dev/null 2>&1; then
              # Get files changed since last successful build
              CHANGED_FILES=$(git diff --name-only "$LAST_BUILD_SHA" HEAD 2>/dev/null || echo "force-rebuild")
            else
              echo "Last build SHA not found in history, checking all PR changes"
              CHANGED_FILES="force-rebuild"
            fi
          else
            # No cache, check all PR changes
            echo "No build cache found, checking all PR changes"
            
            if [[ "${{ github.event_name }}" == "pull_request" ]]; then
              # For PRs, check ALL changes in the PR
              git fetch origin ${{ github.event.pull_request.base.ref }}:refs/remotes/origin/${{ github.event.pull_request.base.ref }} --depth=50
              MERGE_BASE=$(git merge-base origin/${{ github.event.pull_request.base.ref }} HEAD 2>/dev/null || echo "")
              
              if [[ -n "$MERGE_BASE" ]]; then
                CHANGED_FILES=$(git diff --name-only $MERGE_BASE HEAD 2>/dev/null || echo "")
              else
                CHANGED_FILES=$(git diff --name-only origin/${{ github.event.pull_request.base.ref }} HEAD 2>/dev/null || echo "")
              fi
            else
              CHANGED_FILES="force-rebuild"
            fi
          fi
          
          # Debug output
          echo "Event: ${{ github.event_name }}, Action: ${{ github.event.action }}"
          echo "Current SHA: $CURRENT_SHA"
          echo "Last Build SHA: $LAST_BUILD_SHA"
          if [[ "$CHANGED_FILES" != "force-rebuild" ]]; then
            echo "Total files changed: $(echo "$CHANGED_FILES" | wc -l)"
            echo "Changed files:"
            echo "$CHANGED_FILES" | head -20
          fi
          
          # Check for backend-related changes
          if [[ "$CHANGED_FILES" == "force-rebuild" ]]; then
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "🔨 Forcing backend rebuild"
          elif [[ -z "$CHANGED_FILES" ]]; then
            # No changes detected and we have a valid last build
            if [[ -n "$LAST_BUILD_IMAGE" ]] && [[ "$LAST_BUILD_IMAGE" != "null" ]]; then
              echo "changed=false" >> $GITHUB_OUTPUT
              echo "cached_image=$LAST_BUILD_IMAGE" >> $GITHUB_OUTPUT
              echo "♻️ No backend changes detected, will use cached image: $LAST_BUILD_IMAGE"
            else
              echo "changed=true" >> $GITHUB_OUTPUT
              echo "⚠️ No changes but no cached image available, forcing rebuild"
            fi
          elif echo "$CHANGED_FILES" | grep -E '^(app/|requirements\.txt|Dockerfile\.backend|alembic/|pyproject\.toml|poetry\.lock)' > /dev/null 2>&1; then
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "✅ Backend changes detected"
            echo "Backend files changed:"
            echo "$CHANGED_FILES" | grep -E '^(app/|requirements\.txt|Dockerfile\.backend|alembic/|pyproject\.toml|poetry\.lock)' | head -10
          else
            # No backend changes, use cached image if available
            if [[ -n "$LAST_BUILD_IMAGE" ]] && [[ "$LAST_BUILD_IMAGE" != "null" ]]; then
              echo "changed=false" >> $GITHUB_OUTPUT
              echo "cached_image=$LAST_BUILD_IMAGE" >> $GITHUB_OUTPUT
              echo "♻️ No backend changes detected, will use cached image: $LAST_BUILD_IMAGE"
            else
              echo "changed=true" >> $GITHUB_OUTPUT
              echo "⚠️ No backend changes but no cached image available, forcing rebuild"
            fi
          fi
        env:
          GH_TOKEN: ${{ github.token }}

      - name: Build and push backend container
        id: backend-build
        run: |
          PR_NUMBER=${{ github.event.pull_request.number || github.event.inputs.pr_number }}
          IMAGE_TAG="pr-${PR_NUMBER}-${{ github.sha }}"
          IMAGE_URL="${{ env.GCP_REGION }}-docker.pkg.dev/${{ env.GCP_PROJECT_ID }}/staging/backend:${IMAGE_TAG}"
          
          # Check if we should use cached image
          if [[ "${{ steps.backend-changes.outputs.changed }}" == "false" ]]; then
            CACHED_IMAGE="${{ steps.backend-changes.outputs.cached_image }}"
            if [[ -n "$CACHED_IMAGE" ]] && [[ "$CACHED_IMAGE" != "null" ]]; then
              echo "♻️ Using cached backend image: $CACHED_IMAGE"
              echo "backend_image=$CACHED_IMAGE" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi
          
          # Build new image
          echo "🔨 Building new backend image..."
          DOCKER_BUILDKIT=1 timeout 600 docker build \
            --progress=plain \
            --network=host \
            -t $IMAGE_URL \
            -f Dockerfile.backend . || {
            echo "Docker build timed out after 10 minutes"
            echo "This might be due to network issues or pip hanging"
            exit 1
          }
          
          # Push with timeout
          timeout 300 docker push $IMAGE_URL || {
            echo "Docker push timed out after 5 minutes"
            exit 1
          }
          
          echo "backend_image=$IMAGE_URL" >> $GITHUB_OUTPUT
          
          # Update build cache
          CACHE_KEY="backend-build-cache-pr-${PR_NUMBER}"
          echo '{"sha": "'${{ github.sha }}'", "image": "'$IMAGE_URL'", "timestamp": "'$(date -u +"%Y-%m-%dT%H:%M:%SZ")'"}' | \
            gsutil cp - "gs://${{ env.GCP_PROJECT_ID }}-terraform-state/build-cache/${CACHE_KEY}.json"
          echo "✅ Build cache updated"

  build-frontend:
    name: Build Frontend Container
    needs: check-eligibility
    if: needs.check-eligibility.outputs.should_deploy == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      frontend_image: ${{ steps.frontend-build.outputs.frontend_image }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
          fetch-depth: 0

      - name: Cache gcloud SDK
        uses: actions/cache@v4
        with:
          path: |
            ~/.config/gcloud
            ~/google-cloud-sdk
          key: gcloud-sdk-${{ runner.os }}-${{ hashFiles('.github/workflows/staging-environment.yml') }}
          restore-keys: |
            gcloud-sdk-${{ runner.os }}-

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_STAGING_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}
          skip_install: false

      - name: Configure Docker for GCP
        run: |
          gcloud auth configure-docker ${{ env.GCP_REGION }}-docker.pkg.dev

      - name: Check for frontend changes
        id: frontend-changes
        run: |
          PR_NUMBER=${{ github.event.pull_request.number || github.event.inputs.pr_number }}
          CURRENT_SHA=${{ github.sha }}
          
          # Try to get the last successful build SHA from cache
          CACHE_KEY="frontend-build-cache-pr-${PR_NUMBER}"
          LAST_BUILD_SHA=""
          LAST_BUILD_IMAGE=""
          
          # Check if we have a cached build record in GCS
          if gsutil -q stat "gs://${{ env.GCP_PROJECT_ID }}-terraform-state/build-cache/${CACHE_KEY}.json" 2>/dev/null; then
            CACHE_DATA=$(gsutil cat "gs://${{ env.GCP_PROJECT_ID }}-terraform-state/build-cache/${CACHE_KEY}.json" 2>/dev/null || echo "{}")
            LAST_BUILD_SHA=$(echo "$CACHE_DATA" | jq -r '.sha // ""')
            LAST_BUILD_IMAGE=$(echo "$CACHE_DATA" | jq -r '.image // ""')
            echo "Found cached build: SHA=$LAST_BUILD_SHA"
          fi
          
          # Initialize as changed by default (safe for first commits or when unsure)
          CHANGED="true"
          CHANGED_FILES=""
          
          # If we have a last build SHA, check if files changed since then
          if [[ -n "$LAST_BUILD_SHA" ]] && [[ "$LAST_BUILD_SHA" != "null" ]]; then
            echo "Checking for changes since last successful build at $LAST_BUILD_SHA"
            
            # Check if the commit exists in our history
            if git rev-parse "$LAST_BUILD_SHA" >/dev/null 2>&1; then
              # Get files changed since last successful build
              CHANGED_FILES=$(git diff --name-only "$LAST_BUILD_SHA" HEAD 2>/dev/null || echo "force-rebuild")
            else
              echo "Last build SHA not found in history, checking all PR changes"
              CHANGED_FILES="force-rebuild"
            fi
          else
            # No cache, check all PR changes
            echo "No build cache found, checking all PR changes"
            
            if [[ "${{ github.event_name }}" == "pull_request" ]]; then
              # For PRs, check ALL changes in the PR
              git fetch origin ${{ github.event.pull_request.base.ref }}:refs/remotes/origin/${{ github.event.pull_request.base.ref }} --depth=50 2>/dev/null || true
              MERGE_BASE=$(git merge-base origin/${{ github.event.pull_request.base.ref }} HEAD 2>/dev/null || echo "")
              
              if [[ -n "$MERGE_BASE" ]]; then
                CHANGED_FILES=$(git diff --name-only $MERGE_BASE HEAD 2>/dev/null || echo "")
              else
                CHANGED_FILES=$(git diff --name-only origin/${{ github.event.pull_request.base.ref }} HEAD 2>/dev/null || echo "")
              fi
            else
              CHANGED_FILES="force-rebuild"
            fi
          fi
          
          # Debug output
          echo "Event: ${{ github.event_name }}, Action: ${{ github.event.action }}"
          echo "Current SHA: $CURRENT_SHA"
          echo "Last Build SHA: $LAST_BUILD_SHA"
          if [[ "$CHANGED_FILES" != "force-rebuild" ]]; then
            echo "Total files changed: $(echo "$CHANGED_FILES" | wc -l)"
            echo "Changed files:"
            echo "$CHANGED_FILES" | head -20
          fi
          
          # Check for frontend-related changes
          if [[ "$CHANGED_FILES" == "force-rebuild" ]]; then
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "🔨 Forcing frontend rebuild"
          elif [[ -z "$CHANGED_FILES" ]]; then
            # No changes detected and we have a valid last build
            if [[ -n "$LAST_BUILD_IMAGE" ]] && [[ "$LAST_BUILD_IMAGE" != "null" ]]; then
              echo "changed=false" >> $GITHUB_OUTPUT
              echo "cached_image=$LAST_BUILD_IMAGE" >> $GITHUB_OUTPUT
              echo "♻️ No frontend changes detected, will use cached image: $LAST_BUILD_IMAGE"
            else
              echo "changed=true" >> $GITHUB_OUTPUT
              echo "⚠️ No changes but no cached image available, forcing rebuild"
            fi
          elif echo "$CHANGED_FILES" | grep -E '^frontend/' > /dev/null 2>&1; then
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "✅ Frontend changes detected"
            echo "Frontend files changed:"
            echo "$CHANGED_FILES" | grep -E '^frontend/' | head -10
          else
            # No frontend changes, use cached image if available
            if [[ -n "$LAST_BUILD_IMAGE" ]] && [[ "$LAST_BUILD_IMAGE" != "null" ]]; then
              echo "changed=false" >> $GITHUB_OUTPUT
              echo "cached_image=$LAST_BUILD_IMAGE" >> $GITHUB_OUTPUT
              echo "♻️ No frontend changes detected, will use cached image: $LAST_BUILD_IMAGE"
            else
              echo "changed=true" >> $GITHUB_OUTPUT
              echo "⚠️ No frontend changes but no cached image available, forcing rebuild"
            fi
          fi
        env:
          GH_TOKEN: ${{ github.token }}

      - name: Build and push frontend container
        id: frontend-build
        run: |
          PR_NUMBER=${{ github.event.pull_request.number || github.event.inputs.pr_number }}
          IMAGE_TAG="pr-${PR_NUMBER}-${{ github.sha }}"
          IMAGE_URL="${{ env.GCP_REGION }}-docker.pkg.dev/${{ env.GCP_PROJECT_ID }}/staging/frontend:${IMAGE_TAG}"
          
          # Check if we should use cached image
          if [[ "${{ steps.frontend-changes.outputs.changed }}" == "false" ]]; then
            CACHED_IMAGE="${{ steps.frontend-changes.outputs.cached_image }}"
            if [[ -n "$CACHED_IMAGE" ]] && [[ "$CACHED_IMAGE" != "null" ]]; then
              echo "♻️ Using cached frontend image: $CACHED_IMAGE"
              echo "frontend_image=$CACHED_IMAGE" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi
          
          # Build new image
          echo "🔨 Building new frontend image..."
          
          # Check if we're in the root directory and Dockerfile.frontend is in frontend/
          if [[ -f "frontend/Dockerfile.frontend" ]]; then
            cd frontend
            DOCKER_BUILDKIT=1 timeout 600 docker build \
              --progress=plain \
              --network=host \
              -t $IMAGE_URL \
              -f Dockerfile.frontend \
              --build-arg NEXT_PUBLIC_API_URL=https://pr-${PR_NUMBER}-api.${{ env.STAGING_DOMAIN }} . || {
              echo "Frontend Docker build timed out after 10 minutes"
              exit 1
            }
            cd ..
          elif [[ -f "Dockerfile.frontend" ]]; then
            # Fallback if Dockerfile.frontend is in root
            DOCKER_BUILDKIT=1 timeout 600 docker build \
              --progress=plain \
              --network=host \
              -t $IMAGE_URL \
              -f Dockerfile.frontend \
              --build-arg NEXT_PUBLIC_API_URL=https://pr-${PR_NUMBER}-api.${{ env.STAGING_DOMAIN }} . || {
              echo "Frontend Docker build timed out after 10 minutes"
              exit 1
            }
          else
            echo "❌ Error: Dockerfile.frontend not found in expected locations"
            exit 1
          fi
          
          # Push with timeout
          timeout 300 docker push $IMAGE_URL || {
            echo "Docker push timed out after 5 minutes"
            exit 1
          }
          
          echo "frontend_image=$IMAGE_URL" >> $GITHUB_OUTPUT
          
          # Update build cache
          CACHE_KEY="frontend-build-cache-pr-${PR_NUMBER}"
          echo '{"sha": "'${{ github.sha }}'", "image": "'$IMAGE_URL'", "timestamp": "'$(date -u +"%Y-%m-%dT%H:%M:%SZ")'"}' | \
            gsutil cp - "gs://${{ env.GCP_PROJECT_ID }}-terraform-state/build-cache/${CACHE_KEY}.json"
          echo "✅ Build cache updated"

  deploy-staging:
    name: Deploy Staging Environment
    needs: [check-eligibility, build-backend, build-frontend]
    if: needs.check-eligibility.outputs.should_deploy == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Reduced from 45 since builds are now parallel
    environment:
      name: staging-pr-${{ github.event.pull_request.number || github.event.inputs.pr_number }}
      url: https://pr-${{ github.event.pull_request.number || github.event.inputs.pr_number }}.${{ env.STAGING_DOMAIN }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
          fetch-depth: 0  # Fetch all history for proper change detection

      # Cache gcloud SDK installation to save ~20-30 seconds
      - name: Cache gcloud SDK
        uses: actions/cache@v4
        with:
          path: |
            ~/.config/gcloud
            ~/google-cloud-sdk
          key: gcloud-sdk-${{ runner.os }}-${{ hashFiles('.github/workflows/staging-environment.yml') }}
          restore-keys: |
            gcloud-sdk-${{ runner.os }}-

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_STAGING_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}
          skip_install: false  # Still install if not cached

      - name: Configure OAuth for PR Environment
        id: oauth
        run: |
          PR_NUMBER=${{ github.event.pull_request.number || github.event.inputs.pr_number }}
          
          # Set OAuth environment variables for this PR
          echo "PR_NUMBER=${PR_NUMBER}" >> $GITHUB_ENV
          echo "GOOGLE_OAUTH_CLIENT_ID_STAGING=${{ secrets.GOOGLE_OAUTH_CLIENT_ID_STAGING }}" >> $GITHUB_ENV
          echo "GOOGLE_OAUTH_CLIENT_SECRET_STAGING=${{ secrets.GOOGLE_OAUTH_CLIENT_SECRET_STAGING }}" >> $GITHUB_ENV
          echo "USE_OAUTH_PROXY=true" >> $GITHUB_ENV
          echo "OAUTH_PROXY_URL=https://auth.staging.netrasystems.ai" >> $GITHUB_ENV
          
          # Generate URLs for this PR environment
          echo "frontend_url=https://pr-${PR_NUMBER}.${{ env.STAGING_DOMAIN }}" >> $GITHUB_OUTPUT
          echo "api_url=https://pr-${PR_NUMBER}-api.${{ env.STAGING_DOMAIN }}" >> $GITHUB_OUTPUT
          
          echo "✅ OAuth configured for PR #${PR_NUMBER}"

      - name: Load staging configuration
        id: config
        run: |
          # Get the numerical project ID for Secret Manager
          PROJECT_ID_NUMERICAL_STAGING=$(gcloud projects describe ${{ env.GCP_PROJECT_ID }} --format="value(projectNumber)")
          echo "project_id_numerical=$PROJECT_ID_NUMERICAL_STAGING" >> $GITHUB_OUTPUT
          echo "Using numerical project ID: $PROJECT_ID_NUMERICAL_STAGING"
          
          # Load configuration with defaults
          if [ -f .github/staging.yml ]; then
            TEST_LEVEL=$(yq e '.default_test_level // "integration"' .github/staging.yml)
            MAX_INSTANCES=$(yq e '.resource_limits.compute.max_instances // 3' .github/staging.yml)
          else
            TEST_LEVEL="integration"
            MAX_INSTANCES="3"
          fi
          
          echo "test_level=$TEST_LEVEL" >> $GITHUB_OUTPUT
          echo "max_instances=$MAX_INSTANCES" >> $GITHUB_OUTPUT

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Terraform Init
        working-directory: ./terraform/staging
        timeout-minutes: 5
        run: |
          # Check for state lock and clear if stale
          LOCK_FILE="gs://${{ env.GCP_PROJECT_ID }}-terraform-state/staging/pr-${{ github.event.pull_request.number || github.event.inputs.pr_number }}/default.tflock"
          
          # Check if lock exists and is older than 30 minutes
          if gsutil stat "$LOCK_FILE" 2>/dev/null; then
            LOCK_AGE=$(gsutil stat "$LOCK_FILE" | grep "Creation time:" | cut -d':' -f2- | xargs -I {} date -d "{}" +%s)
            CURRENT_TIME=$(date +%s)
            AGE_MINUTES=$(( ($CURRENT_TIME - $LOCK_AGE) / 60 ))
            
            if [ $AGE_MINUTES -gt 30 ]; then
              echo "⚠️ Found stale lock file (${AGE_MINUTES} minutes old), removing..."
              gsutil rm "$LOCK_FILE" || true
            else
              echo "⚠️ Active lock file found (${AGE_MINUTES} minutes old)"
              echo "Another Terraform operation may be in progress"
            fi
          fi
          
          # Initialize with lock timeout
          terraform init \
            -backend-config="bucket=${{ env.GCP_PROJECT_ID }}-terraform-state" \
            -backend-config="prefix=staging/pr-${{ github.event.pull_request.number || github.event.inputs.pr_number }}" \
            -lock-timeout=120s

      - name: Terraform Plan
        working-directory: ./terraform/staging
        timeout-minutes: 5
        env:
          TF_VAR_project_id: ${{ env.GCP_PROJECT_ID }}
          TF_VAR_project_id_numerical: ${{ steps.config.outputs.project_id_numerical }}
          TF_VAR_region: ${{ env.GCP_REGION }}
          TF_VAR_pr_number: ${{ github.event.pull_request.number || github.event.inputs.pr_number }}
          TF_VAR_backend_image: ${{ needs.build-backend.outputs.backend_image }}
          TF_VAR_frontend_image: ${{ needs.build-frontend.outputs.frontend_image }}
          TF_VAR_max_instances: ${{ steps.config.outputs.max_instances }}
          TF_VAR_domain: ${{ env.STAGING_DOMAIN }}
          TF_VAR_postgres_password: ${{ secrets.STAGING_DB_PASSWORD }}
          TF_VAR_clickhouse_password: ${{ secrets.CLICKHOUSE_PASSWORD || 'placeholder_password' }}
        run: |
          # Create secure tfvars file to prevent secrets in logs
          cat > terraform.tfvars << 'EOF'
          project_id = "${{ env.GCP_PROJECT_ID }}"
          project_id_numerical = "${{ steps.config.outputs.project_id_numerical }}"
          region = "${{ env.GCP_REGION }}"
          pr_number = "${{ github.event.pull_request.number || github.event.inputs.pr_number }}"
          backend_image = "${{ needs.build-backend.outputs.backend_image }}"
          frontend_image = "${{ needs.build-frontend.outputs.frontend_image }}"
          max_instances = ${{ steps.config.outputs.max_instances }}
          domain = "${{ env.STAGING_DOMAIN }}"
          EOF
          
          # Add sensitive variables through environment variables only
          echo "postgres_password = \"${TF_VAR_postgres_password}\"" >> terraform.tfvars
          echo "clickhouse_password = \"${TF_VAR_clickhouse_password}\"" >> terraform.tfvars
          
          # Secure the file
          chmod 600 terraform.tfvars
          
          # Run plan with secure variables file
          terraform plan -var-file=terraform.tfvars -lock-timeout=120s -out=tfplan
          
          # Clean up sensitive file
          rm -f terraform.tfvars

      - name: Terraform Apply
        working-directory: ./terraform/staging
        id: terraform
        timeout-minutes: 20  # Terraform should complete within 20 minutes
        run: |
          # Set timeout for terraform apply itself
          timeout 1200 terraform apply -auto-approve tfplan || {
            echo "Terraform apply timed out after 20 minutes"
            echo "Attempting to show current state..."
            terraform show
            exit 1
          }
          
          # Capture outputs
          BACKEND_URL=$(terraform output -raw backend_url 2>/dev/null || echo "")
          FRONTEND_URL=$(terraform output -raw frontend_url 2>/dev/null || echo "")
          DATABASE_NAME=$(terraform output -raw database_name 2>/dev/null || echo "")
          
          echo "backend_url=$BACKEND_URL" >> $GITHUB_OUTPUT
          echo "frontend_url=$FRONTEND_URL" >> $GITHUB_OUTPUT
          echo "database_name=$DATABASE_NAME" >> $GITHUB_OUTPUT
          
          # Export connection details for next steps
          echo "DB_NAME=$DATABASE_NAME" >> $GITHUB_ENV
          echo "DB_USER=user_pr_${{ github.event.pull_request.number || github.event.inputs.pr_number }}" >> $GITHUB_ENV

      - name: Run database migrations
        timeout-minutes: 5  # Migrations should complete quickly
        run: |
          PR_NUMBER=${{ github.event.pull_request.number || github.event.inputs.pr_number }}
          
          # Get the SQL instance connection name from shared infrastructure
          SQL_INSTANCE_CONNECTION=$(cd terraform/staging && terraform output -raw -state=terraform.tfstate sql_instance_connection 2>/dev/null || echo "")
          
          if [ -z "$SQL_INSTANCE_CONNECTION" ]; then
            echo "Warning: Could not get SQL instance connection from Terraform state"
            SQL_INSTANCE_CONNECTION="${{ env.GCP_PROJECT_ID }}:${{ env.GCP_REGION }}:staging-shared-postgres"
          fi
          
          # Set database URL for migrations using Cloud SQL proxy format
          export DATABASE_URL="postgresql://user_pr_${PR_NUMBER}:${{ secrets.STAGING_DB_PASSWORD }}@/netra_pr_${PR_NUMBER}?host=/cloudsql/${SQL_INSTANCE_CONNECTION}"
          
          echo "Using DATABASE_URL format for Cloud SQL migrations"
          
          # Install cloud_sql_proxy if not already installed
          if ! command -v cloud_sql_proxy &> /dev/null; then
            echo "Installing Cloud SQL Proxy..."
            wget -q https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy
            chmod +x cloud_sql_proxy
            sudo mv cloud_sql_proxy /usr/local/bin/
          fi
          
          # Start Cloud SQL proxy in background
          cloud_sql_proxy -instances=${SQL_INSTANCE_CONNECTION}=tcp:5432 &
          PROXY_PID=$!
          sleep 5
          
          # Update DATABASE_URL to use localhost for migration
          export DATABASE_URL="postgresql://user_pr_${PR_NUMBER}:${{ secrets.STAGING_DB_PASSWORD }}@localhost:5432/netra_pr_${PR_NUMBER}"
          
          # Wait for database to be ready (max 2 minutes)
          echo "Waiting for database to be ready..."
          for i in {1..24}; do
            if pg_isready -h localhost -p 5432 -U user_pr_${PR_NUMBER} 2>/dev/null; then
              echo "Database is ready!"
              break
            fi
            if [ $i -eq 24 ]; then
              echo "Database did not become ready in time"
              kill $PROXY_PID 2>/dev/null || true
              exit 1
            fi
            echo "Attempt $i/24: Database not ready, waiting 5 seconds..."
            sleep 5
          done
          
          # Run Alembic migrations with timeout
          cd app
          timeout 180 alembic upgrade head || {
            echo "Database migrations timed out after 3 minutes"
            kill $PROXY_PID 2>/dev/null || true
            exit 1
          }
          
          # Stop Cloud SQL proxy
          kill $PROXY_PID 2>/dev/null || true

      - name: Seed test data
        timeout-minutes: 3  # Data seeding should be quick
        run: |
          PR_NUMBER=${{ github.event.pull_request.number || github.event.inputs.pr_number }}
          REDIS_DB_INDEX=$((PR_NUMBER % 16))
          
          # Get connection details from Terraform
          SQL_INSTANCE_CONNECTION=$(cd terraform/staging && terraform output -raw -state=terraform.tfstate sql_instance_connection 2>/dev/null || echo "")
          
          if [ -z "$SQL_INSTANCE_CONNECTION" ]; then
            SQL_INSTANCE_CONNECTION="${{ env.GCP_PROJECT_ID }}:${{ env.GCP_REGION }}:staging-shared-postgres"
          fi
          
          # Start Cloud SQL proxy for seeding
          cloud_sql_proxy -instances=${SQL_INSTANCE_CONNECTION}=tcp:5432 &
          PROXY_PID=$!
          sleep 5
          
          export DATABASE_URL="postgresql://user_pr_${PR_NUMBER}:${{ secrets.STAGING_DB_PASSWORD }}@localhost:5432/netra_pr_${PR_NUMBER}"
          export REDIS_URL="redis://10.0.0.2:6379/${REDIS_DB_INDEX}"
          export CLICKHOUSE_PASSWORD="${{ secrets.CLICKHOUSE_PASSWORD }}"
          export CLICKHOUSE_URL="clickhouse://default:${{ secrets.CLICKHOUSE_PASSWORD }}@xedvrr4c3r.us-central1.gcp.clickhouse.cloud:8443/default?secure=1"
          
          # Check if seed script exists
          if [ -f "scripts/seed_staging_data.py" ]; then
            timeout 150 python scripts/seed_staging_data.py \
              --pr-number "${PR_NUMBER}" || {
              echo "Data seeding timed out after 2.5 minutes"
              kill $PROXY_PID 2>/dev/null || true
              exit 1
            }
          else
            echo "Seed script not found, skipping data seeding"
          fi
          
          # Stop Cloud SQL proxy
          kill $PROXY_PID 2>/dev/null || true

      - name: Wait for services to be healthy
        timeout-minutes: 10  # Services should be up within 10 minutes
        run: |
          STAGING_URL="${{ steps.terraform.outputs.frontend_url }}"
          API_URL="${{ steps.terraform.outputs.backend_url }}"
          
          echo "Waiting for frontend at $STAGING_URL..."
          for i in {1..30}; do
            HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" --max-time 10 "$STAGING_URL" 2>/dev/null || echo "000")
            if [[ "$HTTP_CODE" == "200" ]] || [[ "$HTTP_CODE" == "301" ]] || [[ "$HTTP_CODE" == "302" ]]; then
              echo "✅ Frontend is ready! (HTTP $HTTP_CODE)"
              break
            fi
            if [ $i -eq 30 ]; then
              echo "❌ Frontend did not become ready in time (last HTTP code: $HTTP_CODE)"
              echo "Checking Cloud Run service status..."
              gcloud run services describe frontend-pr-${{ github.event.pull_request.number || github.event.inputs.pr_number }} \
                --region=${{ env.GCP_REGION }} --format="value(status.conditions[0].message)" || true
              # Don't fail immediately, continue with API check
            fi
            echo "Attempt $i/30: Frontend not ready yet (HTTP $HTTP_CODE)..."
            sleep 10
          done
          
          echo "Waiting for API at $API_URL/health..."
          for i in {1..40}; do  # Give API more time to start
            HEALTH_RESPONSE=$(curl -s --max-time 10 "$API_URL/health/" 2>/dev/null || echo "")
            if echo "$HEALTH_RESPONSE" | grep -q "healthy\|ok\|ready"; then
              echo "✅ API is ready!"
              break
            fi
            if [ $i -eq 40 ]; then
              echo "⚠️ API health check timed out, but continuing anyway"
              echo "Last health response: $HEALTH_RESPONSE"
              echo "Checking recent Cloud Run service logs..."
              gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=backend-pr-${{ github.event.pull_request.number || github.event.inputs.pr_number }}" \
                --limit=30 --format=json --project=${{ env.GCP_PROJECT_ID }} | jq -r '.[].textPayload // .[].jsonPayload.message // empty' | head -50 || true
              # Continue despite health check failure - the service might still be functional
            fi
            echo "Attempt $i/40: API not ready yet..."
            sleep 10
          done
          
          # Final verification with detailed debugging if needed
          echo "Final service verification..."
          echo "Frontend status:"
          curl -s --max-time 5 -I "$STAGING_URL" || echo "Frontend not responding"
          echo ""
          echo "API status:"
          curl -s --max-time 5 "$API_URL/health/" || echo "API health endpoint not responding"
          echo ""

      - name: Run staging tests
        id: tests
        timeout-minutes: 15  # Tests should complete within 15 minutes
        run: |
          export STAGING_URL="${{ steps.terraform.outputs.frontend_url }}"
          export STAGING_API_URL="${{ steps.terraform.outputs.backend_url }}"
          
          # Run test suite with staging flag
          python test_runner.py \
            --level "${{ steps.config.outputs.test_level }}" \
            --staging \
            --report-format json \
            --output test_results.json
          
          # Extract summary for PR comment
          PASSED=$(jq -r '.summary.passed' test_results.json)
          FAILED=$(jq -r '.summary.failed' test_results.json)
          SKIPPED=$(jq -r '.summary.skipped' test_results.json)
          DURATION=$(jq -r '.summary.duration' test_results.json)
          
          echo "test_passed=$PASSED" >> $GITHUB_OUTPUT
          echo "test_failed=$FAILED" >> $GITHUB_OUTPUT
          echo "test_skipped=$SKIPPED" >> $GITHUB_OUTPUT
          echo "test_duration=$DURATION" >> $GITHUB_OUTPUT

      # UPDATED FOR V4: Use actions/upload-artifact@v4
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-pr-${{ github.event.pull_request.number || github.event.inputs.pr_number }}-${{ github.run_id }}-${{ github.run_attempt }}
          path: |
            test_results.json
            test_reports/
          retention-days: 7
          compression-level: 6  # New v4 feature for compression control

      - name: Update PR comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const pr_number = context.payload.pull_request.number;
            const staging_url = '${{ steps.terraform.outputs.frontend_url }}';
            const api_url = '${{ steps.terraform.outputs.backend_url }}';
            const test_passed = '${{ steps.tests.outputs.test_passed }}';
            const test_failed = '${{ steps.tests.outputs.test_failed }}';
            const test_duration = '${{ steps.tests.outputs.test_duration }}';
            
            const test_status = test_failed === '0' ? '✅' : '❌';
            
            const comment = `## 🚀 Staging Environment Ready
            
            **Environment:** pr-${pr_number}
            **Status:** ${test_status} Deployed and Tested
            
            ### 🔗 Access URLs
            - **Frontend:** ${staging_url}
            - **API:** ${api_url}/docs
            - **Health:** ${api_url}/health
            
            ### 🧪 Test Results
            - **Passed:** ${test_passed} tests
            - **Failed:** ${test_failed} tests
            - **Duration:** ${test_duration}s
            - **Test Level:** ${{ steps.config.outputs.test_level }}
            
            ### 📊 Resource Configuration
            - **Max Instances:** ${{ steps.config.outputs.max_instances }}
            - **Region:** ${{ env.GCP_REGION }}
            
            ### 🔐 Access Control
            This staging environment is protected and only accessible to:
            - PR author and reviewers
            - Repository maintainers
            
            ---
            
            *This environment will be automatically destroyed when the PR is closed or merged.*
            *Last updated: ${new Date().toISOString()}*`;
            
            // Find and update or create comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: pr_number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('Staging Environment')
            );
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: pr_number,
                body: comment
              });
            }

      - name: Set commit status
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const test_failed = '${{ steps.tests.outputs.test_failed }}';
            const state = test_failed === '0' ? 'success' : 'failure';
            
            await github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.payload.pull_request.head.sha,
              state: state,
              target_url: '${{ steps.terraform.outputs.frontend_url }}',
              description: `Staging deployed with ${test_failed} test failures`,
              context: 'staging/deployment'
            });

  destroy-staging:
    name: Destroy Staging Environment
    needs: check-eligibility
    if: |
      needs.check-eligibility.outputs.should_deploy == 'false' && 
      (github.event.action == 'closed' || github.event.inputs.action == 'destroy')
    runs-on: ubuntu-latest
    timeout-minutes: 20  # Destruction should be quick
    # Don't cancel destroy operations - they must complete to clean up resources
    concurrency:
      group: staging-destroy-pr-${{ github.event.pull_request.number || github.event.inputs.pr_number }}
      cancel-in-progress: false
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_STAGING_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Export logs before destruction
        continue-on-error: true  # Continue even if log export fails
        run: |
          PR_NUMBER=${{ github.event.pull_request.number || github.event.inputs.pr_number }}
          
          # Export Cloud Run logs (may fail if permissions are insufficient)
          if gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=staging-pr-${PR_NUMBER}" \
            --format=json \
            --project=${{ env.GCP_PROJECT_ID }} \
            > staging-logs-pr-${PR_NUMBER}.json 2>/dev/null; then
            echo "✅ Logs exported for PR ${PR_NUMBER}"
          else
            echo "⚠️ Could not export logs (insufficient permissions or no logs found)"
            echo "{}" > staging-logs-pr-${PR_NUMBER}.json  # Create empty file for artifact
          fi

      # UPDATED FOR V4: Use actions/upload-artifact@v4 with unique naming
      - name: Upload logs
        uses: actions/upload-artifact@v4
        with:
          name: staging-logs-pr-${{ github.event.pull_request.number || github.event.inputs.pr_number }}-${{ github.run_id }}
          path: staging-logs-*.json
          retention-days: 7
          compression-level: 9  # Maximum compression for logs

      - name: Terraform Destroy
        working-directory: ./terraform/staging
        env:
          TF_VAR_postgres_password: ${{ secrets.STAGING_DB_PASSWORD }}
          TF_VAR_clickhouse_password: ${{ secrets.CLICKHOUSE_PASSWORD || 'placeholder_password' }}
        run: |
          terraform init \
            -backend-config="bucket=${{ env.GCP_PROJECT_ID }}-terraform-state" \
            -backend-config="prefix=staging/pr-${{ github.event.pull_request.number || github.event.inputs.pr_number }}"
          
          # Create secure tfvars file for destroy operation
          cat > terraform.tfvars << 'EOF'
          project_id = "${{ env.GCP_PROJECT_ID }}"
          region = "${{ env.GCP_REGION }}"
          pr_number = "${{ github.event.pull_request.number || github.event.inputs.pr_number }}"
          backend_image = "${{ needs.build-backend.outputs.backend_image || 'gcr.io/PROJECT_ID/backend:latest' }}"
          frontend_image = "${{ needs.build-frontend.outputs.frontend_image || 'gcr.io/PROJECT_ID/frontend:latest' }}"
          max_instances = 1
          domain = "${{ env.STAGING_DOMAIN }}"
          EOF
          
          # Add sensitive variables securely
          echo "postgres_password = \"${TF_VAR_postgres_password}\"" >> terraform.tfvars
          echo "clickhouse_password = \"${TF_VAR_clickhouse_password}\"" >> terraform.tfvars
          
          # Secure the file
          chmod 600 terraform.tfvars
          
          # Run destroy with secure variables
          terraform destroy -auto-approve -var-file=terraform.tfvars
          
          # Clean up sensitive file
          rm -f terraform.tfvars

      - name: Clean up container images and build cache
        run: |
          PR_NUMBER=${{ github.event.pull_request.number || github.event.inputs.pr_number }}
          
          # Delete backend images
          gcloud artifacts docker images delete \
            ${{ env.GCP_REGION }}-docker.pkg.dev/${{ env.GCP_PROJECT_ID }}/staging/backend:pr-${PR_NUMBER}-* \
            --quiet || true
          
          # Delete frontend images
          gcloud artifacts docker images delete \
            ${{ env.GCP_REGION }}-docker.pkg.dev/${{ env.GCP_PROJECT_ID }}/staging/frontend:pr-${PR_NUMBER}-* \
            --quiet || true
          
          # Clean up build cache files
          echo "Cleaning up build cache for PR #${PR_NUMBER}..."
          gsutil rm "gs://${{ env.GCP_PROJECT_ID }}-terraform-state/build-cache/backend-build-cache-pr-${PR_NUMBER}.json" 2>/dev/null || true
          gsutil rm "gs://${{ env.GCP_PROJECT_ID }}-terraform-state/build-cache/frontend-build-cache-pr-${PR_NUMBER}.json" 2>/dev/null || true
          echo "✅ Build cache cleaned up"

      - name: Update PR comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const pr_number = context.payload.pull_request.number;
            
            const comment = `## 🧹 Staging Environment Destroyed
            
            **Environment:** pr-${pr_number}
            **Status:** ✅ Successfully cleaned up
            **Destroyed at:** ${new Date().toISOString()}
            
            All resources associated with this staging environment have been removed.
            Logs have been archived and are available as workflow artifacts for 7 days.`;
            
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: pr_number,
              body: comment
            });

  cleanup-stale-environments:
    name: Cleanup Stale Environments
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_STAGING_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}

      - name: Find and destroy stale environments
        run: |
          # List all Cloud Run services with staging prefix
          SERVICES=$(gcloud run services list \
            --platform=managed \
            --region=${{ env.GCP_REGION }} \
            --format="value(name)" \
            --filter="name:staging-pr-*")
          
          for SERVICE in $SERVICES; do
            # Extract PR number from service name
            PR_NUMBER=$(echo $SERVICE | grep -oP 'pr-\K[0-9]+')
            
            # Check if PR is still open
            PR_STATE=$(gh pr view $PR_NUMBER --json state -q .state 2>/dev/null || echo "CLOSED")
            
            if [[ "$PR_STATE" == "CLOSED" ]] || [[ "$PR_STATE" == "MERGED" ]]; then
              echo "Cleaning up stale environment for PR #$PR_NUMBER (state: $PR_STATE)"
              
              # Trigger destroy workflow
              gh workflow run staging-environment.yml \
                -f action=destroy \
                -f pr_number=$PR_NUMBER
            fi
          done
        env:
          GH_TOKEN: ${{ github.token }}

  # NEW JOB FOR V4: Download artifacts from other workflows
  download-previous-artifacts:
    name: Download Previous Test Results
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'redeploy'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      # UPDATED FOR V4: Download artifacts from previous runs with github-token
      - name: Download previous test results
        uses: actions/download-artifact@v4
        with:
          name: test-results-pr-${{ github.event.inputs.pr_number }}-*
          path: ./previous-test-results
          github-token: ${{ secrets.GITHUB_TOKEN }}  # Required for cross-run downloads in v4
          
      - name: Process previous results
        run: |
          echo "Processing previous test results..."
          ls -la ./previous-test-results/
          # Add processing logic here