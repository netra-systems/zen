name: AI Auto-Fix

# Disabled - only smoke tests and staging workflows are active
# on:
#   workflow_run:
#     workflows: ["Smoke Tests", "Unit Tests", "Integration Tests"]
#     types: [completed]
#   issue_comment:
#     types: [created]
#   workflow_dispatch:
#     inputs:
#       test_run_id:
#         description: 'Test run ID to fix'
#         required: true
#       max_attempts:
#         description: 'Maximum fix attempts'
#         required: false
#         default: '3'
#       dry_run:
#         description: 'Run in dry-run mode (ACT compatible)'
#         required: false
#         default: false
#         type: boolean

env:
  # Workflow configuration
  DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}

permissions:
  contents: write
  pull-requests: write
  issues: write
  actions: read

jobs:
  check-trigger:
    name: Check Trigger Conditions
    runs-on: warp-custom-default
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
      pr_number: ${{ steps.check.outputs.pr_number }}
      test_failures: ${{ steps.check.outputs.test_failures }}
      is_act: ${{ steps.environment.outputs.is_act }}
      
    steps:
      - name: Detect Environment
        id: environment
        run: |
          if [ "${ACT:-false}" = "true" ] || [ "${{ env.DRY_RUN }}" = "true" ]; then
            echo "is_act=true" >> $GITHUB_OUTPUT
            echo "🧪 Running in ACT/dry-run mode - using mock responses"
          else
            echo "is_act=false" >> $GITHUB_OUTPUT
            echo "🚀 Running in GitHub Actions - using real API calls"
          fi
      - name: Check Comment Trigger
        if: github.event_name == 'issue_comment'
        id: comment-check
        run: |
          if [[ "${{ github.event.comment.body }}" == *"@autofix"* ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "pr_number=${{ github.event.issue.number }}" >> $GITHUB_OUTPUT
          fi
          
      - name: Check Workflow Run Trigger
        if: github.event_name == 'workflow_run'
        id: workflow-check
        run: |
          if [[ "${{ github.event.workflow_run.conclusion }}" == "failure" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            # Extract PR number from workflow run
            echo "pr_number=${{ github.event.workflow_run.pull_requests[0].number }}" >> $GITHUB_OUTPUT
          fi
          
      - name: Check Manual Trigger
        if: github.event_name == 'workflow_dispatch'
        id: manual-check
        run: |
          echo "should_run=true" >> $GITHUB_OUTPUT
          
      - name: Consolidate Checks
        id: check
        run: |
          if [[ "${{ steps.comment-check.outputs.should_run }}" == "true" ]] || \
             [[ "${{ steps.workflow-check.outputs.should_run }}" == "true" ]] || \
             [[ "${{ steps.manual-check.outputs.should_run }}" == "true" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
          else
            echo "should_run=false" >> $GITHUB_OUTPUT
          fi
          
  analyze-failures:
    name: Analyze Test Failures
    needs: check-trigger
    if: needs.check-trigger.outputs.should_run == 'true'
    runs-on: warp-custom-default
    outputs:
      fixable_tests: ${{ steps.analyze.outputs.fixable }}
      fix_strategy: ${{ steps.analyze.outputs.strategy }}
      
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Download Test Results
        uses: actions/download-artifact@v4
        with:
          pattern: '*-test-results-*'
          path: test-results/
          
      - name: Analyze Failures
        id: analyze
        run: |
          python scripts/ci/analyze_failures.py \
            --results-dir test-results/ \
            --output-json failure-analysis.json
            
          # Extract fixable tests
          fixable=$(jq -r '.fixable_tests | @json' failure-analysis.json)
          echo "fixable=$fixable" >> $GITHUB_OUTPUT
          
          # Determine fix strategy
          high_confidence=$(jq -r '.high_confidence_count' failure-analysis.json)
          if [ "$high_confidence" -gt 0 ]; then
            echo "strategy=targeted" >> $GITHUB_OUTPUT
          else
            echo "strategy=comprehensive" >> $GITHUB_OUTPUT
          fi
          
      - name: Upload Analysis
        uses: actions/upload-artifact@v4
        with:
          name: failure-analysis-${{ github.run_id }}
          path: failure-analysis.json
          retention-days: 7
          
  generate-fixes:
    name: Generate AI Fixes
    needs: [check-trigger, analyze-failures]
    if: needs.analyze-failures.outputs.fixable_tests != '[]'
    runs-on: warp-custom-default
    strategy:
      max-parallel: 3
      matrix:
        test: ${{ fromJSON(needs.analyze-failures.outputs.fixable_tests) }}
    
    env:
      IS_ACT_ENV: ${{ needs.check-trigger.outputs.is_act }}
        
    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          ref: ${{ github.event.pull_request.head.ref || github.ref }}
          
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Install Dependencies
        run: |
          pip install anthropic google-generativeai openai
          pip install -r requirements.txt
          
      - name: Generate Fix - Attempt 1 (Claude)
        id: fix-claude
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          if [ "${{ env.IS_ACT_ENV }}" = "true" ]; then
            echo "🧪 ACT/Dry-run mode: Generating mock Claude fix patch"
            cat > fix-claude.patch << 'EOF'
          --- a/${{ matrix.test.file }}
          +++ b/${{ matrix.test.file }}
          @@ -1,3 +1,4 @@
          +# Mock fix applied by Claude (ACT simulation)
           def test_function():
          -    assert False  # Mock failing test
          +    assert True   # Mock fix applied
               pass
          EOF
            echo "✅ Mock Claude fix generated successfully"
          else
            echo "🚀 Real mode: Calling Claude API"
            python scripts/ci/generate_fix.py \
              --test "${{ matrix.test.file }}" \
              --error "${{ matrix.test.error }}" \
              --provider claude \
              --output fix-claude.patch
          fi
        continue-on-error: true
        
      - name: Generate Fix - Attempt 2 (Gemini)
        if: steps.fix-claude.outcome == 'failure'
        id: fix-gemini
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        run: |
          if [ "${{ env.IS_ACT_ENV }}" = "true" ]; then
            echo "🧪 ACT/Dry-run mode: Generating mock Gemini fix patch"
            cat > fix-gemini.patch << 'EOF'
          --- a/${{ matrix.test.file }}
          +++ b/${{ matrix.test.file }}
          @@ -1,3 +1,4 @@
          +# Mock fix applied by Gemini (ACT simulation)
           def test_function():
          -    assert False  # Mock failing test
          +    assert True   # Mock fix applied
               pass
          EOF
            echo "✅ Mock Gemini fix generated successfully"
          else
            echo "🚀 Real mode: Calling Gemini API"
            python scripts/ci/generate_fix.py \
              --test "${{ matrix.test.file }}" \
              --error "${{ matrix.test.error }}" \
              --provider gemini \
              --output fix-gemini.patch
          fi
        continue-on-error: true
        
      - name: Generate Fix - Attempt 3 (GPT-4)
        if: steps.fix-claude.outcome == 'failure' && steps.fix-gemini.outcome == 'failure'
        id: fix-gpt
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          if [ "${{ env.IS_ACT_ENV }}" = "true" ]; then
            echo "🧪 ACT/Dry-run mode: Generating mock GPT-4 fix patch"
            cat > fix-gpt.patch << 'EOF'
          --- a/${{ matrix.test.file }}
          +++ b/${{ matrix.test.file }}
          @@ -1,3 +1,4 @@
          +# Mock fix applied by GPT-4 (ACT simulation)
           def test_function():
          -    assert False  # Mock failing test
          +    assert True   # Mock fix applied
               pass
          EOF
            echo "✅ Mock GPT-4 fix generated successfully"
          else
            echo "🚀 Real mode: Calling OpenAI API"
            python scripts/ci/generate_fix.py \
              --test "${{ matrix.test.file }}" \
              --error "${{ matrix.test.error }}" \
              --provider gpt4 \
              --output fix-gpt.patch
          fi
        continue-on-error: true
        
      - name: Select Best Fix
        id: select-fix
        run: |
          if [ -f "fix-claude.patch" ]; then
            cp fix-claude.patch selected-fix.patch
            echo "provider=claude" >> $GITHUB_OUTPUT
          elif [ -f "fix-gemini.patch" ]; then
            cp fix-gemini.patch selected-fix.patch
            echo "provider=gemini" >> $GITHUB_OUTPUT
          elif [ -f "fix-gpt.patch" ]; then
            cp fix-gpt.patch selected-fix.patch
            echo "provider=gpt4" >> $GITHUB_OUTPUT
          else
            echo "No fix generated"
            exit 1
          fi
          
      - name: Apply and Validate Fix
        id: validate
        run: |
          if [ "${{ env.IS_ACT_ENV }}" = "true" ]; then
            echo "🧪 ACT/Dry-run mode: Simulating fix validation"
            echo "Would apply patch: selected-fix.patch"
            cat selected-fix.patch
            echo "✅ Mock validation successful"
          else
            echo "🚀 Real mode: Applying and validating fix"
            # Apply the patch
            git apply selected-fix.patch
            
            # Run the specific test to validate
            python -m pytest "${{ matrix.test.file }}" -v
            
            # Run smoke tests to ensure no regression
            python test_runner.py --level smoke --json-output smoke-results.json
          fi
          
      - name: Commit Fix
        if: steps.validate.outcome == 'success'
        run: |
          if [ "${{ env.IS_ACT_ENV }}" = "true" ]; then
            echo "🧪 ACT/Dry-run mode: Would commit fix"
            echo "Provider: ${{ steps.select-fix.outputs.provider }}"
            echo "Test: ${{ matrix.test.name }}"
            echo "✅ Mock commit successful"
          else
            echo "🚀 Real mode: Committing fix"
            git config user.name "Netra AI Auto-Fix"
            git config user.email "autofix@netra.ai"
            
            git add -A
            git commit -m "fix: Auto-fix test failure in ${{ matrix.test.name }}
            
            Automated fix generated by ${{ steps.select-fix.outputs.provider }}
            Error type: ${{ matrix.test.error_type }}
            Confidence: ${{ matrix.test.confidence }}%
            
            Original error: ${{ matrix.test.error_summary }}"
            
            git push
          fi
          
      - name: Report Success
        if: steps.validate.outcome == 'success'
        run: |
          if [ "${{ env.IS_ACT_ENV }}" = "true" ]; then
            echo "🧪 ACT/Dry-run mode: Would post success comment"
            echo "### ✅ Auto-Fix Applied Successfully (ACT Simulation)"
            echo "**Test:** ${{ matrix.test.name }}"
            echo "**Provider:** ${{ steps.select-fix.outputs.provider }}"
            echo "**Confidence:** ${{ matrix.test.confidence }}%"
            echo "The fix has been validated and committed. Tests are re-running."
          else
            # Use GitHub CLI in real mode since github-script might not work well in all environments
            gh issue comment "${{ needs.check-trigger.outputs.pr_number }}" --body "### ✅ Auto-Fix Applied Successfully

            **Test:** \`${{ matrix.test.name }}\`
            **Provider:** ${{ steps.select-fix.outputs.provider }}
            **Confidence:** ${{ matrix.test.confidence }}%

            The fix has been validated and committed. Tests are re-running."
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
            
      - name: Report Failure
        if: steps.validate.outcome == 'failure'
        run: |
          if [ "${{ env.IS_ACT_ENV }}" = "true" ]; then
            echo "🧪 ACT/Dry-run mode: Would post failure comment"
            echo "### ⚠️ Auto-Fix Failed (ACT Simulation)"
            echo "**Test:** ${{ matrix.test.name }}"
            echo "**Attempted Providers:** Claude, Gemini, GPT-4"
            echo "Manual intervention required. Creating issue for tracking."
          else
            gh issue comment "${{ needs.check-trigger.outputs.pr_number }}" --body "### ⚠️ Auto-Fix Failed

            **Test:** \`${{ matrix.test.name }}\`
            **Attempted Providers:** Claude, Gemini, GPT-4

            Manual intervention required. Creating issue for tracking."
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
            
  create-tracking-issue:
    name: Create Tracking Issue
    needs: [check-trigger, generate-fixes]
    if: failure()
    runs-on: warp-custom-default
    
    env:
      IS_ACT_ENV: ${{ needs.check-trigger.outputs.is_act }}
    
    steps:
      - name: Create Issue
        run: |
          if [ "${{ env.IS_ACT_ENV }}" = "true" ]; then
            echo "🧪 ACT/Dry-run mode: Would create tracking issue"
            echo "## Auto-Fix Failure Report (ACT Simulation)"
            echo "### Context"
            echo "- **PR:** #${{ needs.check-trigger.outputs.pr_number }}"
            echo "- **Run ID:** ${{ github.run_id }}"
            echo "- **Timestamp:** $(date -u +%Y-%m-%dT%H:%M:%SZ)"
            echo "### Failed Tests"
            echo "Tests that could not be automatically fixed require manual attention."
            echo "### Action Required"
            echo "Please review the test failures and apply manual fixes."
            echo "cc @${{ github.actor }}"
          else
            gh issue create \
              --title "Auto-fix failed for tests in PR #${{ needs.check-trigger.outputs.pr_number }}" \
              --body "## Auto-Fix Failure Report

            ### Context
            - **PR:** #${{ needs.check-trigger.outputs.pr_number }}
            - **Run ID:** ${{ github.run_id }}
            - **Timestamp:** $(date -u +%Y-%m-%dT%H:%M:%SZ)

            ### Failed Tests
            Tests that could not be automatically fixed require manual attention.

            ### Action Required
            Please review the test failures and apply manual fixes.

            cc @${{ github.actor }}" \
              --label "bug,auto-fix-failed,needs-attention"
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}