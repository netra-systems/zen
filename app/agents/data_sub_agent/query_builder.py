"""Modernized Query Builder with BaseExecutionInterface support."""

import time
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List
from dataclasses import dataclass

from app.agents.base.interface import (
    BaseExecutionInterface, ExecutionContext, ExecutionResult, 
    ExecutionStatus, WebSocketManagerProtocol
)
from app.agents.state import DeepAgentState
from app.logging_config import central_logger as logger


@dataclass
class QueryExecutionRequest:
    """Request parameters for query execution."""
    query_type: str
    user_id: int
    parameters: Dict[str, Any]
    execution_timeout: float = 30.0


@dataclass 
class QueryExecutionMetrics:
    """Query execution performance metrics."""
    query_build_time_ms: float = 0.0
    query_validation_time_ms: float = 0.0
    total_execution_time_ms: float = 0.0
    query_complexity_score: int = 0


class QueryBuilder(BaseExecutionInterface):
    """Build optimized ClickHouse queries with modern execution patterns."""
    
    # Marker to identify queries from this builder
    QUERY_SOURCE_MARKER = "/* Generated by Netra QueryBuilder */"
    
    def __init__(self, websocket_manager: Optional[WebSocketManagerProtocol] = None):
        """Initialize modernized query builder."""
        super().__init__("QueryBuilder", websocket_manager)
        self._init_query_registry()
        self._init_performance_tracking()
        
    def _init_query_registry(self) -> None:
        """Initialize query type registry for validation."""
        self.supported_query_types = {
            'performance_metrics', 'anomaly_detection', 
            'correlation_analysis', 'usage_patterns'
        }
        
    def _init_performance_tracking(self) -> None:
        """Initialize performance tracking components."""
        self.query_metrics = QueryExecutionMetrics()
        self.execution_history: List[Dict[str, Any]] = []
        
    async def execute_core_logic(self, context: ExecutionContext) -> Dict[str, Any]:
        """Execute query building with performance tracking."""
        start_time = time.time()
        request = self._extract_request_from_context(context)
        await self.send_status_update(context, "building", "Building optimized query")
        query = await self._build_query_with_tracking(request, context)
        execution_time_ms = (time.time() - start_time) * 1000
        return self._create_execution_result(query, request, execution_time_ms)
        
    async def validate_preconditions(self, context: ExecutionContext) -> bool:
        """Validate query execution preconditions."""
        try:
            request = self._extract_request_from_context(context)
            return self._validate_query_request(request)
        except Exception as e:
            logger.error(f"Precondition validation failed: {e}")
            return False
            
    def _extract_request_from_context(self, context: ExecutionContext) -> QueryExecutionRequest:
        """Extract query request from execution context."""
        state_dict = context.metadata.get('request', {})
        return QueryExecutionRequest(
            query_type=state_dict.get('query_type', 'performance_metrics'),
            user_id=int(state_dict.get('user_id', context.user_id or 0)),
            parameters=state_dict.get('parameters', {}),
            execution_timeout=state_dict.get('timeout', 30.0)
        )
        
    def _validate_query_request(self, request: QueryExecutionRequest) -> bool:
        """Validate query request parameters."""
        if request.query_type not in self.supported_query_types:
            return False
        return request.user_id > 0 and request.parameters is not None
        
    async def _build_query_with_tracking(self, request: QueryExecutionRequest, 
                                       context: ExecutionContext) -> str:
        """Build query with performance tracking."""
        build_start = time.time()
        query = self._route_query_building(request)
        self.query_metrics.query_build_time_ms = (time.time() - build_start) * 1000
        await self.send_status_update(context, "built", f"Query built for {request.query_type}")
        return query
        
    def _route_query_building(self, request: QueryExecutionRequest) -> str:
        """Route to appropriate query building method."""
        params = request.parameters
        if request.query_type == 'performance_metrics':
            return self._build_performance_query(request.user_id, params)
        elif request.query_type == 'anomaly_detection':
            return self._build_anomaly_query(request.user_id, params)
        elif request.query_type == 'correlation_analysis':
            return self._build_correlation_query(request.user_id, params)
        elif request.query_type == 'usage_patterns':
            return self._build_usage_query(request.user_id, params)
        else:
            raise ValueError(f"Unsupported query type: {request.query_type}")
            
    def _create_execution_result(self, query: str, request: QueryExecutionRequest,
                               execution_time_ms: float) -> Dict[str, Any]:
        """Create standardized execution result."""
        self._update_execution_history(request, execution_time_ms)
        return {
            'query': query,
            'query_type': request.query_type,
            'user_id': request.user_id,
            'execution_time_ms': execution_time_ms,
            'complexity_score': self._calculate_complexity_score(query),
            'build_metrics': self.query_metrics.__dict__
        }
        
    def _update_execution_history(self, request: QueryExecutionRequest,
                                execution_time_ms: float) -> None:
        """Update query execution history."""
        history_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'query_type': request.query_type,
            'user_id': request.user_id,
            'execution_time_ms': execution_time_ms
        }
        self.execution_history.append(history_entry)
        self._trim_execution_history()
        
    def _trim_execution_history(self) -> None:
        """Trim execution history to last 100 entries."""
        if len(self.execution_history) > 100:
            self.execution_history = self.execution_history[-100:]
            
    def _calculate_complexity_score(self, query: str) -> int:
        """Calculate query complexity score for performance tracking."""
        score = 0
        complexity_indicators = ['WITH', 'GROUP BY', 'ORDER BY', 'LIMIT', 'JOIN']
        for indicator in complexity_indicators:
            score += query.upper().count(indicator)
        return min(score, 10)  # Cap at 10
        
    def _build_performance_query(self, user_id: int, params: Dict[str, Any]) -> str:
        """Build performance metrics query from parameters."""
        return self.build_performance_metrics_query(
            user_id=user_id,
            workload_id=params.get('workload_id'),
            start_time=params.get('start_time'),
            end_time=params.get('end_time'),
            aggregation_level=params.get('aggregation_level', 'minute')
        )
        
    def _build_anomaly_query(self, user_id: int, params: Dict[str, Any]) -> str:
        """Build anomaly detection query from parameters."""
        return self.build_anomaly_detection_query(
            user_id=user_id,
            metric_name=params.get('metric_name', 'latency_ms'),
            start_time=params.get('start_time'),
            end_time=params.get('end_time'),
            z_score_threshold=params.get('z_score_threshold', 2.0)
        )
        
    def _build_correlation_query(self, user_id: int, params: Dict[str, Any]) -> str:
        """Build correlation analysis query from parameters."""
        return self.build_correlation_analysis_query(
            user_id=user_id,
            metric1=params.get('metric1', 'latency_ms'),
            metric2=params.get('metric2', 'throughput'),
            start_time=params.get('start_time'),
            end_time=params.get('end_time')
        )
        
    def _build_usage_query(self, user_id: int, params: Dict[str, Any]) -> str:
        """Build usage patterns query from parameters."""
        return self.build_usage_patterns_query(
            user_id=user_id,
            days_back=params.get('days_back', 30)
        )
        
    async def execute_with_reliability(self, request: QueryExecutionRequest) -> ExecutionResult:
        """Execute query building with reliability patterns."""
        context = ExecutionContext(
            run_id=f"query_{int(time.time() * 1000)}",
            agent_name=self.agent_name,
            state=DeepAgentState(),
            user_id=str(request.user_id),
            metadata={'request': request.__dict__}
        )
        
        try:
            if not await self.validate_preconditions(context):
                return ExecutionResult(
                    success=False,
                    status=ExecutionStatus.FAILED,
                    error="Precondition validation failed"
                )
                
            result_data = await self.execute_core_logic(context)
            return ExecutionResult(
                success=True,
                status=ExecutionStatus.COMPLETED,
                result=result_data,
                execution_time_ms=result_data.get('execution_time_ms', 0)
            )
            
        except Exception as e:
            logger.error(f"Query execution failed: {e}")
            return ExecutionResult(
                success=False,
                status=ExecutionStatus.FAILED,
                error=str(e)
            )
    
    @staticmethod
    def build_performance_metrics_query(
        user_id: int,
        workload_id: Optional[str],
        start_time: datetime,
        end_time: datetime,
        aggregation_level: str = "minute"
    ) -> str:
        """Build query for performance metrics"""
        time_function = QueryBuilder._get_time_function(aggregation_level)
        workload_filter = QueryBuilder._build_workload_filter(workload_id)
        select_clause = QueryBuilder._build_performance_select_clause(time_function)
        subquery = QueryBuilder._build_performance_subquery(user_id, start_time, end_time, workload_filter)
        return QueryBuilder._assemble_performance_query(select_clause, subquery)
    
    @staticmethod
    def _get_time_function(aggregation_level: str) -> str:
        """Get ClickHouse time function for aggregation level."""
        time_functions = {
            "second": "toStartOfSecond", "minute": "toStartOfMinute",
            "hour": "toStartOfHour", "day": "toStartOfDay"
        }
        return time_functions.get(aggregation_level, "toStartOfMinute")
    
    @staticmethod
    def _build_workload_filter(workload_id: Optional[str]) -> str:
        """Build workload filter clause."""
        return f"AND workload_id = '{workload_id}'" if workload_id else ""
    
    @staticmethod
    def _build_performance_select_clause(time_function: str) -> str:
        """Build SELECT clause for performance metrics."""
        basic_fields = f"{time_function}(timestamp) as time_bucket, count() as event_count"
        latency_fields = QueryBuilder._build_latency_fields()
        throughput_fields = QueryBuilder._build_throughput_fields()
        cost_fields = QueryBuilder._build_cost_fields()
        return f"SELECT {basic_fields}, {latency_fields}, {throughput_fields}, {cost_fields}, uniqExact(workload_id) as unique_workloads"
    
    @staticmethod
    def _build_latency_fields() -> str:
        """Build latency-related fields for SELECT clause."""
        return ("quantileIf(0.5, toFloat64(metric_value), has_latency) as latency_p50, "
                "quantileIf(0.95, toFloat64(metric_value), has_latency) as latency_p95, "
                "quantileIf(0.99, toFloat64(metric_value), has_latency) as latency_p99")
    
    @staticmethod
    def _build_throughput_fields() -> str:
        """Build throughput-related fields for SELECT clause."""
        return ("avgIf(toFloat64(throughput_value), has_throughput) as avg_throughput, "
                "maxIf(toFloat64(throughput_value), has_throughput) as peak_throughput")
    
    @staticmethod
    def _build_cost_fields() -> str:
        """Build cost and error fields for SELECT clause."""
        return ("countIf(event_type = 'error') / count() * 100 as error_rate, "
                "sumIf(toFloat64(cost_value), has_cost) / 100.0 as total_cost")
    
    @staticmethod
    def _build_performance_subquery(user_id: int, start_time: datetime, end_time: datetime, workload_filter: str) -> str:
        """Build subquery for performance metrics."""
        select_part = QueryBuilder._build_subquery_select_part()
        from_part = QueryBuilder._build_subquery_from_part(user_id, start_time, end_time, workload_filter)
        return f"FROM ({select_part} {from_part})"
    
    @staticmethod
    def _build_subquery_select_part() -> str:
        """Build SELECT part of subquery."""
        index_fields = QueryBuilder._build_index_fields()
        value_fields = QueryBuilder._build_value_fields()
        flag_fields = QueryBuilder._build_flag_fields()
        return f"SELECT *, {index_fields}, {value_fields}, {flag_fields}"
    
    @staticmethod
    def _build_index_fields() -> str:
        """Build index fields for subquery."""
        return ("arrayFirstIndex(x -> x = 'latency_ms', metrics.name) as idx, "
                "arrayFirstIndex(x -> x = 'throughput', metrics.name) as idx2, "
                "arrayFirstIndex(x -> x = 'cost_cents', metrics.name) as idx3")
    
    @staticmethod
    def _build_value_fields() -> str:
        """Build value fields for subquery."""
        return ("if(idx > 0, arrayElement(metrics.value, idx), 0.0) as metric_value, "
                "if(idx2 > 0, arrayElement(metrics.value, idx2), 0.0) as throughput_value, "
                "if(idx3 > 0, arrayElement(metrics.value, idx3), 0.0) as cost_value")
    
    @staticmethod
    def _build_flag_fields() -> str:
        """Build flag fields for subquery."""
        return "idx > 0 as has_latency, idx2 > 0 as has_throughput, idx3 > 0 as has_cost"
    
    @staticmethod
    def _build_subquery_from_part(user_id: int, start_time: datetime, end_time: datetime, workload_filter: str) -> str:
        """Build FROM part of subquery."""
        return (f"FROM workload_events WHERE user_id = {user_id} "
                f"AND timestamp >= '{start_time.isoformat()}' "
                f"AND timestamp <= '{end_time.isoformat()}' {workload_filter}")
    
    @staticmethod
    def _assemble_performance_query(select_clause: str, subquery: str) -> str:
        """Assemble complete performance query."""
        return f"""{QueryBuilder.QUERY_SOURCE_MARKER}
        {select_clause} {subquery}
        GROUP BY time_bucket ORDER BY time_bucket DESC LIMIT 10000"""
    
    @staticmethod
    def build_anomaly_detection_query(
        user_id: int,
        metric_name: str,
        start_time: datetime,
        end_time: datetime,
        z_score_threshold: float = 2.0
    ) -> str:
        """Build query for anomaly detection"""
        baseline_clause = QueryBuilder._build_anomaly_baseline_clause(user_id, metric_name, start_time)
        main_query = QueryBuilder._build_anomaly_main_query(user_id, metric_name, start_time, end_time, z_score_threshold)
        return QueryBuilder._assemble_anomaly_query(baseline_clause, main_query)
    
    @staticmethod
    def _build_anomaly_baseline_clause(user_id: int, metric_name: str, start_time: datetime) -> str:
        """Build baseline CTE for anomaly detection."""
        baseline_start = (start_time - timedelta(days=7)).isoformat()
        select_part = QueryBuilder._build_baseline_select_part(metric_name)
        where_part = QueryBuilder._build_baseline_where_part(user_id, baseline_start, start_time)
        return f"WITH baseline AS ({select_part} {where_part})"
    
    @staticmethod
    def _build_baseline_select_part(metric_name: str) -> str:
        """Build SELECT part of baseline CTE."""
        return (f"SELECT arrayFirstIndex(x -> x = '{metric_name}', metrics.name) as idx, "
                "avg(if(idx > 0, arrayElement(metrics.value, idx), 0.0)) as mean_val, "
                "stddevPop(if(idx > 0, arrayElement(metrics.value, idx), 0.0)) as std_val FROM workload_events")
    
    @staticmethod
    def _build_baseline_where_part(user_id: int, baseline_start: str, start_time: datetime) -> str:
        """Build WHERE part of baseline CTE."""
        return (f"WHERE user_id = {user_id} AND timestamp >= '{baseline_start}' "
                f"AND timestamp < '{start_time.isoformat()}'")
    
    @staticmethod
    def _build_anomaly_main_query(user_id: int, metric_name: str, start_time: datetime, end_time: datetime, z_score_threshold: float) -> str:
        """Build main SELECT for anomaly detection."""
        select_part = QueryBuilder._build_anomaly_select_part(metric_name, z_score_threshold)
        where_part = QueryBuilder._build_anomaly_where_part(user_id, start_time, end_time)
        return f"{select_part} FROM workload_events, baseline {where_part} ORDER BY abs(z_score) DESC LIMIT 100"
    
    @staticmethod
    def _build_anomaly_select_part(metric_name: str, z_score_threshold: float) -> str:
        """Build SELECT part of anomaly detection."""
        return (f"SELECT timestamp, arrayFirstIndex(x -> x = '{metric_name}', metrics.name) as idx, "
                "if(idx > 0, arrayElement(metrics.value, idx), 0.0) as metric_value, "
                "if(baseline.std_val > 0, (toFloat64(metric_value) - baseline.mean_val) / baseline.std_val, 0.0) as z_score, "
                f"abs(z_score) > {z_score_threshold} as is_anomaly")
    
    @staticmethod
    def _build_anomaly_where_part(user_id: int, start_time: datetime, end_time: datetime) -> str:
        """Build WHERE part of anomaly detection."""
        return (f"WHERE user_id = {user_id} AND timestamp >= '{start_time.isoformat()}' "
                f"AND timestamp <= '{end_time.isoformat()}' AND is_anomaly = 1")
    
    @staticmethod
    def _assemble_anomaly_query(baseline_clause: str, main_query: str) -> str:
        """Assemble complete anomaly detection query."""
        return f"{QueryBuilder.QUERY_SOURCE_MARKER} {baseline_clause} {main_query}"
    
    @staticmethod
    def build_correlation_analysis_query(
        user_id: int,
        metric1: str,
        metric2: str,
        start_time: datetime,
        end_time: datetime
    ) -> str:
        """Build query for correlation analysis between two metrics"""
        select_part = QueryBuilder._build_correlation_select_part()
        subquery_part = QueryBuilder._build_correlation_subquery(user_id, metric1, metric2, start_time, end_time)
        return f"{QueryBuilder.QUERY_SOURCE_MARKER} {select_part} FROM ({subquery_part}) WHERE idx1 > 0 AND idx2 > 0"
    
    @staticmethod
    def _build_correlation_select_part() -> str:
        """Build SELECT part of correlation query."""
        return ("SELECT corr(toFloat64(m1_value), toFloat64(m2_value)) as correlation_coefficient, "
                "count() as sample_size, avg(toFloat64(m1_value)) as metric1_avg, "
                "avg(toFloat64(m2_value)) as metric2_avg, stddevPop(toFloat64(m1_value)) as metric1_std, "
                "stddevPop(toFloat64(m2_value)) as metric2_std")
    
    @staticmethod
    def _build_correlation_subquery(user_id: int, metric1: str, metric2: str, start_time: datetime, end_time: datetime) -> str:
        """Build subquery for correlation analysis."""
        select_indices = f"arrayFirstIndex(x -> x = '{metric1}', metrics.name) as idx1, arrayFirstIndex(x -> x = '{metric2}', metrics.name) as idx2"
        select_values = "if(idx1 > 0, arrayElement(metrics.value, idx1), 0.0) as m1_value, if(idx2 > 0, arrayElement(metrics.value, idx2), 0.0) as m2_value"
        where_clause = f"WHERE user_id = {user_id} AND timestamp >= '{start_time.isoformat()}' AND timestamp <= '{end_time.isoformat()}'"
        return f"SELECT {select_indices}, {select_values} FROM workload_events {where_clause}"
    
    @staticmethod
    def build_usage_patterns_query(
        user_id: int,
        days_back: int = 30
    ) -> str:
        """Build query for usage pattern analysis"""
        select_part = QueryBuilder._build_usage_select_part()
        subquery_part = QueryBuilder._build_usage_subquery(user_id, days_back)
        return f"{QueryBuilder.QUERY_SOURCE_MARKER} {select_part} FROM ({subquery_part}) GROUP BY day_of_week, hour_of_day ORDER BY day_of_week, hour_of_day"
    
    @staticmethod
    def _build_usage_select_part() -> str:
        """Build SELECT part of usage patterns query."""
        return ("SELECT toDayOfWeek(timestamp) as day_of_week, toHour(timestamp) as hour_of_day, "
                "count() as event_count, uniqExact(workload_id) as unique_workloads, "
                "uniqExact(event_category) as unique_categories, sumIf(toFloat64(cost_value), has_cost) / 100.0 as total_cost")
    
    @staticmethod
    def _build_usage_subquery(user_id: int, days_back: int) -> str:
        """Build subquery for usage patterns analysis."""
        select_fields = ("SELECT timestamp, workload_id, event_category, "
                        "arrayFirstIndex(x -> x = 'cost_cents', metrics.name) as idx, "
                        "if(idx > 0, arrayElement(metrics.value, idx), 0.0) as cost_value, idx > 0 as has_cost")
        where_clause = f"FROM workload_events WHERE user_id = {user_id} AND timestamp >= now() - INTERVAL {days_back} DAY"
        return f"{select_fields} {where_clause}"