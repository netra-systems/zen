"""Query builder for ClickHouse queries."""

from datetime import datetime, timedelta
from typing import Optional, Dict, Any


class QueryBuilder:
    """Build optimized ClickHouse queries"""
    
    # Marker to identify queries from this builder
    QUERY_SOURCE_MARKER = "/* Generated by Netra QueryBuilder */"
    
    @staticmethod
    def build_performance_metrics_query(
        user_id: int,
        workload_id: Optional[str],
        start_time: datetime,
        end_time: datetime,
        aggregation_level: str = "minute"
    ) -> str:
        """Build query for performance metrics"""
        time_function = QueryBuilder._get_time_function(aggregation_level)
        workload_filter = QueryBuilder._build_workload_filter(workload_id)
        select_clause = QueryBuilder._build_performance_select_clause(time_function)
        subquery = QueryBuilder._build_performance_subquery(user_id, start_time, end_time, workload_filter)
        return QueryBuilder._assemble_performance_query(select_clause, subquery)
    
    @staticmethod
    def _get_time_function(aggregation_level: str) -> str:
        """Get ClickHouse time function for aggregation level."""
        time_functions = {
            "second": "toStartOfSecond",
            "minute": "toStartOfMinute",
            "hour": "toStartOfHour",
            "day": "toStartOfDay"
        }
        return time_functions.get(aggregation_level, "toStartOfMinute")
    
    @staticmethod
    def _build_workload_filter(workload_id: Optional[str]) -> str:
        """Build workload filter clause."""
        return f"AND workload_id = '{workload_id}'" if workload_id else ""
    
    @staticmethod
    def _build_performance_select_clause(time_function: str) -> str:
        """Build SELECT clause for performance metrics."""
        basic_fields = f"{time_function}(timestamp) as time_bucket, count() as event_count"
        latency_fields = QueryBuilder._build_latency_fields()
        throughput_fields = QueryBuilder._build_throughput_fields()
        cost_fields = QueryBuilder._build_cost_fields()
        return f"SELECT {basic_fields}, {latency_fields}, {throughput_fields}, {cost_fields}, uniqExact(workload_id) as unique_workloads"
    
    @staticmethod
    def _build_performance_subquery(user_id: int, start_time: datetime, end_time: datetime, workload_filter: str) -> str:
        """Build subquery for performance metrics."""
        return f"""
        FROM (
            SELECT
                *,
                arrayFirstIndex(x -> x = 'latency_ms', metrics.name) as idx,
                arrayFirstIndex(x -> x = 'throughput', metrics.name) as idx2,
                arrayFirstIndex(x -> x = 'cost_cents', metrics.name) as idx3,
                if(idx > 0, arrayElement(metrics.value, idx), 0.0) as metric_value,
                if(idx2 > 0, arrayElement(metrics.value, idx2), 0.0) as throughput_value,
                if(idx3 > 0, arrayElement(metrics.value, idx3), 0.0) as cost_value,
                idx > 0 as has_latency,
                idx2 > 0 as has_throughput,
                idx3 > 0 as has_cost
            FROM workload_events
            WHERE user_id = {user_id}
                AND timestamp >= '{start_time.isoformat()}'
                AND timestamp <= '{end_time.isoformat()}'
                {workload_filter}
        )"""
    
    @staticmethod
    def _assemble_performance_query(select_clause: str, subquery: str) -> str:
        """Assemble complete performance query."""
        return f"""
        {QueryBuilder.QUERY_SOURCE_MARKER}
        {select_clause}
        {subquery}
        GROUP BY time_bucket
        ORDER BY time_bucket DESC
        LIMIT 10000
        """
    
    @staticmethod
    def build_anomaly_detection_query(
        user_id: int,
        metric_name: str,
        start_time: datetime,
        end_time: datetime,
        z_score_threshold: float = 2.0
    ) -> str:
        """Build query for anomaly detection"""
        baseline_clause = QueryBuilder._build_anomaly_baseline_clause(user_id, metric_name, start_time)
        main_query = QueryBuilder._build_anomaly_main_query(user_id, metric_name, start_time, end_time, z_score_threshold)
        return QueryBuilder._assemble_anomaly_query(baseline_clause, main_query)
    
    @staticmethod
    def _build_anomaly_baseline_clause(user_id: int, metric_name: str, start_time: datetime) -> str:
        """Build baseline CTE for anomaly detection."""
        baseline_start = (start_time - timedelta(days=7)).isoformat()
        return f"""WITH baseline AS (
            SELECT
                arrayFirstIndex(x -> x = '{metric_name}', metrics.name) as idx,
                avg(if(idx > 0, arrayElement(metrics.value, idx), 0.0)) as mean_val,
                stddevPop(if(idx > 0, arrayElement(metrics.value, idx), 0.0)) as std_val
            FROM workload_events
            WHERE user_id = {user_id}
                AND timestamp >= '{baseline_start}'
                AND timestamp < '{start_time.isoformat()}'
        )"""
    
    @staticmethod
    def _build_anomaly_main_query(user_id: int, metric_name: str, start_time: datetime, end_time: datetime, z_score_threshold: float) -> str:
        """Build main SELECT for anomaly detection."""
        return f"""SELECT
            timestamp,
            arrayFirstIndex(x -> x = '{metric_name}', metrics.name) as idx,
            if(idx > 0, arrayElement(metrics.value, idx), 0.0) as metric_value,
            if(baseline.std_val > 0, (toFloat64(metric_value) - baseline.mean_val) / baseline.std_val, 0.0) as z_score,
            abs(z_score) > {z_score_threshold} as is_anomaly
        FROM workload_events, baseline
        WHERE user_id = {user_id}
            AND timestamp >= '{start_time.isoformat()}'
            AND timestamp <= '{end_time.isoformat()}'
            AND is_anomaly = 1
        ORDER BY abs(z_score) DESC
        LIMIT 100"""
    
    @staticmethod
    def _assemble_anomaly_query(baseline_clause: str, main_query: str) -> str:
        """Assemble complete anomaly detection query."""
        return f"""
        {QueryBuilder.QUERY_SOURCE_MARKER}
        {baseline_clause}
        {main_query}
        """
    
    @staticmethod
    def build_correlation_analysis_query(
        user_id: int,
        metric1: str,
        metric2: str,
        start_time: datetime,
        end_time: datetime
    ) -> str:
        """Build query for correlation analysis between two metrics"""
        
        return f"""
        {QueryBuilder.QUERY_SOURCE_MARKER}
        SELECT
            corr(toFloat64(m1_value), toFloat64(m2_value)) as correlation_coefficient,
            count() as sample_size,
            avg(toFloat64(m1_value)) as metric1_avg,
            avg(toFloat64(m2_value)) as metric2_avg,
            stddevPop(toFloat64(m1_value)) as metric1_std,
            stddevPop(toFloat64(m2_value)) as metric2_std
        FROM (
            SELECT
                arrayFirstIndex(x -> x = '{metric1}', metrics.name) as idx1,
                arrayFirstIndex(x -> x = '{metric2}', metrics.name) as idx2,
                if(idx1 > 0, arrayElement(metrics.value, idx1), 0.0) as m1_value,
                if(idx2 > 0, arrayElement(metrics.value, idx2), 0.0) as m2_value
            FROM workload_events
            WHERE user_id = {user_id}
                AND timestamp >= '{start_time.isoformat()}'
                AND timestamp <= '{end_time.isoformat()}'
        )
        WHERE idx1 > 0 AND idx2 > 0
        """
    
    @staticmethod
    def build_usage_patterns_query(
        user_id: int,
        days_back: int = 30
    ) -> str:
        """Build query for usage pattern analysis"""
        
        return f"""
        {QueryBuilder.QUERY_SOURCE_MARKER}
        SELECT
            toDayOfWeek(timestamp) as day_of_week,
            toHour(timestamp) as hour_of_day,
            count() as event_count,
            uniqExact(workload_id) as unique_workloads,
            uniqExact(event_category) as unique_categories,
            sumIf(toFloat64(cost_value), has_cost) / 100.0 as total_cost
        FROM (
            SELECT
                timestamp,
                workload_id,
                event_category,
                arrayFirstIndex(x -> x = 'cost_cents', metrics.name) as idx,
                if(idx > 0, arrayElement(metrics.value, idx), 0.0) as cost_value,
                idx > 0 as has_cost
            FROM workload_events
            WHERE user_id = {user_id}
                AND timestamp >= now() - INTERVAL {days_back} DAY
        )
        GROUP BY day_of_week, hour_of_day
        ORDER BY day_of_week, hour_of_day
        """