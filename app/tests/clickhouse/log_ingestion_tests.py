"""
ClickHouse Realistic Log Ingestion Tests
Tests realistic log ingestion patterns and operations
"""

import pytest
import asyncio
import uuid
import random
from datetime import datetime, timedelta
from typing import List, Dict
from unittest.mock import patch, AsyncMock

from app.db.clickhouse_query_fixer import validate_clickhouse_query


class TestRealisticLogIngestion:
    """Test realistic log ingestion patterns"""
    
    @pytest.fixture
    def generate_realistic_logs(self):
        """Generate realistic log entries"""
        log_types = ["INFO", "WARNING", "ERROR", "DEBUG"]
        components = ["api", "worker", "scheduler", "llm_manager", "agent"]
        
        def _generate(count: int) -> List[Dict]:
            logs = []
            base_time = datetime.now() - timedelta(hours=1)
            
            for i in range(count):
                timestamp = base_time + timedelta(seconds=i * 0.1)
                logs.append({
                    "timestamp": timestamp.isoformat(),
                    "level": random.choice(log_types),
                    "component": random.choice(components),
                    "message": f"Log message {i}",
                    "metadata": {
                        "request_id": str(uuid.uuid4()),
                        "user_id": random.randint(1, 100),
                        "latency_ms": random.uniform(10, 500) if random.random() > 0.5 else None
                    }
                })
            return logs
        return _generate

    async def test_streaming_log_ingestion(self, generate_realistic_logs):
        """Test streaming ingestion of logs"""
        logs = generate_realistic_logs(1000)
        
        with patch('app.db.clickhouse.get_clickhouse_client') as mock_client:
            mock_instance = AsyncMock()
            mock_client.return_value.__aenter__.return_value = mock_instance
            
            # Simulate batch inserts
            batch_size = 100
            for i in range(0, len(logs), batch_size):
                batch = logs[i:i+batch_size]
                
                # Simulate the insert
                query = f"""
                INSERT INTO netra_app_internal_logs 
                (timestamp, level, component, message, metadata)
                VALUES
                """
                
                await mock_instance.execute(query, batch)
            
            # Verify batches were inserted
            assert mock_instance.execute.call_count == 10  # 1000 logs / 100 batch size

    async def test_log_pattern_recognition(self):
        """Test pattern recognition across large log volumes"""
        # Simulate pattern detection query
        pattern_query = """
        WITH log_patterns AS (
            SELECT 
                component,
                level,
                extractAllGroups(message, '(\\w+Exception|Error: \\w+|Failed to \\w+)')[1] as error_pattern,
                count() as occurrence_count,
                avg(JSONExtractFloat(metadata, 'latency_ms')) as avg_latency
            FROM netra_app_internal_logs
            WHERE timestamp >= now() - INTERVAL 1 HOUR
                AND level IN ('ERROR', 'WARNING')
            GROUP BY component, level, error_pattern
            HAVING occurrence_count > 5
        )
        SELECT * FROM log_patterns
        ORDER BY occurrence_count DESC
        LIMIT 100
        """
        
        # Verify query is valid
        is_valid, error = validate_clickhouse_query(pattern_query)
        assert is_valid, f"Pattern query validation failed: {error}"