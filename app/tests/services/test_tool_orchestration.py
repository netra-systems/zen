"""
Focused tests for Tool Orchestration - chain execution and coordination
Tests simple chains, complex chains, and parallel tool orchestration
MODULAR VERSION: <300 lines, all functions â‰¤8 lines
"""

import pytest
import asyncio
from datetime import datetime, UTC
from typing import Dict, List, Any, Optional
from unittest.mock import AsyncMock, MagicMock, patch
from enum import Enum

from langchain_core.tools import BaseTool
from app.services.tool_registry import ToolRegistry
from app.core.exceptions_base import NetraException


class MockAdvancedTool(BaseTool):
    """Advanced mock tool with lifecycle management"""
    
    def __init__(self, name: str, description: str = "", **kwargs):
        super().__init__(name=name, description=description, **kwargs)
        self.call_count = 0
        self.last_called = None
        self.dependencies = kwargs.get('dependencies', [])
        
    def _run(self, query: str) -> str:
        self.call_count += 1
        self.last_called = datetime.now(UTC)
        return f"Result from {self.name}: {query}"
    
    async def _arun(self, query: str) -> str:
        return self._run(query)


class ToolOrchestrator:
    """Orchestrates tool execution and coordination"""
    
    def __init__(self):
        self.execution_history = []
        self.active_chains = {}
        
    async def execute_chain(self, chain_config: Dict[str, Any]) -> Any:
        """Execute tool chain based on configuration"""
        chain_id = chain_config.get('id', 'default')
        steps = chain_config.get('steps', [])
        
        results = []
        for step in steps:
            result = await self._execute_step(step)
            results.append(result)
            
        execution_record = {
            'chain_id': chain_id,
            'results': results,
            'timestamp': datetime.now(UTC),
            'step_count': len(steps)
        }
        self.execution_history.append(execution_record)
        
        return execution_record
    
    async def _execute_step(self, step: Dict[str, Any]) -> Any:
        """Execute single step in tool chain"""
        tool_name = step.get('tool')
        input_data = step.get('input', '')
        
        # Mock tool execution
        await asyncio.sleep(0.01)  # Simulate processing time
        return f"Result from {tool_name}: {input_data}"
    
    async def execute_parallel_chain(self, parallel_config: Dict[str, Any]) -> Any:
        """Execute tools in parallel"""
        parallel_steps = parallel_config.get('parallel_steps', [])
        
        tasks = [self._execute_step(step) for step in parallel_steps]
        results = await asyncio.gather(*tasks)
        
        return {
            'parallel_id': parallel_config.get('id', 'parallel_default'),
            'results': results,
            'execution_time': datetime.now(UTC)
        }


class UnifiedToolRegistry:
    """Unified registry with orchestration support"""
    
    def __init__(self):
        self.registries = {}
        self.tool_orchestrator = ToolOrchestrator()
        
    def add_registry(self, name: str, registry: ToolRegistry):
        """Add a tool registry to unified management"""
        self.registries[name] = registry
    
    async def orchestrate_tool_chain(self, chain_config: Dict[str, Any]) -> Any:
        """Orchestrate execution of tool chain"""
        return await self.tool_orchestrator.execute_chain(chain_config)
    
    async def orchestrate_parallel_tools(self, parallel_config: Dict[str, Any]) -> Any:
        """Orchestrate parallel tool execution"""
        return await self.tool_orchestrator.execute_parallel_chain(parallel_config)


class TestToolOrchestration:
    """Test tool orchestration functionality"""
    
    @pytest.fixture
    def unified_registry(self):
        """Create unified tool registry for testing"""
        return UnifiedToolRegistry()
    
    @pytest.fixture
    def sample_tools(self):
        """Create sample tools for testing"""
        return {
            'data_analysis': MockAdvancedTool("data_analyzer", "Analyzes data sets"),
            'supply_chain': MockAdvancedTool("supply_optimizer", "Optimizes supply chains"),
            'risk_assessment': MockAdvancedTool("risk_evaluator", "Evaluates risks"),
            'cost_calculator': MockAdvancedTool("cost_calc", "Calculates costs")
        }
    
    @pytest.fixture
    def orchestrator(self):
        """Create tool orchestrator for testing"""
        return ToolOrchestrator()

    def _create_simple_chain_config(self):
        """Create simple tool chain configuration"""
        return {
            'id': 'simple_chain',
            'steps': [
                {'tool': 'data_analyzer', 'input': 'test_data'},
                {'tool': 'risk_evaluator', 'input': 'analysis_result'}
            ]
        }

    def _validate_simple_chain_result(self, result, expected_steps):
        """Validate simple tool chain execution result"""
        assert result is not None
        assert 'chain_id' in result
        assert 'results' in result
        assert 'timestamp' in result
        assert len(result['results']) == expected_steps
        
        for step_result in result['results']:
            assert isinstance(step_result, str)
            assert 'Result from' in step_result

    async def test_tool_orchestration_simple_chain(self, unified_registry, sample_tools):
        """Test simple tool chain orchestration"""
        chain_config = self._create_simple_chain_config()
        result = await unified_registry.orchestrate_tool_chain(chain_config)
        self._validate_simple_chain_result(result, len(chain_config['steps']))

    def _create_complex_chain_config(self):
        """Create complex tool chain configuration"""
        return {
            'id': 'complex_chain',
            'steps': [
                {'tool': 'data_analyzer', 'input': 'raw_data'},
                {'tool': 'supply_optimizer', 'input': 'analyzed_data'},
                {'tool': 'risk_evaluator', 'input': 'optimized_plan'},
                {'tool': 'cost_calc', 'input': 'risk_assessment'}
            ]
        }

    def _validate_complex_chain_result(self, result, config):
        """Validate complex tool chain execution result"""
        assert result is not None
        assert result['chain_id'] == config['id']
        assert len(result['results']) == len(config['steps'])
        assert 'step_count' in result
        assert result['step_count'] == len(config['steps'])

    async def test_tool_orchestration_complex_chain(self, unified_registry, sample_tools):
        """Test complex tool chain orchestration"""
        chain_config = self._create_complex_chain_config()
        result = await unified_registry.orchestrate_tool_chain(chain_config)
        self._validate_complex_chain_result(result, chain_config)

    def _create_parallel_chain_config(self):
        """Create parallel tool execution configuration"""
        return {
            'id': 'parallel_execution',
            'parallel_steps': [
                {'tool': 'data_analyzer', 'input': 'dataset_1'},
                {'tool': 'risk_evaluator', 'input': 'dataset_2'},
                {'tool': 'cost_calc', 'input': 'dataset_3'}
            ]
        }

    def _validate_parallel_execution_result(self, result, config):
        """Validate parallel tool execution result"""
        assert result is not None
        assert 'parallel_id' in result
        assert 'results' in result
        assert 'execution_time' in result
        assert len(result['results']) == len(config['parallel_steps'])
        
        # All parallel steps should have completed
        for step_result in result['results']:
            assert isinstance(step_result, str)
            assert 'Result from' in step_result

    async def test_parallel_tool_orchestration(self, unified_registry, sample_tools):
        """Test parallel tool orchestration"""
        parallel_config = self._create_parallel_chain_config()
        result = await unified_registry.orchestrate_parallel_tools(parallel_config)
        self._validate_parallel_execution_result(result, parallel_config)

    def _create_sequential_vs_parallel_configs(self):
        """Create configurations for sequential vs parallel comparison"""
        steps = [
            {'tool': 'tool_1', 'input': 'input_1'},
            {'tool': 'tool_2', 'input': 'input_2'},
            {'tool': 'tool_3', 'input': 'input_3'}
        ]
        
        sequential_config = {'id': 'sequential', 'steps': steps}
        parallel_config = {'id': 'parallel', 'parallel_steps': steps}
        
        return sequential_config, parallel_config

    def _validate_execution_time_comparison(self, sequential_time, parallel_time):
        """Validate execution time comparison between sequential and parallel"""
        # Parallel should generally be faster or similar
        # Allow some tolerance for test execution variance
        tolerance_factor = 1.5
        assert parallel_time <= sequential_time * tolerance_factor

    async def test_sequential_vs_parallel_performance(self, orchestrator):
        """Test performance comparison between sequential and parallel execution"""
        sequential_config, parallel_config = self._create_sequential_vs_parallel_configs()
        
        # Measure sequential execution time
        start_time = datetime.now(UTC)
        await orchestrator.execute_chain(sequential_config)
        sequential_time = (datetime.now(UTC) - start_time).total_seconds()
        
        # Measure parallel execution time  
        start_time = datetime.now(UTC)
        await orchestrator.execute_parallel_chain(parallel_config)
        parallel_time = (datetime.now(UTC) - start_time).total_seconds()
        
        self._validate_execution_time_comparison(sequential_time, parallel_time)

    def _create_mixed_orchestration_config(self):
        """Create mixed orchestration configuration"""
        return {
            'id': 'mixed_orchestration',
            'phases': [
                {
                    'type': 'sequential',
                    'steps': [
                        {'tool': 'data_preparation', 'input': 'raw_data'}
                    ]
                },
                {
                    'type': 'parallel',
                    'parallel_steps': [
                        {'tool': 'analyzer_1', 'input': 'prepared_data'},
                        {'tool': 'analyzer_2', 'input': 'prepared_data'}
                    ]
                },
                {
                    'type': 'sequential', 
                    'steps': [
                        {'tool': 'aggregator', 'input': 'analysis_results'}
                    ]
                }
            ]
        }

    async def _execute_mixed_orchestration(self, orchestrator, config):
        """Execute mixed orchestration configuration"""
        results = []
        
        for phase in config['phases']:
            if phase['type'] == 'sequential':
                phase_result = await orchestrator.execute_chain(phase)
            else:  # parallel
                phase_result = await orchestrator.execute_parallel_chain(phase)
            results.append(phase_result)
        
        return {
            'orchestration_id': config['id'],
            'phase_results': results,
            'total_phases': len(config['phases'])
        }

    def _validate_mixed_orchestration_result(self, result, config):
        """Validate mixed orchestration execution result"""
        assert result is not None
        assert result['orchestration_id'] == config['id']
        assert len(result['phase_results']) == len(config['phases'])
        assert result['total_phases'] == len(config['phases'])

    async def test_mixed_sequential_parallel_orchestration(self, orchestrator):
        """Test mixed sequential and parallel orchestration"""
        config = self._create_mixed_orchestration_config()
        result = await self._execute_mixed_orchestration(orchestrator, config)
        self._validate_mixed_orchestration_result(result, config)

    def _create_dependency_chain_config(self):
        """Create tool chain with dependencies"""
        return {
            'id': 'dependency_chain',
            'steps': [
                {'tool': 'data_loader', 'input': 'source', 'dependencies': []},
                {'tool': 'data_validator', 'input': 'loaded_data', 'dependencies': ['data_loader']},
                {'tool': 'data_processor', 'input': 'valid_data', 'dependencies': ['data_validator']},
                {'tool': 'report_generator', 'input': 'processed_data', 'dependencies': ['data_processor']}
            ]
        }

    def _validate_dependency_chain_execution(self, result, config):
        """Validate dependency chain execution order"""
        assert result is not None
        assert len(result['results']) == len(config['steps'])
        
        # Each step should have executed in order
        for i, step_result in enumerate(result['results']):
            expected_tool = config['steps'][i]['tool']
            assert expected_tool in step_result

    async def test_dependency_aware_orchestration(self, orchestrator):
        """Test orchestration with tool dependencies"""
        config = self._create_dependency_chain_config()
        result = await orchestrator.execute_chain(config)
        self._validate_dependency_chain_execution(result, config)

    def _validate_orchestration_history_tracking(self, orchestrator, expected_executions):
        """Validate orchestration history tracking"""
        assert len(orchestrator.execution_history) == expected_executions
        
        for execution in orchestrator.execution_history:
            assert 'chain_id' in execution
            assert 'results' in execution
            assert 'timestamp' in execution

    async def test_orchestration_history_tracking(self, orchestrator):
        """Test tracking of orchestration execution history"""
        config1 = self._create_simple_chain_config()
        config2 = self._create_complex_chain_config()
        
        await orchestrator.execute_chain(config1)
        await orchestrator.execute_chain(config2)
        
        self._validate_orchestration_history_tracking(orchestrator, 2)